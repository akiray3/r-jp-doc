x
stdin(), stdout() and stderr() are standardconnections corresponding to input, output and error on the consolerespectively (and not necessarily to file streams).  They are text-modeconnections of class "terminal" which cannot be opened orclosed, and are read-only, write-only and write-only respectively.The stdout() and stderr() connections can bere-directed by sink (and in some circumstances theoutput from stdout() can be split: see the help page).The encoding for stdin() when redirected canbe set by the command-line flag --encoding.nullfile() returns filename of the null device ("/dev/null"on Unix, "nul:" on Windows).showConnections returns a matrix of information.  If aconnection object has been lost or forgotten, getConnectionwill take a row number from the table and return a connection objectfor that connection, which can be used to close the connection,for example.  However, if there is no R level object referring to theconnection it will be closed automatically at the next garbagecollection (except for gzcon connections).closeAllConnections closes (and destroys) all userconnections, restoring all sink diversions as it doesso.isatty returns true if the connection is one of the class"terminal" connections and it is apparently connected to aterminal, otherwise false.  This may not be reliable in embeddedapplications, including GUI consoles.
The default type of quoting supported under Unix-alikes is that forthe Bourne shell sh.  If the string does not contain singlequotes, we can just surround it with single quotes.  Otherwise, thestring is surrounded in double quotes, which suppresses all specialmeanings of metacharacters except dollar, backquote and backslash, sothese (and of course double quote) are preceded by backslash.  Thistype of quoting is also appropriate for bash, ksh andzsh.The other type of quoting is for the C-shell (csh andtcsh).  Once again, if the string does not contain singlequotes, we can just surround it with single quotes.  If it doescontain single quotes, we can use double quotes provided it does notcontain dollar or backquote (and we need to escape backslash,exclamation mark and double quote).  As a last resort, we need tosplit the string into pieces not containing single quotes and surroundeach with single quotes, and the single quotes with double quotes.In Windows, command line interpretation is done by the applicationas well as the shell.  It may depend on the compiler used:Microsoft's rules for the C run-time are given at https://docs.microsoft.com/en-us/previous-versions/ms880421(v=msdn.10).It may depend onthe whim of the programmer of the application: check itsdocumentation.  The type = "cmd" quoting surrounds the stringby double quotes and escapes internal double quotes by a backslash.As Windows path names cannot contain double quotes, this makesshQuote safe for use with many applications when used withsystem or system2.  The Windowscmd.exe shell (used by default with shell)uses type = "cmd2" quoting:  special characters are prefixedwith "^".  In some cases, two types of quoting should beused:  first for the application, and then type = "cmd2" for cmd.exe.  See the examples below.  
This is an internal generic primitive function: methodscan be defined for it directly or via theMath group generic.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
These are generic functions: methods can be defined for themindividually or via the Math groupgeneric.Note that for rounding off a 5, the IEC 60559 standard (see also‘IEEE 754’) is expected to be used, ‘go to the even digit’.Therefore round(0.5) is 0 and round(-1.5) is-2.  However, this is dependent on OS services and onrepresentation error (since e.g. 0.15 is not representedexactly, the rounding rule applies to the represented number and notto the printed number, and so round(0.15, 1) could be either0.1 or 0.2).Rounding to a negative number of digits means rounding to a power often, so for example round(x, digits = -2) rounds to the nearesthundred.For signif the recognized values of digits are1...22, and non-missing values are rounded to the nearestinteger in that range.  Complex numbers are rounded to retain thespecified number of digits in the larger of the components.  Eachelement of the vector is rounded individually, unlike printing.These are all primitive functions.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
FUN is found by a call to match.fun and typicallyis specified as a function or a symbol (e.g., a backquoted name) or acharacter string specifying a function to be searched for from theenvironment of the call to lapply.Function FUN must be able to accept as input any of theelements of X.  If the latter is an atomic vector, FUNwill always be passed a length-one vector of the same type as X.Arguments in ... cannot have the same name as any of theother arguments, and care may be needed to avoid partial matching toFUN.  In general-purpose code it is good practice to name thefirst two arguments X and FUN if ... is passedthrough: this both avoids partial matching to FUN and ensuresthat a sensible error message is given if arguments named X orFUN are passed through ....Simplification in sapply is only attempted if X haslength greater than zero and if the return values from all elementsof X are all of the same (positive) length.  If the commonlength is one the result is a vector, and if greater than one is amatrix with a column corresponding to each element of X.Simplification is always done in vapply.  This functionchecks that all values of FUN are compatible with theFUN.VALUE, in that they must have the same length and type.(Types may be promoted to a higher type within the ordering logical< integer < double < complex, but not demoted.)Users of S4 classes should pass a list to lapply andvapply: the internal coercion is done by the as.list inthe base namespace and not one defined by a user (e.g., by setting S4methods on the base function).
The arc-tangent of two arguments atan2(y, x) returns the anglebetween the x-axis and the vector from the origin to (x, y),i.e., for positive arguments atan2(y, x) == atan(y/x).Angles are in radians, not degrees, for the standard versions (i.e., aright angle is π/2), and in ‘half-rotations’ forcospi etc.cospi(x), sinpi(x), and tanpi(x) are accuratefor x values which are multiples of a half.All except atan2 are internal generic primitivefunctions: methods can be defined for them individually or via theMath group generic.These are all wrappers to system calls of the same name (with prefixc for complex arguments) where available.  (cospi,sinpi, and tanpi are part of a C11 extensionand provided by e.g. macOS and Solaris: where not yetavailable call to cos etc are used, with special casesfor  multiples of a half.)
double creates a double-precision vector of the specifiedlength.  The elements of the vector are all equal to 0.It is identical to numeric.as.double is a generic function.  It is identical toas.numeric.  Methods should return an object of base type"double".is.double is a test of double type.R has no single precision data type.  All real numbers arestored in double precision format.  The functions as.singleand single are identical to as.double and doubleexcept they set the attribute Csingle that is used in the.C and .Fortran interface, and they areintended only to be used in that context.
These are internal generic primitive functions: methodscan be defined for them individually or via theMath group generic.Branch cuts are consistent with the inverse trigonometric functionsasin et seq, and agree with those defined in Abramowitzand Stegun, figure 4.7, page 86.   The behaviour actually on the cutsfollows the C99 standard which requires continuity coming round theendpoint in a counter-clockwise direction.
sink diverts R output to a connection (and must be used againto finish such a diversion, see below!).  If file is acharacter string, a file connection with that name will be establishedfor the duration of the diversion.Normal R output (to connection stdout) is diverted bythe default type = "output".  Only prompts and (most)messages continue to appear on the console.  Messages sent tostderr() (including those from message,warning and stop) can be diverted bysink(type = "message") (see below).sink() or sink(file = NULL) ends the last diversion (ofthe specified type).  There is a stack of diversions for normaloutput, so output reverts to the previous diversion (if there wasone).  The stack is of up to 21 connections (20 diversions).If file is a connection it will be opened if necessary (in"wt" mode) and closed once it is removed from the stack ofdiversions.split = TRUE only splits R output (via Rvprintf) andthe default output from writeLines: it does not splitall output that might be sent to stdout().Sink-ing the messages stream should be done only with great care.For that stream file must be an already open connection, andthere is no stack of connections.If file is a character string, the file will be opened usingthe current encoding.  If you want a different encoding (e.g., torepresent strings which have been stored in UTF-8), use afile connection — but some ways to produce R outputwill already have converted such strings to the current encoding.
sink diverts R output to a connection (and must be used againto finish such a diversion, see below!).  If file is acharacter string, a file connection with that name will be establishedfor the duration of the diversion.Normal R output (to connection stdout) is diverted bythe default type = "output".  Only prompts and (most)messages continue to appear on the console.  Messages sent tostderr() (including those from message,warning and stop) can be diverted bysink(type = "message") (see below).sink() or sink(file = NULL) ends the last diversion (ofthe specified type).  There is a stack of diversions for normaloutput, so output reverts to the previous diversion (if there wasone).  The stack is of up to 21 connections (20 diversions).If file is a connection it will be opened if necessary (in"wt" mode) and closed once it is removed from the stack ofdiversions.split = TRUE only splits R output (via Rvprintf) andthe default output from writeLines: it does not splitall output that might be sent to stdout().Sink-ing the messages stream should be done only with great care.For that stream file must be an already open connection, andthere is no stack of connections.If file is a character string, the file will be opened usingthe current encoding.  If you want a different encoding (e.g., torepresent strings which have been stored in UTF-8), use afile connection — but some ways to produce R outputwill already have converted such strings to the current encoding.
The arc-tangent of two arguments atan2(y, x) returns the anglebetween the x-axis and the vector from the origin to (x, y),i.e., for positive arguments atan2(y, x) == atan(y/x).Angles are in radians, not degrees, for the standard versions (i.e., aright angle is π/2), and in ‘half-rotations’ forcospi etc.cospi(x), sinpi(x), and tanpi(x) are accuratefor x values which are multiples of a half.All except atan2 are internal generic primitivefunctions: methods can be defined for them individually or via theMath group generic.These are all wrappers to system calls of the same name (with prefixc for complex arguments) where available.  (cospi,sinpi, and tanpi are part of a C11 extensionand provided by e.g. macOS and Solaris: where not yetavailable call to cos etc are used, with special casesfor  multiples of a half.)
If MARGIN gives a single dimension, then all elements of slicenumber i with respect to this have value i.  In general,slice numbers are obtained by numbering all combinations of indices inthe dimensions given by MARGIN in column-major order.  I.e.,with m_1, ..., m_k the dimension numbers (elements ofMARGIN) sliced by and d_{m_1}, ..., d_{m_k} thecorresponding extents, and n_1 = 1, n_2 = d_{m_1}, ...,n_k = d_{m_1} … d_{m_{k-1}},the number of the slice where dimension m_1 has value i_1,..., dimension m_k has value i_k is 1 + n_1 (i_1 - 1) +    … + n_k (i_k - 1).
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
The values in write are recycled if necessary to make up alogical vector the same length as socklist. Socket connectionscan appear more than once in socklist; this can be useful ifyou want to determine whether a socket is available for reading orwriting.
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
a or b can be complex, but this uses double complexarithmetic which might not be available on all platforms.The row and column names of the result are taken from the column namesof a and of b respectively.  If b is missing thecolumn names of the result are the row names of a.  No check ismade that the column names of a and the row names of bare equal.For back-compatibility a can be a (real) QR decomposition,although qr.solve should be called in that case.qr.solve can handle non-square systems.Unsuccessful results from the underlying LAPACK code will result in anerror giving a positive error code: these can only be interpreted bydetailed study of the FORTRAN code.
a or b can be complex, but this uses double complexarithmetic which might not be available on all platforms.The row and column names of the result are taken from the column namesof a and of b respectively.  If b is missing thecolumn names of the result are the row names of a.  No check ismade that the column names of a and the row names of bare equal.For back-compatibility a can be a (real) QR decomposition,although qr.solve should be called in that case.qr.solve can handle non-square systems.Unsuccessful results from the underlying LAPACK code will result in anerror giving a positive error code: these can only be interpreted bydetailed study of the FORTRAN code.
The QR decomposition plays an important role in manystatistical techniques.  In particular it can be used to solve theequation \bold{Ax} = \bold{b} for given matrix \bold{A},and vector \bold{b}.  It is useful for computing regressioncoefficients and in applying the Newton-Raphson algorithm.The functions qr.coef, qr.resid, and qr.fittedreturn the coefficients, residuals and fitted values obtained whenfitting y to the matrix with QR decomposition qr.(If pivoting is used, some of the coefficients will be NA.)qr.qy and qr.qty return Q %*% y andt(Q) %*% y, where Q is the (complete) \bold{Q} matrix.All the above functions keep dimnames (and names) ofx and y if there are any.solve.qr is the method for solve for qr objects.qr.solve solves systems of equations via the QR decomposition:if a is a QR decomposition it is the same as solve.qr,but if a is a rectangular matrix the QR decomposition iscomputed first.  Either will handle over- and under-determinedsystems, providing a least-squares fit if appropriate.is.qr returns TRUE if x is a listand inherits from "qr".It is not possible to coerce objects to mode "qr".  Objectseither are QR decompositions or they are not.The LINPACK interface is restricted to matrices x with lessthan 2^31 elements.qr.fitted and qr.resid only support the LINPACK interface.Unsuccessful results from the underlying LAPACK code will result in anerror giving a positive error code: these can only be interpreted bydetailed study of the FORTRAN code.
sort is a generic function for which methods can be written,and sort.int is the internal method which is compatiblewith S if only the first three arguments are used.The default sort method makes use of order forclassed objects, which in turn makes use of the generic functionxtfrm (and can be slow unless a xtfrm method hasbeen defined or is.numeric(x) is true).Complex values are sorted first by the real part, then the imaginarypart.The "auto" method selects "radix" for short (less than2^31 elements) numeric vectors, integer vectors, logicalvectors and factors; otherwise, "shell".Except for method "radix",the sort order for character vectors will depend on the collatingsequence of the locale in use: see Comparison.The sort order for factors is the order of their levels (which isparticularly appropriate for ordered factors).If partial is not NULL, it is taken to contain indicesof elements of the result which are to be placed in their correctpositions in the sorted array by partial sorting.  For each of theresult values in a specified position, any values smaller than thatone are guaranteed to have a smaller index in the sorted array and anyvalues which are greater are guaranteed to have a bigger index in thesorted array.  (This is included for efficiency, and many of theoptions are not available for partial sorting.  It is onlysubstantially more efficient if partial has a handful ofelements, and a full sort is done (a Quicksort if possible) if thereare more than 10.)  Names are discarded for partial sorting.Method "shell" uses Shellsort (an O(n^{4/3}) variant fromSedgewick (1986)).  If x has names a stable modification isused, so ties are not reordered.  (This only matters if names arepresent.)Method "quick" uses Singleton (1969)'s implementation ofHoare's Quicksort method and is only available when x isnumeric (double or integer) and partial is NULL.  (Forother types of x Shellsort is used, silently.)  It is normallysomewhat faster than Shellsort (perhaps 50% faster on vectors oflength a million and twice as fast at a billion) but has poorperformance in the rare worst case.  (Peto's modification using apseudo-random midpoint is used to make the worst case rarer.)  This isnot a stable sort, and ties may be reordered.Method "radix" relies on simple hashing to scale time linearlywith the input size, i.e., its asymptotic time complexity is O(n). Thespecific variant and its implementation originated from the data.tablepackage and are due to Matt Dowle and Arun Srinivasan.  For smallinputs (< 200), the implementation uses an insertion sort (O(n^2))that operates in-place to avoid the allocation overhead of the radixsort. For integer vectors of range less than 100,000, it switches to asimpler and faster linear time counting sort. In all cases, the sortis stable; the order of ties is preserved. It is the default methodfor integer vectors and factors.The "radix" method generally outperforms the other methods,especially for character vectors and small integers. Compared to quicksort, it is slightly faster for vectors with large integer or realvalues (but unlike quick sort, radix is stable and supports allna.last options). The implementation is orders of magnitudefaster than shell sort for character vectors, in part thanks to cleveruse of the internal CHARSXP table.However, there are some caveats with the radix sort:If x is a character vector, all elements must sharethe same encoding. Only UTF-8 (including ASCII) and Latin-1encodings are supported. Collation always follows the "C" locale.Long vectors (with more than 2^32 elements) and complexvectors are not supported yet.
sort is a generic function for which methods can be written,and sort.int is the internal method which is compatiblewith S if only the first three arguments are used.The default sort method makes use of order forclassed objects, which in turn makes use of the generic functionxtfrm (and can be slow unless a xtfrm method hasbeen defined or is.numeric(x) is true).Complex values are sorted first by the real part, then the imaginarypart.The "auto" method selects "radix" for short (less than2^31 elements) numeric vectors, integer vectors, logicalvectors and factors; otherwise, "shell".Except for method "radix",the sort order for character vectors will depend on the collatingsequence of the locale in use: see Comparison.The sort order for factors is the order of their levels (which isparticularly appropriate for ordered factors).If partial is not NULL, it is taken to contain indicesof elements of the result which are to be placed in their correctpositions in the sorted array by partial sorting.  For each of theresult values in a specified position, any values smaller than thatone are guaranteed to have a smaller index in the sorted array and anyvalues which are greater are guaranteed to have a bigger index in thesorted array.  (This is included for efficiency, and many of theoptions are not available for partial sorting.  It is onlysubstantially more efficient if partial has a handful ofelements, and a full sort is done (a Quicksort if possible) if thereare more than 10.)  Names are discarded for partial sorting.Method "shell" uses Shellsort (an O(n^{4/3}) variant fromSedgewick (1986)).  If x has names a stable modification isused, so ties are not reordered.  (This only matters if names arepresent.)Method "quick" uses Singleton (1969)'s implementation ofHoare's Quicksort method and is only available when x isnumeric (double or integer) and partial is NULL.  (Forother types of x Shellsort is used, silently.)  It is normallysomewhat faster than Shellsort (perhaps 50% faster on vectors oflength a million and twice as fast at a billion) but has poorperformance in the rare worst case.  (Peto's modification using apseudo-random midpoint is used to make the worst case rarer.)  This isnot a stable sort, and ties may be reordered.Method "radix" relies on simple hashing to scale time linearlywith the input size, i.e., its asymptotic time complexity is O(n). Thespecific variant and its implementation originated from the data.tablepackage and are due to Matt Dowle and Arun Srinivasan.  For smallinputs (< 200), the implementation uses an insertion sort (O(n^2))that operates in-place to avoid the allocation overhead of the radixsort. For integer vectors of range less than 100,000, it switches to asimpler and faster linear time counting sort. In all cases, the sortis stable; the order of ties is preserved. It is the default methodfor integer vectors and factors.The "radix" method generally outperforms the other methods,especially for character vectors and small integers. Compared to quicksort, it is slightly faster for vectors with large integer or realvalues (but unlike quick sort, radix is stable and supports allna.last options). The implementation is orders of magnitudefaster than shell sort for character vectors, in part thanks to cleveruse of the internal CHARSXP table.However, there are some caveats with the radix sort:If x is a character vector, all elements must sharethe same encoding. Only UTF-8 (including ASCII) and Latin-1encodings are supported. Collation always follows the "C" locale.Long vectors (with more than 2^32 elements) and complexvectors are not supported yet.
sort is a generic function for which methods can be written,and sort.int is the internal method which is compatiblewith S if only the first three arguments are used.The default sort method makes use of order forclassed objects, which in turn makes use of the generic functionxtfrm (and can be slow unless a xtfrm method hasbeen defined or is.numeric(x) is true).Complex values are sorted first by the real part, then the imaginarypart.The "auto" method selects "radix" for short (less than2^31 elements) numeric vectors, integer vectors, logicalvectors and factors; otherwise, "shell".Except for method "radix",the sort order for character vectors will depend on the collatingsequence of the locale in use: see Comparison.The sort order for factors is the order of their levels (which isparticularly appropriate for ordered factors).If partial is not NULL, it is taken to contain indicesof elements of the result which are to be placed in their correctpositions in the sorted array by partial sorting.  For each of theresult values in a specified position, any values smaller than thatone are guaranteed to have a smaller index in the sorted array and anyvalues which are greater are guaranteed to have a bigger index in thesorted array.  (This is included for efficiency, and many of theoptions are not available for partial sorting.  It is onlysubstantially more efficient if partial has a handful ofelements, and a full sort is done (a Quicksort if possible) if thereare more than 10.)  Names are discarded for partial sorting.Method "shell" uses Shellsort (an O(n^{4/3}) variant fromSedgewick (1986)).  If x has names a stable modification isused, so ties are not reordered.  (This only matters if names arepresent.)Method "quick" uses Singleton (1969)'s implementation ofHoare's Quicksort method and is only available when x isnumeric (double or integer) and partial is NULL.  (Forother types of x Shellsort is used, silently.)  It is normallysomewhat faster than Shellsort (perhaps 50% faster on vectors oflength a million and twice as fast at a billion) but has poorperformance in the rare worst case.  (Peto's modification using apseudo-random midpoint is used to make the worst case rarer.)  This isnot a stable sort, and ties may be reordered.Method "radix" relies on simple hashing to scale time linearlywith the input size, i.e., its asymptotic time complexity is O(n). Thespecific variant and its implementation originated from the data.tablepackage and are due to Matt Dowle and Arun Srinivasan.  For smallinputs (< 200), the implementation uses an insertion sort (O(n^2))that operates in-place to avoid the allocation overhead of the radixsort. For integer vectors of range less than 100,000, it switches to asimpler and faster linear time counting sort. In all cases, the sortis stable; the order of ties is preserved. It is the default methodfor integer vectors and factors.The "radix" method generally outperforms the other methods,especially for character vectors and small integers. Compared to quicksort, it is slightly faster for vectors with large integer or realvalues (but unlike quick sort, radix is stable and supports allna.last options). The implementation is orders of magnitudefaster than shell sort for character vectors, in part thanks to cleveruse of the internal CHARSXP table.However, there are some caveats with the radix sort:If x is a character vector, all elements must sharethe same encoding. Only UTF-8 (including ASCII) and Latin-1encodings are supported. Collation always follows the "C" locale.Long vectors (with more than 2^32 elements) and complexvectors are not supported yet.
In the case of ties in the first vector, values in the second are usedto break the ties.  If the values are still tied, values in the laterarguments are used to break the tie (see the first example).The sort used is stable (except for method = "quick"),so any unresolved ties will be left in their original ordering.Complex values are sorted first by the real part, then the imaginarypart.Except for method "radix", the sort order for character vectorswill depend on the collating sequence of the locale in use: seeComparison.The "shell" method is generally the safest bet and is thedefault method, except for short factors, numeric vectors,integer vectors and logical vectors, where "radix" is assumed.Method "radix" stably sorts logical,numeric and character vectors in linear time. It outperforms the othermethods, although there are caveats (see sort).  Method"quick" for sort.list is only supported for numericx with na.last = NA, is not stable, and is slower than"radix". partial = NULL is supported for compatibility with otherimplementations of S, but no other values are accepted and ordering isalways complete.For a classed R object, the sort order is taken fromxtfrm: as its help page notes, this can be slow unless asuitable method has been defined or is.numeric(x) istrue.  For factors, this sorts on the internal codes, which isparticularly appropriate for ordered factors.
sort is a generic function for which methods can be written,and sort.int is the internal method which is compatiblewith S if only the first three arguments are used.The default sort method makes use of order forclassed objects, which in turn makes use of the generic functionxtfrm (and can be slow unless a xtfrm method hasbeen defined or is.numeric(x) is true).Complex values are sorted first by the real part, then the imaginarypart.The "auto" method selects "radix" for short (less than2^31 elements) numeric vectors, integer vectors, logicalvectors and factors; otherwise, "shell".Except for method "radix",the sort order for character vectors will depend on the collatingsequence of the locale in use: see Comparison.The sort order for factors is the order of their levels (which isparticularly appropriate for ordered factors).If partial is not NULL, it is taken to contain indicesof elements of the result which are to be placed in their correctpositions in the sorted array by partial sorting.  For each of theresult values in a specified position, any values smaller than thatone are guaranteed to have a smaller index in the sorted array and anyvalues which are greater are guaranteed to have a bigger index in thesorted array.  (This is included for efficiency, and many of theoptions are not available for partial sorting.  It is onlysubstantially more efficient if partial has a handful ofelements, and a full sort is done (a Quicksort if possible) if thereare more than 10.)  Names are discarded for partial sorting.Method "shell" uses Shellsort (an O(n^{4/3}) variant fromSedgewick (1986)).  If x has names a stable modification isused, so ties are not reordered.  (This only matters if names arepresent.)Method "quick" uses Singleton (1969)'s implementation ofHoare's Quicksort method and is only available when x isnumeric (double or integer) and partial is NULL.  (Forother types of x Shellsort is used, silently.)  It is normallysomewhat faster than Shellsort (perhaps 50% faster on vectors oflength a million and twice as fast at a billion) but has poorperformance in the rare worst case.  (Peto's modification using apseudo-random midpoint is used to make the worst case rarer.)  This isnot a stable sort, and ties may be reordered.Method "radix" relies on simple hashing to scale time linearlywith the input size, i.e., its asymptotic time complexity is O(n). Thespecific variant and its implementation originated from the data.tablepackage and are due to Matt Dowle and Arun Srinivasan.  For smallinputs (< 200), the implementation uses an insertion sort (O(n^2))that operates in-place to avoid the allocation overhead of the radixsort. For integer vectors of range less than 100,000, it switches to asimpler and faster linear time counting sort. In all cases, the sortis stable; the order of ties is preserved. It is the default methodfor integer vectors and factors.The "radix" method generally outperforms the other methods,especially for character vectors and small integers. Compared to quicksort, it is slightly faster for vectors with large integer or realvalues (but unlike quick sort, radix is stable and supports allna.last options). The implementation is orders of magnitudefaster than shell sort for character vectors, in part thanks to cleveruse of the internal CHARSXP table.However, there are some caveats with the radix sort:If x is a character vector, all elements must sharethe same encoding. Only UTF-8 (including ASCII) and Latin-1encodings are supported. Collation always follows the "C" locale.Long vectors (with more than 2^32 elements) and complexvectors are not supported yet.
Note that running code via source differs in a few respectsfrom entering it at the R command line.  Since expressions are notexecuted at the top level, auto-printing is not done.  So you willneed to include explicit print calls for things you want to beprinted (and remember that this includes plotting by lattice,FAQ Q7.22).  Since the complete file is parsed before any of it isrun, syntax errors result in none of the code being run.  If an erroroccurs in running a syntactically correct script, anything assignedinto the workspace by code that has been run will be kept (just asfrom the command line), but diagnostic information such astraceback() will contain additional calls towithVisible.All versions of R accept input from a connection with end of linemarked by LF (as used on Unix), CRLF (as used on DOS/Windows) or CR(as used on classic Mac OS) and map this to newline.  The final linecan be incomplete, that is missing the final end-of-line marker.If keep.source is true (the default in interactive use), thesource of functions is kept so they can be listed exactly as input.Unlike input from a console, lines in the file or on a connection cancontain an unlimited number of characters.When skip.echo > 0, that many comment lines at the start ofthe file will not be echoed.  This does not affect the execution ofthe code at all.  If there are executable lines within the firstskip.echo lines, echoing will start with the first of them.If echo is true and a deparsed expression exceedsmax.deparse.length, that many characters are output followed by .... [TRUNCATED] .
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
Dates are represented as the number of days since 1970-01-01, withnegative values for earlier dates.  They are always printedfollowing the rules of the current Gregorian calendar, even thoughthat calendar was not in use long ago (it was adopted in 1752 inGreat Britain and its colonies).It is intended that the date should be an integer, but this is notenforced in the internal representation.  Fractional days will beignored when printing.  It is possible to produce fractional days viathe mean method or by adding or subtracting (seeOps.Date).From the many methods, see methods(class = "Date"), a few aredocumented separately, see below.
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
There are two basic classes of date/times.  Class "POSIXct"represents the (signed) number of seconds since the beginning of 1970(in the UTC time zone) as a numeric vector.  Class "POSIXlt" isa named list of vectors representing0–61: seconds.0–59: minutes.0–23: hours.1–31: day of the month0–11: months after the first of the year.years since 1900.0–6 day of the week, starting on Sunday.0–365: day of the year (365 only in leap years).Daylight Saving Time flag.  Positive if inforce, zero if not, negative if unknown.(Optional.) The abbreviation for the time zone inforce at that time: "" if unknown (but "" might alsobe used for UTC).(Optional.) The offset in seconds from GMT:positive values are East of the meridian.  Usually NA ifunknown, but 0 could mean unknown.(The last two components are not present for times in UTC and areplatform-dependent: they are supported on platforms based on BSD orglibc (including Linux and macOS) and those using thetzcode implementation shipped with R (including Windows). Butthey are not necessarily set.).  Note that the internal list structureis somewhat hidden, as many methods (includinglength(x), print() and str)apply to the abstract date-time vector, as for "POSIXct".  Asfrom R 3.5.0, one can extract and replace single componentsvia [ indexing with two indices (see the examples).  Theclasses correspond to the POSIX/C99 constructs of ‘calendartime’ (the time_t data type) and ‘local time’ (orbroken-down time, the struct tm data type), from which theyalso inherit their names.  The components of "POSIXlt" areinteger vectors, except sec and zone."POSIXct" is more convenient for including in data frames, and"POSIXlt" is closer to human-readable forms.  A virtual class"POSIXt" exists from which both of the classes inherit: it isused to allow operations such as subtraction to mix the two classes.Components wday and yday of "POSIXlt" are forinformation, and are not used in the conversion to calendar time.However, isdst is needed to distinguish times at the end ofDST: typically 1am to 2am occurs twice, first in DST and then instandard time.  At all other times isdst can be deduced fromthe first six values, but the behaviour if it is set incorrectly isplatform-dependent.Logical comparisons and some arithmetic operations are available forboth classes.  One can add or subtract a number of seconds from adate-time object, but not add two date-time objects.  Subtraction oftwo date-time objects is equivalent to using difftime.Be aware that "POSIXlt" objects will be interpreted as being inthe current time zone for these operations unless a time zone has beenspecified."POSIXlt" objects will often have an attribute "tzone",a character vector of length 3 giving the time zone name (from the TZenvironment variable or argument tz of functions creating"POSIXlt" objects; "" marks the current time zone)and the names of the base time zoneand the alternate (daylight-saving) time zone.  Sometimes this mayjust be of length one, giving the time zone name."POSIXct" objects may also have an attribute "tzone", acharacter vector of length one.  If set to a non-empty value, it willdetermine how the object is converted to class "POSIXlt" and inparticular how it is printed.  This is usually desirable, but if youwant to specify an object in a particular time zone but to be printedin the current time zone you may want to remove the "tzone"attribute (e.g., by c(x)).Unfortunately, the conversion is complicated by the operation of timezones and leap seconds (according to this version of R's data,27 days have been 86401 seconds long sofar, the last being on (actually, immediately before)2017-01-01: the times of theextra seconds are in the object .leap.seconds).  The details ofthis are entrusted to the OS services where possible.  It seems thatsome rare systems used to use leap seconds, but all known currentplatforms ignore them (as required by POSIX).  This is detected andcorrected for at build time, so "POSIXct" times used by R donot include leap seconds on any platform.Using c on "POSIXlt" objects converts them to thecurrent time zone, and on "POSIXct" objects drops any"tzone" attributes, unless they are all marked with the sametime zone.A few times have specific issues.  First, the leap seconds are ignored,and real times such as "2005-12-31 23:59:60" are (probably)treated as the next second.  However, they will never be generated byR, and are unlikely to arise as input.  Second, on some OSes there isa problem in the POSIX/C99 standard with "1969-12-31 23:59:59 UTC",which is -1 in calendar time and that value is on those OSesalso used as an error code.  Thus as.POSIXct("1969-12-31  23:59:59", format = "%Y-%m-%d %H:%M:%S", tz = "UTC") may giveNA, and hence as.POSIXct("1969-12-31 23:59:59",  tz = "UTC") will give "1969-12-31 23:59:00".  Other OSes(including the code used by R on Windows) report errors separatelyand so are able to handle that time as valid.The print methods respect options("max.print").
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
sprintf is a wrapper for the system sprintf C-libraryfunction.  Attempts are made to check that the mode of the valuespassed match the format supplied, and R's special values (NA,Inf, -Inf and NaN) are handled correctly.gettextf is a convenience function which provides C-stylestring formatting with possible translation of the format string.The arguments (including fmt) are recycled if possible a wholenumber of times to the length of the longest, and then the formattingis done in parallel.  Zero-length arguments are allowed and will givea zero-length result.  All arguments are evaluated even if unused, andhence some types (e.g., "symbol" or "language", seetypeof) are not allowed. Arguments unused by fmtresult in a warning. (The format %.0s can be used to“skip” an argument.)The following is abstracted from Kernighan and Ritchie (seeReferences): however the actual implementation will follow the C99standard and fine details (especially the behaviour under user error)may depend on the platform. References to numbered arguments come fromPOSIX.The string fmt contains normal characters,which are passed through to the output string, and also conversionspecifications which operate on the arguments provided through....  The allowed conversion specifications start with a% and end with one of the letters in the setaAdifeEgGosxX%.  These letters denote the following types:Integervalue, o being octal, x and X being hexadecimal (using the same case fora-f as the code).  Numeric variables with exactly integervalues will be coerced to integer.  Formats d and ican also be used for logical variables, which will be converted to0, 1 or NA.Double precision value, in “fixedpoint” decimal notation of the form "[-]mmm.ddd".  The number ofdecimal places ("d") is specified by the precision: the default is 6;a precision of 0 suppresses the decimal point.  Non-finite valuesare converted to NA, NaN or (perhaps a sign followedby) Inf.Double precision value, in“exponential” decimal notation of theform [-]m.ddde[+-]xx or [-]m.dddE[+-]xx.Double precision value, in %e or%E format if the exponent is less than -4 or greater than orequal to the precision, and %f format otherwise.(The precision (default 6) specifies the number ofsignificant digits here, whereas in %f, %e, it isthe number of digits after the decimal point.)Double precision value, in binary notationof the form [-]0xh.hhhp[+-]d.  This is a binary fractionexpressed in hex multiplied by a (decimal) power of 2.  The numberof hex digits after the decimal point is specified by the precision:the default is enough digits to represent exactly the internalbinary representation.  Non-finite values are converted to NA,NaN or (perhaps a sign followed by) Inf.  Format%a uses lower-case for x, p and the hexvalues: format %A uses upper-case.This should be supported on all platforms as it is a feature of C99.The format is not uniquely defined: although it would be possibleto make the leading h always zero or one, this is notalways done.  Most systems will suppress trailing zeros, but a fewdo not.  On a well-written platform, for normal numbers there willbe a leading one before the decimal point plus (by default) 13hexadecimal digits, hence 53 bits.  The treatment of denormalized(aka ‘subnormal’) numbers is very platform-dependent.Character string.  Character NAs areconverted to "NA".Literal % (none of the extra formattingcharacters given below are permitted in this case).Conversion by as.character is used for non-characterarguments with s and by as.double fornon-double arguments with f, e, E, g, G.  NB: the length isdetermined before conversion, so do not rely on the internalcoercion if this would change the length.  The coercion is done onlyonce, so if length(fmt) > 1 then all elements must expect thesame types of arguments.In addition, between the initial % and the terminatingconversion character there may be, in any order:Two numbers separated by a period, denoting thefield width (m) and the precision (n).Left adjustment of converted argument in its field.Always print number with sign: by default onlynegative numbers are printed with a sign.Prefix a space if the first character is not a sign.For numbers, pad to the field width with leadingzeros. For characters, this zero-pads on some platforms and isignored on others.specifies “alternate output” for numbers, itsaction depending on the type:For x or X, 0x or 0X will be prefixedto a non-zero result.  For e, e, f, gand G, the output will always have a decimal point; forg and G, trailing zeros will not be removed.Further, immediately after % may come 1$ to 99$to refer to a numbered argument: this allows arguments to bereferenced out of order and is mainly intended for translators oferror messages.  If this is done it is best if all formats arenumbered: if not the unnumbered ones process the arguments in order.See the examples.  This notation allows arguments to be used more thanonce, in which case they must be used as the same type (integer,double or character).A field width or precision (but not both) may be indicated by anasterisk *: in this case an argument specifies the desirednumber.  A negative field width is taken as a '-' flag followed by apositive field width.  A negative precision is treated as if theprecision were omitted.  The argument should be integer, but a doubleargument will be coerced to integer.There is a limit of 8192 bytes on elements of fmt, and onstrings included from a single %letter conversionspecification.Field widths and precisions of %s conversions are interpretedas bytes, not characters, as described in the C standard.The C doubles used for R numerical vectors have signed zeros, whichsprintf may output as -0, -0.000 ....
These are internal generic primitive functions: methodscan be defined for them individually or via theMath group generic.  For complexarguments (and the default method), z, abs(z) ==  Mod(z) and sqrt(z) == z^0.5.abs(x) returns an integer vector when x isinteger or logical.
The purpose of the functions is to provide a simple means of markupfor quoting text to be used in the R output, e.g., in warnings orerror messages.The choice of the appropriate quotation marks depends on both thelocale and the available character sets.  Older Unix/X11 fontsdisplayed the grave accent (ASCII code 0x60) and the apostrophe (0x27)in a way that they could also be used as matching open and closesingle quotation marks.  Using modern fonts, or non-Unix systems,these characters no longer produce matching glyphs.  Unicode providesleft and right single quotation mark characters (U+2018 and U+2019);if Unicode markup cannot be assumed to be available, it seems goodpractice to use the apostrophe as a non-directional single quotationmark.Similarly, Unicode has left and right double quotation mark characters(U+201C and U+201D); if only ASCII's typewriter characteristics can beemployed, than the ASCII quotation mark (0x22) should be used as boththe left and right double quotation mark.Some other locales also have the directional quotation marks, notablyon Windows.  TeX uses grave and apostrophe for the directional singlequotation marks, and doubled grave and doubled apostrophe for thedirectional double quotation marks.What rendering is used depends on q which by default depends onthe options setting for useFancyQuotes.  If thisis FALSE then the undirectionalASCII quotation style is used.  If this is TRUE (the default),Unicode directional quotes are used are used where available(currently, UTF-8 locales on Unix-alikes and all Windows localesexcept C): if set to "UTF-8" UTF-8 markup is used(whatever the current locale). If set to "TeX", TeX-stylemarkup is used.  Finally, if this is set to a character vector oflength four, the first two entries are used for beginning and endingsingle quotes and the second two for beginning and ending doublequotes: this can be used to implement non-English quoting conventionssuch as the use of guillemets.Where fancy quotes are used, you should be aware that they may not berendered correctly as not all fonts include the requisite glyphs: forexample some have directional single quotes but not directional doublequotes.
These functions and classes handle source code references.The srcfile function produces an object of classsrcfile, which contains the name and directory of a source codefile, along with its timestamp, for use in source level debugging (notyet implemented) and source echoing.  The encoding of the file issaved; see file for a discussion of encodings, andiconvlist for a list of allowable encodings on your platform.The srcfilecopy function produces an object of the descendantclass srcfilecopy, which saves the source lines in a charactervector.  It copies the value of the isFile argument, to helpdebuggers identify whether this text comes from a real file in thefile system.The srcfilealias function produces an object of the descendantclass srcfilealias, which gives an alternate name to anothersrcfile.  This is produced by the parser when a #line directiveis used.The getSrcLines function reads the specified lines fromsrcfile.The srcref function produces an object of classsrcref, which describes a range of characters in asrcfile.The lloc value gives the following values:Bytes (elements 2, 4) andcolumns (elements 5, 6) may be different due to multibytecharacters.  If only four values are given, the columns and bytesare assumed to match.  Lines (elements 1, 3) and parsed lines(elements 7, 8) may differ if a #line directive is used incode:  the former will respect the directive, the latter will justcount lines.  If only 4 or 6 elements are given, the parsed lineswill be assumed to match the lines.Methods are defined for print, summary, open,and close for classes srcfile and srcfilecopy.The open method opens its internal file connection ata particular line; if it was already open, it will be repositionedto that line.Methods are defined for print, summary andas.character for class srcref.  The as.charactermethod will read the associated source file to obtain the textcorresponding to the reference.  If the to argument is given,it should be a second srcref that follows the first, in thesame file; they will be treated as one reference to the wholerange.  The exact behaviour depends on theclass of the source file.  If the source file inherits fromclass srcfilecopy, the lines are taken from the saved copyusing the “parsed” line counts.  If not, an attemptis made to read the file, and the original line numbers of thesrcref record (i.e., elements 1 and 3) are used.  If an erroroccurs (e.g., the file no longer exists), text like<srcref: "file" chars 1:1 to 2:10> will be returned instead,indicating the line:column ranges of the first and lastcharacter.  The summary method defaults to this type ofdisplay.Lists of srcref objects may be attached to expressions as the"srcref" attribute.  (The list of srcref objects should be the samelength as the expression.)  By default, expressions are printed byprint.default using the associated srcref.  Tosee deparsed code instead, call print with argumentuseSource = FALSE.  If a srcref objectis printed with useSource = FALSE, the <srcref: ...>record will be printed..isOpen is intended for internal use:  it checks whether theconnection associated with a srcfile object is open.
These functions and classes handle source code references.The srcfile function produces an object of classsrcfile, which contains the name and directory of a source codefile, along with its timestamp, for use in source level debugging (notyet implemented) and source echoing.  The encoding of the file issaved; see file for a discussion of encodings, andiconvlist for a list of allowable encodings on your platform.The srcfilecopy function produces an object of the descendantclass srcfilecopy, which saves the source lines in a charactervector.  It copies the value of the isFile argument, to helpdebuggers identify whether this text comes from a real file in thefile system.The srcfilealias function produces an object of the descendantclass srcfilealias, which gives an alternate name to anothersrcfile.  This is produced by the parser when a #line directiveis used.The getSrcLines function reads the specified lines fromsrcfile.The srcref function produces an object of classsrcref, which describes a range of characters in asrcfile.The lloc value gives the following values:Bytes (elements 2, 4) andcolumns (elements 5, 6) may be different due to multibytecharacters.  If only four values are given, the columns and bytesare assumed to match.  Lines (elements 1, 3) and parsed lines(elements 7, 8) may differ if a #line directive is used incode:  the former will respect the directive, the latter will justcount lines.  If only 4 or 6 elements are given, the parsed lineswill be assumed to match the lines.Methods are defined for print, summary, open,and close for classes srcfile and srcfilecopy.The open method opens its internal file connection ata particular line; if it was already open, it will be repositionedto that line.Methods are defined for print, summary andas.character for class srcref.  The as.charactermethod will read the associated source file to obtain the textcorresponding to the reference.  If the to argument is given,it should be a second srcref that follows the first, in thesame file; they will be treated as one reference to the wholerange.  The exact behaviour depends on theclass of the source file.  If the source file inherits fromclass srcfilecopy, the lines are taken from the saved copyusing the “parsed” line counts.  If not, an attemptis made to read the file, and the original line numbers of thesrcref record (i.e., elements 1 and 3) are used.  If an erroroccurs (e.g., the file no longer exists), text like<srcref: "file" chars 1:1 to 2:10> will be returned instead,indicating the line:column ranges of the first and lastcharacter.  The summary method defaults to this type ofdisplay.Lists of srcref objects may be attached to expressions as the"srcref" attribute.  (The list of srcref objects should be the samelength as the expression.)  By default, expressions are printed byprint.default using the associated srcref.  Tosee deparsed code instead, call print with argumentuseSource = FALSE.  If a srcref objectis printed with useSource = FALSE, the <srcref: ...>record will be printed..isOpen is intended for internal use:  it checks whether theconnection associated with a srcfile object is open.
These functions and classes handle source code references.The srcfile function produces an object of classsrcfile, which contains the name and directory of a source codefile, along with its timestamp, for use in source level debugging (notyet implemented) and source echoing.  The encoding of the file issaved; see file for a discussion of encodings, andiconvlist for a list of allowable encodings on your platform.The srcfilecopy function produces an object of the descendantclass srcfilecopy, which saves the source lines in a charactervector.  It copies the value of the isFile argument, to helpdebuggers identify whether this text comes from a real file in thefile system.The srcfilealias function produces an object of the descendantclass srcfilealias, which gives an alternate name to anothersrcfile.  This is produced by the parser when a #line directiveis used.The getSrcLines function reads the specified lines fromsrcfile.The srcref function produces an object of classsrcref, which describes a range of characters in asrcfile.The lloc value gives the following values:Bytes (elements 2, 4) andcolumns (elements 5, 6) may be different due to multibytecharacters.  If only four values are given, the columns and bytesare assumed to match.  Lines (elements 1, 3) and parsed lines(elements 7, 8) may differ if a #line directive is used incode:  the former will respect the directive, the latter will justcount lines.  If only 4 or 6 elements are given, the parsed lineswill be assumed to match the lines.Methods are defined for print, summary, open,and close for classes srcfile and srcfilecopy.The open method opens its internal file connection ata particular line; if it was already open, it will be repositionedto that line.Methods are defined for print, summary andas.character for class srcref.  The as.charactermethod will read the associated source file to obtain the textcorresponding to the reference.  If the to argument is given,it should be a second srcref that follows the first, in thesame file; they will be treated as one reference to the wholerange.  The exact behaviour depends on theclass of the source file.  If the source file inherits fromclass srcfilecopy, the lines are taken from the saved copyusing the “parsed” line counts.  If not, an attemptis made to read the file, and the original line numbers of thesrcref record (i.e., elements 1 and 3) are used.  If an erroroccurs (e.g., the file no longer exists), text like<srcref: "file" chars 1:1 to 2:10> will be returned instead,indicating the line:column ranges of the first and lastcharacter.  The summary method defaults to this type ofdisplay.Lists of srcref objects may be attached to expressions as the"srcref" attribute.  (The list of srcref objects should be the samelength as the expression.)  By default, expressions are printed byprint.default using the associated srcref.  Tosee deparsed code instead, call print with argumentuseSource = FALSE.  If a srcref objectis printed with useSource = FALSE, the <srcref: ...>record will be printed..isOpen is intended for internal use:  it checks whether theconnection associated with a srcfile object is open.
These functions and classes handle source code references.The srcfile function produces an object of classsrcfile, which contains the name and directory of a source codefile, along with its timestamp, for use in source level debugging (notyet implemented) and source echoing.  The encoding of the file issaved; see file for a discussion of encodings, andiconvlist for a list of allowable encodings on your platform.The srcfilecopy function produces an object of the descendantclass srcfilecopy, which saves the source lines in a charactervector.  It copies the value of the isFile argument, to helpdebuggers identify whether this text comes from a real file in thefile system.The srcfilealias function produces an object of the descendantclass srcfilealias, which gives an alternate name to anothersrcfile.  This is produced by the parser when a #line directiveis used.The getSrcLines function reads the specified lines fromsrcfile.The srcref function produces an object of classsrcref, which describes a range of characters in asrcfile.The lloc value gives the following values:Bytes (elements 2, 4) andcolumns (elements 5, 6) may be different due to multibytecharacters.  If only four values are given, the columns and bytesare assumed to match.  Lines (elements 1, 3) and parsed lines(elements 7, 8) may differ if a #line directive is used incode:  the former will respect the directive, the latter will justcount lines.  If only 4 or 6 elements are given, the parsed lineswill be assumed to match the lines.Methods are defined for print, summary, open,and close for classes srcfile and srcfilecopy.The open method opens its internal file connection ata particular line; if it was already open, it will be repositionedto that line.Methods are defined for print, summary andas.character for class srcref.  The as.charactermethod will read the associated source file to obtain the textcorresponding to the reference.  If the to argument is given,it should be a second srcref that follows the first, in thesame file; they will be treated as one reference to the wholerange.  The exact behaviour depends on theclass of the source file.  If the source file inherits fromclass srcfilecopy, the lines are taken from the saved copyusing the “parsed” line counts.  If not, an attemptis made to read the file, and the original line numbers of thesrcref record (i.e., elements 1 and 3) are used.  If an erroroccurs (e.g., the file no longer exists), text like<srcref: "file" chars 1:1 to 2:10> will be returned instead,indicating the line:column ranges of the first and lastcharacter.  The summary method defaults to this type ofdisplay.Lists of srcref objects may be attached to expressions as the"srcref" attribute.  (The list of srcref objects should be the samelength as the expression.)  By default, expressions are printed byprint.default using the associated srcref.  Tosee deparsed code instead, call print with argumentuseSource = FALSE.  If a srcref objectis printed with useSource = FALSE, the <srcref: ...>record will be printed..isOpen is intended for internal use:  it checks whether theconnection associated with a srcfile object is open.
standardGeneric dispatches the method defined for a genericfunction named f, using the actual arguments in the frame from whichit is called.The argument fdef is inserted (automatically) when dispatchingmethods for a primitive function.  If present, it must always be the functiondefinition for the corresponding generic.  Don't insert this argumentby hand, as there is no validity checking and miss-specifying thefunction definition will cause certain failure.For more, use the methods package, and see the documentation inGenericFunctions.
startsWith() is equivalent to but much faster thanor alsowhere prefix is not to contain special regular expressioncharacters (and for grepl, x does not contain missingvalues, see below).The code has an optimized branch for the most common usage in whichprefix or suffix is of length one, and is furtheroptimized in a UTF-8 or 8-byte locale if that is an ASCII string.
stdin(), stdout() and stderr() are standardconnections corresponding to input, output and error on the consolerespectively (and not necessarily to file streams).  They are text-modeconnections of class "terminal" which cannot be opened orclosed, and are read-only, write-only and write-only respectively.The stdout() and stderr() connections can bere-directed by sink (and in some circumstances theoutput from stdout() can be split: see the help page).The encoding for stdin() when redirected canbe set by the command-line flag --encoding.nullfile() returns filename of the null device ("/dev/null"on Unix, "nul:" on Windows).showConnections returns a matrix of information.  If aconnection object has been lost or forgotten, getConnectionwill take a row number from the table and return a connection objectfor that connection, which can be used to close the connection,for example.  However, if there is no R level object referring to theconnection it will be closed automatically at the next garbagecollection (except for gzcon connections).closeAllConnections closes (and destroys) all userconnections, restoring all sink diversions as it doesso.isatty returns true if the connection is one of the class"terminal" connections and it is apparently connected to aterminal, otherwise false.  This may not be reliable in embeddedapplications, including GUI consoles.
stdin(), stdout() and stderr() are standardconnections corresponding to input, output and error on the consolerespectively (and not necessarily to file streams).  They are text-modeconnections of class "terminal" which cannot be opened orclosed, and are read-only, write-only and write-only respectively.The stdout() and stderr() connections can bere-directed by sink (and in some circumstances theoutput from stdout() can be split: see the help page).The encoding for stdin() when redirected canbe set by the command-line flag --encoding.nullfile() returns filename of the null device ("/dev/null"on Unix, "nul:" on Windows).showConnections returns a matrix of information.  If aconnection object has been lost or forgotten, getConnectionwill take a row number from the table and return a connection objectfor that connection, which can be used to close the connection,for example.  However, if there is no R level object referring to theconnection it will be closed automatically at the next garbagecollection (except for gzcon connections).closeAllConnections closes (and destroys) all userconnections, restoring all sink diversions as it doesso.isatty returns true if the connection is one of the class"terminal" connections and it is apparently connected to aterminal, otherwise false.  This may not be reliable in embeddedapplications, including GUI consoles.
stdin(), stdout() and stderr() are standardconnections corresponding to input, output and error on the consolerespectively (and not necessarily to file streams).  They are text-modeconnections of class "terminal" which cannot be opened orclosed, and are read-only, write-only and write-only respectively.The stdout() and stderr() connections can bere-directed by sink (and in some circumstances theoutput from stdout() can be split: see the help page).The encoding for stdin() when redirected canbe set by the command-line flag --encoding.nullfile() returns filename of the null device ("/dev/null"on Unix, "nul:" on Windows).showConnections returns a matrix of information.  If aconnection object has been lost or forgotten, getConnectionwill take a row number from the table and return a connection objectfor that connection, which can be used to close the connection,for example.  However, if there is no R level object referring to theconnection it will be closed automatically at the next garbagecollection (except for gzcon connections).closeAllConnections closes (and destroys) all userconnections, restoring all sink diversions as it doesso.isatty returns true if the connection is one of the class"terminal" connections and it is apparently connected to aterminal, otherwise false.  This may not be reliable in embeddedapplications, including GUI consoles.
The error action is controlled by error handlers established withinthe executing code and by the current default error handler set byoptions(error=).  The error is first signaled as if usingsignalCondition().  If there are no handlers or if all handlersreturn, then the error message is printed (ifoptions("show.error.messages") is true) and the default errorhandler is used.  The default behaviour (the NULLerror-handler) in interactive use is to return to the top levelprompt or the top level browser, and in non-interactive use to(effectively) call q("no", status = 1, runLast = FALSE).The default handler stores the error message in a buffer; it can beretrieved by geterrmessage().  It also stores a trace ofthe call stack that can be retrieved by traceback().Errors will be truncated to getOption("warning.length")characters, default 1000.If a condition object is supplied it should be the only argument, andfurther arguments will be ignored, with a warning.
This function is intended for use in regression tests or also argumentchecking of functions, in particular to make them easier to read.stopifnot(A, B) or equivalently stopifnot(exprs= {A ;      B}) are conceptually equivalent to Since R version 3.6.0, stopifnot() no longer handles potentialerrors or warnings (by tryCatch() etc) for each singleexpressionand may use sys.call(<n>) to get a meaningful and shorterror message in case an expression did not evaluate to all TRUE.  Thisprovides considerably less overhead.Since R version 3.5.0, expressions are evaluated sequentially,and hence evaluation stops as soon as there is a “non-TRUE”, asindicated by the above conceptual equivalence statement.Also, since R version 3.5.0, stopifnot(exprs = { ... }) can be usedalternatively and may be preferable in the case of severalexpressions, as they are more conveniently evaluated interactively(“no extraneous , ”).Since R version 3.4.0, when an expression (from ...) is nottrue and is a call to all.equal, the errormessage will report the (first part of the) differences reported byall.equal(*).
Both mode and storage.mode return a character stringgiving the (storage) mode of the object — often the same — bothrelying on the output of typeof(x), see the examplebelow.mode(x) <- "newmode" changes the mode of object x tonewmode.  This is only supported if there is an appropriateas.newmode function, for example"logical", "integer", "double", "complex","raw", "character", "list", "expression","name", "symbol" and "function".  Attributes arepreserved (but see below).storage.mode(x) <- "newmode" is a more efficient primitiveversion of mode<-, which works for "newmode" which isone of the internal types (see typeof), but not for"single".  Attributes are preserved.As storage mode "single" is only a pseudo-mode in R, it willnot be reported by mode or storage.mode: useattr(object, "Csingle") to examine this.  However,mode<- can be used to set the mode to "single",which sets the real mode to "double" and the "Csingle"attribute to TRUE.  Setting any other mode will remove thisattribute.Note (in the examples below) that some calls have mode"(" which is S compatible.
Both mode and storage.mode return a character stringgiving the (storage) mode of the object — often the same — bothrelying on the output of typeof(x), see the examplebelow.mode(x) <- "newmode" changes the mode of object x tonewmode.  This is only supported if there is an appropriateas.newmode function, for example"logical", "integer", "double", "complex","raw", "character", "list", "expression","name", "symbol" and "function".  Attributes arepreserved (but see below).storage.mode(x) <- "newmode" is a more efficient primitiveversion of mode<-, which works for "newmode" which isone of the internal types (see typeof), but not for"single".  Attributes are preserved.As storage mode "single" is only a pseudo-mode in R, it willnot be reported by mode or storage.mode: useattr(object, "Csingle") to examine this.  However,mode<- can be used to set the mode to "single",which sets the real mode to "double" and the "Csingle"attribute to TRUE.  Setting any other mode will remove thisattribute.Note (in the examples below) that some calls have mode"(" which is S compatible.
If text has length greater than zero (after coercion) it is used inpreference to file.All versions of R accept input from a connection with end of linemarked by LF (as used on Unix), CRLF (as used on DOS/Windows)or CR (as used on classic Mac OS).  The final line can be incomplete,that is missing the final EOL marker.When input is taken from the console, n = NULL is equivalent ton = 1, and n < 0 will read until an EOF character isread.  (The EOF character is Ctrl-Z for the Windows front-ends.)  Theline-length limit is 4095 bytes when reading from the console (whichmay impose a lower limit: see ‘An Introduction to R’).The default for srcfile is set as follows.  Ifkeep.source is not TRUE, srcfiledefaults to a character string, either "<text>" or onederived from file.  When keep.source isTRUE, if text is used, srcfile will be set to asrcfilecopy containing the text.  If a characterstring is used for file, a srcfile objectreferring to that file will be used.When srcfile is a character string, error messages willinclude the name, but source reference information will not be addedto the result.  When srcfile is a srcfileobject, source reference information will be retained.for a character vectors, str2expression(s) corresponds toparse(text = s, keep.source=FALSE), which is always oftype (typeof) and class expression.for a character strings, str2lang(s) corresponds toparse(text = s, keep.source=FALSE)[[1]] (plus a checkthat both s and the parse(*) result are of length one)which is typically a call but may also be a symbol akaname, NULL or an atomic constant such as2, 1L, or TRUE.  Put differently, the value ofstr2lang(.) is a call or one of its parts, in short“a call or simpler”.Currently, encoding is not handled in str2lang() andstr2expression().
If text has length greater than zero (after coercion) it is used inpreference to file.All versions of R accept input from a connection with end of linemarked by LF (as used on Unix), CRLF (as used on DOS/Windows)or CR (as used on classic Mac OS).  The final line can be incomplete,that is missing the final EOL marker.When input is taken from the console, n = NULL is equivalent ton = 1, and n < 0 will read until an EOF character isread.  (The EOF character is Ctrl-Z for the Windows front-ends.)  Theline-length limit is 4095 bytes when reading from the console (whichmay impose a lower limit: see ‘An Introduction to R’).The default for srcfile is set as follows.  Ifkeep.source is not TRUE, srcfiledefaults to a character string, either "<text>" or onederived from file.  When keep.source isTRUE, if text is used, srcfile will be set to asrcfilecopy containing the text.  If a characterstring is used for file, a srcfile objectreferring to that file will be used.When srcfile is a character string, error messages willinclude the name, but source reference information will not be addedto the result.  When srcfile is a srcfileobject, source reference information will be retained.for a character vectors, str2expression(s) corresponds toparse(text = s, keep.source=FALSE), which is always oftype (typeof) and class expression.for a character strings, str2lang(s) corresponds toparse(text = s, keep.source=FALSE)[[1]] (plus a checkthat both s and the parse(*) result are of length one)which is typically a call but may also be a symbol akaname, NULL or an atomic constant such as2, 1L, or TRUE.  Put differently, the value ofstr2lang(.) is a call or one of its parts, in short“a call or simpler”.Currently, encoding is not handled in str2lang() andstr2expression().
The format and as.character methods and strftimeconvert objects from the classes "POSIXlt" and"POSIXct" to character vectors.strptime converts character vectors to class "POSIXlt":its input x is first converted by as.character.Each input string is processed as far as necessary for the formatspecified: any trailing characters are ignored.strftime is a wrapper for format.POSIXlt, and it andformat.POSIXct first convert to class "POSIXlt" bycalling as.POSIXlt (so they also work for class"Date").  Note that only that conversion depends on thetime zone.The usual vector re-cycling rules are applied to x andformat so the answer will be of length of the longer of thesevectors.Locale-specific conversions to and from character strings are usedwhere appropriate and available.  This affects the names of the daysand months, the AM/PM indicator (if used) and the separators in outputformats such as %x and %X, via the setting ofthe LC_TIME locale category.  The ‘currentlocale’ of the descriptions might mean the locale in use at the startof the R session or when these functions are first used.  (For input,the locale-specific conversions can be changed by callingSys.setlocale with category LC_TIME (orLC_ALL).  For output, what happens depends on the OS butusually works.)The details of the formats are platform-specific, but the following arelikely to be widely available: most are defined by the POSIX standard.A conversion specification is introduced by %, usuallyfollowed by a single letter or O or E and then a singleletter.  Any character in the format string not part of a conversionspecification is interpreted literally (and %% gives%).  Widely implemented conversion specifications includeAbbreviated weekday name in the currentlocale on this platform.  (Also matches full name on input:in some locales there are no abbreviations of names.)Full weekday name in the current locale.  (Alsomatches abbreviated name on input.)Abbreviated month name in the current locale onthis platform.  (Also matches full name on input: insome locales there are no abbreviations of names.)Full month name in the current locale.  (Alsomatches abbreviated name on input.)Date and time.   Locale-specific on output,"%a %b %e %H:%M:%S %Y" on input.Century (00–99): the integer part of the yeardivided by 100.Day of the month as decimal number (01–31).Date format such as %m/%d/%y: the C99standard says it should be that exact format (but not all OSescomply).Day of the month as decimal number (1–31), witha leading space for a single-digit number.Equivalent to %Y-%m-%d (the ISO 8601 dateformat).The last two digits of the week-based year(see %V).  (Accepted but ignored on input.)The week-based year (see %V) as a decimalnumber.  (Accepted but ignored on input.)Equivalent to %b.Hours as decimal number (00–23).  As a specialexception strings such as 24:00:00 are accepted for input,since ISO 8601 allows these.Hours as decimal number (01–12).Day of year as decimal number (001–366):  Forinput, 366 is only valid in a leap year.Month as decimal number (01–12).Minute as decimal number (00–59).Newline on output, arbitrary whitespace on input.AM/PM indicator in the locale.  Used inconjunction with %I and not with %H.  Anempty string in some locales (for example on some OSes,non-English European locales including Russia). The behaviour isundefined if used for input in such a locale.Some platforms accept %P for output, which uses a lower-caseversion (%p may also use lower case): others will outputP.For output, the 12-hour clock time (using thelocale's AM or PM): only defined in some locales, and on some OSesmisleading in locales which do not define an AM/PM indicator.For input, equivalent to %I:%M:%S %p.Equivalent to %H:%M.Second as integer (00–61), allowing forup to two leap-seconds (but POSIX-compliant implementationswill ignore leap seconds).Tab on output, arbitrary whitespace on input.Equivalent to %H:%M:%S.Weekday as a decimal number (1–7, Monday is 1).Week of the year as decimal number (00–53) usingSunday as the first day 1 of the week (and typically with thefirst Sunday of the year as day 1 of week 1).  The US convention.Week of the year as decimal number (01–53) asdefined in ISO 8601.If the week (starting on Monday) containing 1 January has four ormore days in the new year, then it is considered week 1.  Otherwise, itis the last week of the previous year, and the next week is week1.  (Accepted but ignored on input.)Weekday as decimal number (0–6, Sunday is 0).Week of the year as decimal number (00–53) usingMonday as the first day of week (and typically with thefirst Monday of the year as day 1 of week 1).  The UK convention.Date.  Locale-specific on output,"%y/%m/%d" on input.Time.  Locale-specific on output,"%H:%M:%S" on input.Year without century (00–99).  On input, values00 to 68 are prefixed by 20 and 69 to 99 by 19 – that is thebehaviour specified by the 2018 POSIX standard, but it doesalso say ‘it is expected that in a future version thedefault century inferred from a 2-digit year will change’.Year with century.  Note that whereas there was nozero in the original Gregorian calendar, ISO 8601:2004 defines itto be valid (interpreted as 1BC): seehttps://en.wikipedia.org/wiki/0_(year).  However, the standardsalso say that years before 1582 in its calendar should only be usedwith agreement of the parties involved.For input, only years 0:9999 are accepted.Signed offset in hours and minutesfrom UTC, so -0800 is 8 hours behind UTC. Values up to+1400 are accepted.  (Standard only for output.  For inputR currently supports it on all platforms.)(Output only.)  Time zone abbreviation as acharacter string (empty if not available).  This may not be reliablewhen a time zone has changed abbreviations over the years.Where leading zeros are shown they will be used on output but areoptional on input.  Names are matched case-insensitively on input:whether they are capitalized on output depends on the platform and thelocale.  Note that abbreviated names are platform-specific (althoughthe standards specify that in the C locale they must be thefirst three letters of the capitalized English name: this conventionis widely used in English-language locales but for example the Frenchmonth abbreviations are not the same on any two of Linux, macOS, Solarisand Windows). Knowing what the abbreviations are is essentialif you wish to use %a, %b or %h as part of aninput format: see the examples for how to check.When %z or %Z is used for output with anobject with an assigned time zone an attempt is made to use the valuesfor that time zone — but it is not guaranteed to succeed.Not in the standards and less widely implemented areThe 24-hour clock time with single digits precededby a blank.The 12-hour clock time with single digits precededby a blank.(Output only.) The number of seconds since theepoch.(Output only.) Similar to %c, often"%a %b %e %H:%M:%S %Z %Y". May depend on the locale.For output there are also %O[dHImMUVwWy] which may emitnumbers in an alternative locale-dependent format (e.g., romannumerals), and %E[cCyYxX] which can use an alternative‘era’ (e.g., a different religious calendar).  Which of theseare supported is OS-dependent.  These are accepted for input, but withthe standard interpretation.Specific to R is %OSn, which for output gives the secondstruncated to 0 <= n <= 6 decimal places (and if %OS isnot followed by a digit, it uses the setting ofgetOption("digits.secs"), or if that is unset, n =  0).  Further, for strptime %OS will input secondsincluding fractional seconds.  Note that %S does not readfractional parts on output. The behaviour of other conversion specifications (and even if othercharacter sequences commencing with % are conversionspecifications) is system-specific.  Some systems document that theuse of multi-byte characters in format is unsupported: UTF-8locales are unlikely to cause a problem.
The format and as.character methods and strftimeconvert objects from the classes "POSIXlt" and"POSIXct" to character vectors.strptime converts character vectors to class "POSIXlt":its input x is first converted by as.character.Each input string is processed as far as necessary for the formatspecified: any trailing characters are ignored.strftime is a wrapper for format.POSIXlt, and it andformat.POSIXct first convert to class "POSIXlt" bycalling as.POSIXlt (so they also work for class"Date").  Note that only that conversion depends on thetime zone.The usual vector re-cycling rules are applied to x andformat so the answer will be of length of the longer of thesevectors.Locale-specific conversions to and from character strings are usedwhere appropriate and available.  This affects the names of the daysand months, the AM/PM indicator (if used) and the separators in outputformats such as %x and %X, via the setting ofthe LC_TIME locale category.  The ‘currentlocale’ of the descriptions might mean the locale in use at the startof the R session or when these functions are first used.  (For input,the locale-specific conversions can be changed by callingSys.setlocale with category LC_TIME (orLC_ALL).  For output, what happens depends on the OS butusually works.)The details of the formats are platform-specific, but the following arelikely to be widely available: most are defined by the POSIX standard.A conversion specification is introduced by %, usuallyfollowed by a single letter or O or E and then a singleletter.  Any character in the format string not part of a conversionspecification is interpreted literally (and %% gives%).  Widely implemented conversion specifications includeAbbreviated weekday name in the currentlocale on this platform.  (Also matches full name on input:in some locales there are no abbreviations of names.)Full weekday name in the current locale.  (Alsomatches abbreviated name on input.)Abbreviated month name in the current locale onthis platform.  (Also matches full name on input: insome locales there are no abbreviations of names.)Full month name in the current locale.  (Alsomatches abbreviated name on input.)Date and time.   Locale-specific on output,"%a %b %e %H:%M:%S %Y" on input.Century (00–99): the integer part of the yeardivided by 100.Day of the month as decimal number (01–31).Date format such as %m/%d/%y: the C99standard says it should be that exact format (but not all OSescomply).Day of the month as decimal number (1–31), witha leading space for a single-digit number.Equivalent to %Y-%m-%d (the ISO 8601 dateformat).The last two digits of the week-based year(see %V).  (Accepted but ignored on input.)The week-based year (see %V) as a decimalnumber.  (Accepted but ignored on input.)Equivalent to %b.Hours as decimal number (00–23).  As a specialexception strings such as 24:00:00 are accepted for input,since ISO 8601 allows these.Hours as decimal number (01–12).Day of year as decimal number (001–366):  Forinput, 366 is only valid in a leap year.Month as decimal number (01–12).Minute as decimal number (00–59).Newline on output, arbitrary whitespace on input.AM/PM indicator in the locale.  Used inconjunction with %I and not with %H.  Anempty string in some locales (for example on some OSes,non-English European locales including Russia). The behaviour isundefined if used for input in such a locale.Some platforms accept %P for output, which uses a lower-caseversion (%p may also use lower case): others will outputP.For output, the 12-hour clock time (using thelocale's AM or PM): only defined in some locales, and on some OSesmisleading in locales which do not define an AM/PM indicator.For input, equivalent to %I:%M:%S %p.Equivalent to %H:%M.Second as integer (00–61), allowing forup to two leap-seconds (but POSIX-compliant implementationswill ignore leap seconds).Tab on output, arbitrary whitespace on input.Equivalent to %H:%M:%S.Weekday as a decimal number (1–7, Monday is 1).Week of the year as decimal number (00–53) usingSunday as the first day 1 of the week (and typically with thefirst Sunday of the year as day 1 of week 1).  The US convention.Week of the year as decimal number (01–53) asdefined in ISO 8601.If the week (starting on Monday) containing 1 January has four ormore days in the new year, then it is considered week 1.  Otherwise, itis the last week of the previous year, and the next week is week1.  (Accepted but ignored on input.)Weekday as decimal number (0–6, Sunday is 0).Week of the year as decimal number (00–53) usingMonday as the first day of week (and typically with thefirst Monday of the year as day 1 of week 1).  The UK convention.Date.  Locale-specific on output,"%y/%m/%d" on input.Time.  Locale-specific on output,"%H:%M:%S" on input.Year without century (00–99).  On input, values00 to 68 are prefixed by 20 and 69 to 99 by 19 – that is thebehaviour specified by the 2018 POSIX standard, but it doesalso say ‘it is expected that in a future version thedefault century inferred from a 2-digit year will change’.Year with century.  Note that whereas there was nozero in the original Gregorian calendar, ISO 8601:2004 defines itto be valid (interpreted as 1BC): seehttps://en.wikipedia.org/wiki/0_(year).  However, the standardsalso say that years before 1582 in its calendar should only be usedwith agreement of the parties involved.For input, only years 0:9999 are accepted.Signed offset in hours and minutesfrom UTC, so -0800 is 8 hours behind UTC. Values up to+1400 are accepted.  (Standard only for output.  For inputR currently supports it on all platforms.)(Output only.)  Time zone abbreviation as acharacter string (empty if not available).  This may not be reliablewhen a time zone has changed abbreviations over the years.Where leading zeros are shown they will be used on output but areoptional on input.  Names are matched case-insensitively on input:whether they are capitalized on output depends on the platform and thelocale.  Note that abbreviated names are platform-specific (althoughthe standards specify that in the C locale they must be thefirst three letters of the capitalized English name: this conventionis widely used in English-language locales but for example the Frenchmonth abbreviations are not the same on any two of Linux, macOS, Solarisand Windows). Knowing what the abbreviations are is essentialif you wish to use %a, %b or %h as part of aninput format: see the examples for how to check.When %z or %Z is used for output with anobject with an assigned time zone an attempt is made to use the valuesfor that time zone — but it is not guaranteed to succeed.Not in the standards and less widely implemented areThe 24-hour clock time with single digits precededby a blank.The 12-hour clock time with single digits precededby a blank.(Output only.) The number of seconds since theepoch.(Output only.) Similar to %c, often"%a %b %e %H:%M:%S %Z %Y". May depend on the locale.For output there are also %O[dHImMUVwWy] which may emitnumbers in an alternative locale-dependent format (e.g., romannumerals), and %E[cCyYxX] which can use an alternative‘era’ (e.g., a different religious calendar).  Which of theseare supported is OS-dependent.  These are accepted for input, but withthe standard interpretation.Specific to R is %OSn, which for output gives the secondstruncated to 0 <= n <= 6 decimal places (and if %OS isnot followed by a digit, it uses the setting ofgetOption("digits.secs"), or if that is unset, n =  0).  Further, for strptime %OS will input secondsincluding fractional seconds.  Note that %S does not readfractional parts on output. The behaviour of other conversion specifications (and even if othercharacter sequences commencing with % are conversionspecifications) is system-specific.  Some systems document that theuse of multi-byte characters in format is unsupported: UTF-8locales are unlikely to cause a problem.
The elements of x and times will be recycled asnecessary (if one has no elements, and empty character vector isreturned).  Missing elements in x or times result inmissing elements of the return value.
Argument split will be coerced to character, soyou will see uses with split = NULL to meansplit = character(0), including in the examples below.Note that splitting into single characters can be done viasplit = character(0) or split = ""; the two areequivalent.  The definition of ‘character’ here depends on thelocale: in a single-byte locale it is a byte, and in a multi-bytelocale it is the unit represented by a ‘wide character’ (almostalways a Unicode code point).A missing value of split does not split the correspondingelement(s) of x at all.The algorithm applied to each input string isNote that this means that if there is a match at the beginning of a(non-empty) string, the first element of the output is "", butif there is a match at the end of the string, the output is the sameas with the match removed.Invalid inputs in the current locale are warned about up to 5 times.
Conversion is based on the C library function strtol.For the default base = 0L, the base chosen from the stringrepresentation of that element of x, so different elements canhave different bases (see the first example).  The standard C rulesfor choosing the base are that octal constants (prefix 0 notfollowed by x or X) and hexadecimal constants (prefix0x or 0X) are interpreted as base 8 and16; all other strings are interpreted as base 10.For a base greater than 10, letters a to z (orA to Z) are used to represent 10 to 35.
‘Width’ is interpreted as the display width in a monospacedfont.  What happens with non-printable characters (such as backspace, tab)is implementation-dependent and may depend on the locale (e.g., theymay be included in the count or they may be omitted).Using this function rather than substr is important whenthere might be double-width (e.g., Chinese/Japanese/Korean) charactersin the character vector.
Adding a  class "factor" will ensure that numeric codes aregiven integer storage mode.For historical reasons (these names are used when deparsing),attributes ".Dim", ".Dimnames", ".Names",".Tsp" and ".Label" are renamed to "dim","dimnames", "names", "tsp" and "levels".It is possible to give the same tag more than once, in which case thelast value assigned wins.  As with other ways of assigning attributes,using tag = NULL removes attribute tag from .Data ifit is present.
Whitespace (space, tab or newline characters) in the input isdestroyed.  Double spaces after periods, question and explanationmarks (thought as representing sentence ends) are preserved.Currently, possible sentence ends at line breaks are not consideredspecially.Indentation is relative to the number of characters in the prefixstring.
Arguments which should be character strings or character vectors arecoerced to character if possible.Each of these functions operates in one of three modes:fixed = TRUE: use exact matching.perl = TRUE: use Perl-style regular expressions.fixed = FALSE, perl = FALSE: use POSIX 1003.2extended regular expressions (the default).See the help pages on regular expression for details of thedifferent types of regular expressions.The two *sub functions differ only in that sub replacesonly the first occurrence of a pattern whereas gsubreplaces all occurrences.  If replacement containsbackreferences which are not defined in pattern the result isundefined (but most often the backreference is taken to be "").For regexpr, gregexpr, regexec and gregexecit is an error for pattern to be NA, otherwise NAis permitted and gives an NA match.Both grep and grepl take missing values in x asnot matching a non-missing pattern.The main effect of useBytes = TRUE is to avoid errors/warningsabout invalid inputs and spurious matches in multibyte locales, butfor regexpr it changes the interpretation of the output.  Itinhibits the conversion of inputs with marked encodings, and is forcedif any input is found which is marked as "bytes" (seeEncoding).Caseless matching does not make much sense for bytes in a multibytelocale, and you should expect it only to work for ASCII characters ifuseBytes = TRUE.regexpr and gregexpr with perl = TRUE allowPython-style named captures, but not for long vector inputs.Invalid inputs in the current locale are warned about up to 5 times.Caseless matching with perl = TRUE for non-ASCII charactersdepends on the PCRE library being compiled with ‘Unicodeproperty support’, which PCRE2 is by default.
This is a generic function, with methods supplied for matrices, dataframes and vectors (including lists).  Packages and users can addfurther methods.For ordinary vectors, the result is simplyx[subset & !is.na(subset)].For data frames, the subset argument works on the rows.  Notethat subset will be evaluated in the data frame, so columns canbe referred to (by name) as variables in the expression (see the examples).The select argument exists only for the methods for data framesand matrices.  It works by first replacing column names in theselection expression with the corresponding column numbers in the dataframe and then using the resulting integer vector to index thecolumns.  This allows the use of the standard indexing conventions sothat for example ranges of columns can be specified easily, or singlecolumns can be dropped (see the examples).The drop argument is passed on to the indexing method formatrices and data frames: note that the default for matrices isdifferent from that for indexing.Factors may have empty levels after subsetting; unused levels arenot automatically removed.  See droplevels for a way todrop all unused levels from a data frame.
This is a generic function, with methods supplied for matrices, dataframes and vectors (including lists).  Packages and users can addfurther methods.For ordinary vectors, the result is simplyx[subset & !is.na(subset)].For data frames, the subset argument works on the rows.  Notethat subset will be evaluated in the data frame, so columns canbe referred to (by name) as variables in the expression (see the examples).The select argument exists only for the methods for data framesand matrices.  It works by first replacing column names in theselection expression with the corresponding column numbers in the dataframe and then using the resulting integer vector to index thecolumns.  This allows the use of the standard indexing conventions sothat for example ranges of columns can be specified easily, or singlecolumns can be dropped (see the examples).The drop argument is passed on to the indexing method formatrices and data frames: note that the default for matrices isdifferent from that for indexing.Factors may have empty levels after subsetting; unused levels arenot automatically removed.  See droplevels for a way todrop all unused levels from a data frame.
This is a generic function, with methods supplied for matrices, dataframes and vectors (including lists).  Packages and users can addfurther methods.For ordinary vectors, the result is simplyx[subset & !is.na(subset)].For data frames, the subset argument works on the rows.  Notethat subset will be evaluated in the data frame, so columns canbe referred to (by name) as variables in the expression (see the examples).The select argument exists only for the methods for data framesand matrices.  It works by first replacing column names in theselection expression with the corresponding column numbers in the dataframe and then using the resulting integer vector to index thecolumns.  This allows the use of the standard indexing conventions sothat for example ranges of columns can be specified easily, or singlecolumns can be dropped (see the examples).The drop argument is passed on to the indexing method formatrices and data frames: note that the default for matrices isdifferent from that for indexing.Factors may have empty levels after subsetting; unused levels arenot automatically removed.  See droplevels for a way todrop all unused levels from a data frame.
This is a generic function, with methods supplied for matrices, dataframes and vectors (including lists).  Packages and users can addfurther methods.For ordinary vectors, the result is simplyx[subset & !is.na(subset)].For data frames, the subset argument works on the rows.  Notethat subset will be evaluated in the data frame, so columns canbe referred to (by name) as variables in the expression (see the examples).The select argument exists only for the methods for data framesand matrices.  It works by first replacing column names in theselection expression with the corresponding column numbers in the dataframe and then using the resulting integer vector to index thecolumns.  This allows the use of the standard indexing conventions sothat for example ranges of columns can be specified easily, or singlecolumns can be dropped (see the examples).The drop argument is passed on to the indexing method formatrices and data frames: note that the default for matrices isdifferent from that for indexing.Factors may have empty levels after subsetting; unused levels arenot automatically removed.  See droplevels for a way todrop all unused levels from a data frame.
The typical use of substitute is to create informative labelsfor data sets and plots.The myplot example below shows a simple use of this facility.It uses the functions deparse and substituteto create labels for a plot which are character string versionsof the actual arguments to the function myplot.Substitution takes place by examining each component of the parse treeas follows: If it is not a bound symbol in env, it isunchanged.  If it is a promise object, i.e., a formal argument to afunction or explicitly created using delayedAssign(),the expression slot of the promise replaces the symbol.  If it is anordinary variable, its value is substituted, unless env is.GlobalEnv in which case the symbol is left unchanged.Both quote and substitute are ‘special’primitive functions which do not evaluate their arguments.
substring is compatible with S, with first andlast instead of start and stop.For vector arguments, it expands the arguments cyclically to thelength of the longest provided none are of zero length.When extracting, if start is larger than the string length then"" is returned.For the extraction functions, x or text will beconverted to a character vector by as.character if it is notalready one.For the replacement functions, if start is larger than thestring length then no replacement is done.  If the portion to bereplaced is longer than the replacement string, then only theportion the length of the string is replaced.If any argument is an NA element, the corresponding element ofthe answer is NA.Elements of the result will be have the encoding declared as that ofthe current locale (see Encoding) if the correspondinginput had a declared Latin-1 or UTF-8 encoding and the current localeis either Latin-1 or UTF-8.If an input element has declared "bytes" encoding (seeEncoding, the subsetting is done in units of bytes notcharacters.
substring is compatible with S, with first andlast instead of start and stop.For vector arguments, it expands the arguments cyclically to thelength of the longest provided none are of zero length.When extracting, if start is larger than the string length then"" is returned.For the extraction functions, x or text will beconverted to a character vector by as.character if it is notalready one.For the replacement functions, if start is larger than thestring length then no replacement is done.  If the portion to bereplaced is longer than the replacement string, then only theportion the length of the string is replaced.If any argument is an NA element, the corresponding element ofthe answer is NA.Elements of the result will be have the encoding declared as that ofthe current locale (see Encoding) if the correspondinginput had a declared Latin-1 or UTF-8 encoding and the current localeis either Latin-1 or UTF-8.If an input element has declared "bytes" encoding (seeEncoding, the subsetting is done in units of bytes notcharacters.
substring is compatible with S, with first andlast instead of start and stop.For vector arguments, it expands the arguments cyclically to thelength of the longest provided none are of zero length.When extracting, if start is larger than the string length then"" is returned.For the extraction functions, x or text will beconverted to a character vector by as.character if it is notalready one.For the replacement functions, if start is larger than thestring length then no replacement is done.  If the portion to bereplaced is longer than the replacement string, then only theportion the length of the string is replaced.If any argument is an NA element, the corresponding element ofthe answer is NA.Elements of the result will be have the encoding declared as that ofthe current locale (see Encoding) if the correspondinginput had a declared Latin-1 or UTF-8 encoding and the current localeis either Latin-1 or UTF-8.If an input element has declared "bytes" encoding (seeEncoding, the subsetting is done in units of bytes notcharacters.
substring is compatible with S, with first andlast instead of start and stop.For vector arguments, it expands the arguments cyclically to thelength of the longest provided none are of zero length.When extracting, if start is larger than the string length then"" is returned.For the extraction functions, x or text will beconverted to a character vector by as.character if it is notalready one.For the replacement functions, if start is larger than thestring length then no replacement is done.  If the portion to bereplaced is longer than the replacement string, then only theportion the length of the string is replaced.If any argument is an NA element, the corresponding element ofthe answer is NA.Elements of the result will be have the encoding declared as that ofthe current locale (see Encoding) if the correspondinginput had a declared Latin-1 or UTF-8 encoding and the current localeis either Latin-1 or UTF-8.If an input element has declared "bytes" encoding (seeEncoding, the subsetting is done in units of bytes notcharacters.
This is a generic function: methods can be defined for itdirectly or via the Summary group generic.For this to work properly, the arguments ... should beunnamed, and dispatch is on the first argument.If na.rm is FALSE an NA or NaN value inany of the arguments will cause a value of NA or NaN tobe returned, otherwise NA and NaN values are ignored.Logical true values are regarded as one, false values as zero.For historical reasons, NULL is accepted and treated as if itwere integer(0).Loss of accuracy can occur when summing values of different signs:this can even occur for sufficiently long integer inputs if thepartial sums would cause integer overflow.  Where possibleextended-precision accumulators are used, typically well supportedwith C99 and newer, but possibly platform-dependent.
For factors, the frequency of the first maxsum - 1most frequent levels is shown, and the less frequent levels aresummarized in "(Others)" (resulting in at most maxsumfrequencies).The functions summary.lm and summary.glm are examplesof particular methods which summarize the results produced bylm and glm.
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
For factors, the frequency of the first maxsum - 1most frequent levels is shown, and the less frequent levels aresummarized in "(Others)" (resulting in at most maxsumfrequencies).The functions summary.lm and summary.glm are examplesof particular methods which summarize the results produced bylm and glm.
There are four groups for which S3 methods can be written,namely the "Math", "Ops", "Summary" and"Complex" groups.  These are not R objects in base R, butmethods can be supplied for them and base R containsfactor, data.frame anddifftime methods for the first three groups.  (There isalso a ordered method for Ops,POSIXt and Date methods for Mathand Ops, package_version methods for Opsand Summary, as well as a ts method forOps in package stats.) Group "Math":abs, sign, sqrt,floor, ceiling, trunc,round, signifexp, log,  expm1, log1p,cos, sin, tan,cospi, sinpi, tanpi,acos, asin, atancosh, sinh, tanh,acosh, asinh, atanhlgamma, gamma, digamma, trigammacumsum, cumprod, cummax, cumminMembers of this group dispatch on x.  Most members acceptonly one argument, but members log, round andsignif accept one or two arguments, and trunc acceptsone or more. Group "Ops":"+", "-", "*", "/","^", "%%", "%/%""&", "|", "!""==", "!=","<", "<=", ">=", ">"This group contains both binary and unary operators (+,- and !): when a unary operator is encountered theOps method is called with one argument and e2 ismissing.The classes of both arguments are considered in dispatching anymember of this group.  For each argument its vector of classes isexamined to see if there is a matching specific (preferred) orOps method.  If a method is found for just one argument orthe same method is found for both, it is used.If different methods are found, there is a warning about‘incompatible methods’: in that case or if no method is foundfor either argument the internal method is used.Note that the data.frame methods for the comparison("Compare": ==, <, ...) and logic("Logic": & | and !) operators return alogical matrix instead of a data frame, forconvenience and back compatibility.If the members of this group are called as functions, any argumentnames are removed to ensure that positional matching is always used. Group "Summary":all, anysum, prodmin, maxrangeMembers of this group dispatch on the first argument supplied.Note that the data.frame methods for the"Summary" and "Math" groups require “numeric-alike”columns x, i.e., fulfilling  Group "Complex":Arg, Conj, Im, Mod, ReMembers of this group dispatch on z.Note that a method will be used for one of these groups or one of itsmembers only if it corresponds to a "class" attribute,as the internal code dispatches on oldClass and not onclass.  This is for efficiency: having to dispatch on,say, Ops.integer would be too slow.The number of arguments supplied for primitive members of the"Math" group generic methods is not checked prior to dispatch.There is no lazy evaluation of arguments for group-generic functions.
Dates are represented as the number of days since 1970-01-01, withnegative values for earlier dates.  They are always printedfollowing the rules of the current Gregorian calendar, even thoughthat calendar was not in use long ago (it was adopted in 1752 inGreat Britain and its colonies).It is intended that the date should be an integer, but this is notenforced in the internal representation.  Fractional days will beignored when printing.  It is possible to produce fractional days viathe mean method or by adding or subtracting (seeOps.Date).From the many methods, see methods(class = "Date"), a few aredocumented separately, see below.
Dates are represented as the number of days since 1970-01-01, withnegative values for earlier dates.  They are always printedfollowing the rules of the current Gregorian calendar, even thoughthat calendar was not in use long ago (it was adopted in 1752 inGreat Britain and its colonies).It is intended that the date should be an integer, but this is notenforced in the internal representation.  Fractional days will beignored when printing.  It is possible to produce fractional days viathe mean method or by adding or subtracting (seeOps.Date).From the many methods, see methods(class = "Date"), a few aredocumented separately, see below.
For factors, the frequency of the first maxsum - 1most frequent levels is shown, and the less frequent levels aresummarized in "(Others)" (resulting in at most maxsumfrequencies).The functions summary.lm and summary.glm are examplesof particular methods which summarize the results produced bylm and glm.
Function difftime calculates a difference of two date/timeobjects and returns an object of class "difftime" with anattribute indicating the units.  TheMath group method providesround, signif, floor,ceiling, trunc, abs, andsign methods for objects of this class, and there aremethods for the group-generic (seeOps) logical and arithmeticoperations.If units = "auto", a suitable set of units is chosen, the largestpossible (excluding "weeks") in which all the absolutedifferences are greater than one.Subtraction of date-time objects gives an object of this class,by calling difftime with units = "auto".  Alternatively,as.difftime() works on character-coded or numeric timeintervals; in the latter case, units must be specified, andformat has no effect.Limited arithmetic is available on "difftime" objects: they canbe added or subtracted, and multiplied or divided by a numeric vector.In addition, adding or subtracting a numeric vector by a"difftime" object implicitly converts the numeric vector to a"difftime" object with the same units as the "difftime"object.  There are methods for mean andsum (via the Summarygroup generic), and diff via diff.defaultbuilding on the "difftime" method for arithmetic, notably-.The units of a "difftime" object can be extracted by theunits function, which also has a replacement form.  If theunits are changed, the numerical value is scaled accordingly.  Thereplacement version keeps attributes such as names and dimensions.Note that units = "days" means a period of 24 hours, hencetakes no account of Daylight Savings Time.  Differences in objectsof class "Date" are computed as if in the UTC time zone.The as.double method returns the numeric value expressed inthe specified units.  Using  units = "auto" means the units of theobject.The format method simply formats the numeric value and appendsthe units as a text string.
For factors, the frequency of the first maxsum - 1most frequent levels is shown, and the less frequent levels aresummarized in "(Others)" (resulting in at most maxsumfrequencies).The functions summary.lm and summary.glm are examplesof particular methods which summarize the results produced bylm and glm.
The type of the vector x is not restricted; it only must havean as.character method and be sortable (byorder).Ordered factors differ from factors only in their class, but methodsand the model-fitting functions treat the two classes quite differently.The encoding of the vector happens as follows.  First all the valuesin exclude are removed from levels. If x[i]equals levels[j], then the i-th element of the result isj.  If no match is found for x[i] in levels(which will happen for excluded values) then the i-th elementof the result is set to NA.Normally the ‘levels’ used as an attribute of the result arethe reduced set of levels after removing those in exclude, butthis can be altered by supplying labels.  This should eitherbe a set of new labels for the levels, or a character string, inwhich case the levels are that character string with a sequencenumber appended.factor(x, exclude = NULL) applied to a factor withoutNAs is a no-operation unless there are unused levels: inthat case, a factor with the reduced level set is returned.  Ifexclude is used, since R version 3.4.0, excluding non-existingcharacter levels is equivalent to excluding nothing, and whenexclude is a character vector, that isapplied to the levels of x.Alternatively, exclude can be factor with the same level set asx and will exclude the levels present in exclude.The codes of a factor may contain NA.  For a numericx, set exclude = NULL to make NA an extralevel (prints as <NA>); by default, this is the last level.If NA is a level, the way to set a code to be missing (asopposed to the code of the missing level) is touse is.na on the left-hand-side of an assignment (as inis.na(f)[i] <- TRUE; indexing inside is.na does not work).Under those circumstances missing values are currently printed as<NA>, i.e., identical to entries of level NA.is.factor is generic: you can write methods to handlespecific classes of objects, see InternalMethods.Where levels is not supplied, unique is called.Since factors typically have quite a small number of levels, for largevectors x it is helpful to supply nmax as an upper boundon the number of unique values.Since R 4.1.0, when using c to combine a (possiblyordered) factor with other objects, if all objects are (possiblyordered) factors, the result will be a factor with levels the union ofthe level sets of the elements, in the order the levels occur in thelevel sets of the elements (which means that if all the elements havethe same level set, that is the level set of the result), equivalentto how unlist operates on a list of factor objects.
For factors, the frequency of the first maxsum - 1most frequent levels is shown, and the less frequent levels aresummarized in "(Others)" (resulting in at most maxsumfrequencies).The functions summary.lm and summary.glm are examplesof particular methods which summarize the results produced bylm and glm.
Numeric versions are sequences of one or more non-negative integers,usually (e.g., in package ‘DESCRIPTION’ files) represented ascharacter strings with the elements of the sequence concatenated andseparated by single . or - characters.  R packageversions consist of at least two such integers, an R system versionof exactly three (major, minor and patchlevel).Functions numeric_version, package_version andR_system_version create a representation from such strings (ifsuitable) which allows for coercion and testing, combination,comparison, summaries (min/max), inclusion in data frames,subscripting, and printing.  The classes can hold a vector of suchrepresentations.getRversion returns the version of the running R as an Rsystem version object.The [[ operator extracts or replaces a single version.  Toaccess the integers of a version use two indices: see the examples.
The type of the vector x is not restricted; it only must havean as.character method and be sortable (byorder).Ordered factors differ from factors only in their class, but methodsand the model-fitting functions treat the two classes quite differently.The encoding of the vector happens as follows.  First all the valuesin exclude are removed from levels. If x[i]equals levels[j], then the i-th element of the result isj.  If no match is found for x[i] in levels(which will happen for excluded values) then the i-th elementof the result is set to NA.Normally the ‘levels’ used as an attribute of the result arethe reduced set of levels after removing those in exclude, butthis can be altered by supplying labels.  This should eitherbe a set of new labels for the levels, or a character string, inwhich case the levels are that character string with a sequencenumber appended.factor(x, exclude = NULL) applied to a factor withoutNAs is a no-operation unless there are unused levels: inthat case, a factor with the reduced level set is returned.  Ifexclude is used, since R version 3.4.0, excluding non-existingcharacter levels is equivalent to excluding nothing, and whenexclude is a character vector, that isapplied to the levels of x.Alternatively, exclude can be factor with the same level set asx and will exclude the levels present in exclude.The codes of a factor may contain NA.  For a numericx, set exclude = NULL to make NA an extralevel (prints as <NA>); by default, this is the last level.If NA is a level, the way to set a code to be missing (asopposed to the code of the missing level) is touse is.na on the left-hand-side of an assignment (as inis.na(f)[i] <- TRUE; indexing inside is.na does not work).Under those circumstances missing values are currently printed as<NA>, i.e., identical to entries of level NA.is.factor is generic: you can write methods to handlespecific classes of objects, see InternalMethods.Where levels is not supplied, unique is called.Since factors typically have quite a small number of levels, for largevectors x it is helpful to supply nmax as an upper boundon the number of unique values.Since R 4.1.0, when using c to combine a (possiblyordered) factor with other objects, if all objects are (possiblyordered) factors, the result will be a factor with levels the union ofthe level sets of the elements, in the order the levels occur in thelevel sets of the elements (which means that if all the elements havethe same level set, that is the level set of the result), equivalentto how unlist operates on a list of factor objects.
There are two basic classes of date/times.  Class "POSIXct"represents the (signed) number of seconds since the beginning of 1970(in the UTC time zone) as a numeric vector.  Class "POSIXlt" isa named list of vectors representing0–61: seconds.0–59: minutes.0–23: hours.1–31: day of the month0–11: months after the first of the year.years since 1900.0–6 day of the week, starting on Sunday.0–365: day of the year (365 only in leap years).Daylight Saving Time flag.  Positive if inforce, zero if not, negative if unknown.(Optional.) The abbreviation for the time zone inforce at that time: "" if unknown (but "" might alsobe used for UTC).(Optional.) The offset in seconds from GMT:positive values are East of the meridian.  Usually NA ifunknown, but 0 could mean unknown.(The last two components are not present for times in UTC and areplatform-dependent: they are supported on platforms based on BSD orglibc (including Linux and macOS) and those using thetzcode implementation shipped with R (including Windows). Butthey are not necessarily set.).  Note that the internal list structureis somewhat hidden, as many methods (includinglength(x), print() and str)apply to the abstract date-time vector, as for "POSIXct".  Asfrom R 3.5.0, one can extract and replace single componentsvia [ indexing with two indices (see the examples).  Theclasses correspond to the POSIX/C99 constructs of ‘calendartime’ (the time_t data type) and ‘local time’ (orbroken-down time, the struct tm data type), from which theyalso inherit their names.  The components of "POSIXlt" areinteger vectors, except sec and zone."POSIXct" is more convenient for including in data frames, and"POSIXlt" is closer to human-readable forms.  A virtual class"POSIXt" exists from which both of the classes inherit: it isused to allow operations such as subtraction to mix the two classes.Components wday and yday of "POSIXlt" are forinformation, and are not used in the conversion to calendar time.However, isdst is needed to distinguish times at the end ofDST: typically 1am to 2am occurs twice, first in DST and then instandard time.  At all other times isdst can be deduced fromthe first six values, but the behaviour if it is set incorrectly isplatform-dependent.Logical comparisons and some arithmetic operations are available forboth classes.  One can add or subtract a number of seconds from adate-time object, but not add two date-time objects.  Subtraction oftwo date-time objects is equivalent to using difftime.Be aware that "POSIXlt" objects will be interpreted as being inthe current time zone for these operations unless a time zone has beenspecified."POSIXlt" objects will often have an attribute "tzone",a character vector of length 3 giving the time zone name (from the TZenvironment variable or argument tz of functions creating"POSIXlt" objects; "" marks the current time zone)and the names of the base time zoneand the alternate (daylight-saving) time zone.  Sometimes this mayjust be of length one, giving the time zone name."POSIXct" objects may also have an attribute "tzone", acharacter vector of length one.  If set to a non-empty value, it willdetermine how the object is converted to class "POSIXlt" and inparticular how it is printed.  This is usually desirable, but if youwant to specify an object in a particular time zone but to be printedin the current time zone you may want to remove the "tzone"attribute (e.g., by c(x)).Unfortunately, the conversion is complicated by the operation of timezones and leap seconds (according to this version of R's data,27 days have been 86401 seconds long sofar, the last being on (actually, immediately before)2017-01-01: the times of theextra seconds are in the object .leap.seconds).  The details ofthis are entrusted to the OS services where possible.  It seems thatsome rare systems used to use leap seconds, but all known currentplatforms ignore them (as required by POSIX).  This is detected andcorrected for at build time, so "POSIXct" times used by R donot include leap seconds on any platform.Using c on "POSIXlt" objects converts them to thecurrent time zone, and on "POSIXct" objects drops any"tzone" attributes, unless they are all marked with the sametime zone.A few times have specific issues.  First, the leap seconds are ignored,and real times such as "2005-12-31 23:59:60" are (probably)treated as the next second.  However, they will never be generated byR, and are unlikely to arise as input.  Second, on some OSes there isa problem in the POSIX/C99 standard with "1969-12-31 23:59:59 UTC",which is -1 in calendar time and that value is on those OSesalso used as an error code.  Thus as.POSIXct("1969-12-31  23:59:59", format = "%Y-%m-%d %H:%M:%S", tz = "UTC") may giveNA, and hence as.POSIXct("1969-12-31 23:59:59",  tz = "UTC") will give "1969-12-31 23:59:00".  Other OSes(including the code used by R on Windows) report errors separatelyand so are able to handle that time as valid.The print methods respect options("max.print").
There are two basic classes of date/times.  Class "POSIXct"represents the (signed) number of seconds since the beginning of 1970(in the UTC time zone) as a numeric vector.  Class "POSIXlt" isa named list of vectors representing0–61: seconds.0–59: minutes.0–23: hours.1–31: day of the month0–11: months after the first of the year.years since 1900.0–6 day of the week, starting on Sunday.0–365: day of the year (365 only in leap years).Daylight Saving Time flag.  Positive if inforce, zero if not, negative if unknown.(Optional.) The abbreviation for the time zone inforce at that time: "" if unknown (but "" might alsobe used for UTC).(Optional.) The offset in seconds from GMT:positive values are East of the meridian.  Usually NA ifunknown, but 0 could mean unknown.(The last two components are not present for times in UTC and areplatform-dependent: they are supported on platforms based on BSD orglibc (including Linux and macOS) and those using thetzcode implementation shipped with R (including Windows). Butthey are not necessarily set.).  Note that the internal list structureis somewhat hidden, as many methods (includinglength(x), print() and str)apply to the abstract date-time vector, as for "POSIXct".  Asfrom R 3.5.0, one can extract and replace single componentsvia [ indexing with two indices (see the examples).  Theclasses correspond to the POSIX/C99 constructs of ‘calendartime’ (the time_t data type) and ‘local time’ (orbroken-down time, the struct tm data type), from which theyalso inherit their names.  The components of "POSIXlt" areinteger vectors, except sec and zone."POSIXct" is more convenient for including in data frames, and"POSIXlt" is closer to human-readable forms.  A virtual class"POSIXt" exists from which both of the classes inherit: it isused to allow operations such as subtraction to mix the two classes.Components wday and yday of "POSIXlt" are forinformation, and are not used in the conversion to calendar time.However, isdst is needed to distinguish times at the end ofDST: typically 1am to 2am occurs twice, first in DST and then instandard time.  At all other times isdst can be deduced fromthe first six values, but the behaviour if it is set incorrectly isplatform-dependent.Logical comparisons and some arithmetic operations are available forboth classes.  One can add or subtract a number of seconds from adate-time object, but not add two date-time objects.  Subtraction oftwo date-time objects is equivalent to using difftime.Be aware that "POSIXlt" objects will be interpreted as being inthe current time zone for these operations unless a time zone has beenspecified."POSIXlt" objects will often have an attribute "tzone",a character vector of length 3 giving the time zone name (from the TZenvironment variable or argument tz of functions creating"POSIXlt" objects; "" marks the current time zone)and the names of the base time zoneand the alternate (daylight-saving) time zone.  Sometimes this mayjust be of length one, giving the time zone name."POSIXct" objects may also have an attribute "tzone", acharacter vector of length one.  If set to a non-empty value, it willdetermine how the object is converted to class "POSIXlt" and inparticular how it is printed.  This is usually desirable, but if youwant to specify an object in a particular time zone but to be printedin the current time zone you may want to remove the "tzone"attribute (e.g., by c(x)).Unfortunately, the conversion is complicated by the operation of timezones and leap seconds (according to this version of R's data,27 days have been 86401 seconds long sofar, the last being on (actually, immediately before)2017-01-01: the times of theextra seconds are in the object .leap.seconds).  The details ofthis are entrusted to the OS services where possible.  It seems thatsome rare systems used to use leap seconds, but all known currentplatforms ignore them (as required by POSIX).  This is detected andcorrected for at build time, so "POSIXct" times used by R donot include leap seconds on any platform.Using c on "POSIXlt" objects converts them to thecurrent time zone, and on "POSIXct" objects drops any"tzone" attributes, unless they are all marked with the sametime zone.A few times have specific issues.  First, the leap seconds are ignored,and real times such as "2005-12-31 23:59:60" are (probably)treated as the next second.  However, they will never be generated byR, and are unlikely to arise as input.  Second, on some OSes there isa problem in the POSIX/C99 standard with "1969-12-31 23:59:59 UTC",which is -1 in calendar time and that value is on those OSesalso used as an error code.  Thus as.POSIXct("1969-12-31  23:59:59", format = "%Y-%m-%d %H:%M:%S", tz = "UTC") may giveNA, and hence as.POSIXct("1969-12-31 23:59:59",  tz = "UTC") will give "1969-12-31 23:59:00".  Other OSes(including the code used by R on Windows) report errors separatelyand so are able to handle that time as valid.The print methods respect options("max.print").
There are two basic classes of date/times.  Class "POSIXct"represents the (signed) number of seconds since the beginning of 1970(in the UTC time zone) as a numeric vector.  Class "POSIXlt" isa named list of vectors representing0–61: seconds.0–59: minutes.0–23: hours.1–31: day of the month0–11: months after the first of the year.years since 1900.0–6 day of the week, starting on Sunday.0–365: day of the year (365 only in leap years).Daylight Saving Time flag.  Positive if inforce, zero if not, negative if unknown.(Optional.) The abbreviation for the time zone inforce at that time: "" if unknown (but "" might alsobe used for UTC).(Optional.) The offset in seconds from GMT:positive values are East of the meridian.  Usually NA ifunknown, but 0 could mean unknown.(The last two components are not present for times in UTC and areplatform-dependent: they are supported on platforms based on BSD orglibc (including Linux and macOS) and those using thetzcode implementation shipped with R (including Windows). Butthey are not necessarily set.).  Note that the internal list structureis somewhat hidden, as many methods (includinglength(x), print() and str)apply to the abstract date-time vector, as for "POSIXct".  Asfrom R 3.5.0, one can extract and replace single componentsvia [ indexing with two indices (see the examples).  Theclasses correspond to the POSIX/C99 constructs of ‘calendartime’ (the time_t data type) and ‘local time’ (orbroken-down time, the struct tm data type), from which theyalso inherit their names.  The components of "POSIXlt" areinteger vectors, except sec and zone."POSIXct" is more convenient for including in data frames, and"POSIXlt" is closer to human-readable forms.  A virtual class"POSIXt" exists from which both of the classes inherit: it isused to allow operations such as subtraction to mix the two classes.Components wday and yday of "POSIXlt" are forinformation, and are not used in the conversion to calendar time.However, isdst is needed to distinguish times at the end ofDST: typically 1am to 2am occurs twice, first in DST and then instandard time.  At all other times isdst can be deduced fromthe first six values, but the behaviour if it is set incorrectly isplatform-dependent.Logical comparisons and some arithmetic operations are available forboth classes.  One can add or subtract a number of seconds from adate-time object, but not add two date-time objects.  Subtraction oftwo date-time objects is equivalent to using difftime.Be aware that "POSIXlt" objects will be interpreted as being inthe current time zone for these operations unless a time zone has beenspecified."POSIXlt" objects will often have an attribute "tzone",a character vector of length 3 giving the time zone name (from the TZenvironment variable or argument tz of functions creating"POSIXlt" objects; "" marks the current time zone)and the names of the base time zoneand the alternate (daylight-saving) time zone.  Sometimes this mayjust be of length one, giving the time zone name."POSIXct" objects may also have an attribute "tzone", acharacter vector of length one.  If set to a non-empty value, it willdetermine how the object is converted to class "POSIXlt" and inparticular how it is printed.  This is usually desirable, but if youwant to specify an object in a particular time zone but to be printedin the current time zone you may want to remove the "tzone"attribute (e.g., by c(x)).Unfortunately, the conversion is complicated by the operation of timezones and leap seconds (according to this version of R's data,27 days have been 86401 seconds long sofar, the last being on (actually, immediately before)2017-01-01: the times of theextra seconds are in the object .leap.seconds).  The details ofthis are entrusted to the OS services where possible.  It seems thatsome rare systems used to use leap seconds, but all known currentplatforms ignore them (as required by POSIX).  This is detected andcorrected for at build time, so "POSIXct" times used by R donot include leap seconds on any platform.Using c on "POSIXlt" objects converts them to thecurrent time zone, and on "POSIXct" objects drops any"tzone" attributes, unless they are all marked with the sametime zone.A few times have specific issues.  First, the leap seconds are ignored,and real times such as "2005-12-31 23:59:60" are (probably)treated as the next second.  However, they will never be generated byR, and are unlikely to arise as input.  Second, on some OSes there isa problem in the POSIX/C99 standard with "1969-12-31 23:59:59 UTC",which is -1 in calendar time and that value is on those OSesalso used as an error code.  Thus as.POSIXct("1969-12-31  23:59:59", format = "%Y-%m-%d %H:%M:%S", tz = "UTC") may giveNA, and hence as.POSIXct("1969-12-31 23:59:59",  tz = "UTC") will give "1969-12-31 23:59:00".  Other OSes(including the code used by R on Windows) report errors separatelyand so are able to handle that time as valid.The print methods respect options("max.print").
There are two basic classes of date/times.  Class "POSIXct"represents the (signed) number of seconds since the beginning of 1970(in the UTC time zone) as a numeric vector.  Class "POSIXlt" isa named list of vectors representing0–61: seconds.0–59: minutes.0–23: hours.1–31: day of the month0–11: months after the first of the year.years since 1900.0–6 day of the week, starting on Sunday.0–365: day of the year (365 only in leap years).Daylight Saving Time flag.  Positive if inforce, zero if not, negative if unknown.(Optional.) The abbreviation for the time zone inforce at that time: "" if unknown (but "" might alsobe used for UTC).(Optional.) The offset in seconds from GMT:positive values are East of the meridian.  Usually NA ifunknown, but 0 could mean unknown.(The last two components are not present for times in UTC and areplatform-dependent: they are supported on platforms based on BSD orglibc (including Linux and macOS) and those using thetzcode implementation shipped with R (including Windows). Butthey are not necessarily set.).  Note that the internal list structureis somewhat hidden, as many methods (includinglength(x), print() and str)apply to the abstract date-time vector, as for "POSIXct".  Asfrom R 3.5.0, one can extract and replace single componentsvia [ indexing with two indices (see the examples).  Theclasses correspond to the POSIX/C99 constructs of ‘calendartime’ (the time_t data type) and ‘local time’ (orbroken-down time, the struct tm data type), from which theyalso inherit their names.  The components of "POSIXlt" areinteger vectors, except sec and zone."POSIXct" is more convenient for including in data frames, and"POSIXlt" is closer to human-readable forms.  A virtual class"POSIXt" exists from which both of the classes inherit: it isused to allow operations such as subtraction to mix the two classes.Components wday and yday of "POSIXlt" are forinformation, and are not used in the conversion to calendar time.However, isdst is needed to distinguish times at the end ofDST: typically 1am to 2am occurs twice, first in DST and then instandard time.  At all other times isdst can be deduced fromthe first six values, but the behaviour if it is set incorrectly isplatform-dependent.Logical comparisons and some arithmetic operations are available forboth classes.  One can add or subtract a number of seconds from adate-time object, but not add two date-time objects.  Subtraction oftwo date-time objects is equivalent to using difftime.Be aware that "POSIXlt" objects will be interpreted as being inthe current time zone for these operations unless a time zone has beenspecified."POSIXlt" objects will often have an attribute "tzone",a character vector of length 3 giving the time zone name (from the TZenvironment variable or argument tz of functions creating"POSIXlt" objects; "" marks the current time zone)and the names of the base time zoneand the alternate (daylight-saving) time zone.  Sometimes this mayjust be of length one, giving the time zone name."POSIXct" objects may also have an attribute "tzone", acharacter vector of length one.  If set to a non-empty value, it willdetermine how the object is converted to class "POSIXlt" and inparticular how it is printed.  This is usually desirable, but if youwant to specify an object in a particular time zone but to be printedin the current time zone you may want to remove the "tzone"attribute (e.g., by c(x)).Unfortunately, the conversion is complicated by the operation of timezones and leap seconds (according to this version of R's data,27 days have been 86401 seconds long sofar, the last being on (actually, immediately before)2017-01-01: the times of theextra seconds are in the object .leap.seconds).  The details ofthis are entrusted to the OS services where possible.  It seems thatsome rare systems used to use leap seconds, but all known currentplatforms ignore them (as required by POSIX).  This is detected andcorrected for at build time, so "POSIXct" times used by R donot include leap seconds on any platform.Using c on "POSIXlt" objects converts them to thecurrent time zone, and on "POSIXct" objects drops any"tzone" attributes, unless they are all marked with the sametime zone.A few times have specific issues.  First, the leap seconds are ignored,and real times such as "2005-12-31 23:59:60" are (probably)treated as the next second.  However, they will never be generated byR, and are unlikely to arise as input.  Second, on some OSes there isa problem in the POSIX/C99 standard with "1969-12-31 23:59:59 UTC",which is -1 in calendar time and that value is on those OSesalso used as an error code.  Thus as.POSIXct("1969-12-31  23:59:59", format = "%Y-%m-%d %H:%M:%S", tz = "UTC") may giveNA, and hence as.POSIXct("1969-12-31 23:59:59",  tz = "UTC") will give "1969-12-31 23:59:00".  Other OSes(including the code used by R on Windows) report errors separatelyand so are able to handle that time as valid.The print methods respect options("max.print").
proc.time returns five elements for backwards compatibility,but its print method prints a named vector oflength 3.  The first two entries are the total user and system CPUtimes of the current R process and any child processes on which ithas waited, and the third entry is the ‘real’ elapsed timesince the process was started.
These functions and classes handle source code references.The srcfile function produces an object of classsrcfile, which contains the name and directory of a source codefile, along with its timestamp, for use in source level debugging (notyet implemented) and source echoing.  The encoding of the file issaved; see file for a discussion of encodings, andiconvlist for a list of allowable encodings on your platform.The srcfilecopy function produces an object of the descendantclass srcfilecopy, which saves the source lines in a charactervector.  It copies the value of the isFile argument, to helpdebuggers identify whether this text comes from a real file in thefile system.The srcfilealias function produces an object of the descendantclass srcfilealias, which gives an alternate name to anothersrcfile.  This is produced by the parser when a #line directiveis used.The getSrcLines function reads the specified lines fromsrcfile.The srcref function produces an object of classsrcref, which describes a range of characters in asrcfile.The lloc value gives the following values:Bytes (elements 2, 4) andcolumns (elements 5, 6) may be different due to multibytecharacters.  If only four values are given, the columns and bytesare assumed to match.  Lines (elements 1, 3) and parsed lines(elements 7, 8) may differ if a #line directive is used incode:  the former will respect the directive, the latter will justcount lines.  If only 4 or 6 elements are given, the parsed lineswill be assumed to match the lines.Methods are defined for print, summary, open,and close for classes srcfile and srcfilecopy.The open method opens its internal file connection ata particular line; if it was already open, it will be repositionedto that line.Methods are defined for print, summary andas.character for class srcref.  The as.charactermethod will read the associated source file to obtain the textcorresponding to the reference.  If the to argument is given,it should be a second srcref that follows the first, in thesame file; they will be treated as one reference to the wholerange.  The exact behaviour depends on theclass of the source file.  If the source file inherits fromclass srcfilecopy, the lines are taken from the saved copyusing the “parsed” line counts.  If not, an attemptis made to read the file, and the original line numbers of thesrcref record (i.e., elements 1 and 3) are used.  If an erroroccurs (e.g., the file no longer exists), text like<srcref: "file" chars 1:1 to 2:10> will be returned instead,indicating the line:column ranges of the first and lastcharacter.  The summary method defaults to this type ofdisplay.Lists of srcref objects may be attached to expressions as the"srcref" attribute.  (The list of srcref objects should be the samelength as the expression.)  By default, expressions are printed byprint.default using the associated srcref.  Tosee deparsed code instead, call print with argumentuseSource = FALSE.  If a srcref objectis printed with useSource = FALSE, the <srcref: ...>record will be printed..isOpen is intended for internal use:  it checks whether theconnection associated with a srcfile object is open.
These functions and classes handle source code references.The srcfile function produces an object of classsrcfile, which contains the name and directory of a source codefile, along with its timestamp, for use in source level debugging (notyet implemented) and source echoing.  The encoding of the file issaved; see file for a discussion of encodings, andiconvlist for a list of allowable encodings on your platform.The srcfilecopy function produces an object of the descendantclass srcfilecopy, which saves the source lines in a charactervector.  It copies the value of the isFile argument, to helpdebuggers identify whether this text comes from a real file in thefile system.The srcfilealias function produces an object of the descendantclass srcfilealias, which gives an alternate name to anothersrcfile.  This is produced by the parser when a #line directiveis used.The getSrcLines function reads the specified lines fromsrcfile.The srcref function produces an object of classsrcref, which describes a range of characters in asrcfile.The lloc value gives the following values:Bytes (elements 2, 4) andcolumns (elements 5, 6) may be different due to multibytecharacters.  If only four values are given, the columns and bytesare assumed to match.  Lines (elements 1, 3) and parsed lines(elements 7, 8) may differ if a #line directive is used incode:  the former will respect the directive, the latter will justcount lines.  If only 4 or 6 elements are given, the parsed lineswill be assumed to match the lines.Methods are defined for print, summary, open,and close for classes srcfile and srcfilecopy.The open method opens its internal file connection ata particular line; if it was already open, it will be repositionedto that line.Methods are defined for print, summary andas.character for class srcref.  The as.charactermethod will read the associated source file to obtain the textcorresponding to the reference.  If the to argument is given,it should be a second srcref that follows the first, in thesame file; they will be treated as one reference to the wholerange.  The exact behaviour depends on theclass of the source file.  If the source file inherits fromclass srcfilecopy, the lines are taken from the saved copyusing the “parsed” line counts.  If not, an attemptis made to read the file, and the original line numbers of thesrcref record (i.e., elements 1 and 3) are used.  If an erroroccurs (e.g., the file no longer exists), text like<srcref: "file" chars 1:1 to 2:10> will be returned instead,indicating the line:column ranges of the first and lastcharacter.  The summary method defaults to this type ofdisplay.Lists of srcref objects may be attached to expressions as the"srcref" attribute.  (The list of srcref objects should be the samelength as the expression.)  By default, expressions are printed byprint.default using the associated srcref.  Tosee deparsed code instead, call print with argumentuseSource = FALSE.  If a srcref objectis printed with useSource = FALSE, the <srcref: ...>record will be printed..isOpen is intended for internal use:  it checks whether theconnection associated with a srcfile object is open.
If the argument dnn is not supplied, the internal functionlist.names is called to compute the ‘dimname names’.  If thearguments in ... are named, those names are used.  For theremaining arguments, deparse.level = 0 gives an empty name,deparse.level = 1 uses the supplied argument if it is a symbol,and deparse.level = 2 will deparse the argument.Only when exclude is specified (i.e., not by default) andnon-empty, will table potentially drop levels of factorarguments.useNA controls if the table includes counts of NAvalues: the allowed values correspond to never ("no"), only if the count ispositive ("ifany") and even for zero counts ("always").Note the somewhat “pathological” case of two different kinds ofNAs which are treated differently, depending on bothuseNA and exclude, see d.patho in the‘Examples:’ below.Both exclude and useNA operate on an “all or none”basis.  If you want to control the dimensions of a multiway tableseparately, modify each argument using factor oraddNA.Non-factor arguments a are coerced via factor(a,    exclude=exclude).  Since R 3.4.0, care is taken not tocount the excluded values (where they were included in the NAcount, previously).The summary method for class "table" (used for objectscreated by table or xtabs) which gives basicinformation and performs a chi-squared test for independence offactors (note that the function chisq.test currentlyonly handles 2-d tables).
See the description of options("warn") for thecircumstances under which there is a last.warning object andwarnings() is used.  In essence this is if options(warn =    0) and warning has been called at least once.Note that the length(last.warning) is maximallygetOption("nwarnings") (at the time the warnings aregenerated) which is 50 by default.  To increase, use somethinglike It is possible that last.warning refers to the last recordedwarning and not to the last warning, for example if options(warn) hasbeen changed or if a catastrophic error occurred.
message is used for generating ‘simple’ diagnosticmessages which are neither warnings nor errors, but neverthelessrepresented as conditions.  Unlike warnings and errors, a finalnewline is regarded as part of the message, and is optional.The default handler sends the message to thestderr() connection.If a condition object is supplied  to message it should bethe only argument, and further arguments will be ignored, with a warning.While the message is being processed, a muffleMessage restartis available.suppressMessages evaluates its expression in a context thatignores all ‘simple’ diagnostic messages.packageStartupMessage is a variant whose messages can besuppressed separately by suppressPackageStartupMessages.  (Theyare still messages, so can be suppressed by suppressMessages.).makeMessage is a utility used by message, warningand stop to generate a text message from the ...arguments by possible translation (see gettext) andconcatenation (with no separator).
message is used for generating ‘simple’ diagnosticmessages which are neither warnings nor errors, but neverthelessrepresented as conditions.  Unlike warnings and errors, a finalnewline is regarded as part of the message, and is optional.The default handler sends the message to thestderr() connection.If a condition object is supplied  to message it should bethe only argument, and further arguments will be ignored, with a warning.While the message is being processed, a muffleMessage restartis available.suppressMessages evaluates its expression in a context thatignores all ‘simple’ diagnostic messages.packageStartupMessage is a variant whose messages can besuppressed separately by suppressPackageStartupMessages.  (Theyare still messages, so can be suppressed by suppressMessages.).makeMessage is a utility used by message, warningand stop to generate a text message from the ...arguments by possible translation (see gettext) andconcatenation (with no separator).
The result depends on the value ofoptions("warn") and on handlers established in theexecuting code.If a condition object is supplied it should be the only argument, andfurther arguments will be ignored, with a message.warning signals a warning condition by (effectively) callingsignalCondition.  If there are no handlers or if all handlersreturn, then the value of warn = getOption("warn") isused to determine the appropriate action.  If warn is negativewarnings are ignored; if it is zero they are stored and printed afterthe top–level function has completed; if it is one they are printedas they occur and if it is 2 (or larger) warnings are turned intoerrors. Calling warning(immediate. = TRUE) turns warn <=    0 into warn = 1 for this call only.If warn is zero (the default), a read-only variablelast.warning is created.  It contains the warnings which can beprinted via a call to warnings.Warnings will be truncated to getOption("warning.length")characters, default 1000, indicated by [... truncated].While the warning is being processed, a muffleWarning restartis available.  If this restart is invoked with invokeRestart,then warning returns immediately.An attempt is made to coerce other types of inputs to warningto character vectors.suppressWarnings evaluates its expression in a context thatignores all warnings.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
The singular value decomposition plays an important role in manystatistical techniques.  svd and La.svd provide twointerfaces which differ in their return values.Computing the singular vectors is the slow part for large matrices.The computation will be more efficient if both nu <= min(n, p)and nv <= min(n, p), and even more so if both are zero.Unsuccessful results from the underlying LAPACK code will result in anerror giving a positive error code (most often 1): these canonly be interpreted by detailed study of the FORTRAN code but meanthat the algorithm failed to converge.
FUN is found by a call to match.fun.  As in thedefault, binary operators can be supplied if quoted or backquoted.FUN should be a function of two arguments: it will be calledwith arguments x and an array of the same dimensions generatedfrom STATS by aperm.The consistency check among STATS, MARGIN and xis stricter if STATS is an array than if it is a vector.In the vector case, some kinds of recycling are allowed without awarning.  Use sweep(x, MARGIN, as.array(STATS)) if STATSis a vector and you want to be warned if any recycling occurs.
switch works in two distinct ways depending whether the firstargument evaluates to a character string or a number.If the value of EXPR is not a character string it is coerced tointeger.  Note that this also happens for factors, witha warning, as typically the character level is meant.  If the integeris between 1 and nargs()-1 then the corresponding element of... is evaluated and the result returned: thus if the firstargument is 3 then the fourth argument is evaluated andreturned.If EXPR evaluates to a character string then that string ismatched (exactly) to the names of the elements in ....  Ifthere is a match then that element is evaluated unless it is missing,in which case the next non-missing element is evaluated, so forexample switch("cc", a = 1, cc =, cd =, d = 2) evaluates to2.  If there is more than one match, the first matching elementis used.  In the case of no match, if there is an unnamed element of... its value is returned.  (If there is more than one suchargument an error is signaled.)The first argument is always taken to be EXPR: if it is namedits name must (partially) match.A warning is signaled if no alternatives are provided, as this isusually a coding error.This is implemented as a primitive function that only evaluatesits first argument and one other if one is selected.
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
dir.exists checks that the paths exist (in the same sense asfile.exists) and are directories.dir.create creates the last element of the path, unlessrecursive = TRUE.  Trailing path separators are discarded.The mode will be modified by the umask setting in the same wayas for the system function mkdir.  What modes can be set isOS-dependent, and it is unsafe to assume that more than three octaldigits will be used.  For more details see your OS's documentation on thesystem call mkdir, e.g. man 2 mkdir (and not that onthe command-line utility of that name).One of the idiosyncrasies of Windows is that directory creation mayreport success but create a directory with a different name, forexample dir.create("G.S.") creates ‘"G.S"’.  This isundocumented, and what are the precise circumstances is unknown (andmight depend on the version of Windows).  Also avoid directory nameswith a trailing space.Sys.chmod sets the file permissions of one or more files.It may not be supported on a system (when a warning is issued).See the comments for dir.create for how modes are interpreted.Changing mode on a symbolic link is unlikely to work (nor benecessary).  For more details see your OS's documentation on thesystem call chmod, e.g. man 2 chmod (and not that onthe command-line utility of that name).  Whether this changes thepermission of a symbolic link or its target is OS-dependent (althoughto change the target is more common, and POSIX does not support modesfor symbolic links: BSD-based Unixes do, though).Sys.umask sets the umask and returns the previous value:as a special case mode = NA just returns the current value.It may not be supported (when a warning is issued and "0"is returned).  For more details see your OS's documentation on thesystem call umask, e.g. man 2 umask.How modes are handled depends on the file system, even on Unix-alikes(although their documentation is often written assuming a POSIX filesystem).  So treat documentation cautiously if you are using, say, aFAT/FAT32 or network-mounted file system.See files for how file paths with marked encodings are interpreted.  
Sys.time returns an absolute date-time value which can beconverted to various time zones and may return different days.Sys.Date returns the current day in the current time zone.
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
Both arguments will be coerced to character if necessary.Setting unset = NA will enable unset variables and those set tothe value "" to be distinguished, if the OS does.  POSIXrequires the OS to distinguish, and all known current R platforms do.
The locale describes aspects of the internationalization of a program.Initially most aspects of the locale of R are set to "C"(which is the default for the C language and reflects North-Americanusage – also known as "POSIX").  R sets "LC_CTYPE" and"LC_COLLATE", which allow the use of a different character setand alphabetic comparisons in that character set (including the use ofsort), "LC_MONETARY" (for use bySys.localeconv) and "LC_TIME" may affect thebehaviour of as.POSIXlt and strptime andfunctions which use them (but not date).The first seven categories described here are those specified byPOSIX.  "LC_MESSAGES" will be "C" on systems that do notsupport message translation, and is not supported on Windows.  Tryingto use an unsupported category is an error for Sys.setlocale.Note that setting category "LC_ALL" sets only categories"LC_COLLATE", "LC_CTYPE", "LC_MONETARY" and"LC_TIME".Attempts to set an invalid locale are ignored.  There may or may notbe a warning, depending on the OS.Attempts to change the character set (bySys.setlocale("LC_CTYPE", ), if that implies a differentcharacter set) during a session may not work and are likely to lead tosome confusion.Note that the LANGUAGE environment variable has precedence over"LC_MESSAGES" in selecting the language for message translationon most R platforms.On platforms where ICU is used for collation the locale used forcollation can be reset by icuSetCollate.  Except onWindows, the initial setting is taken from the "LC_COLLATE"category, and it is reset when this is changed by a call toSys.setlocale.
NA
This expands tilde (see tilde expansion) and wildcards in file paths. For precise details of wildcards expansion, see yoursystem's documentation on the glob system call.  There is aPOSIX 1003.2 standard (seehttps://pubs.opengroup.org/onlinepubs/9699919799/functions/glob.html)but some OSes will go beyond this.All systems should interpret * (match zero or more characters),? (match a single character) and (probably) [ (begin acharacter class or range).  The handling of pathsending with a separator is system-dependent.  On a POSIX-2008compliant OS they will match directories (only), but as they are notvalid filepaths on Windows, they match nothing there.  (Earlier POSIXstandards allowed them to match files.)The rest of these details are indicative (and based on the POSIXstandard).If a filename starts with . this may need to be matchedexplicitly: for example Sys.glob("*.RData") may or may notmatch ‘.RData’ but will not usually match ‘.aa.RData’.  Notethat this is platform-dependent: e.g. on SolarisSys.glob("*.*") matches ‘.’ and ‘..’.[ begins a character class.  If the first character in[...] is not !, this is a character class which matchesa single character against any of the characters specified.  The classcannot be empty, so ] can be included provided it is first.  Ifthe first character is !, the character class matches a singlecharacter which is none of the specified characters.  Whether. in a character class matches a leading . in thefilename is OS-dependent.Character classes can include ranges such as [A-Z]: include- as a character by having it first or last in a class.  (Theinterpretation of ranges should be locale-specific, so the example isnot a good idea in an Estonian locale.)One can remove the special meaning of ?, *  and[ by preceding them by a backslash (except within acharacter class).
This uses POSIX or Windows system calls.  Note that OS names (sysname) might notbe what you expect: for example macOS identifies itself asDarwin and Solaris as SunOS.Sys.info() returns details of the platform R is running on,whereas R.version gives details of the platform R wasbuilt on: the release and version may well be different.
The functions .subset and .subset2 are essentiallyequivalent to the [ and [[ operators,except that methods dispatch does not take place.  This is to avoidexpensive unclassing when applying the default method to an object.  Theyshould not normally be invoked by end users.  Note that unlike theoperators they are builtins and not specials (all arguments areevaluated) and hence do not allow missing arguments..getRequiredPackages2 attaches all the packages mentioned in theDepends field: failure to find a package is an error.  It alsochecks the versions of the packages found against the Depends field..getRequiredPackages is a wrapper to.getRequiredPackages2 using a ‘DESCRIPTION’ file.The function .isMethodsDispatchOn() returns TRUE ifthe S4 method dispatch has been turned on in the evaluator (usually byloading package methods).  It is meant for R internal use only.sys.save.image is a system function that is called by q()and its GUI analogs; sys.load.image is called by the startup code.These functions should not be called directly and are subject to change.sys.save.image closes all connections first, to ensure that itis able to open a connection to save the image.  This is appropriatewhen called from q() and allies, but reinforces the warningthat it should not be called directly.row.names can be stored internally in compact form..set_row_names(n) generates that form for automatic row namesof length n, to be assigned toattr(<a data frame>, "row.names").  .row_names_infogives information on the internal form of the row names for a dataframe: for details of what information see the argument type..GenericArgsEnv and .ArgsEnv are environments thatcontain closures with the argument lists that the primitives wouldhave had had they been closures.  All the primitives that are internalS3 generics have corresponding members of .GenericArgsEnv andthe remaining non-language-element primitives correspond to.ArgsEnv.  See the ‘R Internals’ manual for furtherdetails.  They are used by args andprint.default and the QC functions codocand checkS3methods.findPackageEnv is invoked by the unserialize code to set asaved environment if possible..TAOCP1997init is the initialization code for the"Knuth-TAOCP" RNG..gt and .gtn are callbacks from rank andis.unsorted used for (S3 or S4) classed objects..primTrace and .primUntrace are the primitivefunctions underlying trace and untracerespectively..Date, .POSIXct, .POSIXlt and .difftimeare class generators..cache_class caches the inheritance of an S4 class for use inS3 method dispatch.  With NULL second argument it returns thecached inheritance, for diagnostic use..popath is a variable created at startup which records wherethe translations package in use is..detach is a ‘bare-bones’ version of detach foruse in other R packages..maskedMsg is a utility called both from attach()and library() for consistency to produce the warning message.Objects starting .C_ and .F_ are references toregistered C and Fortran entry points.Only on Windows:.fixupGFortranStdout and .fixupGFortranStderr are helper functionsto enable standard output and standard error units in gfortran whenexecuting external code via system and system2. By default, theseunits are disabled by the Windows profile when running inside RGui.
Normally R is run without looking at the value of LC_NUMERIC,so the decimal point remains '.'.  So the first three of thesecomponents will only be useful if you have set the locale categoryLC_NUMERIC using Sys.setlocale in the current R session(when R may not work correctly).The monetary components will only be set to non-default values (seethe ‘Examples’ section) if the LC_MONETARY category isset.  It often is not set: set the examples for how to trigger setting it.
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
NA
The functions .subset and .subset2 are essentiallyequivalent to the [ and [[ operators,except that methods dispatch does not take place.  This is to avoidexpensive unclassing when applying the default method to an object.  Theyshould not normally be invoked by end users.  Note that unlike theoperators they are builtins and not specials (all arguments areevaluated) and hence do not allow missing arguments..getRequiredPackages2 attaches all the packages mentioned in theDepends field: failure to find a package is an error.  It alsochecks the versions of the packages found against the Depends field..getRequiredPackages is a wrapper to.getRequiredPackages2 using a ‘DESCRIPTION’ file.The function .isMethodsDispatchOn() returns TRUE ifthe S4 method dispatch has been turned on in the evaluator (usually byloading package methods).  It is meant for R internal use only.sys.save.image is a system function that is called by q()and its GUI analogs; sys.load.image is called by the startup code.These functions should not be called directly and are subject to change.sys.save.image closes all connections first, to ensure that itis able to open a connection to save the image.  This is appropriatewhen called from q() and allies, but reinforces the warningthat it should not be called directly.row.names can be stored internally in compact form..set_row_names(n) generates that form for automatic row namesof length n, to be assigned toattr(<a data frame>, "row.names").  .row_names_infogives information on the internal form of the row names for a dataframe: for details of what information see the argument type..GenericArgsEnv and .ArgsEnv are environments thatcontain closures with the argument lists that the primitives wouldhave had had they been closures.  All the primitives that are internalS3 generics have corresponding members of .GenericArgsEnv andthe remaining non-language-element primitives correspond to.ArgsEnv.  See the ‘R Internals’ manual for furtherdetails.  They are used by args andprint.default and the QC functions codocand checkS3methods.findPackageEnv is invoked by the unserialize code to set asaved environment if possible..TAOCP1997init is the initialization code for the"Knuth-TAOCP" RNG..gt and .gtn are callbacks from rank andis.unsorted used for (S3 or S4) classed objects..primTrace and .primUntrace are the primitivefunctions underlying trace and untracerespectively..Date, .POSIXct, .POSIXlt and .difftimeare class generators..cache_class caches the inheritance of an S4 class for use inS3 method dispatch.  With NULL second argument it returns thecached inheritance, for diagnostic use..popath is a variable created at startup which records wherethe translations package in use is..detach is a ‘bare-bones’ version of detach foruse in other R packages..maskedMsg is a utility called both from attach()and library() for consistency to produce the warning message.Objects starting .C_ and .F_ are references toregistered C and Fortran entry points.Only on Windows:.fixupGFortranStdout and .fixupGFortranStderr are helper functionsto enable standard output and standard error units in gfortran whenexecuting external code via system and system2. By default, theseunits are disabled by the Windows profile when running inside RGui.
Non-standard R names must be quoted in Sys.setenv: see theexamples.  Most platforms (and POSIX) do not allow names containing"=".  Windows does, but the facilities provided by R may nothandle these correctly so they should be avoided.  Most platformsallow setting an environment variable to "", but Windows doesnot and there Sys.setenv(FOO = "") unsets FOO.There may be system-specific limits on the maximum length of thevalues of individual environment variables or of names+values of allenvironment variables.Recent versions of Windows have a maximum length of 32,767 characters for aenvironment variable; however cmd.exe has a limit of 8192characters for a command line, hence set can only set 8188.
This attempts sets the file time to the value specified.On a Unix-alike it uses the system call utimensat if that isavailable, otherwise utimes or utime.  On a POSIX filesystem it sets both the last-access and modification times.Fractional seconds will set as from R 3.4.0 on OSes with therequisite system calls and suitable filesystems.On Windows it uses the system call SetFileTime to set the‘last write time’.  Some Windows file systems only record thetime at a resolution of two seconds.Sys.setFileTime has been vectorized in R 3.6.0.  Earlier versionsof R required path and time to be vectors of length one.
The locale describes aspects of the internationalization of a program.Initially most aspects of the locale of R are set to "C"(which is the default for the C language and reflects North-Americanusage – also known as "POSIX").  R sets "LC_CTYPE" and"LC_COLLATE", which allow the use of a different character setand alphabetic comparisons in that character set (including the use ofsort), "LC_MONETARY" (for use bySys.localeconv) and "LC_TIME" may affect thebehaviour of as.POSIXlt and strptime andfunctions which use them (but not date).The first seven categories described here are those specified byPOSIX.  "LC_MESSAGES" will be "C" on systems that do notsupport message translation, and is not supported on Windows.  Tryingto use an unsupported category is an error for Sys.setlocale.Note that setting category "LC_ALL" sets only categories"LC_COLLATE", "LC_CTYPE", "LC_MONETARY" and"LC_TIME".Attempts to set an invalid locale are ignored.  There may or may notbe a warning, depending on the OS.Attempts to change the character set (bySys.setlocale("LC_CTYPE", ), if that implies a differentcharacter set) during a session may not work and are likely to lead tosome confusion.Note that the LANGUAGE environment variable has precedence over"LC_MESSAGES" in selecting the language for message translationon most R platforms.On platforms where ICU is used for collation the locale used forcollation can be reset by icuSetCollate.  Except onWindows, the initial setting is taken from the "LC_COLLATE"category, and it is reset when this is changed by a call toSys.setlocale.
Using this function allows R to temporarily be given very lowpriority and hence not to interfere with more important foregroundtasks.  A typical use is to allow a process launched from R to setitself up and read its input files before R execution is resumed.The intention is that this function suspends execution of Rexpressions but wakes the process up often enough to respond to GUIevents, typically every half second.  It can be interrupted(e.g. by Ctrl-C or Esc at the R console).There is no guarantee that the process will sleep for the whole of thespecified interval (sleep might be interrupted), and it may well takeslightly longer in real time to resume execution.time must be non-negative (and not NA nor NaN):Inf is allowed (and might be appropriate if the intention is towait indefinitely for an interrupt).  The resolution of the timeinterval is system-dependent, but will normally be 20ms or better.(On modern Unix-alikes it will be better than 1ms.)
For large files, keep.source = FALSE may save quite a bit ofmemory. Disabling only parse data via keep.parse.data = FALSEcan already save a lot.In order for the code being evaluated to use the correct environment(for example, in global assignments), source code in packages shouldcall topenv(), which will return the namespace, if any,the environment set up by sys.source, or the global environmentif a saved image is being used.
.GlobalEnv is given number 0 in the list of frames.Each subsequent function evaluation increases the frame stack by 1.The call, function definition and the environment for evaluationof that function are returned by sys.call, sys.functionand sys.frame with the appropriate index.sys.call, sys.function and sys.frame acceptinteger values for the argument which.  Non-negative values ofwhich are frame numbers starting from .GlobalEnvwhereas negative values are counted back from the frame number of thecurrent evaluation.The parent frame of a function evaluation is the environment in whichthe function was called.  It is not necessarily numbered one less thanthe frame number of the current evaluation, nor is it the environmentwithin which the function was defined.  sys.parent returns thenumber of the parent frame if n is 1 (the default), thegrandparent if n is 2, and so on.  See also the ‘Note’.sys.nframe returns an integer, the number of the current frameas described in the first paragraph.sys.calls and sys.frames give a pairlist of all theactive calls and frames, respectively, and sys.parents returnsan integer vector of indices of the parent frames of each of thoseframes.Notice that even though the sys.xxx functions (exceptsys.status) are interpreted, their contexts are not counted norare they reported.  There is no access to them.sys.status() returns a list with components sys.calls,sys.parents and sys.frames, the results of calls tothose three functions (which will include the call tosys.status: see the first example).sys.on.exit() returns the expression stored for use byon.exit in the function currently being evaluated.(Note that this differs from S, which returns a list of expressionsfor the current frame and its parents.)parent.frame(n) is a convenient shorthand forsys.frame(sys.parent(n)) (implemented slightly more efficiently).
Sys.time returns an absolute date-time value which can beconverted to various time zones and may return different days.Sys.Date returns the current day in the current time zone.
Time zones are a system-specific topic, but these days almost all Rplatforms use similar underlying code, used by Linux, macOS, Solaris,AIX and FreeBSD, and installed with R on Windows.  (Unfortunatelythere are many system-specific errors in the implementations.)  It ispossible to use the R sources' version of the code on Unix-alikes aswell as on Windows: this is the default for macOS and recommended forSolaris.It should be possible to set the current time zone via the environmentvariable TZ: see the section on ‘Time zone names’ forsuitable values.  Sys.timezone() will return the value ofTZ if set initially (and on some OSes it is always set),otherwise it will try to retrieve from the OS a value which if set forTZ would give the initial time zone. (‘Initially’ meansbefore any time-zone functions are used: if TZ is being set tooverride the OS setting or if the ‘try’ does not get thisright, it should be set before the R process is started or (probablyearly enough) in file .Rprofile).If TZ is set but invalid, most platforms default to UTC,the time zone colloquially known as GMT (seehttps://en.wikipedia.org/wiki/Coordinated_Universal_Time).(Some but not all platforms will give a warning for invalid values.)If it is unset or empty the system time zone is used (the onereturned by Sys.timezone).Time zones did not come into use until the middle of the nineteenthcentury and were not widely adopted until the twentieth, anddaylight saving time (DST, also known as summer time)was first introduced in the early twentieth century, most widely in1916.  Over the last 100 years places have changed their affiliationbetween major time zones, have opted out of (or in to) DST in variousyears or adopted DST rule changes late or not at all.  (The UKexperimented with DST throughout 1971, only.)  In a few countries (oneis the Irish Republic) it is the summer time which is the‘standard’ time and a different name is used in winter.  Andthere can be multiple changes during a year, for example for Ramadan.A quite common system implementation of POSIXct is as signed32-bit integers and so only goes back to the end of 1901: on suchsystems R assumes that dates prior to that are in the same time zoneas they were in 1902.  Most of the world had not adopted time zones by1902 (so used local ‘mean time’ based on longitude) but for afew places there had been time-zone changes before then.  64-bitrepresentations are becoming common; unfortunately on some 64-bit OSesthe database information is 32-bit and so only available for the range1901–2038, and incompletely for the end years.As from R 3.5.0, when a time zone location is first found in asession, its value is cached in object .sys.timezone in thebase environment.
dir.exists checks that the paths exist (in the same sense asfile.exists) and are directories.dir.create creates the last element of the path, unlessrecursive = TRUE.  Trailing path separators are discarded.The mode will be modified by the umask setting in the same wayas for the system function mkdir.  What modes can be set isOS-dependent, and it is unsafe to assume that more than three octaldigits will be used.  For more details see your OS's documentation on thesystem call mkdir, e.g. man 2 mkdir (and not that onthe command-line utility of that name).One of the idiosyncrasies of Windows is that directory creation mayreport success but create a directory with a different name, forexample dir.create("G.S.") creates ‘"G.S"’.  This isundocumented, and what are the precise circumstances is unknown (andmight depend on the version of Windows).  Also avoid directory nameswith a trailing space.Sys.chmod sets the file permissions of one or more files.It may not be supported on a system (when a warning is issued).See the comments for dir.create for how modes are interpreted.Changing mode on a symbolic link is unlikely to work (nor benecessary).  For more details see your OS's documentation on thesystem call chmod, e.g. man 2 chmod (and not that onthe command-line utility of that name).  Whether this changes thepermission of a symbolic link or its target is OS-dependent (althoughto change the target is more common, and POSIX does not support modesfor symbolic links: BSD-based Unixes do, though).Sys.umask sets the umask and returns the previous value:as a special case mode = NA just returns the current value.It may not be supported (when a warning is issued and "0"is returned).  For more details see your OS's documentation on thesystem call umask, e.g. man 2 umask.How modes are handled depends on the file system, even on Unix-alikes(although their documentation is often written assuming a POSIX filesystem).  So treat documentation cautiously if you are using, say, aFAT/FAT32 or network-mounted file system.See files for how file paths with marked encodings are interpreted.  
Non-standard R names must be quoted in Sys.setenv: see theexamples.  Most platforms (and POSIX) do not allow names containing"=".  Windows does, but the facilities provided by R may nothandle these correctly so they should be avoided.  Most platformsallow setting an environment variable to "", but Windows doesnot and there Sys.setenv(FOO = "") unsets FOO.There may be system-specific limits on the maximum length of thevalues of individual environment variables or of names+values of allenvironment variables.Recent versions of Windows have a maximum length of 32,767 characters for aenvironment variable; however cmd.exe has a limit of 8192characters for a command line, hence set can only set 8188.
The system command which reports on the full path names ofan executable (including an executable script) as would be executed bya shell, accepting either absolute paths or looking on the path.On Windows an ‘executable’ is a file with extension‘.exe’, ‘.com’, ‘.cmd’ or ‘.bat’.  Such files neednot actually be executable, but they are what systemtries.On a Unix-alike the full path to which (usually‘/usr/bin/which’) is found when R is installed.
This interface has become rather complicated over the years: seesystem2 for a more portable and flexible interfacewhich is recommended for new code.command is parsed as a command plus arguments separated byspaces.  So if the path to the command (or a single argument such as afile path) contains spaces, it must be quoted e.g. byshQuote.Unix-alikes pass the command line to a shell (normally ‘/bin/sh’,and POSIX requires that shell), so command can be anything theshell regards as executable, including shell scripts, and it cancontain multiple commands separated by ;.On Windows, system does not use a shell and there is a separatefunction shell which passes command lines to a shell.If intern is TRUE then popen is used to invoke thecommand and the output collected, line by line, into an Rcharacter vector.  If intern is FALSE thenthe C function system is used to invoke the command.wait is implemented by appending & to the command: thisis in principle shell-dependent, but required by POSIX and so widelysupported.When timeout is non-zero, the command is terminated after the givennumber of seconds.  The termination works for typical commands, but is notguaranteed: it is possible to write a program that would keep runningafter the time is out.  Timeouts can only be set with wait = TRUE.Timeouts cannot be used with interactive commands: the command is run withstandard input redirected from /dev/null and it must not modifyterminal settings.  As long as tty tostop option is disabled, whichit usually is by default, the executed command may write to standardoutput and standard error.  One cannot rely on that the execution time ofthe child processes will be included into user.child andsys.child element of proc_time returned by proc.time. For the time to be included, all child processes have to be waited for bytheir parents, which has to be implemented in the parent applications.The ordering of arguments after the first two has changed from time totime: it is recommended to name all arguments after the first.There are many pitfalls in using system to ascertain if acommand can be run — Sys.which is more suitable.
This checks the existence of the specified files withfile.exists.  So file paths are only returned if thereare sufficient permissions to establish their existence.The unnamed arguments in ... are usually character strings, butif character vectors they are recycled to the same length.This uses find.package to find the package, and hencewith the default lib.loc = NULL looks first for attachedpackages then in each library listed in .libPaths().Note that if a namespace is loaded but the package is not attached,this will look only on .libPaths().
system.time calls the function proc.time,evaluates expr, and then calls proc.time once more,returning the difference between the two proc.time calls.unix.time has been an alias of system.time, forcompatibility with S, and has finally been deprecated in 2016.Timings of evaluations of the same expression can vary considerablydepending on whether the evaluation triggers a garbage collection.  WhengcFirst is TRUE a garbage collection (gc)will be performed immediately before the evaluation of expr.This will usually produce more consistent timings.
Unlike system, command is always quoted byshQuote, so it must be a single command without arguments.For details of how command is found see system.On Windows, env is only supported for commands such asR and make which accept environment variables ontheir command line.Some Unix commands (such as some implementations of ls) changetheir output if they consider it to be piped or redirected:stdout = TRUE uses a pipe whereas stdout =  "some_file_name" uses redirection.Because of the way it is implemented, on a Unix-alike stderr =    TRUE implies stdout = TRUE: a warning is given if this isnot what was specified.When timeout is non-zero, the command is terminated after the givennumber of seconds.  The termination works for typical commands, but is notguaranteed: it is possible to write a program that would keep runningafter the time is out.  Timeouts can only be set with wait = TRUE.Timeouts cannot be used with interactive commands: the command is run withstandard input redirected from /dev/null and it must not modifyterminal settings.  As long as tty tostop option is disabled, whichit usually is by default, the executed command may write to standardoutput and standard error.
This is a generic function for which methods can be written.  Thedescription here applies to the default and "data.frame" methods.A data frame is first coerced to a matrix: see as.matrix.When x is a vector, it is treated as a column, i.e., theresult is a 1-row matrix.
TRUE and FALSE are reserved words denoting logicalconstants in the R language, whereas T and F are globalvariables whose initial values set to these.  All four arelogical(1) vectors.Logical vectors are coerced to integer vectors in contexts where anumerical value is required, with TRUE being mapped to1L, FALSE to 0L and NA to NA_integer_.
This is a generic function for which methods can be written.  Thedescription here applies to the default and "data.frame" methods.A data frame is first coerced to a matrix: see as.matrix.When x is a vector, it is treated as a column, i.e., theresult is a 1-row matrix.
This is a generic function for which methods can be written.  Thedescription here applies to the default and "data.frame" methods.A data frame is first coerced to a matrix: see as.matrix.When x is a vector, it is treated as a column, i.e., theresult is a 1-row matrix.
If the argument dnn is not supplied, the internal functionlist.names is called to compute the ‘dimname names’.  If thearguments in ... are named, those names are used.  For theremaining arguments, deparse.level = 0 gives an empty name,deparse.level = 1 uses the supplied argument if it is a symbol,and deparse.level = 2 will deparse the argument.Only when exclude is specified (i.e., not by default) andnon-empty, will table potentially drop levels of factorarguments.useNA controls if the table includes counts of NAvalues: the allowed values correspond to never ("no"), only if the count ispositive ("ifany") and even for zero counts ("always").Note the somewhat “pathological” case of two different kinds ofNAs which are treated differently, depending on bothuseNA and exclude, see d.patho in the‘Examples:’ below.Both exclude and useNA operate on an “all or none”basis.  If you want to control the dimensions of a multiway tableseparately, modify each argument using factor oraddNA.Non-factor arguments a are coerced via factor(a,    exclude=exclude).  Since R 3.4.0, care is taken not tocount the excluded values (where they were included in the NAcount, previously).The summary method for class "table" (used for objectscreated by table or xtabs) which gives basicinformation and performs a chi-squared test for independence offactors (note that the function chisq.test currentlyonly handles 2-d tables).
tabulate is the workhorse for the table function.If bin is a factor, its internal integer representationis tabulated.If the elements of bin are numeric but not integers,they are truncated by as.integer.
The arc-tangent of two arguments atan2(y, x) returns the anglebetween the x-axis and the vector from the origin to (x, y),i.e., for positive arguments atan2(y, x) == atan(y/x).Angles are in radians, not degrees, for the standard versions (i.e., aright angle is π/2), and in ‘half-rotations’ forcospi etc.cospi(x), sinpi(x), and tanpi(x) are accuratefor x values which are multiples of a half.All except atan2 are internal generic primitivefunctions: methods can be defined for them individually or via theMath group generic.These are all wrappers to system calls of the same name (with prefixc for complex arguments) where available.  (cospi,sinpi, and tanpi are part of a C11 extensionand provided by e.g. macOS and Solaris: where not yetavailable call to cos etc are used, with special casesfor  multiples of a half.)
These are internal generic primitive functions: methodscan be defined for them individually or via theMath group generic.Branch cuts are consistent with the inverse trigonometric functionsasin et seq, and agree with those defined in Abramowitzand Stegun, figure 4.7, page 86.   The behaviour actually on the cutsfollows the C99 standard which requires continuity coming round theendpoint in a counter-clockwise direction.
The arc-tangent of two arguments atan2(y, x) returns the anglebetween the x-axis and the vector from the origin to (x, y),i.e., for positive arguments atan2(y, x) == atan(y/x).Angles are in radians, not degrees, for the standard versions (i.e., aright angle is π/2), and in ‘half-rotations’ forcospi etc.cospi(x), sinpi(x), and tanpi(x) are accuratefor x values which are multiples of a half.All except atan2 are internal generic primitivefunctions: methods can be defined for them individually or via theMath group generic.These are all wrappers to system calls of the same name (with prefixc for complex arguments) where available.  (cospi,sinpi, and tanpi are part of a C11 extensionand provided by e.g. macOS and Solaris: where not yetavailable call to cos etc are used, with special casesfor  multiples of a half.)
If FUN is not NULL, it is passed tomatch.fun, and hence it can be a function or a symbol orcharacter string naming a function.
NA
NA
The length of the result is the maximum of the lengths of the threearguments; values of shorter arguments are recycled.The names are very likely to be unique among calls to tempfilein an R session and across simultaneous R sessions (unlesstmpdir is specified).  The filenames are guaranteed not to becurrently in use.The file name is made by concatenating the path given bytmpdir, the pattern string, a random string in hex anda suffix of fileext.By default, tmpdir will be the directory given bytempdir().  This will be a subdirectory of the per-sessiontemporary directory found by the following rule when the R session isstarted.  The environment variables TMPDIR, TMP andTEMP are checked in turn and the first found which points to awritable directory is used:if none succeeds ‘/tmp’ is used.  The path should not contain spaces.Note that setting any of these environment variables in the R sessionhas no effect on tempdir(): the per-session temporary directoryis created before the interpreter is started.
The length of the result is the maximum of the lengths of the threearguments; values of shorter arguments are recycled.The names are very likely to be unique among calls to tempfilein an R session and across simultaneous R sessions (unlesstmpdir is specified).  The filenames are guaranteed not to becurrently in use.The file name is made by concatenating the path given bytmpdir, the pattern string, a random string in hex anda suffix of fileext.By default, tmpdir will be the directory given bytempdir().  This will be a subdirectory of the per-sessiontemporary directory found by the following rule when the R session isstarted.  The environment variables TMPDIR, TMP andTEMP are checked in turn and the first found which points to awritable directory is used:if none succeeds ‘/tmp’ is used.  The path should not contain spaces.Note that setting any of these environment variables in the R sessionhas no effect on tempdir(): the per-session temporary directoryis created before the interpreter is started.
An input text connection is opened and the character vector is copiedat time the connection object is created, and close destroysthe copy.  object should be the name of a character vector:however, short expressions will be accepted provided they deparse toless than 60 bytes.An output text connection is opened and creates an R character vectorof the given name in the user's workspace or in the calling environment,depending on the value of the local argument.  This object will at alltimes hold the completed lines of output to the connection, andisIncomplete will indicate if there is an incompletefinal line.  Closing the connection will output the final line,complete or not.  (A line is complete once it has been terminated byend-of-line, represented by "\n" in R.)   The output charactervector has locked bindings (see lockBinding) untilclose is called on the connection.  The character vector canalso be retrieved via textConnectionValue, which is theonly way to do so if object = NULL.  If the current locale isdetected as Latin-1 or UTF-8, non-ASCII elements of the character vectorwill be marked accordingly (see Encoding).Opening a text connection with mode = "a" will attempt toappend to an existing character vector with the given name in theuser's workspace or the calling environment.  If none is found (evenif an object exists of the right name but the wrong type) a newcharacter vector will be created, with a warning.You cannot seek on a text connection, and seek willalways return zero as the position.Text connections have slightly unusual semantics: they are alwaysopen, and throwing away an input text connection without closing it(so it get garbage-collected) does not give a warning.
An input text connection is opened and the character vector is copiedat time the connection object is created, and close destroysthe copy.  object should be the name of a character vector:however, short expressions will be accepted provided they deparse toless than 60 bytes.An output text connection is opened and creates an R character vectorof the given name in the user's workspace or in the calling environment,depending on the value of the local argument.  This object will at alltimes hold the completed lines of output to the connection, andisIncomplete will indicate if there is an incompletefinal line.  Closing the connection will output the final line,complete or not.  (A line is complete once it has been terminated byend-of-line, represented by "\n" in R.)   The output charactervector has locked bindings (see lockBinding) untilclose is called on the connection.  The character vector canalso be retrieved via textConnectionValue, which is theonly way to do so if object = NULL.  If the current locale isdetected as Latin-1 or UTF-8, non-ASCII elements of the character vectorwill be marked accordingly (see Encoding).Opening a text connection with mode = "a" will attempt toappend to an existing character vector with the given name in theuser's workspace or the calling environment.  If none is found (evenif an object exists of the right name but the wrong type) a newcharacter vector will be created, with a warning.You cannot seek on a text connection, and seek willalways return zero as the position.Text connections have slightly unusual semantics: they are alwaysopen, and throwing away an input text connection without closing it(so it get garbage-collected) does not give a warning.
chartr translates each character in x that is specifiedin old to the corresponding character specified in new.Ranges are supported in the specifications, but character classes andrepeated characters are not.  If old contains more charactersthan new, an error is signaled; if it contains fewer characters, theextra characters at the end of new are ignored.tolower and toupper convert upper-case characters in acharacter vector to lower-case, or vice versa.  Non-alphabeticcharacters are left unchanged.  More than one character can be mappedto a single upper-case character.casefold is a wrapper for tolower and toupperprovided for compatibility with S-PLUS.
topenv returns the first top level environmentfound when searching envir and its enclosing environments. If notop level environment is found, .GlobalEnv is returned.  Anenvironment is considered top level if it is the internal environmentof a namespace, a package environment in the searchpath, or .GlobalEnv .
This is a generic function for which methods can be written: only thedefault method is described here.  Most methods should honor thewidth argument to specify the maximum display width (as measuredby nchar(type = "width")) of the result.The default method first converts x to character and thenconcatenates the elements separated by ", ".If width is supplied and is not NULL, the default methodreturns the first width - 4 characters of the result with.... appended, if the full result would use more thanwidth characters.
This is a generic function for which methods can be written: only thedefault method is described here.  Most methods should honor thewidth argument to specify the maximum display width (as measuredby nchar(type = "width")) of the result.The default method first converts x to character and thenconcatenates the elements separated by ", ".If width is supplied and is not NULL, the default methodreturns the first width - 4 characters of the result with.... appended, if the full result would use more thanwidth characters.
chartr translates each character in x that is specifiedin old to the corresponding character specified in new.Ranges are supported in the specifications, but character classes andrepeated characters are not.  If old contains more charactersthan new, an error is signaled; if it contains fewer characters, theextra characters at the end of new are ignored.tolower and toupper convert upper-case characters in acharacter vector to lower-case, or vice versa.  Non-alphabeticcharacters are left unchanged.  More than one character can be mappedto a single upper-case character.casefold is a wrapper for tolower and toupperprovided for compatibility with S-PLUS.
The trace function operates by constructing a revised versionof the function (or of the method, if signature is supplied),and assigning the new object back where the original was found.If only the what argument is given, a line of trace printing isproduced for each call to the function (back compatible with theearlier version of trace).The object constructed by trace is from a class that extends"function" and which contains the original, untraced version.A call to untrace re-assigns this version.If the argument tracer or exit is the name of afunction, the tracing expression will be a call to that function, withno arguments.  This is the easiest and most common case, with thefunctions browser and recover thelikeliest candidates; the former browses in the frame of the functionbeing traced, and the latter allows browsing in any of the currentlyactive calls. The arguments tracer and exit are evaluated tosee whether they are functions, but only their names are used in thetracing expressions.  The lookup is done again when the traced functionexecutes, so it may not be tracer or exit that will be calledwhile tracing.The tracer or exit argument can also be an unevaluatedexpression (such as returned by a call to quote orsubstitute).  This expression itself is inserted in thetraced function, so it will typically involve arguments or localobjects in the traced function.  An expression of this form is usefulif you only want to interact when certain conditions apply (and inthis case you probably want to supply print = FALSE in the callto trace also).When the at argument is supplied, it can be a vector ofintegers referring to the substeps of the body of the function (thisonly works if the body of the function is enclosed in { ...}).  Inthis case tracer is not called on entry, but insteadjust before evaluating each of the steps listed in at.  (Hint:you don't want to try to count the steps in the printed version of afunction; instead, look at as.list(body(f)) to get the numbersassociated with the steps in function f.)The at argument can also be a list of integer vectors.  Inthis case, each vector refers to a step nested within another step ofthe function.  For example, at = list(c(3,4))will call the tracer just before the fourth step of the third stepof the function.  See the example below.Using setBreakpoint (from package utils) may be analternative, calling trace(...., at, ...).The exit argument is called during on.exitprocessing.  In an on.exit expression, the experimental returnValue()function may be called to obtain the value about to be returned bythe function. Calling this function in other circumstances will giveundefined results.An intrinsic limitation in the exit argument is that it won'twork if the function itself uses on.exit with add=  FALSE (the default), since the existing calls will override the onesupplied by trace.Tracing does not nest.  Any call to trace replaces previouslytraced versions of that function or method (except for editedversions as discussed below), and untrace alwaysrestores an untraced version.  (Allowing nested tracing has too manypotentials for confusion and for accidentally leaving traced versionsbehind.)When the edit argument is used repeatedly with no call tountrace on the same function or method in between, thepreviously edited version is retained.  If you want to throw awayall the previous tracing and then edit, call untrace before the nextcall to trace.  Editing may be combined with automatictracing; just supply the other arguments such as tracer, andthe edit argument as well.  The edit = TRUE argumentuses the default editor (see edit).Tracing primitive functions (builtins and specials) from the basepackage works, but only by a special mechanism and not veryinformatively.  Tracing a primitive causes the primitive to bereplaced by a function with argument ... (only).  You can get a bitof information out, but not much.  A warning message is issued whentrace is used on a primitive.The practice of saving the traced version of the function back wherethe function came from means that tracing carries over from onesession to another, if the traced function is saved in thesession image.  (In the next session, untrace will remove thetracing.)  On the other hand, functions that were in a package, not inthe global environment, are not saved in the image, so tracing expireswith the session for such functions.Tracing an S4 method is basically just like tracing a function, with theexception that the traced version is stored by a call tosetMethod rather than by direct assignment, and so isthe untraced version after a call to untrace.The version of trace described here is largely compatible withthe version in S-Plus, although the two work by entirely differentmechanisms.  The S-Plus trace uses the session frame, with theresult that tracing never carries over from one session to another (Rdoes not have a session frame).  Another relevant distinction hasnothing directly to do with trace:  The browser in S-Plusallows changes to be made to the frame being browsed, and the changeswill persist after exiting the browser.  The R browser allows changes,but they disappear when the browser exits.  This may be relevant inthat the S-Plus version allows you to experiment with code changesinteractively, but the R version does not.  (A future revision mayinclude a ‘destructive’ browser for R.)
The default display is of the stack of the last uncaught error asstored as a list of calls in .Traceback, whichtraceback prints in a user-friendly format.  The stack ofcalls always contains all function calls and all foreignfunction calls (such as .Call): if profiling is inprogress it will include calls to some primitive functions.  (Callsto builtins are included, but not to specials.)Errors which are caught via try ortryCatch do not generate a traceback, so what is printedis the call sequence for the last uncaught error, and not necessarilyfor the last error.If x is numeric, then the current stack is printed, skippingx entries at the top of the stack.  For example,options(error = function() traceback(3)) will print the stackat the time of the error, skipping the call to traceback() and.traceback()and the error function that called it.Otherwise, x is assumed to be a list or pairlist of calls ordeparsed calls and will be displayed in the same way..traceback() and by extension traceback() may triggerdeparsing of calls.  This is an expensive operationfor large calls so it may be advisable to set max.linesto a reasonable value when such calls are on the call stack.
This functionality is optional, determined at compilation, because itmakes R run a little more slowly even when no objects are beingtraced.  tracemem and untracemem give errors when R is notcompiled with memory profiling; retracemem does not (so it can beleft in code during development).It is enabled in the CRAN macOS and Windows builds of R.When an object is traced any copying of the object by the C functionduplicate produces a message to standard output, as does typecoercion and copying when passing arguments to .C or.Fortran.The message consists of the string tracemem, the identifyingstrings for the object being copied and the new object being created,and a stack trace showing where the duplication occurred.retracemem() is used to indicate that a variable should beconsidered a copy of a previous variable (e.g., after subscripting).The messages can be turned off with tracingState.It is not possible to trace functions, as this would conflict withtrace and it is not useful to trace NULL,environments, promises, weak references, or external pointer objects, asthese are not duplicated.These functions are primitive.
The trace function operates by constructing a revised versionof the function (or of the method, if signature is supplied),and assigning the new object back where the original was found.If only the what argument is given, a line of trace printing isproduced for each call to the function (back compatible with theearlier version of trace).The object constructed by trace is from a class that extends"function" and which contains the original, untraced version.A call to untrace re-assigns this version.If the argument tracer or exit is the name of afunction, the tracing expression will be a call to that function, withno arguments.  This is the easiest and most common case, with thefunctions browser and recover thelikeliest candidates; the former browses in the frame of the functionbeing traced, and the latter allows browsing in any of the currentlyactive calls. The arguments tracer and exit are evaluated tosee whether they are functions, but only their names are used in thetracing expressions.  The lookup is done again when the traced functionexecutes, so it may not be tracer or exit that will be calledwhile tracing.The tracer or exit argument can also be an unevaluatedexpression (such as returned by a call to quote orsubstitute).  This expression itself is inserted in thetraced function, so it will typically involve arguments or localobjects in the traced function.  An expression of this form is usefulif you only want to interact when certain conditions apply (and inthis case you probably want to supply print = FALSE in the callto trace also).When the at argument is supplied, it can be a vector ofintegers referring to the substeps of the body of the function (thisonly works if the body of the function is enclosed in { ...}).  Inthis case tracer is not called on entry, but insteadjust before evaluating each of the steps listed in at.  (Hint:you don't want to try to count the steps in the printed version of afunction; instead, look at as.list(body(f)) to get the numbersassociated with the steps in function f.)The at argument can also be a list of integer vectors.  Inthis case, each vector refers to a step nested within another step ofthe function.  For example, at = list(c(3,4))will call the tracer just before the fourth step of the third stepof the function.  See the example below.Using setBreakpoint (from package utils) may be analternative, calling trace(...., at, ...).The exit argument is called during on.exitprocessing.  In an on.exit expression, the experimental returnValue()function may be called to obtain the value about to be returned bythe function. Calling this function in other circumstances will giveundefined results.An intrinsic limitation in the exit argument is that it won'twork if the function itself uses on.exit with add=  FALSE (the default), since the existing calls will override the onesupplied by trace.Tracing does not nest.  Any call to trace replaces previouslytraced versions of that function or method (except for editedversions as discussed below), and untrace alwaysrestores an untraced version.  (Allowing nested tracing has too manypotentials for confusion and for accidentally leaving traced versionsbehind.)When the edit argument is used repeatedly with no call tountrace on the same function or method in between, thepreviously edited version is retained.  If you want to throw awayall the previous tracing and then edit, call untrace before the nextcall to trace.  Editing may be combined with automatictracing; just supply the other arguments such as tracer, andthe edit argument as well.  The edit = TRUE argumentuses the default editor (see edit).Tracing primitive functions (builtins and specials) from the basepackage works, but only by a special mechanism and not veryinformatively.  Tracing a primitive causes the primitive to bereplaced by a function with argument ... (only).  You can get a bitof information out, but not much.  A warning message is issued whentrace is used on a primitive.The practice of saving the traced version of the function back wherethe function came from means that tracing carries over from onesession to another, if the traced function is saved in thesession image.  (In the next session, untrace will remove thetracing.)  On the other hand, functions that were in a package, not inthe global environment, are not saved in the image, so tracing expireswith the session for such functions.Tracing an S4 method is basically just like tracing a function, with theexception that the traced version is stored by a call tosetMethod rather than by direct assignment, and so isthe untraced version after a call to untrace.The version of trace described here is largely compatible withthe version in S-Plus, although the two work by entirely differentmechanisms.  The S-Plus trace uses the session frame, with theresult that tracing never carries over from one session to another (Rdoes not have a session frame).  Another relevant distinction hasnothing directly to do with trace:  The browser in S-Plusallows changes to be made to the frame being browsed, and the changeswill persist after exiting the browser.  The R browser allows changes,but they disappear when the browser exits.  This may be relevant inthat the S-Plus version allows you to experiment with code changesinteractively, but the R version does not.  (A future revision mayinclude a ‘destructive’ browser for R.)
The ... arguments to transform.data.frame are taggedvector expressions, which are evaluated in the data frame_data.  The tags are matched against names(_data), and forthose that match, the value replace the corresponding variable in_data, and the others are appended to _data.
The ... arguments to transform.data.frame are taggedvector expressions, which are evaluated in the data frame_data.  The tags are matched against names(_data), and forthose that match, the value replace the corresponding variable in_data, and the others are appended to _data.
The ... arguments to transform.data.frame are taggedvector expressions, which are evaluated in the data frame_data.  The tags are matched against names(_data), and forthose that match, the value replace the corresponding variable in_data, and the others are appended to _data.
The functions beta and lbeta return the beta functionand the natural logarithm of the beta function,B(a,b) = Γ(a)Γ(b)/Γ(a+b).The formal definition isintegral_0^1 t^(a-1) (1-t)^(b-1) dt(Abramowitz and Stegun section 6.2.1, page 258).  Note that it is onlydefined in R for non-negative a and b, and is infiniteif either is zero.The functions gamma and lgamma return the gamma functionΓ(x) and the natural logarithm of the absolute value of thegamma function.  The gamma function is defined by(Abramowitz and Stegun section 6.1.1, page 255)Γ(x) = integral_0^Inf t^(x-1) exp(-t) dtfor all real x except zero and negative integers (whenNaN is returned).  There will be a warning on possible loss ofprecision for values which are too close (within about1e-8) to a negative integer less than -10.factorial(x) (x! for non-negative integer x)is defined to be gamma(x+1) and lfactorial to belgamma(x+1).The functions digamma and trigamma return the first and secondderivatives of the logarithm of the gamma function.psigamma(x, deriv) (deriv >= 0) computes thederiv-th derivative of ψ(x).digamma(x) = ψ(x) = d/dx{ln Γ(x)} = Γ'(x) / Γ(x)ψ and its derivatives, the psigamma() functions, areoften called the ‘polygamma’ functions, e.g. inAbramowitz and Stegun (section 6.4.1, page 260); and higherderivatives (deriv = 2:4) have occasionally been called‘tetragamma’, ‘pentagamma’, and ‘hexagamma’.The functions choose and lchoose return binomialcoefficients and the logarithms of their absolute values.  Note thatchoose(n, k) is defined for all real numbers n and integerk.  For k ≥ 1 it is defined asn(n-1)…(n-k+1) / k!,as 1 for k = 0 and as 0 for negative k.Non-integer values of k are rounded to an integer, with a warning.choose(*, k) uses direct arithmetic (instead of[l]gamma calls) for small k, for speed and accuracyreasons.  Note the function combn (packageutils) for enumeration of all possible combinations.The gamma, lgamma, digamma and trigammafunctions are internal generic primitive functions: methods can bedefined for them individually or via theMath group generic.
Internally, sub(re, "", *, perl = TRUE), i.e., PCRElibrary regular expressions are used.For portability, the default ‘whitespace’ is the character class[ \t\r\n] (space, horizontal tab, carriage return,newline).  Alternatively, [\h\v] is a good (PCRE)generalization to match all Unicode horizontal and vertical whitespace characters, see also https://www.pcre.org.
These are generic functions: methods can be defined for themindividually or via the Math groupgeneric.Note that for rounding off a 5, the IEC 60559 standard (see also‘IEEE 754’) is expected to be used, ‘go to the even digit’.Therefore round(0.5) is 0 and round(-1.5) is-2.  However, this is dependent on OS services and onrepresentation error (since e.g. 0.15 is not representedexactly, the rounding rule applies to the represented number and notto the printed number, and so round(0.15, 1) could be either0.1 or 0.2).Rounding to a negative number of digits means rounding to a power often, so for example round(x, digits = -2) rounds to the nearesthundred.For signif the recognized values of digits are1...22, and non-missing values are rounded to the nearestinteger in that range.  Complex numbers are rounded to retain thespecified number of digits in the larger of the components.  Eachelement of the vector is rounded individually, unlike printing.These are all primitive functions.
The time is rounded or truncated to the second, minute, hour, day,month or year.  Time zones are only relevant to days or more, whenmidnight in the current time zone is used.The methods for class "Date" are of little use except to removefractional days.
The time is rounded or truncated to the second, minute, hour, day,month or year.  Time zones are only relevant to days or more, whenmidnight in the current time zone is used.The methods for class "Date" are of little use except to removefractional days.
seek with where = NA returns the current byte offsetof a connection (from the beginning), and with a non-missing whereargument the connection is re-positioned (if possible) to thespecified position.  isSeekable returns whether the connectionin principle supports seek: currently only (possiblygz-compressed) file connections do.where is stored as a real but should represent an integer:non-integer values are likely to be truncated.  Note that the possiblevalues can exceed the largest representable number in an Rinteger on 64-bit builds, and on some 32-bit builds.File connections can be open for both writing/appending, in which caseR keeps separate positions for reading and writing.  Which seekrefers to can be set by its rw argument: the default is thelast mode (reading or writing) which was used.  Most files areonly opened for reading or writing and so default to that state.  If afile is open for both reading and writing but has not been used, thedefault is to give the reading position (0).The initial file position for reading is always at the beginning.The initial position for writing is at the beginning of the filefor modes "r+" and "r+b", otherwise at the end of thefile.  Some platforms only allow writing at the end of the file inthe append modes.  (The reported write position for a file opened inan append mode will typically be unreliable until the file has beenwritten to.)gzfile connections support seek with a number oflimitations, using the file position of the uncompressed file.They do not support origin = "end".  When writing, seeking isonly possible forwards: when reading seeking backwards is supported byrewinding the file and re-reading from its start.If seek is called with a non-NA value of where,any pushback on a text-mode connection is discarded.truncate truncates a file opened for writing at its currentposition.  It works only for file connections, and is notimplemented on all platforms: on others (including Windows) it willnot work for large (> 2Gb) files.None of these should be expected to work on text-mode connections withre-encoding selected.
seek with where = NA returns the current byte offsetof a connection (from the beginning), and with a non-missing whereargument the connection is re-positioned (if possible) to thespecified position.  isSeekable returns whether the connectionin principle supports seek: currently only (possiblygz-compressed) file connections do.where is stored as a real but should represent an integer:non-integer values are likely to be truncated.  Note that the possiblevalues can exceed the largest representable number in an Rinteger on 64-bit builds, and on some 32-bit builds.File connections can be open for both writing/appending, in which caseR keeps separate positions for reading and writing.  Which seekrefers to can be set by its rw argument: the default is thelast mode (reading or writing) which was used.  Most files areonly opened for reading or writing and so default to that state.  If afile is open for both reading and writing but has not been used, thedefault is to give the reading position (0).The initial file position for reading is always at the beginning.The initial position for writing is at the beginning of the filefor modes "r+" and "r+b", otherwise at the end of thefile.  Some platforms only allow writing at the end of the file inthe append modes.  (The reported write position for a file opened inan append mode will typically be unreliable until the file has beenwritten to.)gzfile connections support seek with a number oflimitations, using the file position of the uncompressed file.They do not support origin = "end".  When writing, seeking isonly possible forwards: when reading seeking backwards is supported byrewinding the file and re-reading from its start.If seek is called with a non-NA value of where,any pushback on a text-mode connection is discarded.truncate truncates a file opened for writing at its currentposition.  It works only for file connections, and is notimplemented on all platforms: on others (including Windows) it willnot work for large (> 2Gb) files.None of these should be expected to work on text-mode connections withre-encoding selected.
try evaluates an expression and traps any errors that occurduring the evaluation.  If an error occurs then the errormessage is printed to the stderr connection unlessoptions("show.error.messages") is false orthe call includes silent = TRUE.  The error message is alsostored in a buffer where it can be retrieved bygeterrmessage. (This should not be needed as the value returnedin case of an error contains the error message.)try is implemented using tryCatch; forprogramming, instead of try(expr, silent = TRUE), something liketryCatch(expr, error = function(e) e) (or other simpleerror handler functions) may be more efficient and flexible.It may be useful to set the default for outFile tostdout(), i.e., instead of the default stderr(),notably when try() is used inside a Sweave codechunk and the error message should appear in the resulting document.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
NA
Here, we describe the so called “S3” classes (and methods). For“S4” classes (and methods), see ‘Formal classes’ below.Many R objects have a class attribute, a character vectorgiving the names of the classes from which the object inherits.(Functions oldClass and oldClass<- get and set theattribute, which can also be done directly.)If the object does not have a class attribute, it has an implicitclass, notably "matrix", "array", "function" or"numeric" or the result oftypeof(x) (which is similar to mode(x)),but for type "language" and mode "call",where the following extra classes exist for the corresponding functioncalls:if, while, for, =, <-, (,{, call.Note that for objects x of an implicit (or an S4) class, when a(S3) generic function foo(x) is called, method dispatch may usemore classes than are returned by class(x), e.g., for a numericmatrix, the foo.numeric() method may apply.  The exact fullcharacter vector of the classes whichUseMethod() uses, is available as .class2(x) sinceR version 4.0.0.  (This also applies to S4 objects when S3 dispatch isconsidered, see below.)Beware that using .class2() for other reasons than didactical,diagnostical or for debugging may rather be a misuse than smart.NULL objects (of implicit class "NULL") cannot haveattributes (hence no class attribute) and attempting to assign aclass is an error.When a generic function fun is applied to an object with classattribute c("first", "second"), the system searches for afunction called fun.first and, if it finds it, applies it tothe object.  If no such function is found, a function calledfun.second is tried.  If no class name produces a suitablefunction, the function fun.default is used (if it exists).  Ifthere is no class attribute, the implicit class is tried, then thedefault method.The function class prints the vector of names of classes anobject inherits from.  Correspondingly, class<- sets theclasses an object inherits from.  Assigning an empty character vector orNULL removes the class attribute, as for oldClass<- ordirect attribute setting.  Whereas it is clearer to explicitly assignNULL to remove the class, using an empty vector is more natural ine.g., class(x) <- setdiff(class(x), "ts").unclass returns (a copy of) its argument with its classattribute removed.  (It is not allowed for objects which cannot becopied, namely environments and external pointers.)inherits indicates whether its first argument inherits from anyof the classes specified in the what argument.  If whichis TRUE then an integer vector of the same length aswhat is returned.  Each element indicates the position in theclass(x) matched by the element of what; zero indicatesno match. If which is FALSE then TRUE isreturned by inherits if any of the names in what matchwith any class.isa tests whether x is an object of class(es) as givenin what by using is if x is an S4object, and otherwise giving TRUE iff all elements ofclass(x) are contained in what.All but inherits and isa are primitive functions.
When a function flagged for debugging is entered, normal executionis suspended and the body of function is executed one statement at atime.  A new browser context is initiated for each step(and the previous one destroyed).At the debug prompt the user can enter commands or R expressions,followed by a newline.  The commands are described in thebrowser help topic.To debug a function which is defined inside another function,single-step through to the end of its definition, and then calldebug on its name.If you want to debug a function not starting at the very beginning,use trace(..., at = *) or setBreakpoint.Using debug is persistent, and unless debugging is turned offthe debugger will be entered on every invocation (note that if thefunction is removed and replaced the debug state is not preserved).Use debugonce() to enter the debugger only the next time thefunction is invoked.To debug an S4 method by explicit signature, usesignature. When specified, signature indicates the method offun to be debugged. Note that debugging is implemented slightlydifferently for this case, as it uses the trace machinery, rather thanthe debugging bit. As such, text and condition cannot bespecified in combination with a non-null signature. For methodswhich implement the .local rematching mechanism, the.local closure itself is the one that will be ultimatelydebugged (see isRematched).isdebugged returns TRUE if a) signature is NULLand the closure fun has been debugged, or b) signature is notNULL, fun is an S4 generic, and the method of funfor that signature has been debugged. In all other cases, it returnsFALSE.The number of lines printed for the deparsed call when a function isentered for debugging can be limited by settingoptions(deparse.max.lines).When debugging is enabled on a byte compiled function then theinterpreted version of the function will be used until debugging isdisabled.
Each of union, intersect, setdiff andsetequal will discard any duplicated values in the arguments,and they apply as.vector to their arguments (and soin particular coerce factors to character vectors).is.element(x, y) is identical to x %in% y.
This is a generic function with methods for vectors, data frames andarrays (including matrices).The array method calculates for each element of the dimensionspecified by MARGIN if the remaining dimensions are identicalto those for an earlier element (in row-major order).  This would mostcommonly be used for matrices to find unique rows (the default) or columns(with MARGIN = 2).Note that unlike the Unix command uniq this omitsduplicated and not just repeated elements/rows.  Thatis, an element is omitted if it is equal to any previous element andnot just if it is equal the immediately previous one.  (For thelatter, see rle).Missing values ("NA") are regarded as equal, numeric andcomplex ones differing from NaN; character strings will be compared in a“common encoding”; for details, see match (andduplicated) which use the same concept.Values in incomparables will never be marked as duplicated.This is intended to be used for a fairly small set of values and willnot be efficient for a very large set.When used on a data frame with more than one column, or an array ormatrix when comparing dimensions of length greater than one, thistests for identity of character representations.  This willcatch people who unwisely rely on exact equality of floating-pointnumbers!
This is a generic function with methods for vectors, data frames andarrays (including matrices).The array method calculates for each element of the dimensionspecified by MARGIN if the remaining dimensions are identicalto those for an earlier element (in row-major order).  This would mostcommonly be used for matrices to find unique rows (the default) or columns(with MARGIN = 2).Note that unlike the Unix command uniq this omitsduplicated and not just repeated elements/rows.  Thatis, an element is omitted if it is equal to any previous element andnot just if it is equal the immediately previous one.  (For thelatter, see rle).Missing values ("NA") are regarded as equal, numeric andcomplex ones differing from NaN; character strings will be compared in a“common encoding”; for details, see match (andduplicated) which use the same concept.Values in incomparables will never be marked as duplicated.This is intended to be used for a fairly small set of values and willnot be efficient for a very large set.When used on a data frame with more than one column, or an array ormatrix when comparing dimensions of length greater than one, thistests for identity of character representations.  This willcatch people who unwisely rely on exact equality of floating-pointnumbers!
This is a generic function with methods for vectors, data frames andarrays (including matrices).The array method calculates for each element of the dimensionspecified by MARGIN if the remaining dimensions are identicalto those for an earlier element (in row-major order).  This would mostcommonly be used for matrices to find unique rows (the default) or columns(with MARGIN = 2).Note that unlike the Unix command uniq this omitsduplicated and not just repeated elements/rows.  Thatis, an element is omitted if it is equal to any previous element andnot just if it is equal the immediately previous one.  (For thelatter, see rle).Missing values ("NA") are regarded as equal, numeric andcomplex ones differing from NaN; character strings will be compared in a“common encoding”; for details, see match (andduplicated) which use the same concept.Values in incomparables will never be marked as duplicated.This is intended to be used for a fairly small set of values and willnot be efficient for a very large set.When used on a data frame with more than one column, or an array ormatrix when comparing dimensions of length greater than one, thistests for identity of character representations.  This willcatch people who unwisely rely on exact equality of floating-pointnumbers!
This is a generic function with methods for vectors, data frames andarrays (including matrices).The array method calculates for each element of the dimensionspecified by MARGIN if the remaining dimensions are identicalto those for an earlier element (in row-major order).  This would mostcommonly be used for matrices to find unique rows (the default) or columns(with MARGIN = 2).Note that unlike the Unix command uniq this omitsduplicated and not just repeated elements/rows.  Thatis, an element is omitted if it is equal to any previous element andnot just if it is equal the immediately previous one.  (For thelatter, see rle).Missing values ("NA") are regarded as equal, numeric andcomplex ones differing from NaN; character strings will be compared in a“common encoding”; for details, see match (andduplicated) which use the same concept.Values in incomparables will never be marked as duplicated.This is intended to be used for a fairly small set of values and willnot be efficient for a very large set.When used on a data frame with more than one column, or an array ormatrix when comparing dimensions of length greater than one, thistests for identity of character representations.  This willcatch people who unwisely rely on exact equality of floating-pointnumbers!
This is a generic function with methods for vectors, data frames andarrays (including matrices).The array method calculates for each element of the dimensionspecified by MARGIN if the remaining dimensions are identicalto those for an earlier element (in row-major order).  This would mostcommonly be used for matrices to find unique rows (the default) or columns(with MARGIN = 2).Note that unlike the Unix command uniq this omitsduplicated and not just repeated elements/rows.  Thatis, an element is omitted if it is equal to any previous element andnot just if it is equal the immediately previous one.  (For thelatter, see rle).Missing values ("NA") are regarded as equal, numeric andcomplex ones differing from NaN; character strings will be compared in a“common encoding”; for details, see match (andduplicated) which use the same concept.Values in incomparables will never be marked as duplicated.This is intended to be used for a fairly small set of values and willnot be efficient for a very large set.When used on a data frame with more than one column, or an array ormatrix when comparing dimensions of length greater than one, thistests for identity of character representations.  This willcatch people who unwisely rely on exact equality of floating-pointnumbers!
Numeric versions are sequences of one or more non-negative integers,usually (e.g., in package ‘DESCRIPTION’ files) represented ascharacter strings with the elements of the sequence concatenated andseparated by single . or - characters.  R packageversions consist of at least two such integers, an R system versionof exactly three (major, minor and patchlevel).Functions numeric_version, package_version andR_system_version create a representation from such strings (ifsuitable) which allows for coercion and testing, combination,comparison, summaries (min/max), inclusion in data frames,subscripting, and printing.  The classes can hold a vector of suchrepresentations.getRversion returns the version of the running R as an Rsystem version object.The [[ operator extracts or replaces a single version.  Toaccess the integers of a version use two indices: see the examples.
There are two basic classes of date/times.  Class "POSIXct"represents the (signed) number of seconds since the beginning of 1970(in the UTC time zone) as a numeric vector.  Class "POSIXlt" isa named list of vectors representing0–61: seconds.0–59: minutes.0–23: hours.1–31: day of the month0–11: months after the first of the year.years since 1900.0–6 day of the week, starting on Sunday.0–365: day of the year (365 only in leap years).Daylight Saving Time flag.  Positive if inforce, zero if not, negative if unknown.(Optional.) The abbreviation for the time zone inforce at that time: "" if unknown (but "" might alsobe used for UTC).(Optional.) The offset in seconds from GMT:positive values are East of the meridian.  Usually NA ifunknown, but 0 could mean unknown.(The last two components are not present for times in UTC and areplatform-dependent: they are supported on platforms based on BSD orglibc (including Linux and macOS) and those using thetzcode implementation shipped with R (including Windows). Butthey are not necessarily set.).  Note that the internal list structureis somewhat hidden, as many methods (includinglength(x), print() and str)apply to the abstract date-time vector, as for "POSIXct".  Asfrom R 3.5.0, one can extract and replace single componentsvia [ indexing with two indices (see the examples).  Theclasses correspond to the POSIX/C99 constructs of ‘calendartime’ (the time_t data type) and ‘local time’ (orbroken-down time, the struct tm data type), from which theyalso inherit their names.  The components of "POSIXlt" areinteger vectors, except sec and zone."POSIXct" is more convenient for including in data frames, and"POSIXlt" is closer to human-readable forms.  A virtual class"POSIXt" exists from which both of the classes inherit: it isused to allow operations such as subtraction to mix the two classes.Components wday and yday of "POSIXlt" are forinformation, and are not used in the conversion to calendar time.However, isdst is needed to distinguish times at the end ofDST: typically 1am to 2am occurs twice, first in DST and then instandard time.  At all other times isdst can be deduced fromthe first six values, but the behaviour if it is set incorrectly isplatform-dependent.Logical comparisons and some arithmetic operations are available forboth classes.  One can add or subtract a number of seconds from adate-time object, but not add two date-time objects.  Subtraction oftwo date-time objects is equivalent to using difftime.Be aware that "POSIXlt" objects will be interpreted as being inthe current time zone for these operations unless a time zone has beenspecified."POSIXlt" objects will often have an attribute "tzone",a character vector of length 3 giving the time zone name (from the TZenvironment variable or argument tz of functions creating"POSIXlt" objects; "" marks the current time zone)and the names of the base time zoneand the alternate (daylight-saving) time zone.  Sometimes this mayjust be of length one, giving the time zone name."POSIXct" objects may also have an attribute "tzone", acharacter vector of length one.  If set to a non-empty value, it willdetermine how the object is converted to class "POSIXlt" and inparticular how it is printed.  This is usually desirable, but if youwant to specify an object in a particular time zone but to be printedin the current time zone you may want to remove the "tzone"attribute (e.g., by c(x)).Unfortunately, the conversion is complicated by the operation of timezones and leap seconds (according to this version of R's data,27 days have been 86401 seconds long sofar, the last being on (actually, immediately before)2017-01-01: the times of theextra seconds are in the object .leap.seconds).  The details ofthis are entrusted to the OS services where possible.  It seems thatsome rare systems used to use leap seconds, but all known currentplatforms ignore them (as required by POSIX).  This is detected andcorrected for at build time, so "POSIXct" times used by R donot include leap seconds on any platform.Using c on "POSIXlt" objects converts them to thecurrent time zone, and on "POSIXct" objects drops any"tzone" attributes, unless they are all marked with the sametime zone.A few times have specific issues.  First, the leap seconds are ignored,and real times such as "2005-12-31 23:59:60" are (probably)treated as the next second.  However, they will never be generated byR, and are unlikely to arise as input.  Second, on some OSes there isa problem in the POSIX/C99 standard with "1969-12-31 23:59:59 UTC",which is -1 in calendar time and that value is on those OSesalso used as an error code.  Thus as.POSIXct("1969-12-31  23:59:59", format = "%Y-%m-%d %H:%M:%S", tz = "UTC") may giveNA, and hence as.POSIXct("1969-12-31 23:59:59",  tz = "UTC") will give "1969-12-31 23:59:00".  Other OSes(including the code used by R on Windows) report errors separatelyand so are able to handle that time as valid.The print methods respect options("max.print").
See the description of options("warn") for thecircumstances under which there is a last.warning object andwarnings() is used.  In essence this is if options(warn =    0) and warning has been called at least once.Note that the length(last.warning) is maximallygetOption("nwarnings") (at the time the warnings aregenerated) which is 50 by default.  To increase, use somethinglike It is possible that last.warning refers to the last recordedwarning and not to the last warning, for example if options(warn) hasbeen changed or if a catastrophic error occurred.
Function difftime calculates a difference of two date/timeobjects and returns an object of class "difftime" with anattribute indicating the units.  TheMath group method providesround, signif, floor,ceiling, trunc, abs, andsign methods for objects of this class, and there aremethods for the group-generic (seeOps) logical and arithmeticoperations.If units = "auto", a suitable set of units is chosen, the largestpossible (excluding "weeks") in which all the absolutedifferences are greater than one.Subtraction of date-time objects gives an object of this class,by calling difftime with units = "auto".  Alternatively,as.difftime() works on character-coded or numeric timeintervals; in the latter case, units must be specified, andformat has no effect.Limited arithmetic is available on "difftime" objects: they canbe added or subtracted, and multiplied or divided by a numeric vector.In addition, adding or subtracting a numeric vector by a"difftime" object implicitly converts the numeric vector to a"difftime" object with the same units as the "difftime"object.  There are methods for mean andsum (via the Summarygroup generic), and diff via diff.defaultbuilding on the "difftime" method for arithmetic, notably-.The units of a "difftime" object can be extracted by theunits function, which also has a replacement form.  If theunits are changed, the numerical value is scaled accordingly.  Thereplacement version keeps attributes such as names and dimensions.Note that units = "days" means a period of 24 hours, hencetakes no account of Daylight Savings Time.  Differences in objectsof class "Date" are computed as if in the UTC time zone.The as.double method returns the numeric value expressed inthe specified units.  Using  units = "auto" means the units of theobject.The format method simply formats the numeric value and appendsthe units as a text string.
Function difftime calculates a difference of two date/timeobjects and returns an object of class "difftime" with anattribute indicating the units.  TheMath group method providesround, signif, floor,ceiling, trunc, abs, andsign methods for objects of this class, and there aremethods for the group-generic (seeOps) logical and arithmeticoperations.If units = "auto", a suitable set of units is chosen, the largestpossible (excluding "weeks") in which all the absolutedifferences are greater than one.Subtraction of date-time objects gives an object of this class,by calling difftime with units = "auto".  Alternatively,as.difftime() works on character-coded or numeric timeintervals; in the latter case, units must be specified, andformat has no effect.Limited arithmetic is available on "difftime" objects: they canbe added or subtracted, and multiplied or divided by a numeric vector.In addition, adding or subtracting a numeric vector by a"difftime" object implicitly converts the numeric vector to a"difftime" object with the same units as the "difftime"object.  There are methods for mean andsum (via the Summarygroup generic), and diff via diff.defaultbuilding on the "difftime" method for arithmetic, notably-.The units of a "difftime" object can be extracted by theunits function, which also has a replacement form.  If theunits are changed, the numerical value is scaled accordingly.  Thereplacement version keeps attributes such as names and dimensions.Note that units = "days" means a period of 24 hours, hencetakes no account of Daylight Savings Time.  Differences in objectsof class "Date" are computed as if in the UTC time zone.The as.double method returns the numeric value expressed inthe specified units.  Using  units = "auto" means the units of theobject.The format method simply formats the numeric value and appendsthe units as a text string.
Function difftime calculates a difference of two date/timeobjects and returns an object of class "difftime" with anattribute indicating the units.  TheMath group method providesround, signif, floor,ceiling, trunc, abs, andsign methods for objects of this class, and there aremethods for the group-generic (seeOps) logical and arithmeticoperations.If units = "auto", a suitable set of units is chosen, the largestpossible (excluding "weeks") in which all the absolutedifferences are greater than one.Subtraction of date-time objects gives an object of this class,by calling difftime with units = "auto".  Alternatively,as.difftime() works on character-coded or numeric timeintervals; in the latter case, units must be specified, andformat has no effect.Limited arithmetic is available on "difftime" objects: they canbe added or subtracted, and multiplied or divided by a numeric vector.In addition, adding or subtracting a numeric vector by a"difftime" object implicitly converts the numeric vector to a"difftime" object with the same units as the "difftime"object.  There are methods for mean andsum (via the Summarygroup generic), and diff via diff.defaultbuilding on the "difftime" method for arithmetic, notably-.The units of a "difftime" object can be extracted by theunits function, which also has a replacement form.  If theunits are changed, the numerical value is scaled accordingly.  Thereplacement version keeps attributes such as names and dimensions.Note that units = "days" means a period of 24 hours, hencetakes no account of Daylight Savings Time.  Differences in objectsof class "Date" are computed as if in the UTC time zone.The as.double method returns the numeric value expressed inthe specified units.  Using  units = "auto" means the units of theobject.The format method simply formats the numeric value and appendsthe units as a text string.
Function difftime calculates a difference of two date/timeobjects and returns an object of class "difftime" with anattribute indicating the units.  TheMath group method providesround, signif, floor,ceiling, trunc, abs, andsign methods for objects of this class, and there aremethods for the group-generic (seeOps) logical and arithmeticoperations.If units = "auto", a suitable set of units is chosen, the largestpossible (excluding "weeks") in which all the absolutedifferences are greater than one.Subtraction of date-time objects gives an object of this class,by calling difftime with units = "auto".  Alternatively,as.difftime() works on character-coded or numeric timeintervals; in the latter case, units must be specified, andformat has no effect.Limited arithmetic is available on "difftime" objects: they canbe added or subtracted, and multiplied or divided by a numeric vector.In addition, adding or subtracting a numeric vector by a"difftime" object implicitly converts the numeric vector to a"difftime" object with the same units as the "difftime"object.  There are methods for mean andsum (via the Summarygroup generic), and diff via diff.defaultbuilding on the "difftime" method for arithmetic, notably-.The units of a "difftime" object can be extracted by theunits function, which also has a replacement form.  If theunits are changed, the numerical value is scaled accordingly.  Thereplacement version keeps attributes such as names and dimensions.Note that units = "days" means a period of 24 hours, hencetakes no account of Daylight Savings Time.  Differences in objectsof class "Date" are computed as if in the UTC time zone.The as.double method returns the numeric value expressed inthe specified units.  Using  units = "auto" means the units of theobject.The format method simply formats the numeric value and appendsthe units as a text string.
Functions in standard packages other than the base package are listed inhelp("pkg-deprecated"), where pkg is replaced by thename of the package.
If recursive = FALSE directories are not deleted,not even empty ones.On most platforms ‘file’ includes symbolic links, fifos andsockets.  unlink(x, recursive = TRUE)deletes just the symbolic link if the target of such a link is a directory.Wildcard expansion (normally ‘*’ and ‘?’ are allowed) is done bythe internal code of Sys.glob.  Wildcards never match aleading ‘.’ in the filename, and files ‘.’, ‘..’ and‘~’ will never be considered for deletion.Wildcards will only be expanded if the system supports it.  Mostsystems will support not only ‘*’ and ‘?’ but also characterclasses such as ‘[a-z]’ (see the man pages for the systemcall glob on your OS).  The metacharacters * ? [ canoccur in Unix filenames, and this makes it difficult to useunlink to delete such files (see file.remove),although escaping the metacharacters by backslashes usually works.  Ifa metacharacter matches nothing it is considered as a literalcharacter.recursive = TRUE might not be supported on all platforms, when itwill be ignored, with a warning: however there are no known currentexamples.
unlist is generic: you can write methods to handlespecific classes of objects, see InternalMethods,and note, e.g., relist with the unlist methodfor relistable objects.If recursive = FALSE, the function will not recurse beyond thefirst level items in x.Factors are treated specially.  If all non-list elements of xare factor (or ordered factor) objects then the resultwill be a factor withlevels the union of the level sets of the elements, in the order thelevels occur in the level sets of the elements (which means that ifall the elements have the same level set, that is the level set of theresult).x can be an atomic vector, but then unlist does nothing useful,not even drop names.By default, unlist tries to retain the naminginformation present in x.  If use.names = FALSE allnaming information is dropped.Where possible the list elements are coerced to a common mode duringthe unlisting, and so the result often ends up as a charactervector.  Vectors will be coerced to the highest type of the componentsin the hierarchy NULL < raw < logical < integer < double < complex < character< list < expression: pairlists are treated as lists.A list is a (generic) vector, and the simplified vector might still bea list (and might be unchanged).  Non-vector elements of the list(for example language elements such as names, formulas and calls)are not coerced, and so a list containing one or more of these remains alist.  (The effect of unlisting an lm fit is a list whichhas individual residuals as components.)Note that unlist(x) now returns x unchanged also fornon-vector x, instead of signalling an error in that case.
The functions loadNamespace and attachNamespace areusually called implicitly when library is used to load a namespace and any imports needed.  However it may be useful at times tocall these functions directly.loadNamespace loads the specified name space and registers it inan internal data base.  A request to load a name space when one of thatname is already loaded has no effect.  The arguments have the samemeaning as the corresponding arguments to library, whosehelp page explains the details of how a particular installed packagecomes to be chosen.  After loading, loadNamespace looks for ahook function named .onLoad as an internal variable inthe name space (it should not be exported).  Partial loading is usedto support installation with lazy-loading.Optionally the package licence is checked during loading: see section‘Licenses’ in the help for library.loadNamespace does not attach the name space it loads to thesearch path.  attachNamespace can be used to attach a framecontaining the exported values of a name space to the search path (butthis is almost always done via library).  Thehook function .onAttach is run after the name spaceexports are attached.requireNamespace is a wrapper for loadNamespaceanalogous to require that returns a logical value.loadedNamespaces returns a character vector of the names ofthe loaded name spaces.isNamespaceLoaded(pkg) is equivalent to but more efficient thanpkg %in% loadedNamespaces().unloadNamespace can be used to attempt to force a name space tobe unloaded.  If the name space is attached, it is firstdetached, thereby running a .onDetach or.Last.lib function in the name space if one is exported.  Anerror is signaled and the name space is not unloaded if the name spaceis imported by other loaded name spaces.  If defined, a hook function.onUnload is run before removing the name space from theinternal registry.See the comments in the help for detach about someissues with unloading and reloading name spaces.
The function lockEnvironment locks its environment argument.Locking theenvironment prevents adding or removing variable bindings from theenvironment.  Changing the value of a variable is still possible unlessthe binding has been locked.  The namespace environments of packageswith namespaces are locked when loaded.lockBinding locks individual bindings in the specifiedenvironment.  The value of a locked binding cannot be changed.  Lockedbindings may be removed from an environment unless the environment islocked.makeActiveBinding installs fun in environment envso that getting the value of sym calls fun with noarguments, and assigning to sym calls fun with oneargument, the value to be assigned.  This allows the implementation ofthings like C variables linked to R variables and variables linked todatabases, and is used to implement setRefClass.  It mayalso be useful for making thread-safe versions of some system globals.Currently active bindings are not preserved during package installation,but they can be created in .onLoad.
NA
The function serialize serializes object to the specifiedconnection.  If connection is NULL then object isserialized to a raw vector, which is returned as the result ofserialize.Sharing of reference objects is preserved within the object but notacross separate calls to serialize.unserialize reads an object (as written by serialize)from connection or a raw vector.The refhook functions can be used to customize handling ofnon-system reference objects (all external pointers and weakreferences, and all environments other than namespace and packageenvironments and .GlobalEnv).  The hook function forserialize should return a character vector for references itwants to handle; otherwise it should return NULL.  The hook forunserialize will be called with character vectors supplied toserialize and should return an appropriate object.For a text-mode connection, the default value of ascii is setto TRUE: only ASCII representations can be written to text-modeconnections and attempting to use ascii = FALSE will throw anerror.The format consists of a single line followed by the data: the firstline contains a single character: X for binary serializationand A for ASCII serialization, followed by a new line.  (Theformat used is identical to that used by readRDS.)As almost all systems in current use are little-endian, xdr =  FALSE can be used to avoid byte-shuffling at both ends whentransferring data from one little-endian machine to another (orbetween processes on the same machine).  Depending on the system, thiscan speed up serialization and unserialization by a factor of up to3x.
split and split<- are generic functions with default anddata.frame methods.  The data frame method can also be used tosplit a matrix into a list of matrices, and the replacement formlikewise, provided they are invoked explicitly.unsplit works with lists of vectors or data frames (assumed tohave compatible structure, as if created by split).  It putselements or rows back in the positions given by f.  In the dataframe case, row names are obtained by unsplitting the row namevectors from the elements of value.f is recycled as necessary and if the length of x is nota multiple of the length of f a warning is printed.Any missing values in f are dropped together with thecorresponding values of x.The default method calls interaction when f is alist.  If the levels of the factors contain .the factors may not be split as expected, unless sep is set tostring not present in the factor levels.
The trace function operates by constructing a revised versionof the function (or of the method, if signature is supplied),and assigning the new object back where the original was found.If only the what argument is given, a line of trace printing isproduced for each call to the function (back compatible with theearlier version of trace).The object constructed by trace is from a class that extends"function" and which contains the original, untraced version.A call to untrace re-assigns this version.If the argument tracer or exit is the name of afunction, the tracing expression will be a call to that function, withno arguments.  This is the easiest and most common case, with thefunctions browser and recover thelikeliest candidates; the former browses in the frame of the functionbeing traced, and the latter allows browsing in any of the currentlyactive calls. The arguments tracer and exit are evaluated tosee whether they are functions, but only their names are used in thetracing expressions.  The lookup is done again when the traced functionexecutes, so it may not be tracer or exit that will be calledwhile tracing.The tracer or exit argument can also be an unevaluatedexpression (such as returned by a call to quote orsubstitute).  This expression itself is inserted in thetraced function, so it will typically involve arguments or localobjects in the traced function.  An expression of this form is usefulif you only want to interact when certain conditions apply (and inthis case you probably want to supply print = FALSE in the callto trace also).When the at argument is supplied, it can be a vector ofintegers referring to the substeps of the body of the function (thisonly works if the body of the function is enclosed in { ...}).  Inthis case tracer is not called on entry, but insteadjust before evaluating each of the steps listed in at.  (Hint:you don't want to try to count the steps in the printed version of afunction; instead, look at as.list(body(f)) to get the numbersassociated with the steps in function f.)The at argument can also be a list of integer vectors.  Inthis case, each vector refers to a step nested within another step ofthe function.  For example, at = list(c(3,4))will call the tracer just before the fourth step of the third stepof the function.  See the example below.Using setBreakpoint (from package utils) may be analternative, calling trace(...., at, ...).The exit argument is called during on.exitprocessing.  In an on.exit expression, the experimental returnValue()function may be called to obtain the value about to be returned bythe function. Calling this function in other circumstances will giveundefined results.An intrinsic limitation in the exit argument is that it won'twork if the function itself uses on.exit with add=  FALSE (the default), since the existing calls will override the onesupplied by trace.Tracing does not nest.  Any call to trace replaces previouslytraced versions of that function or method (except for editedversions as discussed below), and untrace alwaysrestores an untraced version.  (Allowing nested tracing has too manypotentials for confusion and for accidentally leaving traced versionsbehind.)When the edit argument is used repeatedly with no call tountrace on the same function or method in between, thepreviously edited version is retained.  If you want to throw awayall the previous tracing and then edit, call untrace before the nextcall to trace.  Editing may be combined with automatictracing; just supply the other arguments such as tracer, andthe edit argument as well.  The edit = TRUE argumentuses the default editor (see edit).Tracing primitive functions (builtins and specials) from the basepackage works, but only by a special mechanism and not veryinformatively.  Tracing a primitive causes the primitive to bereplaced by a function with argument ... (only).  You can get a bitof information out, but not much.  A warning message is issued whentrace is used on a primitive.The practice of saving the traced version of the function back wherethe function came from means that tracing carries over from onesession to another, if the traced function is saved in thesession image.  (In the next session, untrace will remove thetracing.)  On the other hand, functions that were in a package, not inthe global environment, are not saved in the image, so tracing expireswith the session for such functions.Tracing an S4 method is basically just like tracing a function, with theexception that the traced version is stored by a call tosetMethod rather than by direct assignment, and so isthe untraced version after a call to untrace.The version of trace described here is largely compatible withthe version in S-Plus, although the two work by entirely differentmechanisms.  The S-Plus trace uses the session frame, with theresult that tracing never carries over from one session to another (Rdoes not have a session frame).  Another relevant distinction hasnothing directly to do with trace:  The browser in S-Plusallows changes to be made to the frame being browsed, and the changeswill persist after exiting the browser.  The R browser allows changes,but they disappear when the browser exits.  This may be relevant inthat the S-Plus version allows you to experiment with code changesinteractively, but the R version does not.  (A future revision mayinclude a ‘destructive’ browser for R.)
This functionality is optional, determined at compilation, because itmakes R run a little more slowly even when no objects are beingtraced.  tracemem and untracemem give errors when R is notcompiled with memory profiling; retracemem does not (so it can beleft in code during development).It is enabled in the CRAN macOS and Windows builds of R.When an object is traced any copying of the object by the C functionduplicate produces a message to standard output, as does typecoercion and copying when passing arguments to .C or.Fortran.The message consists of the string tracemem, the identifyingstrings for the object being copied and the new object being created,and a stack trace showing where the duplication occurred.retracemem() is used to indicate that a variable should beconsidered a copy of a previous variable (e.g., after subscripting).The messages can be turned off with tracingState.It is not possible to trace functions, as this would conflict withtrace and it is not useful to trace NULL,environments, promises, weak references, or external pointer objects, asthese are not duplicated.These functions are primitive.
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
NA
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
An R object is a data object which has a classattribute (and this can be tested by is.object).A class attribute is a character vector giving the names ofthe classes from which the object inherits.If the object does not have a class attribute, it has an implicitclass.  Matrices and arrays have class "matrix"or"array" followed by the class of the underlying vector.Most vectors have class the result of mode(x), exceptthat integer vectors have class c("integer", "numeric") andreal vectors have class c("double", "numeric").When a function calling UseMethod("fun") is applied to anobject with class attribute c("first", "second"), the systemsearches for a function called fun.first and, if it finds it,applies it to the object.  If no such function is found a functioncalled fun.second is tried.  If no class name produces asuitable function, the function fun.default is used, if itexists, or an error results.Function methods can be used to find out about themethods for a particular generic function or class.UseMethod is a primitive function but uses standard argumentmatching.  It is not the only means of dispatch of methods, for thereare internal generic and group generic functions.UseMethod currently dispatches on the implicit class even forarguments that are not objects, but the other means of dispatch donot.NextMethod invokes the next method (determined by theclass vector, either of the object supplied to the generic, or ofthe first argument to the function containing NextMethod if amethod was invoked directly).  Normally NextMethod is used withonly one argument, generic, but if further arguments aresupplied these modify the call to the next method.NextMethod should not be called except in methods called byUseMethod or from internal generics (seeInternalGenerics).  In particular it will not work insideanonymous calling functions (e.g., get("print.ts")(AirPassengers)).Namespaces can register methods for generic functions.  To supportthis, UseMethod and NextMethod search for methods intwo places: in the environment in which the generic functionis called, and in the registration data base for theenvironment in which the generic is defined (typically a namespace).So methods for a generic function need to be available in theenvironment of the call to the generic, or they must be registered.(It does not matter whether they are visible in the environment inwhich the generic is defined.)  As from R 3.5.0, the registrationdata base is searched after the top level environment (seetopenv) of the calling environment (but before theparents of the top level environment).
These will work in any locale, including on platforms that do nototherwise support multi-byte character sets.Unicode defines a name and a number of all of the glyphs itencompasses: the numbers are called code points: since RFC3629they run from 0 to 0x10FFFF (with about 5% beingassigned by version 13.0 of the Unicode standard and 7% reserved for‘private use’).intToUtf8 does not by default handle surrogate pairs: inputs inthe surrogate ranges are mapped to NA.  They might occur if aUTF-16 byte stream has been read as 2-byte integers (in the correctbyte order), in which case allow_surrogate_pairs = TRUE willtry to interpret them (with unmatched surrogate values still treatedas NA).
These use similar checks to those used by functions such asgrep.validUTF8 ignores any marked encoding (seeEncoding) and so looks directly if the bytes in eachstring are valid UTF-8.  (For the validity of ‘noncharacters’see the help for intToUtf8.)validEnc regards character strings as validly encoded unlesstheir encodings are marked as UTF-8 or they are unmarked and the Rsession is in a UTF-8 or other multi-byte locale.  (The checks inother multi-byte locales depend on the OS and as withiconv not all invalid inputs may be detected.)
These use similar checks to those used by functions such asgrep.validUTF8 ignores any marked encoding (seeEncoding) and so looks directly if the bytes in eachstring are valid UTF-8.  (For the validity of ‘noncharacters’see the help for intToUtf8.)validEnc regards character strings as validly encoded unlesstheir encodings are marked as UTF-8 or they are unmarked and the Rsession is in a UTF-8 or other multi-byte locale.  (The checks inother multi-byte locales depend on the OS and as withiconv not all invalid inputs may be detected.)
FUN is found by a call to match.fun and typicallyis specified as a function or a symbol (e.g., a backquoted name) or acharacter string specifying a function to be searched for from theenvironment of the call to lapply.Function FUN must be able to accept as input any of theelements of X.  If the latter is an atomic vector, FUNwill always be passed a length-one vector of the same type as X.Arguments in ... cannot have the same name as any of theother arguments, and care may be needed to avoid partial matching toFUN.  In general-purpose code it is good practice to name thefirst two arguments X and FUN if ... is passedthrough: this both avoids partial matching to FUN and ensuresthat a sensible error message is given if arguments named X orFUN are passed through ....Simplification in sapply is only attempted if X haslength greater than zero and if the return values from all elementsof X are all of the same (positive) length.  If the commonlength is one the result is a vector, and if greater than one is amatrix with a column corresponding to each element of X.Simplification is always done in vapply.  This functionchecks that all values of FUN are compatible with theFUN.VALUE, in that they must have the same length and type.(Types may be promoted to a higher type within the ordering logical< integer < double < complex, but not demoted.)Users of S4 classes should pass a list to lapply andvapply: the internal coercion is done by the as.list inthe base namespace and not one defined by a user (e.g., by setting S4methods on the base function).
The atomic modes are "logical", "integer","numeric" (synonym "double"), "complex","character" and "raw".If mode = "any", is.vector may return TRUE forthe atomic modes, list and expression.For any mode, it will return FALSE if x has anyattributes except names.  (This is incompatible with S.)  On the otherhand, as.vector removes all attributes including namesfor results of atomic mode (but not those of mode "list" nor"expression").Note that factors are not vectors; is.vector returnsFALSE and as.vector converts a factor to a charactervector for mode = "any".
The arguments named in the vectorize.args argument toVectorize are the arguments passed in the ...  list tomapply.  Only those that are actually passed will bevectorized; default values will not.  See the examples.Vectorize cannot be used with primitive functions as they donot have a value for formals.It also cannot be used with functions that have arguments namedFUN, vectorize.args, SIMPLIFY orUSE.NAMES, as they will interfere with the Vectorizearguments.  See the combn example below for a workaround.  
This gives details of the OS under which R was built, not the oneunder which it is currently running (for which seeSys.info).Note that OS names might not be what you expect: for example macOSMavericks 10.9.4 identifies itself as darwin13.3.0, Linuxusually as linux-gnu and Solaris 10 as solaris2.10.
The result depends on the value ofoptions("warn") and on handlers established in theexecuting code.If a condition object is supplied it should be the only argument, andfurther arguments will be ignored, with a message.warning signals a warning condition by (effectively) callingsignalCondition.  If there are no handlers or if all handlersreturn, then the value of warn = getOption("warn") isused to determine the appropriate action.  If warn is negativewarnings are ignored; if it is zero they are stored and printed afterthe top–level function has completed; if it is one they are printedas they occur and if it is 2 (or larger) warnings are turned intoerrors. Calling warning(immediate. = TRUE) turns warn <=    0 into warn = 1 for this call only.If warn is zero (the default), a read-only variablelast.warning is created.  It contains the warnings which can beprinted via a call to warnings.Warnings will be truncated to getOption("warning.length")characters, default 1000, indicated by [... truncated].While the warning is being processed, a muffleWarning restartis available.  If this restart is invoked with invokeRestart,then warning returns immediately.An attempt is made to coerce other types of inputs to warningto character vectors.suppressWarnings evaluates its expression in a context thatignores all warnings.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
See the description of options("warn") for thecircumstances under which there is a last.warning object andwarnings() is used.  In essence this is if options(warn =    0) and warning has been called at least once.Note that the length(last.warning) is maximallygetOption("nwarnings") (at the time the warnings aregenerated) which is 50 by default.  To increase, use somethinglike It is possible that last.warning refers to the last recordedwarning and not to the last warning, for example if options(warn) hasbeen changed or if a catastrophic error occurred.
NA
NA
NA
NA
NA
NA
break breaks out of a for, while or repeatloop; control is transferred to the first statement outside theinner-most loop. next halts the processing of the currentiteration and advances the looping index.  Both break andnext apply only to the innermost of nested loops.Note that it is a common mistake to forget to put braces ({ .. })around your statements, e.g., after if(..) or for(....).In particular, you should not have a newline between } andelse to avoid a syntax error in entering a if ... elseconstruct at the keyboard or via source.For that reason, one (somewhat extreme) attitude of defensive programmingis to always use braces, e.g., for if clauses.The seq in a for loop is evaluated at the start ofthe loop; changing it subsequently does not affect the loop.  Ifseq has length zero the body of the loop is skipped. Otherwise thevariable var is assigned in turn the value of each element ofseq. You can assign to var within the body of the loop,but this will not affect the next iteration.  When the loop terminates,var remains as a variable containing its latest value.
with is a generic function that evaluates expr in alocal environment constructed from data.  The environment hasthe caller's environment as its parent.  This is useful forsimplifying calls to modeling functions.  (Note: if data isalready an environment then this is used with its existing parent.)Note that assignments within expr take place in the constructedenvironment and not in the user's workspace.within is similar, except that it examines the environmentafter the evaluation of expr and makes the correspondingmodifications to a copy of data (this may fail in the dataframe case if objects are created which cannot be stored in a dataframe), and returns it.  within can be used as an alternativeto transform.
with is a generic function that evaluates expr in alocal environment constructed from data.  The environment hasthe caller's environment as its parent.  This is useful forsimplifying calls to modeling functions.  (Note: if data isalready an environment then this is used with its existing parent.)Note that assignments within expr take place in the constructedenvironment and not in the user's workspace.within is similar, except that it examines the environmentafter the evaluation of expr and makes the correspondingmodifications to a copy of data (this may fail in the dataframe case if objects are created which cannot be stored in a dataframe), and returns it.  within can be used as an alternativeto transform.
Note that running code via source differs in a few respectsfrom entering it at the R command line.  Since expressions are notexecuted at the top level, auto-printing is not done.  So you willneed to include explicit print calls for things you want to beprinted (and remember that this includes plotting by lattice,FAQ Q7.22).  Since the complete file is parsed before any of it isrun, syntax errors result in none of the code being run.  If an erroroccurs in running a syntactically correct script, anything assignedinto the workspace by code that has been run will be kept (just asfrom the command line), but diagnostic information such astraceback() will contain additional calls towithVisible.All versions of R accept input from a connection with end of linemarked by LF (as used on Unix), CRLF (as used on DOS/Windows) or CR(as used on classic Mac OS) and map this to newline.  The final linecan be incomplete, that is missing the final end-of-line marker.If keep.source is true (the default in interactive use), thesource of functions is kept so they can be listed exactly as input.Unlike input from a console, lines in the file or on a connection cancontain an unlimited number of characters.When skip.echo > 0, that many comment lines at the start ofthe file will not be echoed.  This does not affect the execution ofthe code at all.  If there are executable lines within the firstskip.echo lines, echoing will start with the first of them.If echo is true and a deparsed expression exceedsmax.deparse.length, that many characters are output followed by .... [TRUNCATED] .
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
with is a generic function that evaluates expr in alocal environment constructed from data.  The environment hasthe caller's environment as its parent.  This is useful forsimplifying calls to modeling functions.  (Note: if data isalready an environment then this is used with its existing parent.)Note that assignments within expr take place in the constructedenvironment and not in the user's workspace.within is similar, except that it examines the environmentafter the evaluation of expr and makes the correspondingmodifications to a copy of data (this may fail in the dataframe case if objects are created which cannot be stored in a dataframe), and returns it.  within can be used as an alternativeto transform.
with is a generic function that evaluates expr in alocal environment constructed from data.  The environment hasthe caller's environment as its parent.  This is useful forsimplifying calls to modeling functions.  (Note: if data isalready an environment then this is used with its existing parent.)Note that assignments within expr take place in the constructedenvironment and not in the user's workspace.within is similar, except that it examines the environmentafter the evaluation of expr and makes the correspondingmodifications to a copy of data (this may fail in the dataframe case if objects are created which cannot be stored in a dataframe), and returns it.  within can be used as an alternativeto transform.
with is a generic function that evaluates expr in alocal environment constructed from data.  The environment hasthe caller's environment as its parent.  This is useful forsimplifying calls to modeling functions.  (Note: if data isalready an environment then this is used with its existing parent.)Note that assignments within expr take place in the constructedenvironment and not in the user's workspace.within is similar, except that it examines the environmentafter the evaluation of expr and makes the correspondingmodifications to a copy of data (this may fail in the dataframe case if objects are created which cannot be stored in a dataframe), and returns it.  within can be used as an alternativeto transform.
The condition system provides a mechanism for signaling andhandling unusual conditions, including errors and warnings.Conditions are represented as objects that contain informationabout the condition that occurred, such as a message and the call inwhich the condition occurred.  Currently conditions are S3-styleobjects, though this may eventually change.Conditions are objects inheriting from the abstract classcondition.  Errors and warnings are objects inheritingfrom the abstract subclasses error and warning.The class simpleError is the class used by stopand all internal error signals.  Similarly, simpleWarningis used by warning, and simpleMessage is used bymessage.  The constructors by the same names take a stringdescribing the condition as argument and an optional call.  Thefunctions conditionMessage and conditionCall aregeneric functions that return the message and call of a condition.The function errorCondition can beused to construct error conditions of a particular class withadditional fields specified as the ... argument.warningCondition is analogous for warnings.Conditions are signaled by signalCondition.  In addition,the stop and warning functions have been modified toalso accept condition arguments.The function tryCatch evaluates its expression argumentin a context where the handlers provided in the ...argument are available.  The finally expression is thenevaluated in the context in which tryCatch was called; thatis, the handlers supplied to the current tryCatch call arenot active when the finally expression is evaluated.Handlers provided in the ... argument to tryCatchare established for the duration of the evaluation of expr.If no condition is signaled when evaluating expr thentryCatch returns the value of the expression.If a condition is signaled while evaluating expr thenestablished handlers are checked, starting with the most recentlyestablished ones, for one matching the class of the condition.When several handlers are supplied in a single tryCatch thenthe first one is considered more recent than the second.  If ahandler is found then control is transferred to thetryCatch call that established the handler, the handlerfound and all more recent handlers are disestablished, the handleris called with the condition as its argument, and the resultreturned by the handler is returned as the value of thetryCatch call.Calling handlers are established by withCallingHandlers.  Ifa condition is signaled and the applicable handler is a callinghandler, then the handler is called by signalCondition inthe context where the condition was signaled but with the availablehandlers restricted to those below the handler called in thehandler stack.  If the handler returns, then the next handler istried; once the last handler has been tried, signalConditionreturns NULL.globalCallingHandlers establishes calling handlers globally.These handlers are only called as a last resort, after the otherhandlers dynamically registered with withCallingHandlers havebeen invoked. They are called before the error global option(which is the legacy interface for global handling of errors).Registering the same handler multiple times moves that handler ontop of the stack, which ensures that it is called first. Globalhandlers are a good place to define a general purpose logger (forinstance saving the last error object in the global workspace) or ageneral recovery strategy (e.g. installing missing packages via theretry_loadNamespace restart).Like withCallingHandlers and tryCatch,globalCallingHandlers takes named handlers. Unlike thesefunctions, it also has an options-like interface: youcan establish handlers by passing a single list of named handlers.To unregister all global handlers, supply a single 'NULL'. The listof deleted handlers is returned invisibly. Finally, callingglobalCallingHandlers without arguments returns the list ofcurrently established handlers, visibly.User interrupts signal a condition of class interrupt thatinherits directly from class condition before executing thedefault interrupt action.Restarts are used for establishing recovery protocols.  They can beestablished using withRestarts.  One pre-established restart isan abort restart that represents a jump to top level.findRestart and computeRestarts find the availablerestarts.  findRestart returns the most recently establishedrestart of the specified name.  computeRestarts returns alist of all restarts.  Both can be given a condition argument andwill then ignore restarts that do not apply to the condition.invokeRestart transfers control to the point where thespecified restart was established and calls the restart's handler with thearguments, if any, given as additional arguments toinvokeRestart.  The restart argument to invokeRestartcan be a character string, in which case findRestart is usedto find the restart. If no restart is found, an error is thrown.tryInvokeRestart is a variant of invokeRestart thatreturns silently when the restart cannot be found withfindRestart. Because a condition of a given class might besignalled with arbitrary protocols (error, warning, etc), it isrecommended to use this permissive variant whenever you are handlingconditions signalled from a foreign context. For instance, invocationof a "muffleWarning" restart should be optional because thewarning might have been signalled by the user or from a differentpackage with the stop or message protocols. Only useinvokeRestart when you have control of the signalling context,or when it is a logical error if the restart is not available.New restarts for withRestarts can be specified in several ways.The simplest is in name = function form where the function isthe handler to call when the restart is invoked.  Another simplevariant is as name = string where the string is stored in thedescription field of the restart object returned byfindRestart; in this case the handler ignores its argumentsand returns NULL.  The most flexible form of a restartspecification is as a list that can include several fields, includinghandler, description, and test.  Thetest field should contain a function of one argument, acondition, that returns TRUE if the restart applies to thecondition and FALSE if it does not; the default functionreturns TRUE for all conditions.One additional field that can be specified for a restart isinteractive.  This should be a function of no arguments thatreturns a list of arguments to pass to the restart handler.  The listcould be obtained by interacting with the user if necessary.  Thefunction invokeRestartInteractively calls this function toobtain the arguments to use when invoking the restart.  The defaultinteractive method queries the user for values for theformal arguments of the handler function.Interrupts can be suspended while evaluating an expression usingsuspendInterrupts.  Subexpression can be evaluated withinterrupts enabled using allowInterrupts.  These functionscan be used to make sure cleanup handlers cannot be interrupted..signalSimpleWarning, .handleSimpleError, and.tryResumeInterrupt are used internally and should not becalled directly.
The argument, not an expression object, ratheran (unevaluated function) call, is evaluated in thecaller's context.This is a primitive function.
NA
DCF is a simple format for storing databases in plain text files thatcan easily be directly read and written by humans.  DCF is used invarious places to store R system information, like descriptions andcontents of packages.The DCF rules as implemented in R are: A database consists of one or more records, each with one ormore named fields.  Not every record must contain each field.Fields may appear more than once in a record. Regular lines start with a non-whitespace character. Regular lines are of form tag:value, i.e., have a nametag and a value for the field, separated by : (only the first: counts).  The value can be empty (i.e., whitespace only). Lines starting with whitespace are continuation lines (to thepreceding field) if at least one character in the line isnon-whitespace.  Continuation lines where the only non-whitespacecharacter is a . are taken as blank lines (allowing formulti-paragraph field values). Records are separated by one or more empty (i.e., whitespaceonly) lines. Individual lines may not be arbitrarily long; prior to R 3.0.2 thelength limit was approximately 8191 bytes per line.Note that read.dcf(all = FALSE) reads the file byte-by-byte.This allows a ‘DESCRIPTION’ file to be read and only its ASCIIfields used, or its Encoding field used to re-encode theremaining fields.write.dcf does not write NA fields.
These functions can only be used with binary-mode connections.If con is a character string, the functions callfile to obtain a binary-mode file connection which isopened for the duration of the function call.If the connection is open it is read/written from its currentposition.  If it is not open, it is opened for the duration of thecall in an appropriate mode (binary read or write) and then closedagain.  An open connection must be in binary mode.If readBin is called with con a raw vector, the data inthe vector is used as input.  If writeBin is called withcon a raw vector, it is just an indication that a raw vectorshould be returned.If size is specified and not the natural size of the object,each element of the vector is coerced to an appropriate type beforebeing written or as it is read.  Possible sizes are 1, 2, 4 andpossibly 8 for integer or logical vectors, and 4, 8 and possibly 12/16for numeric vectors.  (Note that coercion occurs as signed typesexcept if signed = FALSE when reading integers of sizes 1 and 2.)Changing sizes is unlikely to preserve NAs, and the extendedprecision sizes are unlikely to be portable across platforms.readBin and writeBin read and write C-stylezero-terminated character strings.  Input strings are limited to 10000characters.  readChar and writeChar canbe used to read and write fixed-length strings.  No check is made thatthe string is valid in the current locale's encoding.Handling R's missing and special (Inf, -Inf andNaN) values is discussed in the ‘R Data Import/Export’ manual.Only 2^31 - 1 bytes can be written in a singlecall (and that is the maximum capacity of a raw vector on 32-bitplatforms).‘Endian-ness’ is relevant for size > 1, and shouldalways be set for portable code (the default is only appropriate whenwriting and then reading files on the same platform).
These functions complement readBin andwriteBin which read and write C-style zero-terminatedcharacter strings.  They are for strings of known length, andcan optionally write an end-of-string mark.  They are intended onlyfor character strings valid in the current locale.These functions are intended to be used with binary-mode connections.If con is a character string, the functions callfile to obtain a binary-mode file connection which isopened for the duration of the function call.If the connection is open it is read/written from its currentposition.  If it is not open, it is opened for the duration of thecall in an appropriate mode (binary read or write) and then closedagain.  An open connection must be in binary mode.If readChar is called with con a raw vector, the data inthe vector is used as input.  If writeChar is called withcon a raw vector, it is just an indication that a raw vectorshould be returned.Character strings containing ASCII nul(s) will be readcorrectly by readChar but truncated at the firstnul with a warning.If the character length requested for readChar is longer thanthe data available on the connection, what is available isreturned.  For writeChar if too many characters are requestedthe output is zero-padded, with a warning.Missing strings are written as NA.
If the con is a character string, the function callsfile to obtain a file connection which is opened forthe duration of the function call.(tilde expansion of the file path is done by file.)If the connection is open it is written from its current position.If it is not open, it is opened for the duration of the call in"wt" mode and then closed again.Normally writeLines is used with a text-mode  connection, and thedefault separator is converted to the normal separator for thatplatform (LF on Unix/Linux, CRLF on Windows).  For more control, opena binary connection and specify the precise value you want written tothe file in sep.  For even more control, usewriteChar on a binary connection.useBytes is for expert use.  Normally (when false) characterstrings with marked encodings are converted to the current encodingbefore being passed to the connection (which might do furtherre-encoding).  useBytes = TRUE suppresses the re-encoding ofmarked strings so they are passed byte-by-byte to the connection:this can be useful when strings have already been re-encoded bye.g. iconv.  (It is invoked automatically for stringswith marked encoding "bytes".)
! indicates logical negation (NOT).& and && indicate logical AND and | and ||indicate logical OR.  The shorter form performs elementwisecomparisons in much the same way as arithmetic operators.  The longerform evaluates left to right examining only the first element of eachvector.  Evaluation proceeds only until the result is determined.  Thelonger form is appropriate for programming control-flow and typicallypreferred in if clauses.xor indicates elementwise exclusive OR.isTRUE(x) is the same as{ is.logical(x) && length(x) == 1 && !is.na(x) && x };isFALSE() is defined analogously.  Consequently,if(isTRUE(cond)) may be preferable to if(cond) becauseof NAs.In earlier R versions, isTRUE <- function(x) identical(x, TRUE),had the drawback to be false e.g., for x <- c(val = TRUE).Numeric and complex vectors will be coerced to logical values, withzero being false and all non-zero values being true.  Raw vectors arehandled without any coercion for !, &, | andxor, with these operators being applied bitwise (so ! isthe 1s-complement).The operators !, & and | are generic functions:methods can be written for them individually or via theOps (or S4 Logic, see below)group generic function.  (See Ops forhow dispatch is computed.)NA is a valid logical object.  Where a component ofx or y is NA, the result will be NA if theoutcome is ambiguous.  In other words NA & TRUE evaluates toNA, but NA & FALSE evaluates to FALSE.  See theexamples below.See Syntax for the precedence of these operators: unlike manyother languages (including S) the AND and OR operators do not have thesame precedence (the AND operators have higher precedence than the ORoperators).
xpdrows.data.frame is an auxiliary function which expands therows of a data frame.  It is used by the data frame methods of[<- and [[<- (which perform subscripted assignments on adata frame), and not intended to be called directly.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
Numeric versions are sequences of one or more non-negative integers,usually (e.g., in package ‘DESCRIPTION’ files) represented ascharacter strings with the elements of the sequence concatenated andseparated by single . or - characters.  R packageversions consist of at least two such integers, an R system versionof exactly three (major, minor and patchlevel).Functions numeric_version, package_version andR_system_version create a representation from such strings (ifsuitable) which allows for coercion and testing, combination,comparison, summaries (min/max), inclusion in data frames,subscripting, and printing.  The classes can hold a vector of suchrepresentations.getRversion returns the version of the running R as an Rsystem version object.The [[ operator extracts or replaces a single version.  Toaccess the integers of a version use two indices: see the examples.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
This is a special case of ranking, but as a less general function thanrank is more suitable to be made generic.  The defaultmethod is similar to rank(x, ties.method = "min",    na.last = "keep"), so NA values are given rank NA and alltied values are given equal integer rank.The factor method extracts the codes.The default method will unclass the object ifis.numeric(x) is true but otherwise make use of== and > methods for the class of x[i] (forintegers i), and the is.na method for the class ofx, but might be rather slow when doing so.This is an internal generic primitive, so S3 or S4methods can be written for it.
The first eleven functions create connections.  By default theconnection is not opened (except for a socket connection created bysocketConnection or socketAccept and for server socketconnection created by serverSocket), but maybe opened by setting a non-empty value of argument open.For file the description is a path to the file to be opened(when tilde expansion is done) or a complete URL (when it isthe same as calling url), or "" (the default) or"clipboard" (see the ‘Clipboard’ section).  Use"stdin" to refer to the C-level ‘standard input’ of theprocess (which need not be connected to anything in a console orembedded version of R, and is not in RGui on Windows).  Seealso stdin() for the subtly different R-level concept ofstdin. See nullfile() for a platform-independentway to get filename of the null device.For url the description is a complete URL including scheme(such as http://, https://, ftp:// orfile://).  Method "internal" is that available sinceconnections were introduced..  Method "wininet" is onlyavailable on Windows (it uses the WinINet functions of that OS) andmethod "libcurl" (using the library of that name:https://curl.se/libcurl/) is required on a Unix-alike butoptional on Windows.  Method "default" uses method"internal" for file:// URLs and uses "libcurl"(if available) for ftp:// and ftps:// URLs.  On aUnix-alike it uses "libcurl" for http:// andhttps:// URLs; on Windows "wininet" for http://and https:// URLs (and for ftp:// if "libcurl" isunavailable).  Which methods support which schemes has varied by Rversion – currently "internal" supports file://,http:// (deprecated) and ftp:// (deprecated);"wininet" supports file://, http://,https:// and ftp:// (deprecated).  Proxies can bespecified: see download.file.For gzfile the description is the path to a file compressed bygzip: it can also open for reading uncompressed files andthose compressed by bzip2, xz or lzma.For bzfile the description is the path to a file compressed bybzip2.For xzfile the description is the path to a file compressed byxz (https://en.wikipedia.org/wiki/Xz) or (for readingonly) lzma (https://en.wikipedia.org/wiki/LZMA).unz reads (only) single files within zip files, in binary mode.The description is the full path to the zip file, with ‘.zip’extension if required.For pipe the description is the command line to be piped to orfrom.  This is run in a shell, on Windows that specified by theCOMSPEC environment variable.For fifo the description is the path of the fifo.  (Support forfifo connections is optional but they are available on mostUnix platforms and on Windows.)The intention is that file and gzfile can be usedgenerally for text input (from files, http:// andhttps:// URLs) and binary input respectively.open, close and seek are generic functions: thefollowing applies to the methods relevant to connections.open opens a connection.  In general functions usingconnections will open them if they are not open, but then close themagain, so to leave a connection open call open explicitly.close closes and destroys a connection.  This will happenautomatically in due course (with a warning) if there is no longer anR object referring to the connection.A maximum of 128 connections can be allocated (not necessarily open)at any one time.  Three of these are pre-allocated (seestdout).   The OS will impose limits on the numbers ofconnections of various types, but these are usually larger than 125.flush flushes the output stream of a connection open forwrite/append (where implemented, currently for file and clipboardconnections, stdout and stderr).If for a file or (on most platforms) a fifo connectionthe description is "", the file/fifo is immediately opened (in"w+" mode unless open = "w+b" is specified) and unlinkedfrom the file system.  This provides a temporary file/fifo to write toand then read from.socketConnection(server=TRUE) creates a new temporary server socketlistening on the given port.  As soon as a new socket connection isaccepted on that port, the server socket is automatically closed. serverSocket creates a listening server socket which can be usedfor accepting multiple socket connections by socketAccept.  To stoplistening for new connections, a server socket needs to be closedexplicitly by close.socketConnection and socketAccept support setting ofsocket-specific options. Currently only "no-delay" isimplemented which enables the TCP_NODELAY socket option, causingthe socket to flush send buffers immediately (instead of waiting tocollect all output before sending). This option is useful forprotocols that need fast request/response turn-around times.socketTimeout sets connection timeout of a socket connection.  Anegative timeout can be given to query the old value.
NA
Typical usages areThe first form specifies the line in intercept/slope form(alternatively a can be specified on its own and is taken tocontain the slope and intercept in vector form).The h= and v= forms draw horizontal and vertical linesat the specified coordinates.The coef form specifies the line by a vector containing theslope and intercept.reg is a regression object with a coef method.If this returns a vector of length 1 then the value is taken to be theslope of a line through the origin, otherwise, the first 2 values aretaken to be the intercept and slope.If untf is true, and one or both axes are log-transformed, thena curve is drawn corresponding to a line in original coordinates,otherwise a line is drawn in the transformed coordinate system. Theh and v parameters always refer to original coordinates.The graphical parameters col, lty and lwdcan be specified; see par for details.  For theh= and v= usages they can be vectors of length greaterthan one, recycled as necessary.Specifying an xpd argument for clipping overridesthe global par("xpd") setting used otherwise.
For each i, an arrow is drawn between the point (x0[i],    y0[i]) and the point (x1[i], y1[i]).  The coordinate vectorswill be recycled to the length of the longest.If code = 1 an arrowhead is drawn at (x0[i], y0[i]) and ifcode = 2 an arrowhead is drawn at (x1[i], y1[i]).  Ifcode = 3 a head is drawn at both ends of the arrow.  Unlesslength = 0, when no head is drawn.The graphical parameters col, lty and lwdcan be vectors of length greater than one and will be recycled ifnecessary.The direction of a zero-length arrow is indeterminate, and hence so isthe direction of the arrowheads.  To allow for rounding error,arrowheads are omitted (with a warning) on any arrow of length lessthan 1/1000 inch.
For a two-way contingency table, the signed contribution to Pearson'schi^2 for cell i, j is d_{ij} = (f_{ij} - e_{ij}) / sqrt(e_{ij}),where f_{ij} and e_{ij} are the observed and expectedcounts corresponding to the cell.  In the Cohen-Friendly associationplot, each cell is represented by a rectangle that has (signed) heightproportional to d_{ij} and width proportional tosqrt(e_{ij}), so that the area of the box isproportional to the difference in observed and expected frequencies.The rectangles in each row are positioned relative to a baselineindicating independence (d_{ij} = 0).  If the observed frequencyof a cell is greater than the expected one, the box rises above thebaseline and is shaded in the color specified by the first element ofcol, which defaults to black; otherwise, the box falls belowthe baseline and is shaded in the color specified by the secondelement of col, which defaults to red.A more flexible and extensible implementation of association plotswritten in the grid graphics system is provided in the functionassoc in the contributed package vcd(Meyer, Zeileis and Hornik, 2006).
The axis line is drawn from the lowest to the highest value ofat, but will be clipped at the plot region.  By default, onlyticks which are drawn from points within the plot region (up to atolerance for rounding error) are plotted, but the ticks and theirlabels may well extend outside the plot region.  Use xpd = TRUEor xpd = NA to allow axes to extend further.When at = NULL, pretty tick mark locations are computed internally(the same way axTicks(side) would) frompar("xaxp") or "yaxp" andpar("xlog") (or "ylog").  Note that theselocations may change if an on-screen plot is resized (for example, ifthe plot argument asp (see plot.window) is set.)If labels is not specified, the numeric values supplied orcalculated for at are converted to character strings as if theywere a numeric vector printed by print.default(digits = 7).The code tries hard not to draw overlapping tick labels, and so willomit labels where they would abut or overlap previously drawn labels.This can result in, for example, every other tick being labelled.The ticks are drawn left to right or bottom to top, and space atleast the size of an ‘m’, multiplied by gap.axis,is left between labels.  In previous R versions, this applied onlyfor labels written parallel to the axis direction, hence notfor e.g., las = 2.  Using gap.axis = -1 restores that(buggy) previous behaviour (in the perpendicular case).If either line or pos is set, they (rather thanpar("mgp")[3]) determine the position of the axis line and tickmarks, and the tick labels are placed par("mgp")[2] furtherlines into (or towards for pos) the margin.Several of the graphics parameters affect the way axes are drawn. Thevertical (for sides 1 and 3) positions of the axis and the tick labelsare controlled by mgp[2:3] and mex, the size anddirection of the ticks is controlled by tck and tcl andthe appearance of the tick labels by cex.axis, col.axisand font.axis with orientation controlled by las (butnot srt, unlike S which uses srt if at issupplied and las if it is not).  Note that adj is notsupported and labels are always centered.  See par for details.
This is a generic function.  It works in a slightly non-standard way:if x is supplied and non-NULL it dispatches on x,otherwise if at is supplied and non-NULL it dispatches on at,and the default action is to call axis, omitting argumentx.The idea is that for plots for which either or both of the axes arenumerical but with a special interpretation, the standard plottingfunctions (including boxplot, contour,coplot, filled.contour,pairs, plot.default, rug andstripchart) will set up user coordinates and Axiswill be called to label them appropriately.There are "Date" and "POSIXt" methods which can pass anargument format on to the appropriate axis method (seeaxis.POSIXct).
axis.POSIXct and axis.Date work quite hard to choosesuitable time units (years, months, days, hours, minutes or seconds)and a sensible output format, but this can be overridden by supplyinga format specification.If at is supplied it specifies the locations of the ticksand labels whereas if x is specified a suitable grid of labelsis chosen. Printing of tick labels can be suppressed by usinglabels = FALSE.The date-times for a "POSIXct" input are interpreted in thetime zone give by the "tzone" attribute if there is one,otherwise the current time zone.The way the date-times are rendered (especially month names) iscontrolled by the locale setting of category "LC_TIME" (seeSys.setlocale).
axis.POSIXct and axis.Date work quite hard to choosesuitable time units (years, months, days, hours, minutes or seconds)and a sensible output format, but this can be overridden by supplyinga format specification.If at is supplied it specifies the locations of the ticksand labels whereas if x is specified a suitable grid of labelsis chosen. Printing of tick labels can be suppressed by usinglabels = FALSE.The date-times for a "POSIXct" input are interpreted in thetime zone give by the "tzone" attribute if there is one,otherwise the current time zone.The way the date-times are rendered (especially month names) iscontrolled by the locale setting of category "LC_TIME" (seeSys.setlocale).
The axp, usr, and log arguments must be consistentas their default values (the par(..) results) are.  If youspecify all three (as non-NULL), the graphics environment is not usedat all.  Note that the meaning of axp differs significantlywhen log is TRUE; see the documentation onpar(xaxp = .).axTicks() may be seen as an R implementation of the C functionCreateAtVector() in ‘..../src/main/plot.c’which is called by axis(side, *) when no argumentat is specified or directly by axisTicks() (in packagegrDevices).The delicate case, log = TRUE, now makes use ofaxisTicks unless nintLog = Inf which exists for backcompatibility.
NA
NA
The choice of colour is complicated.  If col was suppliedand is not NA, it is used.  Otherwise, if fg was suppliedand is not NA, it is used.  The final default is par("col").
The generic function boxplot currently has a default method(boxplot.default) and a formula interface (boxplot.formula).If multiple groups are supplied either as multiple arguments or via aformula, parallel boxplots will be plotted, in the order of thearguments or the order of the levels of the factor (seefactor).Missing values are ignored when forming boxplots.
The generic function boxplot currently has a default method(boxplot.default) and a formula interface (boxplot.formula).If multiple groups are supplied either as multiple arguments or via aformula, parallel boxplots will be plotted, in the order of thearguments or the order of the levels of the factor (seefactor).Missing values are ignored when forming boxplots.
NA
NA
cdplot computes the conditional densities of x giventhe levels of y weighted by the marginal distribution of y.The densities are derived cumulatively over the levels of y.This visualization technique is similar to spinograms (see spineplot)and plots P(y | x) against x. The conditional probabilitiesare not derived by discretization (as in the spinogram), but using a smoothingapproach via density.Note, that the estimates of the conditional densities are more reliable forhigh-density regions of x. Conversely, the are less reliable in regionswith only few x observations.
How the clipping rectangle is set depends on the setting ofpar("xpd"): this function changes the current settinguntil the next high-level plotting command resets it.Clipping of lines, rectangles and polygons is done in the graphicsengine, but clipping of text is if possible done in the device, so theeffect of clipping text is device-dependent (and may result in textnot wholly within the clipping region being omitted entirely).Exactly when the clipping region will be reset can be hard topredict.  plot.new always resets it.Functions such as lines and text onlyreset it if par("xpd") has been changed.  However,functions such as box, mtext,title and plot.dendrogram can manipulatethe xpd setting.
The first call to split.screen places R into split-screenmode.  The other split-screen functions only work within this mode.While in this mode, certain other commands should be avoided (see theWarnings section below).  Split-screen mode is exited by the commandclose.screen(all = TRUE).If the current screen is closed, close.screen sets the currentscreen to be the next larger screen number if there is one, otherwiseto the first available screen.
In the case of a single conditioning variable a, when bothrows and columns are unspecified, a ‘close tosquare’ layout is chosen with columns >= rows.In the case of multiple rows, the order of the panelplots is from the bottom and from the left (corresponding toincreasing a, typically).A panel function should not attempt to start a new plot, but just plotwithin a given coordinate system: thus plot and boxplotare not panel functions.The rendering of arguments xlab andylab is not controlled by par argumentscex.lab and font.lab even though they are plotted bymtext rather than title.
contour is a generic function with only a default method inbase R.The methods for positioning the labels on contours are "simple"(draw at the edge of the plot, overlaying the contour line),"edge" (draw at the edge of the plot, embedded in the contourline, with no labels overlapping) and "flattest"(draw on the flattest section of the contour, embedded in thecontour line, with no labels overlapping).  The second and third maynot draw a label on every contour line.For information about vector fonts, see thehelp for text and Hershey.Notice that contour interprets the z matrix as a table off(x[i], y[j]) values, so that the x axis corresponds to rownumber and the y axis to column number, with column 1 at the bottom,i.e. a 90 degree counter-clockwise rotation of the conventional textuallayout.Vector (of length > 1) col, lty, and lwd areapplied along levels and recycled, see the Examples.Alternatively, use contourplot from thelattice package where the formula notation allowsto use vectors x, y, and z of the same length.There is limited control over the axes and frame as argumentscol, lwd and lty refer to the contour lines(rather than being general graphical parameters).  For more control,add contours to a plot, or add axes and frame to a contour plot.
contour is a generic function with only a default method inbase R.The methods for positioning the labels on contours are "simple"(draw at the edge of the plot, overlaying the contour line),"edge" (draw at the edge of the plot, embedded in the contourline, with no labels overlapping) and "flattest"(draw on the flattest section of the contour, embedded in thecontour line, with no labels overlapping).  The second and third maynot draw a label on every contour line.For information about vector fonts, see thehelp for text and Hershey.Notice that contour interprets the z matrix as a table off(x[i], y[j]) values, so that the x axis corresponds to rownumber and the y axis to column number, with column 1 at the bottom,i.e. a 90 degree counter-clockwise rotation of the conventional textuallayout.Vector (of length > 1) col, lty, and lwd areapplied along levels and recycled, see the Examples.Alternatively, use contourplot from thelattice package where the formula notation allowsto use vectors x, y, and z of the same length.There is limited control over the axes and frame as argumentscol, lwd and lty refer to the contour lines(rather than being general graphical parameters).  For more control,add contours to a plot, or add axes and frame to a contour plot.
In the case of a single conditioning variable a, when bothrows and columns are unspecified, a ‘close tosquare’ layout is chosen with columns >= rows.In the case of multiple rows, the order of the panelplots is from the bottom and from the left (corresponding toincreasing a, typically).A panel function should not attempt to start a new plot, but just plotwithin a given coordinate system: thus plot and boxplotare not panel functions.The rendering of arguments xlab andylab is not controlled by par argumentscex.lab and font.lab even though they are plotted bymtext rather than title.
The function or expression expr (for curve) or functionx (for plot) is evaluated at n points equallyspaced over the range [from, to].  The points determined inthis way are then plotted.If either from or to is NULL, it defaults to thecorresponding element of xlim if that is not NULL.What happens when neither from/to nor xlimspecifies both x-limits is a complex story.  Forplot(<function>) and for curve(add = FALSE) the defaultsare (0, 1).  For curve(add = NA) and curve(add =  TRUE) the defaults are taken from the x-limits used for the previousplot.  (This differs from versions of R prior to 2.14.0.)The value of log is used both to specify the plot axes (unlessadd = TRUE) and how ‘equally spaced’ is interpreted: ifthe x component indicates log-scaling, the points at which theexpression or function is plotted are equally spaced on log scale.The default value of log is taken from the current plot whenadd = TRUE, whereas if add = NA the x component is takenfrom the existing plot (if any) and the y component defaults tolinear.  For add = FALSE the default is ""This used to be a quick hack which now seems to serve a useful purpose,but can give bad results for functions which are not smooth.For expensive-to-compute expressions, you should use smarter tools.The way curve handles expr has caused confusion.  Itfirst looks to see if expr is a name (also known as asymbol), in which case it is taken to be the name of a function, andexpr is replaced by a call to expr with a singleargument with name given by xname.  Otherwise it checks thatexpr is either a call or an expression, and thatit contains a reference to the variable given by xname (usingall.vars): anything else is an error.  Then expris evaluated in an environment which supplies a vector of name givenby xname of length n, and should evaluate to an objectof length n.  Note that this means that curve(x, ...) istaken as a request to plot a function named x (and it is usedas such in the function method for plot).The plot method can be called directly as plot.function.
NA
The first call to split.screen places R into split-screenmode.  The other split-screen functions only work within this mode.While in this mode, certain other commands should be avoided (see theWarnings section below).  Split-screen mode is exited by the commandclose.screen(all = TRUE).If the current screen is closed, close.screen sets the currentscreen to be the next larger screen number if there is one, otherwiseto the first available screen.
The values to be plotted can contain NAs.  Rectangles with twoor more corner values are NA are omitted entirely: where thereis a single NA value the triangle opposite the NA isomitted.Values to be plotted can be infinite: the effect is similar to thatdescribed for NA values..filled.contour is a ‘bare bones’ interface to addjust the contour plot to an already-set-up plot region.  It is isintended for programmatic use, and the programmer isresponsible for checking the conditions on the arguments.
The fourfold display is designed for the display of 2 by 2 by ktables.Following suitable standardization, the cell frequenciesf[i,j] of each 2 by 2 table are shown as a quartercircle whose radius is proportional tosqrt(f[i,j]) so that its area is proportional tothe cell frequency.  An association (odds ratio different from 1)between the binary row and column variables is indicated by thetendency of diagonally opposite cells in one direction to differ insize from those in the other direction; color is used to show thisdirection.  Confidence rings for the odds ratio allow a visual test ofthe null of no association; the rings for adjacent quadrants overlapif and only if the observed counts are consistent with the null hypothesis.Typically, the number k corresponds to the number of levels of astratifying variable, and it is of interest to see whether theassociation is homogeneous across strata.  The fourfold displayvisualizes the pattern of association.  Note that the confidence ringsfor the individual odds ratios are not adjusted for multiple testing.
The new page is painted with the background colour(par("bg")), which is often transparent.  For deviceswith a canvas colour (the on-screen devices X11,windows and quartz), the window is first painted with thecanvas colour and then the background colour.There are two hooks called "before.plot.new" and"plot.new" (see setHook) called immediatelybefore and after advancing the frame. The latter is usedin the testing code to annotate the new page. The hook function(s) arecalled with no argument.  (If the value is a character string,get is called on it from within the graphics namespace.)
The coordinate systems areuser coordinates.inches.the device coordinate system.normalized device coordinates.normalized figure coordinates.normalized plot coordinates.normalized inner region coordinates.  (The‘inner region’ is that inside the outer margins.)lines of margin (based on mex).lines of text (based on cex).(These names can be partially matched.)  For the ‘normalized’coordinate systems the lower left has value 0 and the top rightvalue 1.Device coordinates are those in which the device works: they areusually in pixels where that makes sense and in big points (1/72 inch)otherwise (e.g., pdf and postscript).
The coordinate systems areuser coordinates.inches.the device coordinate system.normalized device coordinates.normalized figure coordinates.normalized plot coordinates.normalized inner region coordinates.  (The‘inner region’ is that inside the outer margins.)lines of margin (based on mex).lines of text (based on cex).(These names can be partially matched.)  For the ‘normalized’coordinate systems the lower left has value 0 and the top rightvalue 1.Device coordinates are those in which the device works: they areusually in pixels where that makes sense and in big points (1/72 inch)otherwise (e.g., pdf and postscript).
NA
The definition of histogram differs by source (withcountry-specific biases).  R's default with equi-spaced breaks (alsothe default) is to plot the counts in the cells defined bybreaks.  Thus the height of a rectangle is proportional tothe number of points falling into the cell, as is the areaprovided the breaks are equally-spaced.The default with non-equi-spaced breaks is to givea plot of area one, in which the area of the rectangles is thefraction of the data points falling in the cells.If right = TRUE (default), the histogram cells are intervalsof the form (a, b], i.e., they include their right-hand endpoint,but not their left one, with the exception of the first cell wheninclude.lowest is TRUE.For right = FALSE, the intervals are of the form [a, b),and include.lowest means ‘include highest’.A numerical tolerance of 1e-7 times the median bin size(for more than four bins, otherwise the median is substituted) isapplied when counting entries on the edges of bins.  This is notincluded in the reported breaks nor in the calculation ofdensity.The default for breaks is "Sturges": seenclass.Sturges.  Other names for which algorithmsare supplied are "Scott" and "FD" /"Freedman-Diaconis" (with corresponding functionsnclass.scott and nclass.FD).Case is ignored and partial matching is used.Alternatively, a function can be supplied whichwill compute the intended number of breaks or the actual breakpointsas a function of x.
The definition of histogram differs by source (withcountry-specific biases).  R's default with equi-spaced breaks (alsothe default) is to plot the counts in the cells defined bybreaks.  Thus the height of a rectangle is proportional tothe number of points falling into the cell, as is the areaprovided the breaks are equally-spaced.The default with non-equi-spaced breaks is to givea plot of area one, in which the area of the rectangles is thefraction of the data points falling in the cells.If right = TRUE (default), the histogram cells are intervalsof the form (a, b], i.e., they include their right-hand endpoint,but not their left one, with the exception of the first cell wheninclude.lowest is TRUE.For right = FALSE, the intervals are of the form [a, b),and include.lowest means ‘include highest’.A numerical tolerance of 1e-7 times the median bin size(for more than four bins, otherwise the median is substituted) isapplied when counting entries on the edges of bins.  This is notincluded in the reported breaks nor in the calculation ofdensity.The default for breaks is "Sturges": seenclass.Sturges.  Other names for which algorithmsare supplied are "Scott" and "FD" /"Freedman-Diaconis" (with corresponding functionsnclass.scott and nclass.FD).Case is ignored and partial matching is used.Alternatively, a function can be supplied whichwill compute the intended number of breaks or the actual breakpointsas a function of x.
identify is a generic function, and only the default method isdescribed here.identify is only supported on screen devices such asX11, windows and quartz.  On other devices thecall will do nothing.Clicking near (as defined by tolerance) a point adds it to thelist of identified points.  Points can be identified only once, and ifthe point has already been identified or the click is notnear any of the points a message is printed immediately onthe R console.If plot is TRUE, the point is labelled with thecorresponding element of labels.  If atpen is false (thedefault) the labels are placed below, to the left, above or to theright of the identified point, depending on where the pointer wasrelative to the point.  If atpen is true, thelabels are placed with the bottom left of the string's box at thepointer.For the usual X11 device the identification process isterminated by pressing any mouse button other than the first.For the quartz device the process is terminated bypressing either the pop-up menu equivalent (usually second mousebutton or Ctrl-click) or the ESC key.On most devices which support identify, successful selection ofa point is indicated by a bell sound unlessoptions(locatorBell = FALSE) has been set.If the window is resized or hidden and then exposed before the identificationprocess has terminated, any labels drawn by identifywill disappear.  These will reappear once the identification process hasterminated and the window is resized or hidden and exposed again.This is because the labels drawn by identify are notrecorded in the device's display list until the identification process hasterminated.If you interrupt the identify call this leaves the graphicsdevice in an undefined state, with points labelled but labels notrecorded in the display list.  Copying a device in that statewill give unpredictable results.
The length of x should be equal to the nrow(z)+1 ornrow(z).  In the first case x specifies the boundariesbetween the cells: in the second case x specifies the midpointsof the cells.  Similar reasoning applies to y.  It probablyonly makes sense to specify the midpoints of an equally-spacedgrid.  If you specify just one row or column and a length-one xor y, the whole user area in the corresponding direction isfilled. For logarithmic x or y axes the boundaries betweencells must be specified.Rectangles corresponding to missing values are not plotted (and so aretransparent and (unless add = TRUE) the default backgroundpainted in par("bg") will show through and if that istransparent, the canvas colour will be seen).If breaks is specified then zlim is unused and thealgorithm used follows cut, so intervals are closed onthe right and open on the left except for the lowest interval which isclosed at both ends.The axes (where plotted) make use of the classes of xlim andylim (and hence by default the classes of x andy): this will mean that for example dates are labelled assuch.Notice that image interprets the z matrix as a table off(x[i], y[j]) values, so that the x axis corresponds to rownumber and the y axis to column number, with column 1 at the bottom,i.e. a 90 degree counter-clockwise rotation of the conventionalprinted layout of a matrix.Images for large z on a regular grid are rendered moreefficiently with useRaster = TRUE and can prevent rareanti-aliasing artifacts, but may not be supported by all graphicsdevices.  Some devices (such as postscript and X11(type =  "Xlib")) which do not support semi-transparent colours may emitmissing values as white rather than transparent, and there may belimitations on the size of a raster image.  (Problems with therendering of raster images have been reported by users ofwindows() devices under Remote Desktop, at least under itsdefault settings.)The graphics files in PDF and PostScript can be much smaller underthis option.If useRaster is not specified, raster images are used when thegetOption("preferRaster") is true, the grid is regularand either dev.capabilities("rasterImage")$rasterImageis "yes" or it is "non-missing" and there are no missingvalues.
The length of x should be equal to the nrow(z)+1 ornrow(z).  In the first case x specifies the boundariesbetween the cells: in the second case x specifies the midpointsof the cells.  Similar reasoning applies to y.  It probablyonly makes sense to specify the midpoints of an equally-spacedgrid.  If you specify just one row or column and a length-one xor y, the whole user area in the corresponding direction isfilled. For logarithmic x or y axes the boundaries betweencells must be specified.Rectangles corresponding to missing values are not plotted (and so aretransparent and (unless add = TRUE) the default backgroundpainted in par("bg") will show through and if that istransparent, the canvas colour will be seen).If breaks is specified then zlim is unused and thealgorithm used follows cut, so intervals are closed onthe right and open on the left except for the lowest interval which isclosed at both ends.The axes (where plotted) make use of the classes of xlim andylim (and hence by default the classes of x andy): this will mean that for example dates are labelled assuch.Notice that image interprets the z matrix as a table off(x[i], y[j]) values, so that the x axis corresponds to rownumber and the y axis to column number, with column 1 at the bottom,i.e. a 90 degree counter-clockwise rotation of the conventionalprinted layout of a matrix.Images for large z on a regular grid are rendered moreefficiently with useRaster = TRUE and can prevent rareanti-aliasing artifacts, but may not be supported by all graphicsdevices.  Some devices (such as postscript and X11(type =  "Xlib")) which do not support semi-transparent colours may emitmissing values as white rather than transparent, and there may belimitations on the size of a raster image.  (Problems with therendering of raster images have been reported by users ofwindows() devices under Remote Desktop, at least under itsdefault settings.)The graphics files in PDF and PostScript can be much smaller underthis option.If useRaster is not specified, raster images are used when thegetOption("preferRaster") is true, the grid is regularand either dev.capabilities("rasterImage")$rasterImageis "yes" or it is "non-missing" and there are no missingvalues.
Figure i is allocated a region composed from a subsetof these rows and columns, based on the rows and columnsin which i occurs in mat.The respect argument controls whether a unit column-width isthe same physical measurement on the device as a unit row-height.There is a limit (currently 200) for the numbers of rows and columnsin the layout, and also for the total number of cells (10007).layout.show(n) plots (part of) the current layout, namely theoutlines of the next n figures.lcm is a trivial function, to be used as the interfacefor specifying absolute dimensions for the widths andheights arguments of layout().
Figure i is allocated a region composed from a subsetof these rows and columns, based on the rows and columnsin which i occurs in mat.The respect argument controls whether a unit column-width isthe same physical measurement on the device as a unit row-height.There is a limit (currently 200) for the numbers of rows and columnsin the layout, and also for the total number of cells (10007).layout.show(n) plots (part of) the current layout, namely theoutlines of the next n figures.lcm is a trivial function, to be used as the interfacefor specifying absolute dimensions for the widths andheights arguments of layout().
Figure i is allocated a region composed from a subsetof these rows and columns, based on the rows and columnsin which i occurs in mat.The respect argument controls whether a unit column-width isthe same physical measurement on the device as a unit row-height.There is a limit (currently 200) for the numbers of rows and columnsin the layout, and also for the total number of cells (10007).layout.show(n) plots (part of) the current layout, namely theoutlines of the next n figures.lcm is a trivial function, to be used as the interfacefor specifying absolute dimensions for the widths andheights arguments of layout().
Arguments x, y, legend are interpreted in anon-standard way to allow the coordinates to be specified viaone or two arguments.  If legend is missing and y is notnumeric, it is assumed that the second argument is intended to belegend and that the first argument specifies the coordinates.The coordinates can be specified in any way which is accepted byxy.coords.  If this gives the coordinates of one point,it is used as the top-left coordinate of the rectangle containing thelegend.  If it gives the coordinates of two points, these specifyopposite corners of the rectangle (either pair of corners, in anyorder).The location may also be specified by setting x to a singlekeyword from the list "bottomright", "bottom","bottomleft", "left", "topleft","top", "topright", "right" and"center". This places the legend on the inside of the plotframe at the given location. Partial argument matching is used.  Theoptional inset argument specifies how far the legend is insetfrom the plot margins.  If a single value is given, it is used forboth margins; if two values are given, the first is used for x-distance, the second for y-distance.Attribute arguments such as col, pch, lty, etc,are recycled if necessary: merge is not.  Set entries oflty to 0 or set entries of lwd to NA tosuppress lines in corresponding legend entries; set pchvalues to NA to suppress points.Points are drawn after lines in order that they can cover theline with their background color pt.bg, if applicable.See the examples for how to right-justify labels.Since they are not used for Unicode code points, values -31:-1are silently omitted, as are NA and "" values.
The coordinates can be passed in a plotting structure(a list with x and y components), a two-column matrix, atime series, ....  See xy.coords.  If suppliedseparately, they must be of the same length.The coordinates can contain NA values. If a point containsNA in either its x or y value, it is omitted fromthe plot, and lines are not drawn to or from such points.  Thusmissing values can be used to achieve breaks in lines.For type = "h", col can be a vector and will be recycledas needed.lwd can be a vector: its first element will apply to lines butthe whole vector to symbols (recycled as necessary).
The coordinates can be passed in a plotting structure(a list with x and y components), a two-column matrix, atime series, ....  See xy.coords.  If suppliedseparately, they must be of the same length.The coordinates can contain NA values. If a point containsNA in either its x or y value, it is omitted fromthe plot, and lines are not drawn to or from such points.  Thusmissing values can be used to achieve breaks in lines.For type = "h", col can be a vector and will be recycledas needed.lwd can be a vector: its first element will apply to lines butthe whole vector to symbols (recycled as necessary).
locator is only supported on screen devices such asX11, windows and quartz.  On other devices thecall will do nothing.Unless the process is terminated prematurely by the user (see below)at most n positions are determined.For the usual X11 device the identification process isterminated by pressing any mouse button other than the first.For the quartz device the process is terminated bypressing the ESC key.The current graphics parameters apply just as if plot.defaulthas been called with the same value of type. The plotting ofthe points and lines is subject to clipping, but locations outside thecurrent clipping rectangle will be returned.On most devices which support locator, successful selection ofa point is indicated by a bell sound unlessoptions(locatorBell = FALSE) has been set.If the window is resized or hidden and then exposed before the inputprocess has terminated, any lines or points drawn by locatorwill disappear.  These will reappear once the input process hasterminated and the window is resized or hidden and exposed again.This is because the points and lines drawn by locator are notrecorded in the device's display list until the input process hasterminated.
matplot(x,y, ..) is basically a wrapper for calling (the generic function) plot(x[,1], y[,1], ..)for the first columns (only if add = TRUE). calling (the generic) lines(x[,j], y[,j], ..) forsubsequent columns.Since R 4.0.0, care is taken to keep the class(.) ofx and y, such that the corresponding plot() andlines() methods will be called.Points involving missing values are not plotted.The first column of x is plotted against the first column ofy, the second column of x against the second column ofy, etc.  If one matrix has fewer columns, plotting will cycleback through the columns again.  (In particular, either x ory may be a vector, against which all columns of the otherargument will be plotted.)The first element of col, cex, lty, lwd is used to plot the axesas well as the first line.Because plotting symbols are drawn with lines and because thesefunctions may be changing the line style, you should probably specifylty = 1 when using plotting symbols.
matplot(x,y, ..) is basically a wrapper for calling (the generic function) plot(x[,1], y[,1], ..)for the first columns (only if add = TRUE). calling (the generic) lines(x[,j], y[,j], ..) forsubsequent columns.Since R 4.0.0, care is taken to keep the class(.) ofx and y, such that the corresponding plot() andlines() methods will be called.Points involving missing values are not plotted.The first column of x is plotted against the first column ofy, the second column of x against the second column ofy, etc.  If one matrix has fewer columns, plotting will cycleback through the columns again.  (In particular, either x ory may be a vector, against which all columns of the otherargument will be plotted.)The first element of col, cex, lty, lwd is used to plot the axesas well as the first line.Because plotting symbols are drawn with lines and because thesefunctions may be changing the line style, you should probably specifylty = 1 when using plotting symbols.
matplot(x,y, ..) is basically a wrapper for calling (the generic function) plot(x[,1], y[,1], ..)for the first columns (only if add = TRUE). calling (the generic) lines(x[,j], y[,j], ..) forsubsequent columns.Since R 4.0.0, care is taken to keep the class(.) ofx and y, such that the corresponding plot() andlines() methods will be called.Points involving missing values are not plotted.The first column of x is plotted against the first column ofy, the second column of x against the second column ofy, etc.  If one matrix has fewer columns, plotting will cycleback through the columns again.  (In particular, either x ory may be a vector, against which all columns of the otherargument will be plotted.)The first element of col, cex, lty, lwd is used to plot the axesas well as the first line.Because plotting symbols are drawn with lines and because thesefunctions may be changing the line style, you should probably specifylty = 1 when using plotting symbols.
This is a generic function.  It currently has a default method(mosaicplot.default) and a formula interface(mosaicplot.formula).Extended mosaic displays visualize standardized residuals of aloglinear model for the table by color and outline of the mosaic'stiles.  (Standardized residuals are often referred to a standardnormal distribution.)  Cells representing negative residuals are drawnin shaded of red and with broken borders; positive ones are drawn inblue with solid borders.For the formula method, if data is an object inheriting fromclass "table" or class "ftable" or an array with morethan 2 dimensions, it is taken as a contingency table, and hence allentries should be non-negative.  In this case the left-hand side offormula should be empty and the variables on the right-handside should be taken from the names of the dimnames attribute of thecontingency table.  A marginal table of these variables is computed,and a mosaic plot of that table is produced.Otherwise, data should be a data frame or matrix, list orenvironment containing the variables to be cross-tabulated.  In thiscase, after possibly selecting a subset of the data as specified bythe subset argument, a contingency table is computed from thevariables given in formula, and a mosaic is produced fromthis.See Emerson (1998) for more information and a case study withtelevision viewer data from Nielsen Media Research.Missing values are not supported except via an na.actionfunction when data contains variables to be cross-tabulated.A more flexible and extensible implementation of mosaic plots writtenin the grid graphics system is provided in the functionmosaic in the contributed package vcd(Meyer, Zeileis and Hornik, 2006).
The user coordinates in the outer margins always range from zero toone, and are not affected by the user coordinates in the figureregion(s) — R differs here from other implementations of S.All of the named arguments can be vectors, and recycling will takeplace to plot as many strings as the longest of the vector arguments.Note that a vector adj has a different meaning fromtext.  adj = 0.5 will centre the string, but forouter = TRUE on the device region rather than the plot region.Parameter las will determine the orientation of the string(s).For strings plotted perpendicular to the axis the default justificationis to place the end of the string nearest the axis on the specifiedline.  (Note that this differs from S, which uses srt ifat is supplied and las if it is not.  Parametersrt is ignored in R.)Note that if the text is to be plotted perpendicular to the axis,adj determines the justification of the string and theposition along the axis unless at is specified.Graphics parameter "ylbias" (see par) determineshow the text baseline is placed relative to the nominal line.
The ijth scatterplot contains x[,i] plotted againstx[,j].  The scatterplot can be customised by setting panelfunctions to appear as something completely different. Theoff-diagonal panel functions are passed the appropriate columns ofx as x and y: the diagonal panel function (ifany) is passed a single column, and the text.panel function ispassed a single (x, y) location and the column name.Setting some of these panel functions to NULL isequivalent to not drawing anything there.The graphical parameters pch and col can be usedto specify a vector of plotting symbols and colors to be used in theplots.The graphical parameter oma will be set bypairs.default unless supplied as an argument.A panel function should not attempt to start a new plot, but just plotwithin a given coordinate system: thus plot and boxplotare not panel functions.By default, missing values are passed to the panel functions and willoften be ignored within a panel.  However, for the formula method andna.action = na.omit, all cases which contain a missing values forany of the variables are omitted completely (including when the scalesare selected).Arguments horInd and verInd were introduced in R3.2.0. If given the same value they can be used to select or re-ordervariables: with different ranges of consecutive values they can beused to plot rectangular windows of a full pairs plot; in the lattercase ‘diagonal’ refers to the diagonal of the full plot.
The ijth scatterplot contains x[,i] plotted againstx[,j].  The scatterplot can be customised by setting panelfunctions to appear as something completely different. Theoff-diagonal panel functions are passed the appropriate columns ofx as x and y: the diagonal panel function (ifany) is passed a single column, and the text.panel function ispassed a single (x, y) location and the column name.Setting some of these panel functions to NULL isequivalent to not drawing anything there.The graphical parameters pch and col can be usedto specify a vector of plotting symbols and colors to be used in theplots.The graphical parameter oma will be set bypairs.default unless supplied as an argument.A panel function should not attempt to start a new plot, but just plotwithin a given coordinate system: thus plot and boxplotare not panel functions.By default, missing values are passed to the panel functions and willoften be ignored within a panel.  However, for the formula method andna.action = na.omit, all cases which contain a missing values forany of the variables are omitted completely (including when the scalesare selected).Arguments horInd and verInd were introduced in R3.2.0. If given the same value they can be used to select or re-ordervariables: with different ranges of consecutive values they can beused to plot rectangular windows of a full pairs plot; in the lattercase ‘diagonal’ refers to the diagonal of the full plot.
NA
Each device has its own set of graphical parameters.  If the currentdevice is the null device, par will open a new device beforequerying/setting parameters.  (What device is controlled byoptions("device").)Parameters are queried by giving one or more character vectors ofparameter names to par.par() (no arguments) or par(no.readonly = TRUE) is used toget all the graphical parameters (as a named list).  Theirnames are currently taken from the unexported variablegraphics:::.Pars.R.O. indicates read-only arguments: Thesemay only be used in queries and cannot be set.  ("cin","cra", "csi", "cxy", "din" and"page" are always read-only.)Several parameters can only be set by a call to par():"ask","fig", "fin","lheight","mai", "mar", "mex","mfcol", "mfrow", "mfg","new","oma", "omd", "omi","pin", "plt", "ps", "pty","usr","xlog", "ylog","ylbias"The remaining parameters can also be set as arguments (often via...) to high-level plot functions such asplot.default, plot.window,points, lines, abline,axis, title, text,mtext, segments, symbols,arrows, polygon, rect,box, contour, filled.contourand image.  Such settings will be active during theexecution of the function, only.  However, see the comments onbg, cex, col, lty, lwd andpch which may be taken as arguments to certain plotfunctions rather than as graphical parameters.The meaning of ‘character size’ is not well-defined: this isset up for the device taking pointsize into account but oftennot the actual font family in use.  Internally the corresponding pars(cra, cin, cxy and csi) are used only toset the inter-line spacing used to convert mar and omato physical margins.  (The same inter-line spacing multiplied bylheight is used for multi-line strings in text andstrheight.)Note that graphical parameters are suggestions: plotting functions anddevices need not make use of them (and this is particularly true ofnon-default methods for e.g. plot).
The plots are produced by first transforming the (x,y,z)coordinates to the interval [0,1] using the limits supplied orcomputed from the range of the data.  The surface is then viewedby looking at the origin from a direction defined by thetaand phi.  If theta and phi are both zerothe viewing direction is directly down the negative y axis.Changing theta will vary the azimuth and changing phithe colatitude.There is a hook called "persp" (see setHook)called after the plot is completed, which is used in thetesting code to annotate the plot page.  The hook function(s) arecalled with no argument.Notice that persp interprets the z matrix as a table off(x[i], y[j]) values, so that the x axis corresponds to rownumber and the y axis to column number, with column 1 at the bottom,so that with the standard rotation angles, the top left corner of thematrix is displayed at the left hand side, closest to the user.The sizes and fonts of the axis labels and the annotations forticktype = "detailed" are controlled by graphics parameters"cex.lab"/"font.lab" and"cex.axis"/"font.axis" respectively.The bounding box is drawn with edges of faces facing away from theviewer (and hence at the back of the box) with solid lines and otheredges dashed and on top of the surface.  This (and the plotting of theaxes) assumes that the axis limits are chosen so that the surfaceis within the box, and the function will warn if this is not the case.
NA
Commonly used graphical parameters are:The colors for lines and points.  Multiple colors can bespecified so that each point can be given its own color.  If thereare fewer colors than points they are recycled in the standardfashion.  Lines will all be plotted in the first colour specified.a vector of background colors for open plot symbols, seepoints.  Note: this is not the same settingas par("bg").a vector of plotting characters or symbols:see points.a numerical vector giving the amount by whichplotting characters and symbols should be scaled relative to thedefault.  This works as a multiple of par("cex").NULL and NA are equivalent to 1.0.  Note thatthis does not affect annotation: see below.a vector of line types, see par.settings for main- and sub-title and axis annotation, seetitle and par.a vector of line widths, see par.
Commonly used graphical parameters are:The colors for lines and points.  Multiple colors can bespecified so that each point can be given its own color.  If thereare fewer colors than points they are recycled in the standardfashion.  Lines will all be plotted in the first colour specified.a vector of background colors for open plot symbols, seepoints.  Note: this is not the same settingas par("bg").a vector of plotting characters or symbols:see points.a numerical vector giving the amount by whichplotting characters and symbols should be scaled relative to thedefault.  This works as a multiple of par("cex").NULL and NA are equivalent to 1.0.  Note thatthis does not affect annotation: see below.a vector of line types, see par.settings for main- and sub-title and axis annotation, seetitle and par.a vector of line widths, see par.
The supplied function will be called once for each level of eachfactor in the design and the plot will show these summary values.  Thelevels of a particular factor are shown along a vertical line, and theoverall value of fun() for the response is drawn as ahorizontal line.
The function or expression expr (for curve) or functionx (for plot) is evaluated at n points equallyspaced over the range [from, to].  The points determined inthis way are then plotted.If either from or to is NULL, it defaults to thecorresponding element of xlim if that is not NULL.What happens when neither from/to nor xlimspecifies both x-limits is a complex story.  Forplot(<function>) and for curve(add = FALSE) the defaultsare (0, 1).  For curve(add = NA) and curve(add =  TRUE) the defaults are taken from the x-limits used for the previousplot.  (This differs from versions of R prior to 2.14.0.)The value of log is used both to specify the plot axes (unlessadd = TRUE) and how ‘equally spaced’ is interpreted: ifthe x component indicates log-scaling, the points at which theexpression or function is plotted are equally spaced on log scale.The default value of log is taken from the current plot whenadd = TRUE, whereas if add = NA the x component is takenfrom the existing plot (if any) and the y component defaults tolinear.  For add = FALSE the default is ""This used to be a quick hack which now seems to serve a useful purpose,but can give bad results for functions which are not smooth.For expensive-to-compute expressions, you should use smarter tools.The way curve handles expr has caused confusion.  Itfirst looks to see if expr is a name (also known as asymbol), in which case it is taken to be the name of a function, andexpr is replaced by a call to expr with a singleargument with name given by xname.  Otherwise it checks thatexpr is either a call or an expression, and thatit contains a reference to the variable given by xname (usingall.vars): anything else is an error.  Then expris evaluated in an environment which supplies a vector of name givenby xname of length n, and should evaluate to an objectof length n.  Note that this means that curve(x, ...) istaken as a request to plot a function named x (and it is usedas such in the function method for plot).The plot method can be called directly as plot.function.
The new page is painted with the background colour(par("bg")), which is often transparent.  For deviceswith a canvas colour (the on-screen devices X11,windows and quartz), the window is first painted with thecanvas colour and then the background colour.There are two hooks called "before.plot.new" and"plot.new" (see setHook) called immediatelybefore and after advancing the frame. The latter is usedin the testing code to annotate the new page. The hook function(s) arecalled with no argument.  (If the value is a character string,get is called on it from within the graphics namespace.)
If asp is a finite positive value then the window isset up so that one data unit in the y direction is equal in length toasp * one data unit in the x direction.Note that in this case, par("usr") is no longerdetermined by, e.g., par("xaxs"), but rather by asp andthe device's aspect ratio.  (See what happens if you interactivelyresize the plot device after running the example below!)The special case asp == 1 produces plots where distancesbetween points are represented accurately on screen.  Values withasp > 1 can be used to produce more accurate maps when usinglatitude and longitude.Note that the coordinate ranges will be extended by 4% if theappropriate graphical parameter xaxs or yaxs has value"r" (which is the default).To reverse an axis, use xlim or ylim of the formc(hi, lo).The function attempts to produce a plausible set of scales if one orboth of xlim and ylim is of length one or the two valuesgiven are identical, but it is better to avoid that case.Usually, one should rather use the higher-level functions such asplot, hist, image, ...,instead and refer to their help pages for explanation of thearguments.A side-effect of the call is to set up the usr, xaxp andyaxp graphical parameters.  (It is for the latter two thatlab is used.)
The arguments pch, col, bg, cex,lwd may be vectors and may be recycled, depending ontype: see points and lines forspecifics.  In particular note that lwd is treated as a vectorfor points and as a single (first) value for lines.cex is a numeric factor in addition to par("cex") whichaffects symbols and characters as drawn by type "p","o", "b" and "c".
The coordinates can be passed in a plotting structure(a list with x and y components), a two-column matrix, atime series, ....  See xy.coords.  If suppliedseparately, they must be of the same length.Graphical parameters commonly used areplotting ‘character’, i.e., symbol to use.This can either be a single character or an integer code for oneof a set of graphics symbols.  The full set of S symbols isavailable with pch = 0:18, see the examples below.(NB: R uses circles instead of the octagons used in S.)Value pch = "." (equivalently pch = 46) is handledspecially.  It is a rectangle of side 0.01 inch (scaled bycex).  In addition, if cex = 1 (the default), eachside is at least one pixel (1/72 inch on the pdf,postscript and xfig devices).For other text symbols, cex = 1 corresponds to the defaultfontsize of the device, often specified by an argumentpointsize.  For pch in 0:25 the default sizeis about 75% of the character height (see par("cin")).color code or name, see par.background (fill) color for the open plotsymbols given by pch = 21:25.character (or symbol) expansion: a numerical vector.This works as a multiple of par("cex").line width for drawing symbols see par.Others less commonly used are lty and lwd fortypes such as "b" and "l".The graphical parameters pch, col, bg,cex and lwd can be vectors (which will be recycled asneeded) giving a value for each point plotted.  If lines are to beplotted (e.g., for type = "b") the first element of lwdis used.Points whose x, y, pch, col or cexvalue is NA are omitted from the plot.
The coordinates can be passed in a plotting structure(a list with x and y components), a two-column matrix, atime series, ....  See xy.coords.  If suppliedseparately, they must be of the same length.Graphical parameters commonly used areplotting ‘character’, i.e., symbol to use.This can either be a single character or an integer code for oneof a set of graphics symbols.  The full set of S symbols isavailable with pch = 0:18, see the examples below.(NB: R uses circles instead of the octagons used in S.)Value pch = "." (equivalently pch = 46) is handledspecially.  It is a rectangle of side 0.01 inch (scaled bycex).  In addition, if cex = 1 (the default), eachside is at least one pixel (1/72 inch on the pdf,postscript and xfig devices).For other text symbols, cex = 1 corresponds to the defaultfontsize of the device, often specified by an argumentpointsize.  For pch in 0:25 the default sizeis about 75% of the character height (see par("cin")).color code or name, see par.background (fill) color for the open plotsymbols given by pch = 21:25.character (or symbol) expansion: a numerical vector.This works as a multiple of par("cex").line width for drawing symbols see par.Others less commonly used are lty and lwd fortypes such as "b" and "l".The graphical parameters pch, col, bg,cex and lwd can be vectors (which will be recycled asneeded) giving a value for each point plotted.  If lines are to beplotted (e.g., for type = "b") the first element of lwdis used.Points whose x, y, pch, col or cexvalue is NA are omitted from the plot.
The coordinates can be passed in a plotting structure(a list with x and y components), a two-column matrix,....  See xy.coords.It is assumed that the polygon is to be closed by joining the lastpoint to the first point.The coordinates can contain missing values.  The behaviour is similarto that of lines, except that instead of breaking a lineinto several lines, NA values break the polygon into severalcomplete polygons (including closing the last point to the firstpoint).  See the examples below.When multiple polygons are produced, the values of density,angle, col, border, and lty are recycledin the usual manner.Shading of polygons is only implemented for linear plots: if eitheraxis is on log scale then shading is omitted, with a warning.
The coordinates can be passed in a plotting structure(a list with x and y components), a two-column matrix,....  See xy.coords.It is assumed that the path is to be closed by joining the last point tothe first point.The coordinates can contain missing values.  The behaviour is similarto that of polygon, except that instead of breaking apolygon into several polygons, NA values break the path intoseveral sub-paths (including closing the last point to the first pointin each sub-path).  See the examples below.The distinction between a path and a polygon is that the formercan contain holes, as interpreted by the fill rule; these fill a region ifthe path border encircles it an odd or non-zero number of times,respectively.Hatched shading (as implemented for polygon()) is not(currently) supported.Not all graphics devices support this function: for examplexfig and pictex do not.
The positions supplied, i.e., xleft, ...,are relative to the current plotting region.  If the x-axis goes from100 to 200 then xleft should be larger than 100 and xrightshould be less than 200.  The position vectors will be recycled to thelength of the longest.Plotting raster images is not supported on all devices and may havelimitations where supported, for example (e.g., for postscriptand X11(type = "Xlib") is restricted to opaque colors).Problems with the rendering of raster images have been reported byusers of windows() devices under Remote Desktop, at least underits default settings.You should not expect a raster image to be re-sized when an on-screendevice is re-sized: whether it is is device-dependent.
The positions supplied, i.e., xleft, ...,are relative to the current plotting region.  If the x-axis goes from100 to 200 then xleft must be larger than 100 and xrightmust be less than 200.  The position vectors will be recycled to thelength of the longest.It is a graphics primitive used in hist,barplot, legend, etc.
Because of the way rug is implemented, only values of xthat fall within the plot region are included.  There will be awarning if any finite values are omitted, but non-finite values areomitted silently.
The first call to split.screen places R into split-screenmode.  The other split-screen functions only work within this mode.While in this mode, certain other commands should be avoided (see theWarnings section below).  Split-screen mode is exited by the commandclose.screen(all = TRUE).If the current screen is closed, close.screen sets the currentscreen to be the next larger screen number if there is one, otherwiseto the first available screen.
For each i, a line segment is drawn between the point(x0[i], y0[i]) and the point (x1[i], y1[i]).  Thecoordinate vectors will be recycled to the length of the longest.The graphical parameters col, lty and lwd can bevectors of length greater than one and will be recycled if necessary.
smoothScatter produces a smoothed version of a scatter plot.Two dimensional (kernel density) smoothing is performed bybkde2D from package KernSmooth.See the examples for how to use this function together withpairs.
spineplot creates either a spinogram or a spine plot.  It canbe called via spineplot(x, y) or spineplot(y ~ x) wherey is interpreted to be the dependent variable (and has to becategorical) and x the explanatory variable.  x can beeither categorical (then a spine plot is created) or numerical (then aspinogram is plotted).  Additionally, spineplot can also becalled with only a single argument which then has to be a 2-way table,interpreted to correspond to table(x, y).Both, spine plots and spinograms, are essentially mosaic plots withspecial formatting of spacing and shading.  Conceptually, they plotP(y | x) against P(x).  For the spine plot (where bothx and y are categorical), both quantities are approximatedby the corresponding empirical relative frequencies.  For thespinogram (where x is numerical), x is first discretized(by calling hist with breaks argument) and thenempirical relative frequencies are taken.Thus, spine plots can also be seen as a generalization of stacked barplots where not the heights but the widths of the bars corresponds tothe relative frequencies of x. The heights of the bars thencorrespond to the conditional relative frequencies of y inevery x group. Analogously, spinograms extend stackedhistograms.
The first call to split.screen places R into split-screenmode.  The other split-screen functions only work within this mode.While in this mode, certain other commands should be avoided (see theWarnings section below).  Split-screen mode is exited by the commandclose.screen(all = TRUE).If the current screen is closed, close.screen sets the currentscreen to be the next larger screen number if there is one, otherwiseto the first available screen.
Missing values are treated as 0.Each star plot or segment diagram represents one row of the inputx.  Variables (columns) start on the right and windcounterclockwise around the circle.  The size of the (scaled) columnis shown by the distance from the center to the point on the star orthe radius of the segment representing the variable.Only one page of output is produced.
Infinite and missing values in x are discarded.
Note that the ‘height’ of a string is determined only by thenumber of linefeeds ("\n") it contains: it is the (number oflinefeeds - 1) times the line spacing plus the height of "M" inthe selected font.  For an expression it is the height of thebounding box as computed by plotmath.  Thus in both cases it isan estimate of how far above the final baseline the typesetobject extends.  (It may also extend below the baseline.)  Theinter-line spacing is controlled by cex,par("lheight") and the ‘point size’ (but not theactual font in use).Measurements in "user" units (the default) are only availableafter plot.new has been called – otherwise an error isthrown.
Extensive examples of the use of this kind of plot can be found inBox, Hunter and Hunter or Seber and Wild.The dlab and glab labels may be used instead of xlaband ylab if those are not specified.  dlab applies to thecontinuous data axis (the X axis unless vertical is TRUE),glab to the group axis.
Note that the ‘height’ of a string is determined only by thenumber of linefeeds ("\n") it contains: it is the (number oflinefeeds - 1) times the line spacing plus the height of "M" inthe selected font.  For an expression it is the height of thebounding box as computed by plotmath.  Thus in both cases it isan estimate of how far above the final baseline the typesetobject extends.  (It may also extend below the baseline.)  Theinter-line spacing is controlled by cex,par("lheight") and the ‘point size’ (but not theactual font in use).Measurements in "user" units (the default) are only availableafter plot.new has been called – otherwise an error isthrown.
This is a generic function with default and formula methods.For number[i] == 1, a (slightly enlarged) usual plotting symbol(pch) is drawn.  For number[i] > 1, a small plottingsymbol is drawn and number[i] equi-angular ‘rays’emanate from it.If rotate = TRUE and number[i] >= 2, a random directionis chosen (instead of the y-axis) for the first ray.  The goal is tojitter the orientations of the sunflowers in order toprevent artefactual visual impressions.
Observations which have missing coordinates or missing sizeparameters are not plotted.  The exception to this is stars.In that case, the length of any ray which is NA is resetto zero.Argument inches controls the sizes of the symbols.  IfTRUE (the default), the symbols are scaled so that thelargest dimension of any symbol is one inch.  If a positive number isgiven the symbols are scaled to make largest dimension this size ininches (so TRUE and 1 are equivalent).  If inchesis FALSE, the units are taken to be those of the appropriateaxes.  (For circles, squares and stars the units of the x axis areused.  For boxplots, the lengths of the whiskers are regarded asdimensions alongside width and height when scaling by inches,and are otherwise interpreted in the units of the y axis.)Circles of radius zero are plotted at radius one pixel (which isdevice-dependent).  Circles of a very small non-zero radius may or maynot be visible, and may be smaller than circles of radius zero.  Onwindows devices circles are plotted at radius at least onepixel as some Windows versions omit smaller circles.
labels must be of type character orexpression (or be coercible to such a type).In the latter case, quite a bit ofmathematical notation is available such as sub- and superscripts,greek letters, fractions, etc.adj allows adjustment of the text position with respect to(x, y).Values of 0, 0.5, and 1 specify that (x, y) should align withthe left/bottom, middle andright/top of the text, respectively.  The default is for centered text, i.e.,adj = c(0.5, NA).  Accurate vertical centering needscharacter metric information on individual characters which isonly available on some devices.  Vertical alignment is done slightlydifferently for character strings and for expressions:adj = c(0,0) means to left-justify and to align on the baselinefor strings but on the bottom of the bounding box for expressions.This also affects vertical centering: for strings the centeringexcludes any descenders whereas for expressions it includes them.Using NA for strings centers them, including descenders.The pos and offset arguments can be used in conjunctionwith values returned by identify to recreate an interactivelylabelled plot.Text can be rotated by using graphical parameters srt(see par).  When adj is specified, a non-zerosrt rotates the label about (x, y).  If pos isspecified, srt rotates the text about the point on its boundingbox which is closest to (x, y): top center for pos = 1,right center for pos = 2, bottom center for pos = 3, andleft center for pos = 4.  The pos interface is not asuseful for rotated text because the result is no longer centeredvertically or horizontally with respect to (x, y).  At presentthere is no interface in the graphics package for directlyrotating text about its center which is achievable however by fiddlingwith adj and srt simultaneously.Graphical parameters col, cex and font can bevectors and will then be applied cyclically to the labels (andextra values will be ignored).  NA values of font arereplaced by par("font"), and similarly for col.Labels whose x, y or labels value is NAare omitted from the plot.What happens when font = 5 (the symbol font) is selected can beboth device- and locale-dependent.  Most often labels will beinterpreted in the Adobe symbol encoding, so e.g. "d"is delta, and "\300" is aleph.
labels must be of type character orexpression (or be coercible to such a type).In the latter case, quite a bit ofmathematical notation is available such as sub- and superscripts,greek letters, fractions, etc.adj allows adjustment of the text position with respect to(x, y).Values of 0, 0.5, and 1 specify that (x, y) should align withthe left/bottom, middle andright/top of the text, respectively.  The default is for centered text, i.e.,adj = c(0.5, NA).  Accurate vertical centering needscharacter metric information on individual characters which isonly available on some devices.  Vertical alignment is done slightlydifferently for character strings and for expressions:adj = c(0,0) means to left-justify and to align on the baselinefor strings but on the bottom of the bounding box for expressions.This also affects vertical centering: for strings the centeringexcludes any descenders whereas for expressions it includes them.Using NA for strings centers them, including descenders.The pos and offset arguments can be used in conjunctionwith values returned by identify to recreate an interactivelylabelled plot.Text can be rotated by using graphical parameters srt(see par).  When adj is specified, a non-zerosrt rotates the label about (x, y).  If pos isspecified, srt rotates the text about the point on its boundingbox which is closest to (x, y): top center for pos = 1,right center for pos = 2, bottom center for pos = 3, andleft center for pos = 4.  The pos interface is not asuseful for rotated text because the result is no longer centeredvertically or horizontally with respect to (x, y).  At presentthere is no interface in the graphics package for directlyrotating text about its center which is achievable however by fiddlingwith adj and srt simultaneously.Graphical parameters col, cex and font can bevectors and will then be applied cyclically to the labels (andextra values will be ignored).  NA values of font arereplaced by par("font"), and similarly for col.Labels whose x, y or labels value is NAare omitted from the plot.What happens when font = 5 (the symbol font) is selected can beboth device- and locale-dependent.  Most often labels will beinterpreted in the Adobe symbol encoding, so e.g. "d"is delta, and "\300" is aleph.
The labels passed to title can be character strings orlanguage objects (names, calls or expressions), or a listcontaining the string to be plotted, and a selection of the optionalmodifying graphical parameters cex=, col= andfont=.  Other objects will be coerced byas.graphicsAnnot.The position of main defaults to being vertically centered in(outer) margin 3 and justified horizontally according topar("adj") on the plot region (device region forouter = TRUE).The positions of xlab, ylab and sub areline (default for xlab and ylab beingpar("mgp")[1] and increased by 1 for sub) lines(of height par("mex")) into the appropriate margin, justifiedin the text direction according to par("adj") on theplot/device region.
NA
An X-spline is a line drawn relative to control points.  For eachcontrol point, the line may pass through (interpolate) the controlpoint or it may only approach (approximate) the control point;  thebehaviour is determined by a shape parameter for each control point.If the shape parameter is greater than zero, the spline approximatesthe control points (and is very similar to a cubic B-spline when theshape is 1).  If the shape parameter is less than zero, the splineinterpolates the control points (and is very similar to a Catmull-Romspline when the shape is -1).  If the shape parameter is 0, the splineforms a sharp corner at that control point.For open X-splines, the start and end control points must have a shapeof 0 (and non-zero values are silently converted to zero).For open X-splines, by default the start and end control points arereplicated before the curve is drawn.  A curve is drawn between(interpolating or approximating) the second and third of each set offour control points, so this default behaviour ensures that theresulting curve starts at the first control point you have specifiedand ends at the last control point.  The default behaviour can beturned off via the repEnds argument.
NA
NA
Used in such problems as Thurstonian scaling.  Although not technically matrix addition, as pointed out by Krus, there are many applications where the sum or difference of two vectors or matrices is a useful operation.  An alternative operation for vectors is  outer(x ,y , FUN="+") but this does not work for matrices.  
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
Alpha is one of several estimates of the internal consistency reliability of a test.Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islambda 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep).Alpha and G6 are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  A useful index of the quality of the test that is linear with the number of items and the average correlation is the Signal/Noise ratio where s/n = n r/(1-r)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative functions score.items and  cluster.cor will also score multiple scales and report more useful statistics. “Standardized" alpha is calculated from the inter-item correlations and will differ from raw alpha. Four alternative item-whole correlations are reported, three are conventional, one unique.  raw.r is the correlation of the item with the entire scale, not correcting for item overlap. std.r is the correlation of the item with the entire scale, if each item were standardized.  r.drop is the correlation of the item with the scale composed of the remaining items.  Although each of these are conventional statistics, they have the disadvantage that a) item overlap inflates the first and b) the scale is different for each item when an item is dropped. Thus, the fourth alternative, r.cor, corrects for the item overlap by subtracting the item variance but then replaces this with the best estimate of common variance, the smc. This is similar to a suggestion by Cureton (1966).If some items are to be reversed keyed then they can be specified by either item name or by item location.  (Look at the 3rd and 4th examples.)  Automatic reversal can also be done, and this is based upon the sign of the loadings on the first principal component (Example 5).  This requires the check.keys option to be TRUE.  Previous versions defaulted to have check.keys=TRUE, but some users complained that this made it too easy to find alpha without realizing that some items had been reversed (even though a warning was issued!).  Thus, I have set the default to be check.keys=FALSE with a warning that some items need to be reversed (if this is the case).  To suppress these warnings, set warnings=FALSE.  Scores are based upon the simple averages (or totals) of the items scored. Thus, if some items are missing, the scores reflect just the items answered.  This is particularly problematic if using total scores (with the cumulative=TRUE option).  To impute missing data using either means or medians, use the scoreItems function.   Reversed items are subtracted from the maximum + minimum item response for all the items.When using raw data, standard errors for the raw alpha are calculated using equation 2 and 3 from Duhhachek and Iacobucci (2004).  This is problematic because some simulations suggest these values are too small.  It is probably better to use bootstrapped values.alpha.ci finds confidence intervals using  the Feldt et al. (1987) procedure.  This procedure does not consider the internal structure of the test the way that the Duhachek and Iacobucci (2004) procedure does.  That is, the latter considers the variance of the covariances, while the Feldt procedure is based upon just the mean covariance.  In March, 2022, alpha.ci was finally fixed to follow the Feldt procedure. The confidence intervals reported by alpha use both the Feld and the Duhaceck and Iabocucci precedures.  Note that these match for large values of N, but differ for smaller values.  Because both of these procedures use normal theory, if you really care about confidence intervals, using the boot option (n.iter > 1) is recommended.Bootstrapped resamples are found if n.iter > 1.  These are returned as the boot object.  They may be plotted or described.  The 2.5% and 97.5% values are returned in the boot.ci object.
Alpha is one of several estimates of the internal consistency reliability of a test.Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islambda 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep).Alpha and G6 are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  A useful index of the quality of the test that is linear with the number of items and the average correlation is the Signal/Noise ratio where s/n = n r/(1-r)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative functions score.items and  cluster.cor will also score multiple scales and report more useful statistics. “Standardized" alpha is calculated from the inter-item correlations and will differ from raw alpha. Four alternative item-whole correlations are reported, three are conventional, one unique.  raw.r is the correlation of the item with the entire scale, not correcting for item overlap. std.r is the correlation of the item with the entire scale, if each item were standardized.  r.drop is the correlation of the item with the scale composed of the remaining items.  Although each of these are conventional statistics, they have the disadvantage that a) item overlap inflates the first and b) the scale is different for each item when an item is dropped. Thus, the fourth alternative, r.cor, corrects for the item overlap by subtracting the item variance but then replaces this with the best estimate of common variance, the smc. This is similar to a suggestion by Cureton (1966).If some items are to be reversed keyed then they can be specified by either item name or by item location.  (Look at the 3rd and 4th examples.)  Automatic reversal can also be done, and this is based upon the sign of the loadings on the first principal component (Example 5).  This requires the check.keys option to be TRUE.  Previous versions defaulted to have check.keys=TRUE, but some users complained that this made it too easy to find alpha without realizing that some items had been reversed (even though a warning was issued!).  Thus, I have set the default to be check.keys=FALSE with a warning that some items need to be reversed (if this is the case).  To suppress these warnings, set warnings=FALSE.  Scores are based upon the simple averages (or totals) of the items scored. Thus, if some items are missing, the scores reflect just the items answered.  This is particularly problematic if using total scores (with the cumulative=TRUE option).  To impute missing data using either means or medians, use the scoreItems function.   Reversed items are subtracted from the maximum + minimum item response for all the items.When using raw data, standard errors for the raw alpha are calculated using equation 2 and 3 from Duhhachek and Iacobucci (2004).  This is problematic because some simulations suggest these values are too small.  It is probably better to use bootstrapped values.alpha.ci finds confidence intervals using  the Feldt et al. (1987) procedure.  This procedure does not consider the internal structure of the test the way that the Duhachek and Iacobucci (2004) procedure does.  That is, the latter considers the variance of the covariances, while the Feldt procedure is based upon just the mean covariance.  In March, 2022, alpha.ci was finally fixed to follow the Feldt procedure. The confidence intervals reported by alpha use both the Feld and the Duhaceck and Iabocucci precedures.  Note that these match for large values of N, but differ for smaller values.  Because both of these procedures use normal theory, if you really care about confidence intervals, using the boot option (n.iter > 1) is recommended.Bootstrapped resamples are found if n.iter > 1.  These are returned as the boot object.  They may be plotted or described.  The 2.5% and 97.5% values are returned in the boot.ci object.
setCor returns the SE.residual and degrees of freedom.  These are converted to SSR and then an analysis of variance is used to compare two (or more) models. For omega or fa the change in the ML chisquare statistic as a function of change in df is reported.
The problem of making binary decisions about the state of the world is ubiquitous.  We see this in Null Hypothesis Significance Testing (NHST), medical diagnoses, and selection for occupations.  Variously known as NHST, Signal Detection Theory, clinical Assessment, or college admissions, all of these domains share the same two x two decision task.Although the underlying phenomena are probably continuous, a typical decision or  diagnostic situation makes dichotomous decisions: Accept or Reject, correctly identified, incorrectly identified.   In Signal Detection Theory, the world has two states: Noise versus Signal + Noise.  The decision is whether there is  a signal or not.  In diagnoses, it is whether to diagnose an illness or not given some noisy signal (e.g., an X-Ray, a set of diagnostic tests).In college admissions, we accept some students and reject others.  Four-Five years later we observe who "succeeds" or graduates. All of these decisions lead to four cells based upon a two x two categorization.  Given the true state of the world is Positive or Negative, and a rater assigns positive or negative ratings, then the resulting two by two table has True (Valid) Positives and True (Valid) Negatives on the diagonal and False Positives and False Negatives off the diagonal. When expressed as percentages of the total, then  Base Rates (BR) depend upon the state of the world, but Selection Ratios (SR) are  under the control of the person making the decision and affect the number of False Positives and the number of Valid Positives.Given a two x two table of counts or percentages Unfortunately, although this way of categorizing the data is typical in assessment (e.g., Wiggins 1973), and everything is expressed as percentages of the total, in some decision papers, VP are expressed as the ratio of VP to total positive decisions (e.g., Wickens, 1984).  This requires dividing through by the column totals (and represented as VP* and FP* in the table below).The relationships implied by these data can be summarized as a phi or tetrachoric  correlation between the raters and the world, or as a decision process with several alternative measures.  If we make the assumption that the two dimensions are continuous and were artificially dichotomised, then the tetrachoric correlation is an estimate of the continuous correlation between these two latent dimensions.  If we think of the data as truly representing two states e.g., vaccinated or not vaccinanated, dead or alive, then the phi coefficient is more appropriate.  Sensitivity, Specificity, Accuracy, Area Under the Curve, and d' (d prime). These measures  may be defined as Although only one point is found, we can form a graphical display of VP versus FP as a smooth curve as a function of the decision criterion. The smooth curve assumes normality whereas the other merely are the two line segments between the points (0,0), (FP,VP), (1,1). The resulting correlation between the inferred continuous state of the world and the dichotomous decision process is a biserial correlation.When using table input, the values can be counts and thus greater than 1 or merely probabilities which should add up to 1. Base Rates and Selection Ratios are proportions and thus less than 1. 
When examining multiple measures within subjects, it is sometimes useful to consider the variability of trial by trial observations in addition to the over all between trial variation.  The Mean Square of Successive Differences (mssd) and root mean square of successive differences (rmssd) is just σ^2 = Σ(x_i - x_{i+1})^2 /(n-lag)  Where n-lag is used because there are only n-lag cases.In the case of multiple subjects  (groups) with multiple observations per subject (group), specify the grouping variable will produce output for each group.   Similar functions are available in the matrixStats package. However, the varDiff function in that package is variance of the difference rather than the MeanSquare. This is just the variance and standard deviation applied to the result from the diff function.Perhaps useful when studying mood, the autoR function finds the autocorrelation for each item for the specified lag.  It also returns the rmssd (root means square successive difference). This is done by finding the correlation of the lag data. 
This is essentially a wrapper to the fa and pca combined with the faCor functions.  They are called repeatedly and then the weights from the resulting solutions are used to find the factor/component correlations. Although the default is do all factor solutions from 1 to the nfactors, this can be simplified by specifying just some of the factor solutions. Thus, for the 135 items of the spi, it is more reasonable to ask for 3,5, and 27 item solutions. The function bassAckward.diagram may be called using the diagram function or may be called directly.The output from bassAckward.diagram is the sorted factor structure suitable for using fa.lookup.Although not particularly pretty, it is possible to do Schmid-Leiman rotations at each level.  Specify the rotation as rotate="schmid".
This is essentially a wrapper to the fa and pca combined with the faCor functions.  They are called repeatedly and then the weights from the resulting solutions are used to find the factor/component correlations. Although the default is do all factor solutions from 1 to the nfactors, this can be simplified by specifying just some of the factor solutions. Thus, for the 135 items of the spi, it is more reasonable to ask for 3,5, and 27 item solutions. The function bassAckward.diagram may be called using the diagram function or may be called directly.The output from bassAckward.diagram is the sorted factor structure suitable for using fa.lookup.Although not particularly pretty, it is possible to do Schmid-Leiman rotations at each level.  Specify the rotation as rotate="schmid".
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
There are a number of procedures that can be used for predicting criteria from a set of predictors.  The generic term for this is "machine learning" or "statistical learning".  The basic logic of these procedures is to find a set of items that best predict a criteria according to some fit statistic and then cross validate these items numerous times.  "lasso" regression (least absolute shrinkage and selection) is one such example. bestScales differs from these procedures by unit weighting items chosen from their zero order correlations with the criteria rather than weighting the partial correlations ala regression.  This is an admittedly simple procedure that takes into account the well known finding (Wilks, 1938;  Wainer, 1976; Dawes, 1979; Waller, 2008) that although regression weights are optimal for any particular data set, unit weights are almost as good (fungible) and more robust across sample variation.  Following some suggestions, we have added the ability to find scales where items are weighted by their zero order correlations with the criteria.   This is effectively a comprimise between unit weighting and regression weights (where the weights are the zero order correlations times the inverse of the correlation matrix). This weighted version may be thought of as BISCWIT in contrast to the unit weighted version BISCUIT.To be comparable to other ML algorithms, we now consider multiple solutions (for number of items >= min.item to max.item).  The final scale consists of the number  items which maximize  the validity or at least are within delta * standard error of r of the maximum.  Thus, bestScales will find up to n.items per criterion that have absolute correlations with a criterion greater than cut.  If the overlap option is FALSE (default) the other criteria are not used.  This is an example of “dust bowl empiricism" in that there is no latent construct being measured, just those items that most correlate with a set of criteria. The empirically identified items are then formed into scales (ignoring concepts of internal consistency) which are then correlated with the criteria.  Clearly, bestScales is capitalizing on chance associations.  Thus, we should validate the empirical scales by deriving them on a fraction of the total number of subjects, and cross validating on the remaining subjects. (This is known both as K-fold cross validation and bagging.  Both may be done).  If folds > 1, then a k-fold cross validation is done.  This removes 1/k (a fold) from the sample for the derivation sample and validates on that remaining fold. This is done k-folds times.  Traditional cross validation would thus be a k-fold with k =2.  More modern applications seem to prefer k =10 to have 90% derivation sample and a 10% cross validation sample.The alternative, known as 'bagging' is to do a bootstrap sample (which because it is sampling with replacement will typically extract 1- 1/e = 63.2% of the sample) for the derivation sample (the bag) and then validate it on the remaining 1/e of the sample (the out of bag).  This is done n.iter times. This should be repeated multiple times (n.iter > 1, e.g. n.iter=1000) to get stable cross validations.One can compare the validity of these two approaches by  trying each.  The average predictability of the n.iter samples are shown as are the average validity of the cross validations.   This can only be done if x is a data matrix/ data.frame, not a correlation matrix.  For very large data sets (e.g., those from SAPA) these scales seem very stable. bestScales is effectively a straight forward application of 'bagging' (bootstrap aggregation) and machine learning as well as k-fold validation. The criteria can be the colnames of elements of x, or can be a separate data.frame. bestItems and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).The derived model can be further validated against yet another hold out sample using the predict.psych function if given the best scale object and the new data set. 
There are a number of procedures that can be used for predicting criteria from a set of predictors.  The generic term for this is "machine learning" or "statistical learning".  The basic logic of these procedures is to find a set of items that best predict a criteria according to some fit statistic and then cross validate these items numerous times.  "lasso" regression (least absolute shrinkage and selection) is one such example. bestScales differs from these procedures by unit weighting items chosen from their zero order correlations with the criteria rather than weighting the partial correlations ala regression.  This is an admittedly simple procedure that takes into account the well known finding (Wilks, 1938;  Wainer, 1976; Dawes, 1979; Waller, 2008) that although regression weights are optimal for any particular data set, unit weights are almost as good (fungible) and more robust across sample variation.  Following some suggestions, we have added the ability to find scales where items are weighted by their zero order correlations with the criteria.   This is effectively a comprimise between unit weighting and regression weights (where the weights are the zero order correlations times the inverse of the correlation matrix). This weighted version may be thought of as BISCWIT in contrast to the unit weighted version BISCUIT.To be comparable to other ML algorithms, we now consider multiple solutions (for number of items >= min.item to max.item).  The final scale consists of the number  items which maximize  the validity or at least are within delta * standard error of r of the maximum.  Thus, bestScales will find up to n.items per criterion that have absolute correlations with a criterion greater than cut.  If the overlap option is FALSE (default) the other criteria are not used.  This is an example of “dust bowl empiricism" in that there is no latent construct being measured, just those items that most correlate with a set of criteria. The empirically identified items are then formed into scales (ignoring concepts of internal consistency) which are then correlated with the criteria.  Clearly, bestScales is capitalizing on chance associations.  Thus, we should validate the empirical scales by deriving them on a fraction of the total number of subjects, and cross validating on the remaining subjects. (This is known both as K-fold cross validation and bagging.  Both may be done).  If folds > 1, then a k-fold cross validation is done.  This removes 1/k (a fold) from the sample for the derivation sample and validates on that remaining fold. This is done k-folds times.  Traditional cross validation would thus be a k-fold with k =2.  More modern applications seem to prefer k =10 to have 90% derivation sample and a 10% cross validation sample.The alternative, known as 'bagging' is to do a bootstrap sample (which because it is sampling with replacement will typically extract 1- 1/e = 63.2% of the sample) for the derivation sample (the bag) and then validate it on the remaining 1/e of the sample (the out of bag).  This is done n.iter times. This should be repeated multiple times (n.iter > 1, e.g. n.iter=1000) to get stable cross validations.One can compare the validity of these two approaches by  trying each.  The average predictability of the n.iter samples are shown as are the average validity of the cross validations.   This can only be done if x is a data matrix/ data.frame, not a correlation matrix.  For very large data sets (e.g., those from SAPA) these scales seem very stable. bestScales is effectively a straight forward application of 'bagging' (bootstrap aggregation) and machine learning as well as k-fold validation. The criteria can be the colnames of elements of x, or can be a separate data.frame. bestItems and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).The derived model can be further validated against yet another hold out sample using the predict.psych function if given the best scale object and the new data set. 
This data set is deprecated and users are encouraged to use bfi.It is kept here backward compatability for one more release.The first 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Opennness.  The scoring key is created using  make.keys, the scores are found using  score.items.These five factors are a useful example of using irt.fa to do Item Response Theory based latent factor analysis of the polychoric correlation matrix.  The endorsement plots for each item, as well as the item information functions reveal that the items differ in their quality.The item data were collected using a 6 point response scale: 1 Very Inaccurate2 Moderately Inaccurate3 Slightly Inaccurate4 Slightly Accurate5 Moderately Accurate6 Very Accurateas part of the Synthetic Apeture Personality Assessment (SAPA https://www.sapa-project.org/) project.  To see an example of the data collection technique, visit https://www.sapa-project.org/ or the International Cognitive Ability Resource at https://icar-project.org/.  The items given were sampled from the International Personality Item Pool of Lewis Goldberg using the sampling technique of SAPA.  This is a sample data set taken from the much larger SAPA data bank.
This data set is deprecated and users are encouraged to use bfi.It is kept here backward compatability for one more release.The first 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Opennness.  The scoring key is created using  make.keys, the scores are found using  score.items.These five factors are a useful example of using irt.fa to do Item Response Theory based latent factor analysis of the polychoric correlation matrix.  The endorsement plots for each item, as well as the item information functions reveal that the items differ in their quality.The item data were collected using a 6 point response scale: 1 Very Inaccurate2 Moderately Inaccurate3 Slightly Inaccurate4 Slightly Accurate5 Moderately Accurate6 Very Accurateas part of the Synthetic Apeture Personality Assessment (SAPA https://www.sapa-project.org/) project.  To see an example of the data collection technique, visit https://www.sapa-project.org/ or the International Cognitive Ability Resource at https://icar-project.org/.  The items given were sampled from the International Personality Item Pool of Lewis Goldberg using the sampling technique of SAPA.  This is a sample data set taken from the much larger SAPA data bank.
A trivial, if useful, function to draw back to back histograms/barplots. One for each group.
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
The data are divided into subsets of size=size.  Correlations are then found for each subset and pairs of subsets. The basic loop loops over the subsets. When the size is a integer subset of the number of variables and is a muiltiple of the number of cores, the multiple cores will be used more.  Notice the benefit of 660/80 versus 660/100.  But this breaks down if we try 660/165.  Further notice the benefit when using a smaller subset (55) which led to the 4 cores being used more.  Timings (in seconds) for various problems with 645K subjects on an 8 core Mac Book Pro with a 2.4 GHZ Intell core i9.options(mc.cores=4) (Because we have 8 we can work at the same time as we test this.)First test it with 644,495 subjects and 1/10 of the number of possible variables.  Then test it for somewhat fewer variables.  We also test it with fewer subjects.  Time is roughly linear with number of subjects.One last comparison, 10,000 subjects, showing the effect of getting the proper size value. You can tune on these smaller sets of subjects before trying large problems.Time is roughly linear with the number of cases and increases by the square of the number of variables.  The benefit of more cores is noticeable. It seems as if with 4 cores, we should use sizes to split it into 8 or 12 sets.  Otherwise we don't actually use all cores efficiently. There is some overhead in using multicores.  So for smaller problems (e.g. the 4,000 cases of the  145 items of the psychTools::spi data set, the timings are roughly .44 seconds for bigCor (default size) and .36 for normal cor.  But, fiddling with size gets us to .37 for size = 40.   The cross over point seems to be at roughly 5K subjects.
Uses the generic biplot function to take the output of a factor analysis fa, fa.poly or principal components analysis principal and plot the factor/component scores along with the factor/component loadings.This is an extension of the generic biplot function to allow more control over plotting points in a two space and also to plot three or more factors (two at time).  This will work for objects produced by fa, fa.poly or principal  if they applied to the original data matrix.  If however, one has a correlation matrix based upon the output from tetrachoric or polychoric, and has done either fa or principal on the correlations, then obviously, we can not do a biplot.  However, both of those functions produce a weights matrix, which, in combination with the original data can be used to find the scores by using factor.scores.  Since biplot.psych is looking for two elements of the x object: x$loadings and x$scores, you can create the appropriate object to plot. See the third example.
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See draw.tetra and draw.cor for  illustrations.)This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.tetrachoric and polychoric can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See fa.extension for an application of this.The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).Examples include the lsat6 and lsat7 data sets in the bock data.The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. For finding one polychoric correlation from a table, see the Olsson example (below).polychoric replaces poly.mat and is recommended.   poly.mat was an alternative wrapper to the polycor function. biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (https://www.sapa-project.org/) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (https://www.icar-project.org/) for similar data.Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, rbi = r s* √(pq)/zp .As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).The 'ad hoc' polyserial correlation, rps is just r = r * sqrt(n-1)/n) σ y /∑(zpi)  where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) All of these were inspired by (and adapted from) John Fox's polychor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  For combinations of continous, categorical, and dichotomous variables, see mixed.cor.If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   mixedCor which calls polychoric for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. (Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
NA
The lsat6 data set is analyzed in the ltm package as well as by McDonald (1999). lsat7 is another 1000 subjects on part 7 of the LSAT. Both sets are described by Bock and Lieberman (1970). Both sets are useful examples of testing out IRT procedures and showing the use of tetrachoric correlations and item factor analysis using the irt.fa function.
Cattell (1963) reported on 8 cognitive variables from Thurstone and four from the Institute for Personality Assessment Test (IPAT).  Rindskopf and Rose (1988) use this data set as an example of second order factor analysis. It is thus a nice set for examining alternative solutions such as bifactor rotation, omega hierarchical, as well as esem and interbattery factor analysis.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
 Two artificial correlation matrices from Schmid and Leiman (1957). One real and one artificial covariance matrices from Chen et al. (2006).  Schmid: a 12 x 12 artificial correlation matrix created to show the Schmid-Leiman transformation.  schmid.leiman: A 12 x 12 matrix with communalities on the diagonal.  Treating this as a covariance matrix shows the 6 x 6 factor solution Chen: An 18 x 18 covariance matrix of health related quality of life items from Chen et al. (2006). Number of observations = 403.  The first item is a measure of the quality of life.  The remaining 17 items form four subfactors: The items are (a) Cognition subscale: “Have difficulty reasoningand solving problems?"  “React slowly to things that were said or done?"; “Become confused and start several actions at a time?"  “Forget where youput things or appointments?"; “Have difficulty concentrating?"  (b) Vitalitysubscale: “Feel tired?"  “Have enough energy to do the things you want?" (R) “Feel worn out?" ; “Feel full of pep?" (R). (c) Mental health subscale: “Feelcalm and peaceful?"(R)  “Feel downhearted and blue?"; “Feel veryhappy"(R) ; “Feel very nervous?" ; “Feel so down in the dumps nothing couldcheer you up?  (d) Disease worry subscale: “Were you afraid because of your health?"; “Were you frustrated about your health?"; “Was your health a worry in your life?" . West: A 16 x 16 artificial covariance matrix from Chen et al. (2006).
NA
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
“A common model for representing psychological data is simple structure (Thurstone, 1947). According to one common interpretation, data are simple structured when items or scales have non-zero factor loadings on one and only one factor (Revelle & Rocklin, 1979). Despite the commonplace application of simple structure, some psychological models are defined by a lack of simple structure. Circumplexes (Guttman, 1954) are one kind of model in which simple structure is lacking.“A number of elementary requirements can be teased out of the idea of circumplex structure. First, circumplex structure implies minimally that variables are interrelated; random noise does not a circumplex make. Second, circumplex structure implies that the domain in question is optimally represented by two and only two dimensions. Third, circumplex structure implies that variables do not group or clump along the two axes, as in simple structure, but rather that there are always interstitial variables between any orthogonal pair of axes (Saucier, 1992). In the ideal case, this quality will be reflected in equal spacing of variables along the circumference of the circle (Gurtman, 1994; Wiggins, Steiger, & Gaelick, 1981). Fourth, circumplex structure implies that variables have a constant radius from the center of the circle, which implies that all variables have equal communality on the two circumplex dimensions (Fisher, 1997; Gurtman, 1994). Fifth, circumplex structure implies that all rotations are equally good representations of the domain (Conte & Plutchik, 1981; Larsen & Diener, 1992)." (Acton and Revelle, 2004)Acton and Revelle reviewed the effectiveness of 10 tests of circumplex structure and found that four did a particularly good job of discriminating circumplex structure from simple structure, or circumplexes from ellipsoidal structures. Unfortunately, their work was done in Pascal and is not easily available. Here we release R code to do the four most useful tests:The Gap test of equal spacingFisher's test of equality of axesA test of indifference to RotationA test of equal Variance of squared factor loadings across arbitrary rotations.Included in this set of functions are simple procedure to generate circumplex structured or simple structured data, the four test statistics, and a simple simulation showing the effectiveness of the four procedures.circ.sim.plot compares the four tests for circumplex, ellipsoid and simple structure data as function of the number of variables and the sample size.  What one can see from this plot is that although no one test is sufficient to discriminate these alternative structures, the set of four tests does a very good job of doing so.  When testing a particular data set for structure, comparing the results of all four tests to the simulated data will give a good indication of the structural properties of the data.
“A common model for representing psychological data is simple structure (Thurstone, 1947). According to one common interpretation, data are simple structured when items or scales have non-zero factor loadings on one and only one factor (Revelle & Rocklin, 1979). Despite the commonplace application of simple structure, some psychological models are defined by a lack of simple structure. Circumplexes (Guttman, 1954) are one kind of model in which simple structure is lacking.“A number of elementary requirements can be teased out of the idea of circumplex structure. First, circumplex structure implies minimally that variables are interrelated; random noise does not a circumplex make. Second, circumplex structure implies that the domain in question is optimally represented by two and only two dimensions. Third, circumplex structure implies that variables do not group or clump along the two axes, as in simple structure, but rather that there are always interstitial variables between any orthogonal pair of axes (Saucier, 1992). In the ideal case, this quality will be reflected in equal spacing of variables along the circumference of the circle (Gurtman, 1994; Wiggins, Steiger, & Gaelick, 1981). Fourth, circumplex structure implies that variables have a constant radius from the center of the circle, which implies that all variables have equal communality on the two circumplex dimensions (Fisher, 1997; Gurtman, 1994). Fifth, circumplex structure implies that all rotations are equally good representations of the domain (Conte & Plutchik, 1981; Larsen & Diener, 1992)." (Acton and Revelle, 2004)Acton and Revelle reviewed the effectiveness of 10 tests of circumplex structure and found that four did a particularly good job of discriminating circumplex structure from simple structure, or circumplexes from ellipsoidal structures. Unfortunately, their work was done in Pascal and is not easily available. Here we release R code to do the four most useful tests:The Gap test of equal spacingFisher's test of equality of axesA test of indifference to RotationA test of equal Variance of squared factor loadings across arbitrary rotations.Included in this set of functions are simple procedure to generate circumplex structured or simple structured data, the four test statistics, and a simple simulation showing the effectiveness of the four procedures.circ.sim.plot compares the four tests for circumplex, ellipsoid and simple structure data as function of the number of variables and the sample size.  What one can see from this plot is that although no one test is sufficient to discriminate these alternative structures, the set of four tests does a very good job of doing so.  When testing a particular data set for structure, comparing the results of all four tests to the simulated data will give a good indication of the structural properties of the data.
“A common model for representing psychological data is simple structure (Thurstone, 1947). According to one common interpretation, data are simple structured when items or scales have non-zero factor loadings on one and only one factor (Revelle & Rocklin, 1979). Despite the commonplace application of simple structure, some psychological models are defined by a lack of simple structure. Circumplexes (Guttman, 1954) are one kind of model in which simple structure is lacking.“A number of elementary requirements can be teased out of the idea of circumplex structure. First, circumplex structure implies minimally that variables are interrelated; random noise does not a circumplex make. Second, circumplex structure implies that the domain in question is optimally represented by two and only two dimensions. Third, circumplex structure implies that variables do not group or clump along the two axes, as in simple structure, but rather that there are always interstitial variables between any orthogonal pair of axes (Saucier, 1992). In the ideal case, this quality will be reflected in equal spacing of variables along the circumference of the circle (Gurtman, 1994; Wiggins, Steiger, & Gaelick, 1981). Fourth, circumplex structure implies that variables have a constant radius from the center of the circle, which implies that all variables have equal communality on the two circumplex dimensions (Fisher, 1997; Gurtman, 1994). Fifth, circumplex structure implies that all rotations are equally good representations of the domain (Conte & Plutchik, 1981; Larsen & Diener, 1992). (Acton and Revelle, 2004)Acton and Revelle reviewed the effectiveness of 10 tests of circumplex structure and found that four did a particularly good job of discriminating circumplex structure from simple structure, or circumplexes from ellipsoidal structures. Unfortunately, their work was done in Pascal and is not easily available. Here we release R code to do the four most useful tests:1	The Gap test of equal spacing2	Fisher's test of equality of axes 3	A test of indifference to Rotation4	A test of equal Variance of squared factor loadings across arbitrary rotations.To interpret the values of these various tests, it is useful to compare the particular solution to simulated solutions representing pure cases of circumplex and simple structure.  See the example output from circ.simulation and compare these plots with the results of the circ.test.
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
These  are three of the functions used in the SAPA (https://www.sapa-project.org/) procedures to form synthetic correlation matrices.  Given any correlation matrix of items, it is easy to find the correlation matrix of scales made up of those items. This can also be done from the original data matrix or from the correlation matrix using scoreItems which is probably preferred unless the keys are overlapping.  It is important to remember with SAPA data, that scale correlations should be found from the item correlations, not the raw data.In the case of overlapping keys, (items being scored on multiple scales), scoreOverlap will adjust for this overlap by replacing the overlapping covariances (which are variances when overlapping) with the corresponding best estimate of an item's “true" variance using either the average correlation or the smc estimate for that item.  This parallels the operation done when finding alpha reliability.  This is similar to ideas suggested by Cureton (1966) and Bashaw and Anderson (1966) but uses the smc or the average interitem correlation (default).A typical use in the SAPA project is to form item composites by clustering or factoring (see fa, ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple correlatin procedures using mat.regress.The original correlation is pre and post multiplied by the (transpose) of the keys matrix. If some correlations are missing from the original matrix this will lead to missing values (NA) for scale intercorrelations based upon those lower level correlations. If impute=TRUE (the default), a warning is issued and the correlations are imputed based upon the average correlations of the non-missing elements of each scale. Because the alpha estimate of reliability is based upon the correlations of the items rather than upon the covariances, this estimate of alpha is sometimes called “standardized alpha".  If the raw items are available, it is useful to compare standardized alpha with the raw alpha found using scoreItems.  They will differ substantially only if the items differ a great deal in their variances.   scoreOverlap answers an important question when developing scales and related subscales, or when comparing alternative versions of scales.  For by removing the effect of item overlap, it gives a better estimate the relationship between the latent variables estimated by the observed sum (mean) scores.scoreBy finds the within subject correlations after preprocessing with statsBy.  This is useful if doing an ESM study with multiple occasions for each subject.   It also makes it possible to find the correlations for subsets of subjects. See the example.  Note that it likely that for ESM data with a high level of missingness that the correlation matrices will not be positive-semi-definite. This can lead to composite score correlations that exceed 1.  Smoothing will resolve this problem.scoreBy  is useful when examining multi-level models where we want to examine the correlations within subjects (e.g., for ESM data) or within groups of subjects (when examining the stability of correlational structures by subgroups).  For both cases the data must be processed first by statsBy.  To find the variances of the scales it is necessary to use the cor="cov" option in statsBy.  
The cluster model is similar to the factor model: R is fitted by C'C. Where C <- Cluster definition matrix x the loading matrix.  How well does this model approximate the original correlation matrix and how does this compare to a factor model?The fit statistic is a comparison of the original (squared) correlations to the residual correlations.  Fit = 1 - r*2/r2 where r* is the residual correlation of data - model and model = C'C.
Given a set of items to be scored as (perhaps overlapping) clusters and the intercorrelation matrix of the items, find the clusters and then the correlations of each item with each cluster.  Correct for item overlap by replacing the item variance with its average within cluster inter-item correlation.  Although part of ICLUST, this may be used in any SAPA (https://www.sapa-project.org/) application where we are interested in item-whole correlations of items and composite scales.For information about SAPA see Revelle et al, 2010, 2016.  For information about SAPA based measures of ability, see https://icar-project.org.These loadings are particularly interpretable when sorted by absolute magnitude for each cluster (see ICLUST.sort). 
Results of either a factor analysis or cluster analysis are plotted.  Each item is assigned to its highest loading factor, and then identified by variable name as well as cluster (by color). The cluster assignments can be specified to override the automatic clustering by loading.Both of these functions may be called directly or by calling the generic plot function.  (see example).
Note that because kmeans will not reverse score items, the clusters defined by kmeans will not necessarily match those of ICLUST with the same number of clusters extracted. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
When cateogorical judgments are made with two cateories, a measure of relationship is the phi coefficient.  However, some categorical judgments are made using more than two outcomes.  For example, two diagnosticians might be asked to categorize patients three ways (e.g., Personality disorder, Neurosis, Psychosis) or to categorize the stages of a disease.  Just as base rates affect observed cell frequencies in a two by two table, they need to be considered in the n-way table (Cohen, 1960). Kappa considers the matches on the main diagonal.  A penalty function (weight) may be applied to the off diagonal matches.  If the weights increase by the square of the distance from the diagonal, weighted kappa is similar to an Intra Class Correlation (ICC).Derivations of weighted kappa are sometimes expressed in terms of similarities, and sometimes in terms of dissimilarities. In the latter case, the weights on the diagonal are 1 and the weights off the diagonal are less than one. In this  case, if the weights are 1 - squared distance from the diagonal / k, then the result is similar to the ICC (for any positive k).  cohen.kappa may use either similarity weighting (diagonal = 0) or dissimilarity weighting (diagonal = 1) in order to match various published examples. The input may be a two column data.frame or matrix with columns representing the two judges and rows the subjects being rated. Alternatively, the input may be a square n x n matrix of counts or proportion of matches.  If proportions are used, it is necessary to specify the number of observations (n.obs) in order to correctly find the confidence intervals.The confidence intervals are based upon the variance estimates discussed by Fleiss, Cohen, and Everitt who corrected the formulae of Cohen (1968) and Blashfield.Some data sets will include data with numeric categories with some category values missing completely.  In the sense that kappa is a measure of category relationship, this should not matter.  But when finding weighted kappa, the number of categories weighted will be less than the number of categories potentially in the data.  This can be remedied by specifying the levels parameter.  This is a vector of the levels potentially in the data (even if some are missing).   See the examples.If there are more than 2 raters, then the average of all raters is known as Light's kappa. (Conger, 1980).  
Congruences are the cosines of pairs of vectors defined by a matrix and based at the origin.  Thus, for values that differ only by a scaler the congruence will be 1.For two matrices, F1 and F2, the measure of  congruence, phi, is phi = sum(F1 F2)/sqrt(sum(F1^2) sum(F2^2)) It is an interesting exercise to compare  congruences with the correlations of the respective values. Congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of items are cosines of the vectors based at the mean loading for each column.   phi = sum((F1-a)(F2-b))/sqrt(sum((F1-a)^2) sum((F2-b)^2)) .For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.Input may either be one or two matrices.For congruences of factor or component loading matrices, use factor.congruence.Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.cohen.profile applies the congruence function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). distance finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r >2).
NA
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
When constructing examples for reliability analysis, it is convenient to simulate congeneric data structures.  These are the most simple of item structures, having just one factor. Mainly used for a discussion of reliability theory as well as factor score estimates. The implied covariance matrix is just pattern %*% t(pattern). 
Congruences are the cosines of pairs of vectors defined by a matrix and based at the origin.  Thus, for values that differ only by a scaler the congruence will be 1.For two matrices, F1 and F2, the measure of  congruence, phi, is phi = sum(F1 F2)/sqrt(sum(F1^2) sum(F2^2)) It is an interesting exercise to compare  congruences with the correlations of the respective values. Congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of items are cosines of the vectors based at the mean loading for each column.   phi = sum((F1-a)(F2-b))/sqrt(sum((F1-a)^2) sum((F2-b)^2)) .For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.Input may either be one or two matrices.For congruences of factor or component loading matrices, use factor.congruence.Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.cohen.profile applies the congruence function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). distance finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r >2).
If given a correlation matrix, then confidence intervals are found based upon the sample sizes using the conventional r2z fisher transformation (fisherz and the normal distribution.If given raw data, correlations are found.  If keys are specified (the normal case), then composite scales based upon the correlations are found and reported.  This is the same procedure as done using cluster.cor or scoreItems.Then (with raw data) the data are recreated n.iter times by sampling subjects (rows) with replacement and the correlations (and composite scales) are found again (and again and again).  Mean and standard deviations of these values are calculated based upon the Fisher Z transform of the correlations.  Summary statistics include the original correlations and their confidence intervals.  For those who want the complete set of replications, those are available as an object in the resulting output.Although particularly useful for SAPA (https://www.sapa-project.org/) type data where we have lots of missing data, this will work for any normal data set as well. Although the correlations are shown automatically as a cor.plot, it is possible to show the upper and lower confidence intervals by using cor.plot.upperLowerCi. This will also return, invisibly, a matrix for printing with the lower and upper bounds of the correlations shown below and above the diagonal (see the first example).
When summarizing the correlations of large data bases or when teaching about factor analysis or cluster analysis, it is useful to graphically display the structure of correlation matrices.  This is a simple graphical display using the image function. The difference between mat.plot with a regular image plot is that the primary diagonal goes from the top left to the lower right. zlim defines how to treat the range of possible values. -1 to 1 and the color choice is more reasonable.  Setting it as c(0,1) will lead to negative correlations  treated as zero.  This is advantageous when showing general factor structures, because it makes the 0 white.  There is an interesting case when plotting correlations corrected for attenuation.  Some of these might exceed 1.  In this case, either set zlim = NULL (to use the observed maximum and minimum values) or all values above 1 will be given a slightly darker shade than 1, but do not differ.The default shows a legend for the color coding on the right hand side of the figure.Inspired, in part, by a paper by S. Dray (2008)  on the number of components problem. Modified following suggestions by David Condon and Josh Wilt to use a more meaningful color choice ranging from dark red (-1) through white (0) to dark blue (1). Further modified to allow for color choices using the gr option (suggested by Lorien Elleman). Further modified to include the numerical value of the correlation.  (Inspired by the corrplot package).  These values may be scaled according the the probability values found in cor.ci or corr.test.Unless specified, the font size is dynamically scaled to have a cex =  10/max(nrow(r),ncol(r).  This can produce fairly small fonts for large problems.  The font size of the labels may be adjusted using cex.axis which defaults to one.  By default cor.ci calls corPlotUpperLowerCi and scales the correlations based upon "significance" values.  The correlations plotted are the upper and lower confidence boundaries.  To show the correlations themselves, call corPlot directly.If using the output of corr.test, the upper off diagonal will be scaled by the corrected probability, the lower off diagonal the scaling is the uncorrected probabilities.If using the output of corr.test or cor.ci as input to corPlotUpperLowerCi, the upper off diagonal will be the upper bounds and the lower off diagonal the lower bounds of the confidence intervals.  If adjust=TRUE, these will use the Holm or Bonferroni adjusted values (depending upon corr.test). To compare the elements of two correlation matrices, corPlot the results from lowerUpper.  To do multiple corPlot on the same plot, specify that show.legend=FALSE and keep.par=FALSE.  See the last examples.  Care should be taken when selecting rows and columns from a non-symmetric matrix (e.g., the corrected correlations from scoreItems or scoreOverlap).  To show a factor loading matrix (or any non-symmetric matrix), set symmetric=FALSE.  Otherwise the correlations will be found.
When summarizing the correlations of large data bases or when teaching about factor analysis or cluster analysis, it is useful to graphically display the structure of correlation matrices.  This is a simple graphical display using the image function. The difference between mat.plot with a regular image plot is that the primary diagonal goes from the top left to the lower right. zlim defines how to treat the range of possible values. -1 to 1 and the color choice is more reasonable.  Setting it as c(0,1) will lead to negative correlations  treated as zero.  This is advantageous when showing general factor structures, because it makes the 0 white.  There is an interesting case when plotting correlations corrected for attenuation.  Some of these might exceed 1.  In this case, either set zlim = NULL (to use the observed maximum and minimum values) or all values above 1 will be given a slightly darker shade than 1, but do not differ.The default shows a legend for the color coding on the right hand side of the figure.Inspired, in part, by a paper by S. Dray (2008)  on the number of components problem. Modified following suggestions by David Condon and Josh Wilt to use a more meaningful color choice ranging from dark red (-1) through white (0) to dark blue (1). Further modified to allow for color choices using the gr option (suggested by Lorien Elleman). Further modified to include the numerical value of the correlation.  (Inspired by the corrplot package).  These values may be scaled according the the probability values found in cor.ci or corr.test.Unless specified, the font size is dynamically scaled to have a cex =  10/max(nrow(r),ncol(r).  This can produce fairly small fonts for large problems.  The font size of the labels may be adjusted using cex.axis which defaults to one.  By default cor.ci calls corPlotUpperLowerCi and scales the correlations based upon "significance" values.  The correlations plotted are the upper and lower confidence boundaries.  To show the correlations themselves, call corPlot directly.If using the output of corr.test, the upper off diagonal will be scaled by the corrected probability, the lower off diagonal the scaling is the uncorrected probabilities.If using the output of corr.test or cor.ci as input to corPlotUpperLowerCi, the upper off diagonal will be the upper bounds and the lower off diagonal the lower bounds of the confidence intervals.  If adjust=TRUE, these will use the Holm or Bonferroni adjusted values (depending upon corr.test). To compare the elements of two correlation matrices, corPlot the results from lowerUpper.  To do multiple corPlot on the same plot, specify that show.legend=FALSE and keep.par=FALSE.  See the last examples.  Care should be taken when selecting rows and columns from a non-symmetric matrix (e.g., the corrected correlations from scoreItems or scoreOverlap).  To show a factor loading matrix (or any non-symmetric matrix), set symmetric=FALSE.  Otherwise the correlations will be found.
The smoothing is done by eigen value decomposition.  eigen values < eig.tol are changed to 100  * eig.tol.  The positive eigen values are rescaled to sum to the number of items.  The matrix is recomputed (eigen.vectors %*% diag(eigen.values) %*% t(eigen.vectors) and forced to a correlation matrix using cov2cor. (See Bock, Gibbons and Muraki, 1988 and Wothke, 1993). This does not implement the Knol and ten Berge (1989) solution, nor do nearcor and posdefify in sfmsmisc, not does nearPD in Matrix. As Martin Maechler puts it in the posdedify function, "there are more sophisticated algorithms to solve this and related problems."  cor.smoother examines all of nvar minors of rank nvar-1 by systematically dropping one variable at a time and finding the eigen value decomposition.  It reports those variables, which, when dropped, produce a positive definite matrix.  It also reports the number of negative eigenvalues when each variable is dropped.  Finally, it compares the original correlation matrix to the smoothed correlation matrix and reports those items with absolute deviations great than cut.  These are all hints as to what might be wrong with a correlation matrix.  
The smoothing is done by eigen value decomposition.  eigen values < eig.tol are changed to 100  * eig.tol.  The positive eigen values are rescaled to sum to the number of items.  The matrix is recomputed (eigen.vectors %*% diag(eigen.values) %*% t(eigen.vectors) and forced to a correlation matrix using cov2cor. (See Bock, Gibbons and Muraki, 1988 and Wothke, 1993). This does not implement the Knol and ten Berge (1989) solution, nor do nearcor and posdefify in sfmsmisc, not does nearPD in Matrix. As Martin Maechler puts it in the posdedify function, "there are more sophisticated algorithms to solve this and related problems."  cor.smoother examines all of nvar minors of rank nvar-1 by systematically dropping one variable at a time and finding the eigen value decomposition.  It reports those variables, which, when dropped, produce a positive definite matrix.  It also reports the number of negative eigenvalues when each variable is dropped.  Finally, it compares the original correlation matrix to the smoothed correlation matrix and reports those items with absolute deviations great than cut.  These are all hints as to what might be wrong with a correlation matrix.  
A weighted correlation is just ∑ (wt_k * (x_ik - x_jk)) /sqrt[wt_k ∑(x^2_ik) wt_k ∑(x^2_jk)]  where x_ik is a deviation from the weighted mean.  The weighted correlation is appropriate for correlating aggregated data, where individual data points might reflect the means of a number of observations.  In this case, each point is weighted by its sample size (or alternatively, by the standard error).  If the weights are all equal, the correlation is just a normal Pearson correlation. Used when finding correlations of group means found using statsBy.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
NA
NA
If given a correlation matrix, then confidence intervals are found based upon the sample sizes using the conventional r2z fisher transformation (fisherz and the normal distribution.If given raw data, correlations are found.  If keys are specified (the normal case), then composite scales based upon the correlations are found and reported.  This is the same procedure as done using cluster.cor or scoreItems.Then (with raw data) the data are recreated n.iter times by sampling subjects (rows) with replacement and the correlations (and composite scales) are found again (and again and again).  Mean and standard deviations of these values are calculated based upon the Fisher Z transform of the correlations.  Summary statistics include the original correlations and their confidence intervals.  For those who want the complete set of replications, those are available as an object in the resulting output.Although particularly useful for SAPA (https://www.sapa-project.org/) type data where we have lots of missing data, this will work for any normal data set as well. Although the correlations are shown automatically as a cor.plot, it is possible to show the upper and lower confidence intervals by using cor.plot.upperLowerCi. This will also return, invisibly, a matrix for printing with the lower and upper bounds of the correlations shown below and above the diagonal (see the first example).
In the presence of missing data, Full Information Maximum Likelihood (FIML) is an alternative to simply using the pairwise correlations. The implementation in the lavaan package for structural equation modeling has been adapted for the simpler case of just finding the correlations or covariances.  The pairwise solution for any pair of variables is insensitive to other variables included in the matrix.  On the other hand, the ML solution depends upon the entire set of items being correlated.  This will lead to slightly different solutions for different subsets of variables.  The basic FIML algorithm is to find the pairwise ML solution for covariances and means for every pattern of missingness and then to weight the solution by the size of every unique pattern of missingness.  
When summarizing the correlations of large data bases or when teaching about factor analysis or cluster analysis, it is useful to graphically display the structure of correlation matrices.  This is a simple graphical display using the image function. The difference between mat.plot with a regular image plot is that the primary diagonal goes from the top left to the lower right. zlim defines how to treat the range of possible values. -1 to 1 and the color choice is more reasonable.  Setting it as c(0,1) will lead to negative correlations  treated as zero.  This is advantageous when showing general factor structures, because it makes the 0 white.  There is an interesting case when plotting correlations corrected for attenuation.  Some of these might exceed 1.  In this case, either set zlim = NULL (to use the observed maximum and minimum values) or all values above 1 will be given a slightly darker shade than 1, but do not differ.The default shows a legend for the color coding on the right hand side of the figure.Inspired, in part, by a paper by S. Dray (2008)  on the number of components problem. Modified following suggestions by David Condon and Josh Wilt to use a more meaningful color choice ranging from dark red (-1) through white (0) to dark blue (1). Further modified to allow for color choices using the gr option (suggested by Lorien Elleman). Further modified to include the numerical value of the correlation.  (Inspired by the corrplot package).  These values may be scaled according the the probability values found in cor.ci or corr.test.Unless specified, the font size is dynamically scaled to have a cex =  10/max(nrow(r),ncol(r).  This can produce fairly small fonts for large problems.  The font size of the labels may be adjusted using cex.axis which defaults to one.  By default cor.ci calls corPlotUpperLowerCi and scales the correlations based upon "significance" values.  The correlations plotted are the upper and lower confidence boundaries.  To show the correlations themselves, call corPlot directly.If using the output of corr.test, the upper off diagonal will be scaled by the corrected probability, the lower off diagonal the scaling is the uncorrected probabilities.If using the output of corr.test or cor.ci as input to corPlotUpperLowerCi, the upper off diagonal will be the upper bounds and the lower off diagonal the lower bounds of the confidence intervals.  If adjust=TRUE, these will use the Holm or Bonferroni adjusted values (depending upon corr.test). To compare the elements of two correlation matrices, corPlot the results from lowerUpper.  To do multiple corPlot on the same plot, specify that show.legend=FALSE and keep.par=FALSE.  See the last examples.  Care should be taken when selecting rows and columns from a non-symmetric matrix (e.g., the corrected correlations from scoreItems or scoreOverlap).  To show a factor loading matrix (or any non-symmetric matrix), set symmetric=FALSE.  Otherwise the correlations will be found.
When summarizing the correlations of large data bases or when teaching about factor analysis or cluster analysis, it is useful to graphically display the structure of correlation matrices.  This is a simple graphical display using the image function. The difference between mat.plot with a regular image plot is that the primary diagonal goes from the top left to the lower right. zlim defines how to treat the range of possible values. -1 to 1 and the color choice is more reasonable.  Setting it as c(0,1) will lead to negative correlations  treated as zero.  This is advantageous when showing general factor structures, because it makes the 0 white.  There is an interesting case when plotting correlations corrected for attenuation.  Some of these might exceed 1.  In this case, either set zlim = NULL (to use the observed maximum and minimum values) or all values above 1 will be given a slightly darker shade than 1, but do not differ.The default shows a legend for the color coding on the right hand side of the figure.Inspired, in part, by a paper by S. Dray (2008)  on the number of components problem. Modified following suggestions by David Condon and Josh Wilt to use a more meaningful color choice ranging from dark red (-1) through white (0) to dark blue (1). Further modified to allow for color choices using the gr option (suggested by Lorien Elleman). Further modified to include the numerical value of the correlation.  (Inspired by the corrplot package).  These values may be scaled according the the probability values found in cor.ci or corr.test.Unless specified, the font size is dynamically scaled to have a cex =  10/max(nrow(r),ncol(r).  This can produce fairly small fonts for large problems.  The font size of the labels may be adjusted using cex.axis which defaults to one.  By default cor.ci calls corPlotUpperLowerCi and scales the correlations based upon "significance" values.  The correlations plotted are the upper and lower confidence boundaries.  To show the correlations themselves, call corPlot directly.If using the output of corr.test, the upper off diagonal will be scaled by the corrected probability, the lower off diagonal the scaling is the uncorrected probabilities.If using the output of corr.test or cor.ci as input to corPlotUpperLowerCi, the upper off diagonal will be the upper bounds and the lower off diagonal the lower bounds of the confidence intervals.  If adjust=TRUE, these will use the Holm or Bonferroni adjusted values (depending upon corr.test). To compare the elements of two correlation matrices, corPlot the results from lowerUpper.  To do multiple corPlot on the same plot, specify that show.legend=FALSE and keep.par=FALSE.  See the last examples.  Care should be taken when selecting rows and columns from a non-symmetric matrix (e.g., the corrected correlations from scoreItems or scoreOverlap).  To show a factor loading matrix (or any non-symmetric matrix), set symmetric=FALSE.  Otherwise the correlations will be found.
corr.test uses the cor function to find the correlations, and then applies a t-test to the individual correlations using the formulat = r* sqrt(n-2)/sqrt(1-r^2) se = sqrt((1-r^2)/(n-2))The t and Standard Errors are returned as objects in the result, but are not normally displayed. Confidence intervals are found and printed if using the print(short=FALSE) option.  These are found by using the fisher z transform of the correlation and then taking the range r +/- qnorm(alpha/2) *  se and the standard error of the z transforms is se = sqrt(1/(n-3)).   These values are then back transformed to be in correlation units. They are returned in the ci object. Note that in the case of method=="kendall" since these are the normal theory confidence intervals they are slightly too big.The probability values may be adjusted using the Holm (or other) correction.  If the matrix is symmetric (no y data), then the original p values are reported below the diagonal and the adjusted above the diagonal.  Otherwise, all probabilities are adjusted (unless adjust="none").  This is made explicit in the output. Confidence intervals are shown for raw and adjusted probabilities in the ci object.corr.p may be applied to the results of partial.r if n is set to n - s (where s is the number of variables partialed out)  Fisher, 1924. 
corr.test uses the cor function to find the correlations, and then applies a t-test to the individual correlations using the formulat = r* sqrt(n-2)/sqrt(1-r^2) se = sqrt((1-r^2)/(n-2))The t and Standard Errors are returned as objects in the result, but are not normally displayed. Confidence intervals are found and printed if using the print(short=FALSE) option.  These are found by using the fisher z transform of the correlation and then taking the range r +/- qnorm(alpha/2) *  se and the standard error of the z transforms is se = sqrt(1/(n-3)).   These values are then back transformed to be in correlation units. They are returned in the ci object. Note that in the case of method=="kendall" since these are the normal theory confidence intervals they are slightly too big.The probability values may be adjusted using the Holm (or other) correction.  If the matrix is symmetric (no y data), then the original p values are reported below the diagonal and the adjusted above the diagonal.  Otherwise, all probabilities are adjusted (unless adjust="none").  This is made explicit in the output. Confidence intervals are shown for raw and adjusted probabilities in the ci object.corr.p may be applied to the results of partial.r if n is set to n - s (where s is the number of variables partialed out)  Fisher, 1924. 
Disattenuated correlations may be thought of as  correlations between the latent variables measured by a set of observed variables. That is, what would the correlation be between two (unreliable) variables be if both variables were measured perfectly reliably.This function is mainly used if importing correlations and reliabilities from somewhere else.  If the raw data are available, use score.items, or  cluster.loadings or cluster.cor.Examples of the output of this function are seen in cluster.loadings and cluster.cor
There are several ways to test if a matrix is the identity matrix. The most well known is the chi square test  of Bartlett (1951) and Box (1949). A very straightforward test, discussed by Steiger (1980) is to find the sum of the squared correlations or the sum of the squared Fisher transformed correlations.  Under the null hypothesis that all the correlations are equal, this sum is distributed as chi square.  This is implemented in cortest and cortest.normalYet another test, is the Jennrich(1970) test of the equality of two matrices. This compares the differences between two matrices to the averages of two matrices using a chi square test. This is implemented in cortest.jennrich.Yet another option cortest.mat is to compare the two matrices using an approach analogous to that used in evaluating the adequacy of a factor model.  In factor analysis, the maximum likelihood fit statistic is f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^-1 R|) - n.items. This in turn is converted to a chi square chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f  (see fa.)That is, the model (M = FF' + U2) is compared to the original correlation matrix (R) by a function of M^{-1} R.  By analogy, in the case of two matrices, A and B, cortest.mat finds the chi squares associated with A^{-1}B and A B^{-1}.  The sum of these two χ^2 will also be a χ^2 but with twice the degrees of freedom.  
More useful for pedagogical purposes than actual applications. The Bartlett test is asymptotically chi square distributed.Note that if applied to residuals from factor analysis (fa) or principal components analysis (principal) that the diagonal must be replaced with 1s. This is done automatically if diag=TRUE. (See examples.)  An Alternative way of testing whether a correlation matrix is factorable (i.e., the correlations differ from 0) is the Kaiser-Meyer-Olkin KMO test of factorial adequacy. 
There are several ways to test if a matrix is the identity matrix. The most well known is the chi square test  of Bartlett (1951) and Box (1949). A very straightforward test, discussed by Steiger (1980) is to find the sum of the squared correlations or the sum of the squared Fisher transformed correlations.  Under the null hypothesis that all the correlations are equal, this sum is distributed as chi square.  This is implemented in cortest and cortest.normalYet another test, is the Jennrich(1970) test of the equality of two matrices. This compares the differences between two matrices to the averages of two matrices using a chi square test. This is implemented in cortest.jennrich.Yet another option cortest.mat is to compare the two matrices using an approach analogous to that used in evaluating the adequacy of a factor model.  In factor analysis, the maximum likelihood fit statistic is f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^-1 R|) - n.items. This in turn is converted to a chi square chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f  (see fa.)That is, the model (M = FF' + U2) is compared to the original correlation matrix (R) by a function of M^{-1} R.  By analogy, in the case of two matrices, A and B, cortest.mat finds the chi squares associated with A^{-1}B and A B^{-1}.  The sum of these two χ^2 will also be a χ^2 but with twice the degrees of freedom.  
There are several ways to test if a matrix is the identity matrix. The most well known is the chi square test  of Bartlett (1951) and Box (1949). A very straightforward test, discussed by Steiger (1980) is to find the sum of the squared correlations or the sum of the squared Fisher transformed correlations.  Under the null hypothesis that all the correlations are equal, this sum is distributed as chi square.  This is implemented in cortest and cortest.normalYet another test, is the Jennrich(1970) test of the equality of two matrices. This compares the differences between two matrices to the averages of two matrices using a chi square test. This is implemented in cortest.jennrich.Yet another option cortest.mat is to compare the two matrices using an approach analogous to that used in evaluating the adequacy of a factor model.  In factor analysis, the maximum likelihood fit statistic is f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^-1 R|) - n.items. This in turn is converted to a chi square chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f  (see fa.)That is, the model (M = FF' + U2) is compared to the original correlation matrix (R) by a function of M^{-1} R.  By analogy, in the case of two matrices, A and B, cortest.mat finds the chi squares associated with A^{-1}B and A B^{-1}.  The sum of these two χ^2 will also be a χ^2 but with twice the degrees of freedom.  
There are several ways to test if a matrix is the identity matrix. The most well known is the chi square test  of Bartlett (1951) and Box (1949). A very straightforward test, discussed by Steiger (1980) is to find the sum of the squared correlations or the sum of the squared Fisher transformed correlations.  Under the null hypothesis that all the correlations are equal, this sum is distributed as chi square.  This is implemented in cortest and cortest.normalYet another test, is the Jennrich(1970) test of the equality of two matrices. This compares the differences between two matrices to the averages of two matrices using a chi square test. This is implemented in cortest.jennrich.Yet another option cortest.mat is to compare the two matrices using an approach analogous to that used in evaluating the adequacy of a factor model.  In factor analysis, the maximum likelihood fit statistic is f = log(trace ((FF'+U2)^{-1} R) -  log(|(FF'+U2)^-1 R|) - n.items. This in turn is converted to a chi square chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * factors)/3)) * f  (see fa.)That is, the model (M = FF' + U2) is compared to the original correlation matrix (R) by a function of M^{-1} R.  By analogy, in the case of two matrices, A and B, cortest.mat finds the chi squares associated with A^{-1}B and A B^{-1}.  The sum of these two χ^2 will also be a χ^2 but with twice the degrees of freedom.  
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When data represent angles (such as the hours of peak alertness or peak tension during the day), we need to apply circular statistics rather than the more normal linear statistics (see Jammalamadaka (2006) for a very clear set of examples of circular statistics). The generalization of the mean to circular data is to convert each angle into a vector, average the x and y coordinates, and convert the result back to an angle. A statistic that represents the compactness of the observations is R which is the (normalized) vector length found by adding all of the observations together.  This will achieve a maximum value (1) when all the phase angles are the same and a minimum (0) if the phase angles are distributed uniformly around the clock.  The generalization of Pearson correlation to circular statistics is straight forward and is implemented in cor.circular in the circular package and in circadian.cor here.  Just as the Pearson r is a ratio of covariance to the square root of the product of two variances, so is the circular correlation.  The circular covariance of two circular vectors is defined as the average product of the sines of the deviations from the circular mean.  The variance is thus the average squared sine of the angular deviations from the circular mean.  Circular statistics are used for data that vary over a period (e.g., one day) or over directions (e.g., wind direction or bird flight).  Jammalamadaka and Lund (2006)  give a very good example of the use of circular statistics in calculating wind speed and direction.  The code from CircStats and circular was adapted to allow for analysis of data from various studies of mood over the day.  Those two packages do not seem to handle missing data, nor do they take matrix input, but rather emphasize single vectors.  The cosinor function will either iteratively fit cosines of the angle to the observed data (opti=TRUE) or use the circular by linear regression to estimate the best fitting phase angle.  If cos.t <- cos(time) and sin.t = sin(time) (expressed in hours), then beta.c and beta.s may be found by regression and the phase is sign(beta.c) * acos(beta.c/√(beta.c^2 + beta.s^2)) * 12/piSimulations (see examples) suggest that with incomplete times, perhaps the optimization procedure yields slightly better fits with the correct phase than does the linear model, but the differences are very small. In the presence of noisey data, these advantages seem to reverse.  The recommendation thus seems to be to use the linear model approach (the default).  The fit statistic reported for cosinor is the correlation of the data with the model  [ cos(time - acrophase) ].The circadian.reliability function splits the data for each subject into a first and second half (by default, or into odd and even items) and then finds the best fitting phase for each half.  These are then correlated (using circadian.cor) and this correlation is then adjusted for test length using the conventional Spearman-Brown formula. Returned as object in the output are the statistics for the first and second part, as well as an ANOVA to compare the two halves.circular.mean and circular.cor are just circadian.mean and circadian.cor but with input given in radians rather than hours.The circadian.linear.cor function will correlate a set of circular variables with a set of linear variables.  The first (angle) variables are circular, the second (x) set of variables are linear.  The circadian.F will compare 2 or more groups in terms of their mean position.  This is adapted from the equivalent function in the circular pacakge.  This is clearly a more powerful test the more each group is compact around its mean (large values of R). 
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
A very thorough discussion of the CTA model is available from Revelle (2008). An application of the model is discussed in Revelle and Condon (2015). cta.15 is the version used to produce the figures and analysis in Revelle and Condon (2015).  cta is the most recent version and includes a learning function developed in collaboration with Luke Smillie at the University of Melbourne.The dynamics of action (Atkinson and Birch, 1970)  was a model of how instigating forces elicited action tendencies which in turn elicited actions.  The basic concept was that action tendencies had inertia.  That is, a wish (action tendency) would persist until satisfied and would not change without an instigating force.  The consummatory strength of doing an action was thought  in turn to reduce the action tendency.  Forces could either be instigating or inhibitory (leading to "negaction").Perhaps the simplest example is the action tendency (T)  to eat a pizza.  The instigating forces (F) to eat the pizza include the smell and look of the pizza, and once eating it, the flavor and texture.  However, if eating the pizza, there is also a consummatory force (C) which was thought to reflect both the strength (gusto) of eating the pizza as well as some constant consummatory value of the activity (c).  If not eating the pizza, but in a pizza parlor, the smells and visual cues combine to increase the tendency to eat the pizza.  Once eating it, however, the consummatory effect is no longer zero, and the change in action tendency will be a function of both the instigating forces and the consummatory forces.  These will achieve a balance when instigating forces are equal to the consummatory forces.  The asymptotic strength of eating the pizza reflects this balance and does not require a “set point" or “comparator". To avoid the problems of instigating and consummatory lags and the need for a decision mechanism, it is possible to reparameterize the original DOA model in terms of action tendencies and actions (Revelle, 1986).   Rather than specifying inertia for action tendencies and a choice rule of always expressing the dominant action tendency, it is useful to distinguish between action tendencies (t) and the actions (a) themselves and to  have actions as well as tendencies  having inertial properties. By separating tendencies from actions, and giving them both inertial properties, we avoid the necessity  of a  lag parameter, and by making the decision rule one of mutual inhibition, the process is perhaps easier to understand.  In an environment which affords cues for action (c), cues enhance action tendencies (t) which in turn strengthen actions (a).  This leads to two differential equations, one describing the growth and decay of action tendencies (t), the other of the actions themselves (a). dt = Sc - Ca   andda = Et - Ia.  (See Revelle and Condon (2015) for an extensive discussion of this model.)cta simulates this model, with the addition of a learning parameter such that  activities strengthen the connection between cues and tendencies.  The learning part of the cta model is still under development.  cta.15 represents the state of the cta model as described in the Revelle and Condon (2015) article.
A very thorough discussion of the CTA model is available from Revelle (2008). An application of the model is discussed in Revelle and Condon (2015). cta.15 is the version used to produce the figures and analysis in Revelle and Condon (2015).  cta is the most recent version and includes a learning function developed in collaboration with Luke Smillie at the University of Melbourne.The dynamics of action (Atkinson and Birch, 1970)  was a model of how instigating forces elicited action tendencies which in turn elicited actions.  The basic concept was that action tendencies had inertia.  That is, a wish (action tendency) would persist until satisfied and would not change without an instigating force.  The consummatory strength of doing an action was thought  in turn to reduce the action tendency.  Forces could either be instigating or inhibitory (leading to "negaction").Perhaps the simplest example is the action tendency (T)  to eat a pizza.  The instigating forces (F) to eat the pizza include the smell and look of the pizza, and once eating it, the flavor and texture.  However, if eating the pizza, there is also a consummatory force (C) which was thought to reflect both the strength (gusto) of eating the pizza as well as some constant consummatory value of the activity (c).  If not eating the pizza, but in a pizza parlor, the smells and visual cues combine to increase the tendency to eat the pizza.  Once eating it, however, the consummatory effect is no longer zero, and the change in action tendency will be a function of both the instigating forces and the consummatory forces.  These will achieve a balance when instigating forces are equal to the consummatory forces.  The asymptotic strength of eating the pizza reflects this balance and does not require a “set point" or “comparator". To avoid the problems of instigating and consummatory lags and the need for a decision mechanism, it is possible to reparameterize the original DOA model in terms of action tendencies and actions (Revelle, 1986).   Rather than specifying inertia for action tendencies and a choice rule of always expressing the dominant action tendency, it is useful to distinguish between action tendencies (t) and the actions (a) themselves and to  have actions as well as tendencies  having inertial properties. By separating tendencies from actions, and giving them both inertial properties, we avoid the necessity  of a  lag parameter, and by making the decision rule one of mutual inhibition, the process is perhaps easier to understand.  In an environment which affords cues for action (c), cues enhance action tendencies (t) which in turn strengthen actions (a).  This leads to two differential equations, one describing the growth and decay of action tendencies (t), the other of the actions themselves (a). dt = Sc - Ca   andda = Et - Ia.  (See Revelle and Condon (2015) for an extensive discussion of this model.)cta simulates this model, with the addition of a learning parameter such that  activities strengthen the connection between cues and tendencies.  The learning part of the cta model is still under development.  cta.15 represents the state of the cta model as described in the Revelle and Condon (2015) article.
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
Describe the data using a violin plot. Change alpha to modify the shading.  The grp variable may be used to draw separate violin plots for each of multiple groups.For relatively smallish data sets (< 500-1000), it is informative to also show the actual data points. This done with the dots=TRUE option. The jitter value is arbitrarily set to .05, but making it larger (say .1 or .2) will display more points.
In basic data analysis it is vital to get basic descriptive statistics. Procedures such as summary and Hmisc::describe do so.  The describe function in the psych package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call describeBy to the processing. The results from describe can be used in graphics functions (e.g., error.crosses).The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data.  Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not  for alphanumeric data). If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described.  These variables are marked with an * in the row name.  This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected.  For instance, if education is coded "high school", "some college" , "finished college", then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all).In a typical study, one might read the data in from the clipboard (read.clipboard), show the splom plot of the correlations (pairs.panels), and then describe the data. na.rm=FALSE is equivalent to describe(na.omit(x))  When finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). If we define m_r = [sum(X- mx)^r]/n then Type 1 finds skewness and kurtosis by g_1 = m_3/(m_2)^{3/2}  and g_2 = m_4/(m_2)^2 -3.  Type 2 is G1 = g1 * √{n *(n-1)}/(n-2) and G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)).  Type 3 is b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2} and b2 =  [(n-1)/n]^{3/2} m_4/m_2^2).The additional helper function describeData just scans the data array and reports on whether the data are all numerical, logical/factorial, or categorical.  This is a useful check to run if trying to get descriptive statistics on very large data sets where to improve the speed, the check option is FALSE.  An even faster overview of the data is describeFast which reports the number of total cases, number of complete cases, number of numeric variables and the number which are factors. The fast=TRUE option will lead to a speed up of about 50% for larger problems by not finding all of the statistics (see NOTE)To describe the data for different groups, see describeBy or specify the grouping variable(s) in formula mode (see the examples). 
To get descriptive statistics for several different grouping variables, make sure that group is a list.  In the case of matrix output with multiple grouping variables, the grouping variable values are added to the output.As of July, 2020, the grouping variable(s) may be specified in formula mode (see the examples).The type parameter specifies which version of skew and kurtosis should be found.  See describe for more details. An alternative function (statsBy) returns a list of means, n, and standard deviations for each group.  This is particularly useful if finding weighted correlations of group means using cor.wt. More importantly, it does a proper within and between group decomposition of the correlation.cohen.d  will work for two groups. It converts the data into mean differences and pools the within group standard deviations.  Returns cohen.d statistic as well as the multivariate generalization (Mahalanobis D). 
To get descriptive statistics for several different grouping variables, make sure that group is a list.  In the case of matrix output with multiple grouping variables, the grouping variable values are added to the output.As of July, 2020, the grouping variable(s) may be specified in formula mode (see the examples).The type parameter specifies which version of skew and kurtosis should be found.  See describe for more details. An alternative function (statsBy) returns a list of means, n, and standard deviations for each group.  This is particularly useful if finding weighted correlations of group means using cor.wt. More importantly, it does a proper within and between group decomposition of the correlation.cohen.d  will work for two groups. It converts the data into mean differences and pools the within group standard deviations.  Returns cohen.d statistic as well as the multivariate generalization (Mahalanobis D). 
In basic data analysis it is vital to get basic descriptive statistics. Procedures such as summary and Hmisc::describe do so.  The describe function in the psych package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call describeBy to the processing. The results from describe can be used in graphics functions (e.g., error.crosses).The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data.  Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not  for alphanumeric data). If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described.  These variables are marked with an * in the row name.  This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected.  For instance, if education is coded "high school", "some college" , "finished college", then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all).In a typical study, one might read the data in from the clipboard (read.clipboard), show the splom plot of the correlations (pairs.panels), and then describe the data. na.rm=FALSE is equivalent to describe(na.omit(x))  When finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). If we define m_r = [sum(X- mx)^r]/n then Type 1 finds skewness and kurtosis by g_1 = m_3/(m_2)^{3/2}  and g_2 = m_4/(m_2)^2 -3.  Type 2 is G1 = g1 * √{n *(n-1)}/(n-2) and G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)).  Type 3 is b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2} and b2 =  [(n-1)/n]^{3/2} m_4/m_2^2).The additional helper function describeData just scans the data array and reports on whether the data are all numerical, logical/factorial, or categorical.  This is a useful check to run if trying to get descriptive statistics on very large data sets where to improve the speed, the check option is FALSE.  An even faster overview of the data is describeFast which reports the number of total cases, number of complete cases, number of numeric variables and the number which are factors. The fast=TRUE option will lead to a speed up of about 50% for larger problems by not finding all of the statistics (see NOTE)To describe the data for different groups, see describeBy or specify the grouping variable(s) in formula mode (see the examples). 
In basic data analysis it is vital to get basic descriptive statistics. Procedures such as summary and Hmisc::describe do so.  The describe function in the psych package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call describeBy to the processing. The results from describe can be used in graphics functions (e.g., error.crosses).The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data.  Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not  for alphanumeric data). If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described.  These variables are marked with an * in the row name.  This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected.  For instance, if education is coded "high school", "some college" , "finished college", then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all).In a typical study, one might read the data in from the clipboard (read.clipboard), show the splom plot of the correlations (pairs.panels), and then describe the data. na.rm=FALSE is equivalent to describe(na.omit(x))  When finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). If we define m_r = [sum(X- mx)^r]/n then Type 1 finds skewness and kurtosis by g_1 = m_3/(m_2)^{3/2}  and g_2 = m_4/(m_2)^2 -3.  Type 2 is G1 = g1 * √{n *(n-1)}/(n-2) and G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)).  Type 3 is b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2} and b2 =  [(n-1)/n]^{3/2} m_4/m_2^2).The additional helper function describeData just scans the data array and reports on whether the data are all numerical, logical/factorial, or categorical.  This is a useful check to run if trying to get descriptive statistics on very large data sets where to improve the speed, the check option is FALSE.  An even faster overview of the data is describeFast which reports the number of total cases, number of complete cases, number of numeric variables and the number which are factors. The fast=TRUE option will lead to a speed up of about 50% for larger problems by not finding all of the statistics (see NOTE)To describe the data for different groups, see describeBy or specify the grouping variable(s) in formula mode (see the examples). 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
“Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale." (Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonald has proposed coefficient omega_hierarchical (ω_h) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf compare McDonald's ω_h to Cronbach's α and Revelle's β.  They conclude that ω_h is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   One way to find omega_h is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (schmid) transformation to find general factor loadings, and then find omega_h.  Here we present code to do that.  omega_h differs as a function of how the factors are estimated.  Four options are available, three use the fa function but with different factoring methods: the default does a minres factor solution, fm="pa"  does a principle axes factor analysis  fm="mle" does a maximum likelihood solution; fm="pc" does a principal components analysis using (principal).  For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the scoreItems function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).beta, an alternative to ω_h, is defined as the worst split half reliability (Revelle, 1979).  It can be estimated by using ICLUST (a hierarchical clustering algorithm originally developed for main frames and written in Fortran and that is now part of the psych package.  (For a very complimentary review of why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). The omega function uses exploratory factor analysis to estimate the ω_h coefficient.  It is important to remember that  “A recommendation that should be heeded, regardless of the method chosen to estimate ω_h, is to always examine the pattern of the estimated general factor loadings prior to estimating ω_h. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading ω_h estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137).A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (describe) the $schmid$sl results.Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option="first" or option="second" to the call.Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.In addition to ω_h, another of McDonald's coefficients is ω_t.  This is an estimate of the total reliability of a test. McDonald's ω_t, which is similar to Guttman's λ_6, guttman but uses the estimates of uniqueness (u^2) from factor analysis to find e_j^2. This is based on a decomposition of the variance of a test score, V_x  into four parts: that due to a general factor, \vec{g}, that due to a set of group factors, \vec{f},  (factors common to some but not all of the items), specific factors, \vec{s} unique to each item, and \vec{e}, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). Letting x = cg + Af + Ds + ethen the communality of item_j, based upon general as well as group factors,h_j^2 = c_j^2 + sum(f_ij^2)and the unique variance for the itemu_j^2 = σ_j^2 (1-h_j^2)may be used to estimate the test reliability.That is, if h_j^2 is the communality of item_j, based upon general as well as group factors,  then for standardized items,  e_j^2 = 1 - h_j^2 andω_t = (1 cc' 1 + 1 AA' 1')/(V_x)Because h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.It is important to distinguish here between the two ω coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, ω_t and ω_h.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. ω_h = (1 cc' 1')/VxAnother estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1'). Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  omega is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  omegaSem first calls omega and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  ω_h is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. It is also possible to give omega a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  omegaFromSem takes the output from a sem model and uses it to find ω_h.  The estimate of factor indeterminacy, found by the multiple R^2 of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.Finally, and still being tested, is omegaDirect adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (directSl).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures omega and omegaSem.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.Moral: Finding omega_h is tricky and one should probably compare omega, omegaSem,  omegaDirect and even iclust solutions to understand the differences.The summary of the omega object is a reduced set of the most useful output. The various objects returned from omega include:
Congruences are the cosines of pairs of vectors defined by a matrix and based at the origin.  Thus, for values that differ only by a scaler the congruence will be 1.For two matrices, F1 and F2, the measure of  congruence, phi, is phi = sum(F1 F2)/sqrt(sum(F1^2) sum(F2^2)) It is an interesting exercise to compare  congruences with the correlations of the respective values. Congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of items are cosines of the vectors based at the mean loading for each column.   phi = sum((F1-a)(F2-b))/sqrt(sum((F1-a)^2) sum((F2-b)^2)) .For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.Input may either be one or two matrices.For congruences of factor or component loading matrices, use factor.congruence.Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.cohen.profile applies the congruence function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). distance finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r >2).
A graphic demonstration of the tetrachoric correlation. Used for teaching purposes.  The default values are for a correlation of .5 with cuts at 1 and 1. Any other values are possible.  The code is also a demonstration of how to use the layout function for complex graphics using base graphics. 
A graphic demonstration of the tetrachoric correlation. Used for teaching purposes.  The default values are for a correlation of .5 with cuts at 1 and 1. Any other values are possible.  The code is also a demonstration of how to use the layout function for complex graphics using base graphics. 
When coding demographic information, it is typical to create one variable with multiple categorical values (e.g., ethnicity, college major, occupation). dummy.code will convert these categories into n distinct dummy coded variables.If there are many possible values (e.g., country in the SAPA data set) then specifying top will assign dummy codes to just a subset of the data.If using dummy coded variables as predictors, remember to use n-1 variables.If group is specified, then all values of x that are in group are given the value of 1, otherwise, 0. (Useful for combining a range of science majors into STEM or not. The example forms a dummy code of any smoking at all.)
NA
NA
Ellipse dimensions are calculated from the correlation between the x and y variables and are scaled as sqrt(1+r) and sqrt(1-r). They are then scaled as size[1] and size[2] standard deviation units.   To scale  for 95 and 99 percent confidence use c(1.64,2.32)
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
Drawing the mean +/- a confidence interval is a frequently used function when reporting experimental results. By default, the confidence interval is 1.96 standard errors of the t-distribution.  If within=TRUE, the error bars are corrected for the correlation with the other variables by reducing the variance by a factor of (1-smc).  This allows for comparisons between variables.The error bars are normally calculated from the data using the describe function.  If, alternatively, a matrix of statistics is provided with column headings of values, means, and se, then those values will be used for the plot (using the stats option).  If n is included in the matrix of statistics, then the distribution is drawn for a t distribution for n-1 df.  If n is omitted (NULL) or is NA, then the distribution will be a normal distribution.  If sd is TRUE, then the error bars will represent one standard deviation from the mean rather than be a function of alpha and the standard errors.See the last two examples for the  case of plotting data with statistics from another function. Alternatively, error.bars.tab will take tabulated data and convert to either row, column or overall percentages, and then plot these as percentages with the equivalent standard error (based upon sqrt(pq/N)).In August, 2018, the functionality of error.bars and error.bars.by were  combined so that if groups are specified, then the error bars are done by group.  Furthermore, if the x variable is a formula of the form DV ~ IV, then error.bars.by is called to do the plotting.
Drawing the mean +/- a confidence interval is a frequently used function when reporting experimental results. By default, the confidence interval is 1.96 standard errors (adjusted for the t-distribution). Improved/modified in August, 2018 to allow formula input (see examples) as well as to more properly handle multiple groups. Following a request for better labeling of the grouping variables, the v.lab option is implemented for line graphs as well as bar graphs.  Note that if using multiple grouping variables, the labels are for the variable with the most levels (which should be the first one.)This function was originally just a wrapper for error.bars but has been written to allow groups to be organized either as the x axis or as separate lines.If desired, a barplot with error bars can be shown. Many find this type of plot to be uninformative (e.g.,   https://biostat.mc.vanderbilt.edu/DynamitePlots ) and recommend the more standard dot plot. Note in particular, if choosing to draw barplots, the starting value is 0.0 and setting the ylim parameter can lead to some awkward results if 0 is not included in the ylim range.  Did you really mean to draw a bar plot in this case?For up to three groups, the colors are by default "black", "blue" and "red". For more than 3 groups, they are by default rainbow colors with an alpha factor (transparency) of .5.To make colors semitransparent, set the density to a negative number.  See the last example.
Drawing the mean +/- a confidence interval is a frequently used function when reporting experimental results. By default, the confidence interval is 1.96 standard errors of the t-distribution.  If within=TRUE, the error bars are corrected for the correlation with the other variables by reducing the variance by a factor of (1-smc).  This allows for comparisons between variables.The error bars are normally calculated from the data using the describe function.  If, alternatively, a matrix of statistics is provided with column headings of values, means, and se, then those values will be used for the plot (using the stats option).  If n is included in the matrix of statistics, then the distribution is drawn for a t distribution for n-1 df.  If n is omitted (NULL) or is NA, then the distribution will be a normal distribution.  If sd is TRUE, then the error bars will represent one standard deviation from the mean rather than be a function of alpha and the standard errors.See the last two examples for the  case of plotting data with statistics from another function. Alternatively, error.bars.tab will take tabulated data and convert to either row, column or overall percentages, and then plot these as percentages with the equivalent standard error (based upon sqrt(pq/N)).In August, 2018, the functionality of error.bars and error.bars.by were  combined so that if groups are specified, then the error bars are done by group.  Furthermore, if the x variable is a formula of the form DV ~ IV, then error.bars.by is called to do the plotting.
For an example of two way error bars describing the effects of mood manipulations upon positive and negative affect, see https://personality-project.org/revelle/publications/happy-sad-appendix/FIG.A-6.pdfThe second example shows how error crosses can be done for multiple variables where the grouping variable is found dynamically. The errorCircles example shows how to do this in one step.
Adapted from the dot.chart function to include error bars and to use the output of describe,  describeBy,   statsBy, fa, bestScales or cohen.d.   To speed up multiple plots, the function can work from the output of a previous run.  Thus describeBy will be done and the results can be show for multiple variables.If using the add=TRUE option to add an error.dots plot to a dotplot, note that the order of variables in dot plots goes from last to first (highest y value is actually the last value in a vector.)  Also note that the xlim parameter should be set to make sure the plots line up correctly.
When visualizing the effect of an experimental manipulation or the relationship of multiple groups, it is convenient to plot their means as well as their confidence regions in a two dimensional space.  The diameter of the enclosing circle (ellipse) scales as 1/sqrt(N) * the maximum standard error of all variables.  That is to say, the area of the ellipse reflects sample size.  
Factor analysis as implemented in fa attempts to  summarize the covariance (correlational) structure of a set of variables with a small set of latent variables or “factors".  This solution may be ‘extended’ into a larger space with more variables without changing the original solution (see fa.extension.  Similarly,  the factors of a second set of variables  (the Y set) may be extended into the original (X ) set.  Doing so allows two independent measurement models, a measurement model for X and a measurement model for Y.  These two sets of latent variables may then be correlated  for an Exploratory Structural Equation Model.  (This is exploratory because it is based upon exploratory factor analysis (EFA) rather than a confirmatory factor model (CFA) using more traditional Structural Equation Modeling packages such as sem, lavaan, or Mx.)Although the output seems very similar to that of a normal EFA using  fa, it is actually two independent factor analyses (of the X and the Y sets) that are then mutually extended into each other.  That is, the loadings and structure matrices from sets X and Y are merely combined, and the correlations between the two sets of factors are found.Interbattery factor analysis was developed by Tucker (1958) as a way of comparing the factors in common to two batteries of tests.   (Currently under development and not yet complete). Using some straight forward linear algebra It is easy to find the factors of the intercorrelations between the two sets of variables.  This does not require estimating communalities and is highly related to the procedures of canonical correlation.  The difference between the esem and the interbattery approach is that the first factors the X set and then relates those factors to factors of the Y set.  Interbattery factor analysis, on the other hand, tries to find one set of factors that links both sets but is still distinct from factoring both sets together.  
Factor analysis as implemented in fa attempts to  summarize the covariance (correlational) structure of a set of variables with a small set of latent variables or “factors".  This solution may be ‘extended’ into a larger space with more variables without changing the original solution (see fa.extension.  Similarly,  the factors of a second set of variables  (the Y set) may be extended into the original (X ) set.  Doing so allows two independent measurement models, a measurement model for X and a measurement model for Y.  These two sets of latent variables may then be correlated  for an Exploratory Structural Equation Model.  (This is exploratory because it is based upon exploratory factor analysis (EFA) rather than a confirmatory factor model (CFA) using more traditional Structural Equation Modeling packages such as sem, lavaan, or Mx.)Although the output seems very similar to that of a normal EFA using  fa, it is actually two independent factor analyses (of the X and the Y sets) that are then mutually extended into each other.  That is, the loadings and structure matrices from sets X and Y are merely combined, and the correlations between the two sets of factors are found.Interbattery factor analysis was developed by Tucker (1958) as a way of comparing the factors in common to two batteries of tests.   (Currently under development and not yet complete). Using some straight forward linear algebra It is easy to find the factors of the intercorrelations between the two sets of variables.  This does not require estimating communalities and is highly related to the procedures of canonical correlation.  The difference between the esem and the interbattery approach is that the first factors the X set and then relates those factors to factors of the Y set.  Interbattery factor analysis, on the other hand, tries to find one set of factors that links both sets but is still distinct from factoring both sets together.  
Path diagram representations have become standard in confirmatory factor analysis, but are not yet common in exploratory factor analysis.  Representing factor structures graphically helps some people understand the structure. By default the arrows come from the latent variables to the observed variables.  This is, of course, the factor model.  However, if the class of the object to be drawn is 'principal', then  reverse the direction of the arrows, and the 'latent' variables are no longer latent, and are shown as boxes. For cluster models, the default is to treat them as factors, but if ic =TRUE, then we treat it as a components model.fa.diagram does not use Rgraphviz and is the preferred function.  fa.graph generates dot code to be used by an external graphics program. It does not have all the bells and whistles of fa.diagram, but these may be done in the external editor. Hierarchical (bifactor) models may be drawn by specifying the g parameter as TRUE.  This allows for an graphical displays of various factor transformations with a bifactor structure (e.g., bifactor and biquartimin.  See omega for an alternative way to find these structures.  The het.diagram function will show the case of a hetarchical structure at multiple levels.  It can also be used to show the patterns of correlations between sets of scales (e.g., EPI, NEO, BFI).  The example is for showing the relationship between 3 sets of 4 variables from the Thurstone data set. The parameters l.cex and gap.size are used to adjust the font size of the labels and the gap in the lines.  extension.diagram will draw a fa.extend result with slightly more control than using fa.diagram or the more generic diagram function.In fa.rgraph although a nice graph is drawn for the orthogonal factor case, the oblique factor drawing is acceptable, but is better if cleaned up outside of R or done using fa.diagram. The normal input is taken from the output of either fa or ICLUST. This latter case displays the ICLUST results in terms of the cluster loadings, not in terms of the cluster structure.  Actually an interesting option.It is also possible to just give a factor loading matrix as input.  In this case, supplying a Phi matrix of factor correlations is also possible. It is possible, using fa.graph, to export dot code for an omega solution.  fa.graph should be applied to the schmid$sl object with labels specified as the rownames of schmid$sl.  The results will need editing to make fully compatible with dot language plotting.  To specify the model for a structural equation confirmatory analysis of the results, use structure.diagram instead. 
Factor analysis is an attempt to approximate a correlation or covariance matrix with one of lesser rank.  The basic model is that nRn = nFk kFn' + U2 where k is much less than n. There are many ways to do factor analysis, and maximum likelihood procedures are probably the most commonly preferred (see factanal ).  The existence of uniquenesses is what distinguishes factor analysis from principal components analysis (e.g., principal). If variables are thought to represent a “true" or latent part then factor analysis provides an estimate of the correlations with the latent factor(s) representing the data.  If variables are thought to be measured without error, then principal components provides the most parsimonious description of the data.  Factor loadings will be smaller than component loadings for the later reflect unique error in each variable. The off diagonal residuals for a factor solution will be superior (smaller)  that of a component model.  Factor loadings can be thought of as the asymptotic component loadings as the number of variables loading on each factor increases.  The fa function will do factor analyses using one of six different algorithms: minimum residual (minres, aka ols, uls), principal axes,  alpha factoring, weighted least squares, minimum rank, or maximum likelihood.Principal axes factor analysis has a long history in exploratory analysis and is a straightforward procedure.  Successive eigen value decompositions are done on a correlation matrix with the diagonal replaced with  diag (FF') until ∑(diag(FF')) does not change (very much).  The current limit of max.iter =50 seems to work for most problems, but the Holzinger-Harmon 24 variable problem needs about 203 iterations to converge for a 5 factor solution.  Not all factor programs that do principal axes do iterative solutions.  The example from the SAS manual (Chapter 33) is such a case. To achieve that solution, it is necessary to specify that the max.iter = 1.  Comparing that solution to an iterated one (the default) shows that iterations improve the solution. In addition, fm="mle" produces even better solutions for this example.  Both the RMSEA and the root mean square of the residuals are smaller than the fm="pa" solution.However, simulations of multiple problem sets suggest that fm="pa" tends to produce slightly smaller residuals while having slightly larger RMSEAs than does fm="minres" or fm="mle".    That is, the sum of squared residuals for fm="pa" seem to be slightly smaller than those found using fm="minres" but the RMSEAs are slightly worse when using fm="pa".  That is to say, the "true" minimal residual is probably found by fm="pa". Following extensive correspondence with Hao Wu and Mikko Ronkko, in April, 2017 the derivative of the minres  and uls) fitting was modified.  This leads to slightly smaller residuals (appropriately enough for a method claiming to minimize them) than the prior procedure.  For  consistency with prior analyses, "old.min" was added to give these slightly larger residuals.  The differences between old.min and the newer "minres" and "ols" solutions are at the third to fourth decimal, but none the less, are worth noting. For comparison purposes, the fm="ols" uses empirical first derivatives, while uls and minres use equation based first derivatives.  The results seem to be identical, but the minres and uls solutions require fewer iterations for larger problems and are faster. Thanks to Hao Wu for some very thoughtful help.  Although usually these various algorithms produce equivalent results, there are several data sets included that show large differences between the methods. Schutz produces Heywood and super Heywood cases,   blant leads to very different solutions.  In particular, the minres solution produces smaller residuals than does the mle solution, and the factor.congruence coefficients show very different solutions.A very strong argument against using MLE is found in the chapter by MacCallum, Brown and Cai (2007) who show that OLS approaches produce equivalent solutions most of the time, and better solutions some of the time.  This particularly in the case of models with some unmodeled small factors.  (See sim.minor to generate such data.)Principal axes may be used in cases when maximum likelihood solutions fail to converge, although fm="minres" will also do that and tends to produce better (smaller RMSEA) solutions.The fm="minchi" option is a variation on the "minres" (ols) solution and minimizes the sample size weighted residuals rather than just the residuals. This was developed to handle the problem of data that Massively Missing Completely at Random (MMCAR) which a condition that happens in the SAPA project.A traditional problem in factor analysis was to find the best estimate of the original communalities in order to speed up convergence.  Using the Squared Multiple Correlation (SMC) for each variable will underestimate the original communalities, using 1s will over estimate.  By default, the SMC estimate is used.  Note that in the case of non-invertible matrices, the pseudo-inverse is found so smcs are still estimated. In either case, iterative techniques will tend to converge on a stable solution. If, however, a solution fails to be achieved, it is useful to try again using ones (SMC =FALSE).  Alternatively, a vector of starting values for the communalities may be specified by the SMC option.The iterated principal axes algorithm does not attempt to find the best (as defined by a maximum likelihood criterion) solution, but rather one that converges rapidly using successive eigen value decompositions.  The maximum likelihood criterion of fit and the associated chi square value are reported, and will be (slightly) worse than that found using maximum likelihood procedures.The minimum residual (minres) solution is an unweighted least squares solution that takes a slightly different approach.  It uses the optim function and adjusts the diagonal elements of the correlation matrix to mimimize the squared residual when the factor model is the eigen value decomposition of the reduced matrix.  MINRES and PA will both work when ML will not, for they can be used when the matrix is singular. Although before the change in the derivative,  the MINRES solution was slightly more similar to the ML solution than is the PA solution. With the change in the derivative of the minres fit, the minres, pa and uls solutions are practically identical. To a great extent, the minres and wls solutions follow ideas in the factanal function with the change in the derivative. The weighted least squares (wls) solution weights the residual matrix by 1/ diagonal of the inverse of the correlation matrix.  This has the effect of weighting items with low communalities more than those with high communalities. The generalized least squares (gls) solution weights the residual matrix by the inverse of the correlation matrix.  This has the effect of weighting those variables with low communalities even more than those with high communalities.The maximum likelihood solution takes yet another approach and finds those communality values that minimize the chi square goodness of fit test.  The fm="ml" option provides a maximum likelihood solution following the procedures used in factanal but does not provide all the extra features of that function.  It does, however, produce more expansive output.The minimum rank factor model (MRFA) roughly follows ideas by Shapiro and Ten Berge (2002) and Ten Berge and Kiers (1991).  It makes use of the glb.algebraic procedure contributed by Andreas Moltner.  MRFA attempts to extract factors such that the residual matrix is still positive semi-definite.  This version is still being tested and feedback is most welcome.Alpha factor analysis finds solutions based upon a correlation matrix corrected for communalities and then rescales these to the original correlation matrix.  This procedure is described by Kaiser and Coffey, 1965.  Test cases comparing the output to SPSS suggest that the PA algorithm matches what SPSS calls uls, and that the wls solutions are equivalent in their fits. The wls and gls solutions have slightly larger eigen values, but slightly worse fits of the off diagonal residuals than do the minres or maximum likelihood solutions.  Comparing the results to the examples in Harman (1976), the PA solution with no iterations matches what Harman calls Principal Axes (as does SAS), while the iterated PA solution matches his minres solution.  The minres solution found in psych tends to have slightly smaller off diagonal residuals (as it should) than does the iterated PA solution.   Although for items, it is typical to find factor scores by scoring the salient items (using, e.g., scoreItems) factor scores can be estimated by regression as well as several other means. There are multiple approaches that are possible (see Grice, 2001) and  one taken here was developed by tenBerge et al.(see factor.scores). The alternative, which will match factanal is to find the scores using regression –  Thurstone's least squares regression where the weights are found byW = inverse(R)S where R is the correlation matrix of the variables ans S is the structure matrix.  Then, factor scores are just Fs = X W.In the oblique case, the factor loadings are referred to as Pattern coefficients and are related to the Structure coefficients by S = P Phi and thus P = S Phi^-1.  When estimating factor scores, fa and  factanal differ in that fa finds the factors from the Structure matrix while factanal seems to do it from the Pattern matrix.  Thus, although in the orthogonal case, fa and factanal agree perfectly in their factor score estimates, they do not agree in the case of oblique factors.  Setting oblique.scores = TRUE  will produce factor score estimate that match those of factanal.It is sometimes useful to extend the factor solution to variables that were not factored.  This may be done using fa.extension. Factor extension is typically done in the case where some variables were not appropriate to factor, but factor loadings on the original factors are still desired.  Factor extension is a very powerful procedure in that it allows one to find the factor-criterion correlations without using factor scores. For dichotomous items or polytomous items, it is recommended to analyze the tetrachoric or polychoric correlations rather than the Pearson correlations. This may be done by specifying cor="poly" or cor="tet" or cor="mixed" if the data have a mixture of dichotomous, polytomous, and continous variables.  Analysis of dichotomous or polytomous data may also be done by using  irt.fa or simply setting the cor="poly" option.  In the first case,  the factor analysis results are reported in Item Response Theory (IRT) terms, although the original factor solution is returned in the results. In the later case, a typical factor loadings matrix is returned, but the tetrachoric/polychoric correlation matrix and item statistics are saved for reanalysis by irt.fa. (See also the mixed.cor function to find correlations from a mixture of continuous, dichotomous, and polytomous items.)Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The “cluster” option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional "keys" parameter, the "target" option will rotate to a target supplied as a keys matrix. (See target.rot.)oblimin is implemented in GPArotation by a call to quartimin with delta=0.  This leads to confusion when people refer to quartimin solutions. It is important to note a dirty little secret about factor rotation.  That is the problem of local minima.  Multiple restarts of the rotation algorithms are strongly encouraged (see Nguyen and Waller, N. G. 2021). Two additional target rotation options are available through calls to GPArotation.  These are the targetQ (oblique) and targetT (orthogonal) target rotations of Michael Browne.  See target.rot for more documentation. The "bifactor" rotation implements the Jennrich and Bentler (2011) bifactor rotation by calling the GPForth function in the GPArotation package and using two functions adapted from  the MatLab code of Jennrich and Bentler.  This seems to have a problem with local minima and multiple starting values should be used.There are two varimax rotation functions.  One, Varimax, in the GPArotation package does not by default apply Kaiser normalization.  The other, varimax, in the stats package, does.  It appears that the two rotation functions produce slightly different results even when normalization is set. For consistency with the other rotation functions, Varimax is probably preferred.The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by Phi = Th' ThThere are two ways to handle dichotomous or polytomous responses: fa with the cor="poly" option which will return the tetrachoric or polychoric correlation matrix, as well as the normal factor analysis output, and irt.fa which returns a two parameter irt analysis as well as the normal fa output. When factor analyzing items with dichotomous or polytomous responses, the irt.fa function provides an Item Response Theory representation of the factor output. The factor analysis results are available, however, as an object in the irt.fa output. fa.poly is deprecated, for its functioning is matched by setting cor="poly".  It will produce normal factor analysis output but also will save the polychoric matrix  (rho) and items difficulties (tau) for subsequent irt analyses.  fa.poly will,  by default, find factor scores if the data are available.  The correlations are found using either tetrachoric or polychoric and then this matrix is factored.  Weights from the factors are then applied to the original data to estimate factor scores.The function fa will repeat the analysis n.iter times on a bootstrapped sample of the data (if they exist) or of a simulated data set based upon the observed correlation matrix.  The mean estimate and standard deviation of the estimate are returned and will print the original factor analysis as well as the alpha level confidence intervals for the estimated coefficients.  The bootstrapped solutions are rotated towards the original solution using target.rot. The factor loadings are z-transformed, averaged and then back transformed. This leads to an error in the case of Heywood cases.  The probably better alternative is to just find the mean bootstrapped value and find the confidence intervals based upon the observed range of values. The default is to have n.iter =1 and thus not do bootstrapping.If using polytomous or dichotomous items, it is perhaps more useful  to find the Item Response Theory parameters equivalent to the factor loadings reported in fa.poly by using the irt.fa function.  Some correlation matrices that arise from using pairwise deletion or from tetrachoric or polychoric matrices will not be proper.  That is, they will not be positive semi-definite (all eigen values >= 0).  The cor.smooth function will adjust correlation matrices (smooth them) by making all negative eigen values slightly greater than 0, rescaling the other eigen values to sum to the number of variables, and then recreating the correlation matrix.  See cor.smooth for an example of this problem using the burt data set.One reason for this problem when using tetrachorics or polychorics seems to be the adjustment for continuity.  Setting correct=0 turns this off and seems to produce more proper matrices.For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin KMO test may be found from the correlation matrix or data matrix using the KMO function.  Similarly, the Bartlett's test of Sphericity may be found using the  cortest.bartlett function.For those who want to have an object of the variances accounted for, this is returned invisibly by the print function.  (e.g., p <- print(fa(ability))$Vaccounted ).  This is now returned by the fa function as well (e.g. p <- fa(ability)$Vaccounted ).  Just as communalities may be found by the diagonal of Pattern %*% t(Structure) so can the variance accounted for be found by diagonal ( t(Structure) %*% Pattern.  Note that referred to as SS loadings. The output from the print.psych.fa function displays the factor loadings (from the pattern matrix, the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the factor loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared factor loadings. But for an oblique solution, it is the row sum of the orthogonal factor loadings (remember, that rotations or transformations do not change the communality).  In response to a request from Asghar Minaei who wanted to combine several imputation data sets from mice, fa.pooled was added.  The separate data sets are combined into a list (datasets) which are then factored separately. Each solution is rotated towards the first set in the list.  The results are then reported in terms of pooled loadings and confidence intervals based upon the replications.  fa.sapa simulates the process of doing SAPA (Synthetic Aperture Personality Assessment).  It will do iterative solutions for successive random samples of  fractions (frac) of the data set. This allows us to find the stability of solutions for various sample sizes and various sample rates. Need to specify the number of iterations (n.iter) as well as the percent of data sampled (frac).  
Find the coefficient of factor congruence between two sets of factor loadings. Factor congruences are the cosines of pairs of vectors defined by the loadings matrix and based at the origin.  Thus, for loadings that differ only by a scaler (e.g. the size of the eigen value), the factor congruences will be 1.For factor loading vectors of F1 and F2 the measure of factor congruence, phi, is phi = sum(F1 F2)/sqrt(sum(F1^2) sum(F2^2)) It is an interesting exercise to compare factor congruences with the correlations of factor loadings.  Factor congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of factor loadings are cosines of the vectors based at the mean loading for each factor.   phi = sum((F1-a)(F2-b))/sqrt(sum((F1-a)^2) sum((F2-b)^2)) .For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.Input may either be matrices or factor analysis or principal components analyis output (which includes a loadings object), or a mixture of the two.To compare more than two solutions, x may be a list of matrices, all of which will be compared.Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.cohen.profile applies the congruence function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). distance finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r >2).
Path diagram representations have become standard in confirmatory factor analysis, but are not yet common in exploratory factor analysis.  Representing factor structures graphically helps some people understand the structure. By default the arrows come from the latent variables to the observed variables.  This is, of course, the factor model.  However, if the class of the object to be drawn is 'principal', then  reverse the direction of the arrows, and the 'latent' variables are no longer latent, and are shown as boxes. For cluster models, the default is to treat them as factors, but if ic =TRUE, then we treat it as a components model.fa.diagram does not use Rgraphviz and is the preferred function.  fa.graph generates dot code to be used by an external graphics program. It does not have all the bells and whistles of fa.diagram, but these may be done in the external editor. Hierarchical (bifactor) models may be drawn by specifying the g parameter as TRUE.  This allows for an graphical displays of various factor transformations with a bifactor structure (e.g., bifactor and biquartimin.  See omega for an alternative way to find these structures.  The het.diagram function will show the case of a hetarchical structure at multiple levels.  It can also be used to show the patterns of correlations between sets of scales (e.g., EPI, NEO, BFI).  The example is for showing the relationship between 3 sets of 4 variables from the Thurstone data set. The parameters l.cex and gap.size are used to adjust the font size of the labels and the gap in the lines.  extension.diagram will draw a fa.extend result with slightly more control than using fa.diagram or the more generic diagram function.In fa.rgraph although a nice graph is drawn for the orthogonal factor case, the oblique factor drawing is acceptable, but is better if cleaned up outside of R or done using fa.diagram. The normal input is taken from the output of either fa or ICLUST. This latter case displays the ICLUST results in terms of the cluster loadings, not in terms of the cluster structure.  Actually an interesting option.It is also possible to just give a factor loading matrix as input.  In this case, supplying a Phi matrix of factor correlations is also possible. It is possible, using fa.graph, to export dot code for an omega solution.  fa.graph should be applied to the schmid$sl object with labels specified as the rownames of schmid$sl.  The results will need editing to make fully compatible with dot language plotting.  To specify the model for a structural equation confirmatory analysis of the results, use structure.diagram instead. 
It is sometimes the case that factors are derived from a set of variables (the Fo factor loadings) and we want to see what the loadings of an extended set of variables (Fe) would be. Given the original correlation matrix Ro and the correlation of these original variables with the extension variables of Roe, it is a straight forward calculation to find the loadings Fe of the extended variables on the original factors.  This technique was developed by Dwyer (1937) for the case of adding new variables to a factor analysis without doing all the work over again. But, as discussed by Horn (1973) factor extension is also appropriate when one does not want to include the extension variables in the original factor analysis, but does want to see what the loadings would be anyway.This could be done by estimating the factor scores and then finding the covariances of the extension variables with the factor scores. But if the original data are not available, but just the covariance or correlation matrix is, then the use of fa.extension is most appropriate. The factor analysis results from either fa or omega functions applied to the original correlation matrix is extended to the extended variables given the correlations (Roe) of the extended variables with the original variables.fa.extension assumes that the original factor solution was found by the fa function.For a very nice discussion of the relationship between factor scores, correlation matrices, and the factor loadings in a factor extension, see Horn (1973).The fa.extend function may be thought of as a "seeded" factor analysis.  That is, the variables in the original set are factored, this solution is then extended to the extension set, and the resulting output is presented as if both the original and extended variables were factored together.  This may also be done for an omega analysis. The example of  codefa.extend compares the extended solution to a direct solution of all of the variables using factor.congruence. 
It is sometimes the case that factors are derived from a set of variables (the Fo factor loadings) and we want to see what the loadings of an extended set of variables (Fe) would be. Given the original correlation matrix Ro and the correlation of these original variables with the extension variables of Roe, it is a straight forward calculation to find the loadings Fe of the extended variables on the original factors.  This technique was developed by Dwyer (1937) for the case of adding new variables to a factor analysis without doing all the work over again. But, as discussed by Horn (1973) factor extension is also appropriate when one does not want to include the extension variables in the original factor analysis, but does want to see what the loadings would be anyway.This could be done by estimating the factor scores and then finding the covariances of the extension variables with the factor scores. But if the original data are not available, but just the covariance or correlation matrix is, then the use of fa.extension is most appropriate. The factor analysis results from either fa or omega functions applied to the original correlation matrix is extended to the extended variables given the correlations (Roe) of the extended variables with the original variables.fa.extension assumes that the original factor solution was found by the fa function.For a very nice discussion of the relationship between factor scores, correlation matrices, and the factor loadings in a factor extension, see Horn (1973).The fa.extend function may be thought of as a "seeded" factor analysis.  That is, the variables in the original set are factored, this solution is then extended to the extension set, and the resulting output is presented as if both the original and extended variables were factored together.  This may also be done for an omega analysis. The example of  codefa.extend compares the extended solution to a direct solution of all of the variables using factor.congruence. 
Path diagram representations have become standard in confirmatory factor analysis, but are not yet common in exploratory factor analysis.  Representing factor structures graphically helps some people understand the structure. By default the arrows come from the latent variables to the observed variables.  This is, of course, the factor model.  However, if the class of the object to be drawn is 'principal', then  reverse the direction of the arrows, and the 'latent' variables are no longer latent, and are shown as boxes. For cluster models, the default is to treat them as factors, but if ic =TRUE, then we treat it as a components model.fa.diagram does not use Rgraphviz and is the preferred function.  fa.graph generates dot code to be used by an external graphics program. It does not have all the bells and whistles of fa.diagram, but these may be done in the external editor. Hierarchical (bifactor) models may be drawn by specifying the g parameter as TRUE.  This allows for an graphical displays of various factor transformations with a bifactor structure (e.g., bifactor and biquartimin.  See omega for an alternative way to find these structures.  The het.diagram function will show the case of a hetarchical structure at multiple levels.  It can also be used to show the patterns of correlations between sets of scales (e.g., EPI, NEO, BFI).  The example is for showing the relationship between 3 sets of 4 variables from the Thurstone data set. The parameters l.cex and gap.size are used to adjust the font size of the labels and the gap in the lines.  extension.diagram will draw a fa.extend result with slightly more control than using fa.diagram or the more generic diagram function.In fa.rgraph although a nice graph is drawn for the orthogonal factor case, the oblique factor drawing is acceptable, but is better if cleaned up outside of R or done using fa.diagram. The normal input is taken from the output of either fa or ICLUST. This latter case displays the ICLUST results in terms of the cluster loadings, not in terms of the cluster structure.  Actually an interesting option.It is also possible to just give a factor loading matrix as input.  In this case, supplying a Phi matrix of factor correlations is also possible. It is possible, using fa.graph, to export dot code for an omega solution.  fa.graph should be applied to the schmid$sl object with labels specified as the rownames of schmid$sl.  The results will need editing to make fully compatible with dot language plotting.  To specify the model for a structural equation confirmatory analysis of the results, use structure.diagram instead. 
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
See fa and omega for a discussion of factor analysis and of the case of one higher order factor.
See fa and omega for a discussion of factor analysis and of the case of one higher order factor.
The fa.results$loadings are replaced with sorted loadings.fa.organize takes a factor analysis or components output and reorganizes the factors in the o order. Items are organized in the i order.  This is useful when comparing alternative factor solutions.The flip option works only for the case of matrix input, not for full fa objects. Use the  reflect function.
Cattell's “scree" test is one of most simple tests for the number of factors problem.  Horn's (1965) “parallel" analysis is an equally compelling procedure.  Other procedures for determining the most optimal number of factors include finding the Very Simple Structure (VSS) criterion (VSS ) and Velicer's MAP procedure (included in VSS). Both the VSS and the MAP criteria are included in the nfactors function which also reports the  mean item complexity and the BIC for each of multiple solutions.   fa.parallel plots the eigen values for a principal components and the factor solution (minres by default) and does the same for random matrices of the same size as the original data matrix.  For raw data, the random matrices are 1) a matrix of univariate normal data and 2) random samples (randomized across rows) of the original data.fa.parallelwith the  cor=poly option will do what fa.parallel.polyexplicitly does: parallel analysis for polychoric and tetrachoric factors. If the data are dichotomous, fa.parallel.polywill find tetrachoric correlations for the real and simulated data, otherwise, if the number of categories is less than 10, it will find polychoric correlations.  Note that fa.parallel.poly is slower than fa.parallel because of the complexity of calculating the tetrachoric/polychoric correlations.  The functionality of fa.parallel.poly is included in fa.parallel with cor=poly option (etc.) option but the older fa.parallel.poly is kept for those who call it directly.That is, fa.parallel now will do tetrachorics or polychorics directly if the cor option is set to "tet" or "poly".  As with fa.parallel.poly this will take longer.  The means of (ntrials) random solutions are shown.  Error bars are usually very small and are suppressed by default but can be shown if requested.  If the sim option is set to TRUE (default), then parallel analyses are done on resampled data as well as random normal data. In the interests of speed, the parallel analyses are done just on resampled data if sim=FALSE.    Both procedures tend to agree.  As of version 1.5.4, I added the ability to specify the quantile of the simulated/resampled data, and to plot standard deviations or standard errors.  By default, this is set to the 95th percentile.  Alternative ways to estimate the number of factors problem are discussed in the Very Simple Structure  (Revelle and Rocklin, 1979) documentation (VSS) and include Wayne Velicer's MAP algorithm (Veicer, 1976).  Parallel analysis for factors is actually harder than it seems, for the question is what are the appropriate communalities to use.  If communalities are estimated by the Squared Multiple Correlation (SMC) smc, then the eigen values of the original data will reflect major as well as minor factors (see sim.minor to simulate such data).  Random data will not, of course, have any structure and thus the number of factors will tend to be biased upwards by the presence of the minor factors.  By default, fa.parallel estimates the communalities based upon a one factor minres solution.  Although this will underestimate the communalities, it does seem to lead to better solutions on simulated or real (e.g., the bfi or Harman74) data sets.  For comparability with other algorithms (e.g, the paran function in the paran package), setting smc=TRUE will use smcs as estimates of communalities. This will tend towards identifying more factors than the default option.Yet another option (suggested by Florian Scharf) is to estimate the eigen values based upon a particular factor model (e.g., specify nfactors > 1).   Printing the results will show the eigen values of the original data that are greater than simulated values.A sad observation about parallel analysis is that it is sensitive to sample size.  That is, for large data sets, the eigen values of random data are very close to 1.  This will lead to different estimates of the number of factors as a function of sample size.  Consider factor structure of the bfi data set (the first 25 items are meant to represent a five factor model).  For samples of 200 or less, parallel analysis suggests 5 factors, but for 1000 or more, six factors and components are indicated.  This is not due to an instability of the eigen values of the real data, but rather the closer approximation to 1 of the random data as n increases.Although with nfactors=1, 6 factors are suggested, when specifying nfactors =5, parallel analysis of the bfi suggests 12 factors should be extracted!When simulating dichotomous data in fa.parallel.poly, the simulated data have the same difficulties as the original data.  This functionally means that the simulated and the resampled results will be very similar.  Note that fa.parallel.poly has functionally been replaced with fa.parallel with the cor="poly" option.As with many psych functions, fa.parallel has been changed to allow for multicore processing.  For running a large number of iterations, it is obviously faster to increase the number of cores to the maximum possible (using the options("mc.cores=n) command where n is determined from detectCores(). 
Cattell's “scree" test is one of most simple tests for the number of factors problem.  Horn's (1965) “parallel" analysis is an equally compelling procedure.  Other procedures for determining the most optimal number of factors include finding the Very Simple Structure (VSS) criterion (VSS ) and Velicer's MAP procedure (included in VSS). Both the VSS and the MAP criteria are included in the nfactors function which also reports the  mean item complexity and the BIC for each of multiple solutions.   fa.parallel plots the eigen values for a principal components and the factor solution (minres by default) and does the same for random matrices of the same size as the original data matrix.  For raw data, the random matrices are 1) a matrix of univariate normal data and 2) random samples (randomized across rows) of the original data.fa.parallelwith the  cor=poly option will do what fa.parallel.polyexplicitly does: parallel analysis for polychoric and tetrachoric factors. If the data are dichotomous, fa.parallel.polywill find tetrachoric correlations for the real and simulated data, otherwise, if the number of categories is less than 10, it will find polychoric correlations.  Note that fa.parallel.poly is slower than fa.parallel because of the complexity of calculating the tetrachoric/polychoric correlations.  The functionality of fa.parallel.poly is included in fa.parallel with cor=poly option (etc.) option but the older fa.parallel.poly is kept for those who call it directly.That is, fa.parallel now will do tetrachorics or polychorics directly if the cor option is set to "tet" or "poly".  As with fa.parallel.poly this will take longer.  The means of (ntrials) random solutions are shown.  Error bars are usually very small and are suppressed by default but can be shown if requested.  If the sim option is set to TRUE (default), then parallel analyses are done on resampled data as well as random normal data. In the interests of speed, the parallel analyses are done just on resampled data if sim=FALSE.    Both procedures tend to agree.  As of version 1.5.4, I added the ability to specify the quantile of the simulated/resampled data, and to plot standard deviations or standard errors.  By default, this is set to the 95th percentile.  Alternative ways to estimate the number of factors problem are discussed in the Very Simple Structure  (Revelle and Rocklin, 1979) documentation (VSS) and include Wayne Velicer's MAP algorithm (Veicer, 1976).  Parallel analysis for factors is actually harder than it seems, for the question is what are the appropriate communalities to use.  If communalities are estimated by the Squared Multiple Correlation (SMC) smc, then the eigen values of the original data will reflect major as well as minor factors (see sim.minor to simulate such data).  Random data will not, of course, have any structure and thus the number of factors will tend to be biased upwards by the presence of the minor factors.  By default, fa.parallel estimates the communalities based upon a one factor minres solution.  Although this will underestimate the communalities, it does seem to lead to better solutions on simulated or real (e.g., the bfi or Harman74) data sets.  For comparability with other algorithms (e.g, the paran function in the paran package), setting smc=TRUE will use smcs as estimates of communalities. This will tend towards identifying more factors than the default option.Yet another option (suggested by Florian Scharf) is to estimate the eigen values based upon a particular factor model (e.g., specify nfactors > 1).   Printing the results will show the eigen values of the original data that are greater than simulated values.A sad observation about parallel analysis is that it is sensitive to sample size.  That is, for large data sets, the eigen values of random data are very close to 1.  This will lead to different estimates of the number of factors as a function of sample size.  Consider factor structure of the bfi data set (the first 25 items are meant to represent a five factor model).  For samples of 200 or less, parallel analysis suggests 5 factors, but for 1000 or more, six factors and components are indicated.  This is not due to an instability of the eigen values of the real data, but rather the closer approximation to 1 of the random data as n increases.Although with nfactors=1, 6 factors are suggested, when specifying nfactors =5, parallel analysis of the bfi suggests 12 factors should be extracted!When simulating dichotomous data in fa.parallel.poly, the simulated data have the same difficulties as the original data.  This functionally means that the simulated and the resampled results will be very similar.  Note that fa.parallel.poly has functionally been replaced with fa.parallel with the cor="poly" option.As with many psych functions, fa.parallel has been changed to allow for multicore processing.  For running a large number of iterations, it is obviously faster to increase the number of cores to the maximum possible (using the options("mc.cores=n) command where n is determined from detectCores(). 
Results of either a factor analysis or cluster analysis are plotted.  Each item is assigned to its highest loading factor, and then identified by variable name as well as cluster (by color). The cluster assignments can be specified to override the automatic clustering by loading.Both of these functions may be called directly or by calling the generic plot function.  (see example).
Please see the writeup for fa for all of the functionality in these older functions.
Factor analysis is an attempt to approximate a correlation or covariance matrix with one of lesser rank.  The basic model is that nRn = nFk kFn' + U2 where k is much less than n. There are many ways to do factor analysis, and maximum likelihood procedures are probably the most commonly preferred (see factanal ).  The existence of uniquenesses is what distinguishes factor analysis from principal components analysis (e.g., principal). If variables are thought to represent a “true" or latent part then factor analysis provides an estimate of the correlations with the latent factor(s) representing the data.  If variables are thought to be measured without error, then principal components provides the most parsimonious description of the data.  Factor loadings will be smaller than component loadings for the later reflect unique error in each variable. The off diagonal residuals for a factor solution will be superior (smaller)  that of a component model.  Factor loadings can be thought of as the asymptotic component loadings as the number of variables loading on each factor increases.  The fa function will do factor analyses using one of six different algorithms: minimum residual (minres, aka ols, uls), principal axes,  alpha factoring, weighted least squares, minimum rank, or maximum likelihood.Principal axes factor analysis has a long history in exploratory analysis and is a straightforward procedure.  Successive eigen value decompositions are done on a correlation matrix with the diagonal replaced with  diag (FF') until ∑(diag(FF')) does not change (very much).  The current limit of max.iter =50 seems to work for most problems, but the Holzinger-Harmon 24 variable problem needs about 203 iterations to converge for a 5 factor solution.  Not all factor programs that do principal axes do iterative solutions.  The example from the SAS manual (Chapter 33) is such a case. To achieve that solution, it is necessary to specify that the max.iter = 1.  Comparing that solution to an iterated one (the default) shows that iterations improve the solution. In addition, fm="mle" produces even better solutions for this example.  Both the RMSEA and the root mean square of the residuals are smaller than the fm="pa" solution.However, simulations of multiple problem sets suggest that fm="pa" tends to produce slightly smaller residuals while having slightly larger RMSEAs than does fm="minres" or fm="mle".    That is, the sum of squared residuals for fm="pa" seem to be slightly smaller than those found using fm="minres" but the RMSEAs are slightly worse when using fm="pa".  That is to say, the "true" minimal residual is probably found by fm="pa". Following extensive correspondence with Hao Wu and Mikko Ronkko, in April, 2017 the derivative of the minres  and uls) fitting was modified.  This leads to slightly smaller residuals (appropriately enough for a method claiming to minimize them) than the prior procedure.  For  consistency with prior analyses, "old.min" was added to give these slightly larger residuals.  The differences between old.min and the newer "minres" and "ols" solutions are at the third to fourth decimal, but none the less, are worth noting. For comparison purposes, the fm="ols" uses empirical first derivatives, while uls and minres use equation based first derivatives.  The results seem to be identical, but the minres and uls solutions require fewer iterations for larger problems and are faster. Thanks to Hao Wu for some very thoughtful help.  Although usually these various algorithms produce equivalent results, there are several data sets included that show large differences between the methods. Schutz produces Heywood and super Heywood cases,   blant leads to very different solutions.  In particular, the minres solution produces smaller residuals than does the mle solution, and the factor.congruence coefficients show very different solutions.A very strong argument against using MLE is found in the chapter by MacCallum, Brown and Cai (2007) who show that OLS approaches produce equivalent solutions most of the time, and better solutions some of the time.  This particularly in the case of models with some unmodeled small factors.  (See sim.minor to generate such data.)Principal axes may be used in cases when maximum likelihood solutions fail to converge, although fm="minres" will also do that and tends to produce better (smaller RMSEA) solutions.The fm="minchi" option is a variation on the "minres" (ols) solution and minimizes the sample size weighted residuals rather than just the residuals. This was developed to handle the problem of data that Massively Missing Completely at Random (MMCAR) which a condition that happens in the SAPA project.A traditional problem in factor analysis was to find the best estimate of the original communalities in order to speed up convergence.  Using the Squared Multiple Correlation (SMC) for each variable will underestimate the original communalities, using 1s will over estimate.  By default, the SMC estimate is used.  Note that in the case of non-invertible matrices, the pseudo-inverse is found so smcs are still estimated. In either case, iterative techniques will tend to converge on a stable solution. If, however, a solution fails to be achieved, it is useful to try again using ones (SMC =FALSE).  Alternatively, a vector of starting values for the communalities may be specified by the SMC option.The iterated principal axes algorithm does not attempt to find the best (as defined by a maximum likelihood criterion) solution, but rather one that converges rapidly using successive eigen value decompositions.  The maximum likelihood criterion of fit and the associated chi square value are reported, and will be (slightly) worse than that found using maximum likelihood procedures.The minimum residual (minres) solution is an unweighted least squares solution that takes a slightly different approach.  It uses the optim function and adjusts the diagonal elements of the correlation matrix to mimimize the squared residual when the factor model is the eigen value decomposition of the reduced matrix.  MINRES and PA will both work when ML will not, for they can be used when the matrix is singular. Although before the change in the derivative,  the MINRES solution was slightly more similar to the ML solution than is the PA solution. With the change in the derivative of the minres fit, the minres, pa and uls solutions are practically identical. To a great extent, the minres and wls solutions follow ideas in the factanal function with the change in the derivative. The weighted least squares (wls) solution weights the residual matrix by 1/ diagonal of the inverse of the correlation matrix.  This has the effect of weighting items with low communalities more than those with high communalities. The generalized least squares (gls) solution weights the residual matrix by the inverse of the correlation matrix.  This has the effect of weighting those variables with low communalities even more than those with high communalities.The maximum likelihood solution takes yet another approach and finds those communality values that minimize the chi square goodness of fit test.  The fm="ml" option provides a maximum likelihood solution following the procedures used in factanal but does not provide all the extra features of that function.  It does, however, produce more expansive output.The minimum rank factor model (MRFA) roughly follows ideas by Shapiro and Ten Berge (2002) and Ten Berge and Kiers (1991).  It makes use of the glb.algebraic procedure contributed by Andreas Moltner.  MRFA attempts to extract factors such that the residual matrix is still positive semi-definite.  This version is still being tested and feedback is most welcome.Alpha factor analysis finds solutions based upon a correlation matrix corrected for communalities and then rescales these to the original correlation matrix.  This procedure is described by Kaiser and Coffey, 1965.  Test cases comparing the output to SPSS suggest that the PA algorithm matches what SPSS calls uls, and that the wls solutions are equivalent in their fits. The wls and gls solutions have slightly larger eigen values, but slightly worse fits of the off diagonal residuals than do the minres or maximum likelihood solutions.  Comparing the results to the examples in Harman (1976), the PA solution with no iterations matches what Harman calls Principal Axes (as does SAS), while the iterated PA solution matches his minres solution.  The minres solution found in psych tends to have slightly smaller off diagonal residuals (as it should) than does the iterated PA solution.   Although for items, it is typical to find factor scores by scoring the salient items (using, e.g., scoreItems) factor scores can be estimated by regression as well as several other means. There are multiple approaches that are possible (see Grice, 2001) and  one taken here was developed by tenBerge et al.(see factor.scores). The alternative, which will match factanal is to find the scores using regression –  Thurstone's least squares regression where the weights are found byW = inverse(R)S where R is the correlation matrix of the variables ans S is the structure matrix.  Then, factor scores are just Fs = X W.In the oblique case, the factor loadings are referred to as Pattern coefficients and are related to the Structure coefficients by S = P Phi and thus P = S Phi^-1.  When estimating factor scores, fa and  factanal differ in that fa finds the factors from the Structure matrix while factanal seems to do it from the Pattern matrix.  Thus, although in the orthogonal case, fa and factanal agree perfectly in their factor score estimates, they do not agree in the case of oblique factors.  Setting oblique.scores = TRUE  will produce factor score estimate that match those of factanal.It is sometimes useful to extend the factor solution to variables that were not factored.  This may be done using fa.extension. Factor extension is typically done in the case where some variables were not appropriate to factor, but factor loadings on the original factors are still desired.  Factor extension is a very powerful procedure in that it allows one to find the factor-criterion correlations without using factor scores. For dichotomous items or polytomous items, it is recommended to analyze the tetrachoric or polychoric correlations rather than the Pearson correlations. This may be done by specifying cor="poly" or cor="tet" or cor="mixed" if the data have a mixture of dichotomous, polytomous, and continous variables.  Analysis of dichotomous or polytomous data may also be done by using  irt.fa or simply setting the cor="poly" option.  In the first case,  the factor analysis results are reported in Item Response Theory (IRT) terms, although the original factor solution is returned in the results. In the later case, a typical factor loadings matrix is returned, but the tetrachoric/polychoric correlation matrix and item statistics are saved for reanalysis by irt.fa. (See also the mixed.cor function to find correlations from a mixture of continuous, dichotomous, and polytomous items.)Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The “cluster” option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional "keys" parameter, the "target" option will rotate to a target supplied as a keys matrix. (See target.rot.)oblimin is implemented in GPArotation by a call to quartimin with delta=0.  This leads to confusion when people refer to quartimin solutions. It is important to note a dirty little secret about factor rotation.  That is the problem of local minima.  Multiple restarts of the rotation algorithms are strongly encouraged (see Nguyen and Waller, N. G. 2021). Two additional target rotation options are available through calls to GPArotation.  These are the targetQ (oblique) and targetT (orthogonal) target rotations of Michael Browne.  See target.rot for more documentation. The "bifactor" rotation implements the Jennrich and Bentler (2011) bifactor rotation by calling the GPForth function in the GPArotation package and using two functions adapted from  the MatLab code of Jennrich and Bentler.  This seems to have a problem with local minima and multiple starting values should be used.There are two varimax rotation functions.  One, Varimax, in the GPArotation package does not by default apply Kaiser normalization.  The other, varimax, in the stats package, does.  It appears that the two rotation functions produce slightly different results even when normalization is set. For consistency with the other rotation functions, Varimax is probably preferred.The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by Phi = Th' ThThere are two ways to handle dichotomous or polytomous responses: fa with the cor="poly" option which will return the tetrachoric or polychoric correlation matrix, as well as the normal factor analysis output, and irt.fa which returns a two parameter irt analysis as well as the normal fa output. When factor analyzing items with dichotomous or polytomous responses, the irt.fa function provides an Item Response Theory representation of the factor output. The factor analysis results are available, however, as an object in the irt.fa output. fa.poly is deprecated, for its functioning is matched by setting cor="poly".  It will produce normal factor analysis output but also will save the polychoric matrix  (rho) and items difficulties (tau) for subsequent irt analyses.  fa.poly will,  by default, find factor scores if the data are available.  The correlations are found using either tetrachoric or polychoric and then this matrix is factored.  Weights from the factors are then applied to the original data to estimate factor scores.The function fa will repeat the analysis n.iter times on a bootstrapped sample of the data (if they exist) or of a simulated data set based upon the observed correlation matrix.  The mean estimate and standard deviation of the estimate are returned and will print the original factor analysis as well as the alpha level confidence intervals for the estimated coefficients.  The bootstrapped solutions are rotated towards the original solution using target.rot. The factor loadings are z-transformed, averaged and then back transformed. This leads to an error in the case of Heywood cases.  The probably better alternative is to just find the mean bootstrapped value and find the confidence intervals based upon the observed range of values. The default is to have n.iter =1 and thus not do bootstrapping.If using polytomous or dichotomous items, it is perhaps more useful  to find the Item Response Theory parameters equivalent to the factor loadings reported in fa.poly by using the irt.fa function.  Some correlation matrices that arise from using pairwise deletion or from tetrachoric or polychoric matrices will not be proper.  That is, they will not be positive semi-definite (all eigen values >= 0).  The cor.smooth function will adjust correlation matrices (smooth them) by making all negative eigen values slightly greater than 0, rescaling the other eigen values to sum to the number of variables, and then recreating the correlation matrix.  See cor.smooth for an example of this problem using the burt data set.One reason for this problem when using tetrachorics or polychorics seems to be the adjustment for continuity.  Setting correct=0 turns this off and seems to produce more proper matrices.For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin KMO test may be found from the correlation matrix or data matrix using the KMO function.  Similarly, the Bartlett's test of Sphericity may be found using the  cortest.bartlett function.For those who want to have an object of the variances accounted for, this is returned invisibly by the print function.  (e.g., p <- print(fa(ability))$Vaccounted ).  This is now returned by the fa function as well (e.g. p <- fa(ability)$Vaccounted ).  Just as communalities may be found by the diagonal of Pattern %*% t(Structure) so can the variance accounted for be found by diagonal ( t(Structure) %*% Pattern.  Note that referred to as SS loadings. The output from the print.psych.fa function displays the factor loadings (from the pattern matrix, the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the factor loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared factor loadings. But for an oblique solution, it is the row sum of the orthogonal factor loadings (remember, that rotations or transformations do not change the communality).  In response to a request from Asghar Minaei who wanted to combine several imputation data sets from mice, fa.pooled was added.  The separate data sets are combined into a list (datasets) which are then factored separately. Each solution is rotated towards the first set in the list.  The results are then reported in terms of pooled loadings and confidence intervals based upon the replications.  fa.sapa simulates the process of doing SAPA (Synthetic Aperture Personality Assessment).  It will do iterative solutions for successive random samples of  fractions (frac) of the data set. This allows us to find the stability of solutions for various sample sizes and various sample rates. Need to specify the number of iterations (n.iter) as well as the percent of data sampled (frac).  
This function is inspired by the wprifm function in the profileR package and the citation there to a paper by Davison, Kim and Close (2009).  The basic logic is to extract a means vector from each subject and then to analyze the resulting ipsatized data matrix.  This can be seen as removing acquiecence in the case of personality items, or the general factor, in the case of ability items.  Factors composed of items that are all keyed the same way (e.g., Neuroticism in the bfi data set) will be most affected by this technique. The output is identical to the normal fa output with the addition of two objects:  subject and within.r.  The subject object is just the vector of the mean score for each subject on all the items. within.r is just the correlation of each item with those scores.  
Path diagram representations have become standard in confirmatory factor analysis, but are not yet common in exploratory factor analysis.  Representing factor structures graphically helps some people understand the structure. By default the arrows come from the latent variables to the observed variables.  This is, of course, the factor model.  However, if the class of the object to be drawn is 'principal', then  reverse the direction of the arrows, and the 'latent' variables are no longer latent, and are shown as boxes. For cluster models, the default is to treat them as factors, but if ic =TRUE, then we treat it as a components model.fa.diagram does not use Rgraphviz and is the preferred function.  fa.graph generates dot code to be used by an external graphics program. It does not have all the bells and whistles of fa.diagram, but these may be done in the external editor. Hierarchical (bifactor) models may be drawn by specifying the g parameter as TRUE.  This allows for an graphical displays of various factor transformations with a bifactor structure (e.g., bifactor and biquartimin.  See omega for an alternative way to find these structures.  The het.diagram function will show the case of a hetarchical structure at multiple levels.  It can also be used to show the patterns of correlations between sets of scales (e.g., EPI, NEO, BFI).  The example is for showing the relationship between 3 sets of 4 variables from the Thurstone data set. The parameters l.cex and gap.size are used to adjust the font size of the labels and the gap in the lines.  extension.diagram will draw a fa.extend result with slightly more control than using fa.diagram or the more generic diagram function.In fa.rgraph although a nice graph is drawn for the orthogonal factor case, the oblique factor drawing is acceptable, but is better if cleaned up outside of R or done using fa.diagram. The normal input is taken from the output of either fa or ICLUST. This latter case displays the ICLUST results in terms of the cluster loadings, not in terms of the cluster structure.  Actually an interesting option.It is also possible to just give a factor loading matrix as input.  In this case, supplying a Phi matrix of factor correlations is also possible. It is possible, using fa.graph, to export dot code for an omega solution.  fa.graph should be applied to the schmid$sl object with labels specified as the rownames of schmid$sl.  The results will need editing to make fully compatible with dot language plotting.  To specify the model for a structural equation confirmatory analysis of the results, use structure.diagram instead. 
Factor analysis is an attempt to approximate a correlation or covariance matrix with one of lesser rank.  The basic model is that nRn = nFk kFn' + U2 where k is much less than n. There are many ways to do factor analysis, and maximum likelihood procedures are probably the most commonly preferred (see factanal ).  The existence of uniquenesses is what distinguishes factor analysis from principal components analysis (e.g., principal). If variables are thought to represent a “true" or latent part then factor analysis provides an estimate of the correlations with the latent factor(s) representing the data.  If variables are thought to be measured without error, then principal components provides the most parsimonious description of the data.  Factor loadings will be smaller than component loadings for the later reflect unique error in each variable. The off diagonal residuals for a factor solution will be superior (smaller)  that of a component model.  Factor loadings can be thought of as the asymptotic component loadings as the number of variables loading on each factor increases.  The fa function will do factor analyses using one of six different algorithms: minimum residual (minres, aka ols, uls), principal axes,  alpha factoring, weighted least squares, minimum rank, or maximum likelihood.Principal axes factor analysis has a long history in exploratory analysis and is a straightforward procedure.  Successive eigen value decompositions are done on a correlation matrix with the diagonal replaced with  diag (FF') until ∑(diag(FF')) does not change (very much).  The current limit of max.iter =50 seems to work for most problems, but the Holzinger-Harmon 24 variable problem needs about 203 iterations to converge for a 5 factor solution.  Not all factor programs that do principal axes do iterative solutions.  The example from the SAS manual (Chapter 33) is such a case. To achieve that solution, it is necessary to specify that the max.iter = 1.  Comparing that solution to an iterated one (the default) shows that iterations improve the solution. In addition, fm="mle" produces even better solutions for this example.  Both the RMSEA and the root mean square of the residuals are smaller than the fm="pa" solution.However, simulations of multiple problem sets suggest that fm="pa" tends to produce slightly smaller residuals while having slightly larger RMSEAs than does fm="minres" or fm="mle".    That is, the sum of squared residuals for fm="pa" seem to be slightly smaller than those found using fm="minres" but the RMSEAs are slightly worse when using fm="pa".  That is to say, the "true" minimal residual is probably found by fm="pa". Following extensive correspondence with Hao Wu and Mikko Ronkko, in April, 2017 the derivative of the minres  and uls) fitting was modified.  This leads to slightly smaller residuals (appropriately enough for a method claiming to minimize them) than the prior procedure.  For  consistency with prior analyses, "old.min" was added to give these slightly larger residuals.  The differences between old.min and the newer "minres" and "ols" solutions are at the third to fourth decimal, but none the less, are worth noting. For comparison purposes, the fm="ols" uses empirical first derivatives, while uls and minres use equation based first derivatives.  The results seem to be identical, but the minres and uls solutions require fewer iterations for larger problems and are faster. Thanks to Hao Wu for some very thoughtful help.  Although usually these various algorithms produce equivalent results, there are several data sets included that show large differences between the methods. Schutz produces Heywood and super Heywood cases,   blant leads to very different solutions.  In particular, the minres solution produces smaller residuals than does the mle solution, and the factor.congruence coefficients show very different solutions.A very strong argument against using MLE is found in the chapter by MacCallum, Brown and Cai (2007) who show that OLS approaches produce equivalent solutions most of the time, and better solutions some of the time.  This particularly in the case of models with some unmodeled small factors.  (See sim.minor to generate such data.)Principal axes may be used in cases when maximum likelihood solutions fail to converge, although fm="minres" will also do that and tends to produce better (smaller RMSEA) solutions.The fm="minchi" option is a variation on the "minres" (ols) solution and minimizes the sample size weighted residuals rather than just the residuals. This was developed to handle the problem of data that Massively Missing Completely at Random (MMCAR) which a condition that happens in the SAPA project.A traditional problem in factor analysis was to find the best estimate of the original communalities in order to speed up convergence.  Using the Squared Multiple Correlation (SMC) for each variable will underestimate the original communalities, using 1s will over estimate.  By default, the SMC estimate is used.  Note that in the case of non-invertible matrices, the pseudo-inverse is found so smcs are still estimated. In either case, iterative techniques will tend to converge on a stable solution. If, however, a solution fails to be achieved, it is useful to try again using ones (SMC =FALSE).  Alternatively, a vector of starting values for the communalities may be specified by the SMC option.The iterated principal axes algorithm does not attempt to find the best (as defined by a maximum likelihood criterion) solution, but rather one that converges rapidly using successive eigen value decompositions.  The maximum likelihood criterion of fit and the associated chi square value are reported, and will be (slightly) worse than that found using maximum likelihood procedures.The minimum residual (minres) solution is an unweighted least squares solution that takes a slightly different approach.  It uses the optim function and adjusts the diagonal elements of the correlation matrix to mimimize the squared residual when the factor model is the eigen value decomposition of the reduced matrix.  MINRES and PA will both work when ML will not, for they can be used when the matrix is singular. Although before the change in the derivative,  the MINRES solution was slightly more similar to the ML solution than is the PA solution. With the change in the derivative of the minres fit, the minres, pa and uls solutions are practically identical. To a great extent, the minres and wls solutions follow ideas in the factanal function with the change in the derivative. The weighted least squares (wls) solution weights the residual matrix by 1/ diagonal of the inverse of the correlation matrix.  This has the effect of weighting items with low communalities more than those with high communalities. The generalized least squares (gls) solution weights the residual matrix by the inverse of the correlation matrix.  This has the effect of weighting those variables with low communalities even more than those with high communalities.The maximum likelihood solution takes yet another approach and finds those communality values that minimize the chi square goodness of fit test.  The fm="ml" option provides a maximum likelihood solution following the procedures used in factanal but does not provide all the extra features of that function.  It does, however, produce more expansive output.The minimum rank factor model (MRFA) roughly follows ideas by Shapiro and Ten Berge (2002) and Ten Berge and Kiers (1991).  It makes use of the glb.algebraic procedure contributed by Andreas Moltner.  MRFA attempts to extract factors such that the residual matrix is still positive semi-definite.  This version is still being tested and feedback is most welcome.Alpha factor analysis finds solutions based upon a correlation matrix corrected for communalities and then rescales these to the original correlation matrix.  This procedure is described by Kaiser and Coffey, 1965.  Test cases comparing the output to SPSS suggest that the PA algorithm matches what SPSS calls uls, and that the wls solutions are equivalent in their fits. The wls and gls solutions have slightly larger eigen values, but slightly worse fits of the off diagonal residuals than do the minres or maximum likelihood solutions.  Comparing the results to the examples in Harman (1976), the PA solution with no iterations matches what Harman calls Principal Axes (as does SAS), while the iterated PA solution matches his minres solution.  The minres solution found in psych tends to have slightly smaller off diagonal residuals (as it should) than does the iterated PA solution.   Although for items, it is typical to find factor scores by scoring the salient items (using, e.g., scoreItems) factor scores can be estimated by regression as well as several other means. There are multiple approaches that are possible (see Grice, 2001) and  one taken here was developed by tenBerge et al.(see factor.scores). The alternative, which will match factanal is to find the scores using regression –  Thurstone's least squares regression where the weights are found byW = inverse(R)S where R is the correlation matrix of the variables ans S is the structure matrix.  Then, factor scores are just Fs = X W.In the oblique case, the factor loadings are referred to as Pattern coefficients and are related to the Structure coefficients by S = P Phi and thus P = S Phi^-1.  When estimating factor scores, fa and  factanal differ in that fa finds the factors from the Structure matrix while factanal seems to do it from the Pattern matrix.  Thus, although in the orthogonal case, fa and factanal agree perfectly in their factor score estimates, they do not agree in the case of oblique factors.  Setting oblique.scores = TRUE  will produce factor score estimate that match those of factanal.It is sometimes useful to extend the factor solution to variables that were not factored.  This may be done using fa.extension. Factor extension is typically done in the case where some variables were not appropriate to factor, but factor loadings on the original factors are still desired.  Factor extension is a very powerful procedure in that it allows one to find the factor-criterion correlations without using factor scores. For dichotomous items or polytomous items, it is recommended to analyze the tetrachoric or polychoric correlations rather than the Pearson correlations. This may be done by specifying cor="poly" or cor="tet" or cor="mixed" if the data have a mixture of dichotomous, polytomous, and continous variables.  Analysis of dichotomous or polytomous data may also be done by using  irt.fa or simply setting the cor="poly" option.  In the first case,  the factor analysis results are reported in Item Response Theory (IRT) terms, although the original factor solution is returned in the results. In the later case, a typical factor loadings matrix is returned, but the tetrachoric/polychoric correlation matrix and item statistics are saved for reanalysis by irt.fa. (See also the mixed.cor function to find correlations from a mixture of continuous, dichotomous, and polytomous items.)Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The “cluster” option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional "keys" parameter, the "target" option will rotate to a target supplied as a keys matrix. (See target.rot.)oblimin is implemented in GPArotation by a call to quartimin with delta=0.  This leads to confusion when people refer to quartimin solutions. It is important to note a dirty little secret about factor rotation.  That is the problem of local minima.  Multiple restarts of the rotation algorithms are strongly encouraged (see Nguyen and Waller, N. G. 2021). Two additional target rotation options are available through calls to GPArotation.  These are the targetQ (oblique) and targetT (orthogonal) target rotations of Michael Browne.  See target.rot for more documentation. The "bifactor" rotation implements the Jennrich and Bentler (2011) bifactor rotation by calling the GPForth function in the GPArotation package and using two functions adapted from  the MatLab code of Jennrich and Bentler.  This seems to have a problem with local minima and multiple starting values should be used.There are two varimax rotation functions.  One, Varimax, in the GPArotation package does not by default apply Kaiser normalization.  The other, varimax, in the stats package, does.  It appears that the two rotation functions produce slightly different results even when normalization is set. For consistency with the other rotation functions, Varimax is probably preferred.The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by Phi = Th' ThThere are two ways to handle dichotomous or polytomous responses: fa with the cor="poly" option which will return the tetrachoric or polychoric correlation matrix, as well as the normal factor analysis output, and irt.fa which returns a two parameter irt analysis as well as the normal fa output. When factor analyzing items with dichotomous or polytomous responses, the irt.fa function provides an Item Response Theory representation of the factor output. The factor analysis results are available, however, as an object in the irt.fa output. fa.poly is deprecated, for its functioning is matched by setting cor="poly".  It will produce normal factor analysis output but also will save the polychoric matrix  (rho) and items difficulties (tau) for subsequent irt analyses.  fa.poly will,  by default, find factor scores if the data are available.  The correlations are found using either tetrachoric or polychoric and then this matrix is factored.  Weights from the factors are then applied to the original data to estimate factor scores.The function fa will repeat the analysis n.iter times on a bootstrapped sample of the data (if they exist) or of a simulated data set based upon the observed correlation matrix.  The mean estimate and standard deviation of the estimate are returned and will print the original factor analysis as well as the alpha level confidence intervals for the estimated coefficients.  The bootstrapped solutions are rotated towards the original solution using target.rot. The factor loadings are z-transformed, averaged and then back transformed. This leads to an error in the case of Heywood cases.  The probably better alternative is to just find the mean bootstrapped value and find the confidence intervals based upon the observed range of values. The default is to have n.iter =1 and thus not do bootstrapping.If using polytomous or dichotomous items, it is perhaps more useful  to find the Item Response Theory parameters equivalent to the factor loadings reported in fa.poly by using the irt.fa function.  Some correlation matrices that arise from using pairwise deletion or from tetrachoric or polychoric matrices will not be proper.  That is, they will not be positive semi-definite (all eigen values >= 0).  The cor.smooth function will adjust correlation matrices (smooth them) by making all negative eigen values slightly greater than 0, rescaling the other eigen values to sum to the number of variables, and then recreating the correlation matrix.  See cor.smooth for an example of this problem using the burt data set.One reason for this problem when using tetrachorics or polychorics seems to be the adjustment for continuity.  Setting correct=0 turns this off and seems to produce more proper matrices.For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin KMO test may be found from the correlation matrix or data matrix using the KMO function.  Similarly, the Bartlett's test of Sphericity may be found using the  cortest.bartlett function.For those who want to have an object of the variances accounted for, this is returned invisibly by the print function.  (e.g., p <- print(fa(ability))$Vaccounted ).  This is now returned by the fa function as well (e.g. p <- fa(ability)$Vaccounted ).  Just as communalities may be found by the diagonal of Pattern %*% t(Structure) so can the variance accounted for be found by diagonal ( t(Structure) %*% Pattern.  Note that referred to as SS loadings. The output from the print.psych.fa function displays the factor loadings (from the pattern matrix, the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the factor loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared factor loadings. But for an oblique solution, it is the row sum of the orthogonal factor loadings (remember, that rotations or transformations do not change the communality).  In response to a request from Asghar Minaei who wanted to combine several imputation data sets from mice, fa.pooled was added.  The separate data sets are combined into a list (datasets) which are then factored separately. Each solution is rotated towards the first set in the list.  The results are then reported in terms of pooled loadings and confidence intervals based upon the replications.  fa.sapa simulates the process of doing SAPA (Synthetic Aperture Personality Assessment).  It will do iterative solutions for successive random samples of  fractions (frac) of the data set. This allows us to find the stability of solutions for various sample sizes and various sample rates. Need to specify the number of iterations (n.iter) as well as the percent of data sampled (frac).  
The fa.results$loadings are replaced with sorted loadings.fa.organize takes a factor analysis or components output and reorganizes the factors in the o order. Items are organized in the i order.  This is useful when comparing alternative factor solutions.The flip option works only for the case of matrix input, not for full fa objects. Use the  reflect function.
Combines the goodness of fit tests used in fa and principal into one function.  If the matrix is singular, will smooth the correlation matrix before finding the fit functions. Now will find the RMSEA (root mean square error of approximation) and the alpha confidence intervals similar to a SEM function.  Also reports the root mean square residual.Chi square is found two ways.  The first (STATISTIC) applies the goodness of fit test from Maximum Likelihood objective function (see below).  This assumes multivariate normality.  The second is the empirical chi square based upon the observed residual correlation matrix and the observed sample size for each correlation.  This is found by summing the squared residual correlations time the sample size.  
irt.fa combines several functions into one to make the process of item response analysis easier.  Correlations are found using either tetrachoric or polychoric.  Exploratory factor analyeses with all the normal options are then done using fa.  The results are then organized to be reported in terms of IRT parameters (difficulties and discriminations) as well as the more conventional factor analysis output. In addition, because the correlation step is somewhat slow, reanalyses may be done using the correlation matrix found in the first step.  In this case, if it is desired to use the fm="minchi" factoring method, the number of observations needs to be specified as the matrix resulting from pairwiseCount.The tetrachoric correlation matrix of dichotomous items may be factored using a (e.g.) minimum residual factor analysis function fa and the resulting loadings, λ_i are transformed to discriminations bya = λ / (sqrt(1-λ^2).The difficulty parameter, δ is found from the τ parameter of the tetrachoric or polychoric function.δ = τ / (sqrt(1-λ^2)Similar analyses may be done with discrete item responses using polychoric correlations and distinct estimates of item difficulty (location)  for each item response.The results may be shown graphically using link{plot.irt} for dichotomous items or  link{plot.poly} for polytomous items.  These called by plotting the irt.fa output, see the examples).   For plotting there are three options: type = "ICC" will plot the item characteristic response function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.  Invisible output from the plot function will return tables of item information as a function of several levels of the trait, as well as the standard error of measurement and the reliability at each of those levels.The normal input is just the raw data.  If, however, the correlation matrix has already been found using tetrachoric, polychoric, or a previous analysis using irt.fa then that result can be processed directly.  Because  irt.fa saves the rho and tau matrices from the analysis, subsequent analyses of the same data set are much faster if the input is the object returned on the first run.  A similar feature is available in omega. The output is best seen in terms of graphic displays.  Plot the output from irt.fa to see item and test information functions.  The print function will print the item location and discriminations.  The additional factor analysis output is available as an object in the output and may be printed directly by specifying the $fa object.The irt.select function is a helper function to allow for selecting a subset of a prior analysis for further analysis. First run irt.fa, then select a subset of variables to be analyzed in a subsequent irt.fa analysis.  Perhaps a better approach is to just plot and find the information for selected items.  The plot function for an irt.fa object will plot ICC (item characteristic curves), IIC (item information curves), or test information curves. In addition, by using the "keys" option,  these three kinds of plots can be done for selected items. This is particularly useful when trying to see the information characteristics of short forms of tests based upon the longer form factor analysis.The plot function will also return (invisibly) the informaton at multiple levels of the trait, the average information (area under the curve) as well as the location of the peak information for each item.  These may be then printed or printed in sorted order using the sort option in print.
Multilevel data are endemic in psychological research. In multilevel data, observations are taken on subjects who are nested within some higher level grouping variable.  The data might be experimental (participants are nested within experimental conditions) or observational (students are nested within classrooms, students are nested within college majors.) To analyze this type of data, one uses random effects models or mixed effect models, or more generally, multilevel models.  There are at least two very powerful packages (nlme and multilevel) which allow for complex analysis of hierarchical (multilevel) data structures.  statsBy is a much simpler function to give some of the basic descriptive statistics for two level models.  It is meant to supplement true multilevel modeling.For a group variable (group) for a data.frame or matrix (data), basic descriptive statistics (mean, sd, n) as well as within group correlations (cors=TRUE) are found for each group.  The amount of variance associated with the grouping variable compared to the total variance is the type 1 IntraClass Correlation (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))where npr is the average number of cases within each group. The reliability of the group differences may be found by the ICC2 which reflects how different the means are with respect to the within group variability.  ICC2 = (MSb-MSw)/MSb.Because the mean square between is sensitive to sample size, this estimate will also reflect sample size.Perhaps the most useful part of statsBy is that it decomposes the observed correlations between variables into two parts: the within group and the between group correlation. This follows the decomposition of an observed correlation into the pooled correlation within groups (rwg) and the weighted correlation of the means between groups  discussed by Pedazur (1997) and by Bliese in the multilevel package.  r_{xy} = eta_{x_{wg}} * eta_{y_{wg}} * r_{xy_{wg}}  +  eta_{x_{bg}} * eta_{y_{bg}} * r_{xy_{bg}}  where r_{xy} is the normal correlation which may be decomposed into a within group and between group correlations r_{xy_{wg}} and r_{xy_{bg}} and eta is the correlation of the data with the within group values, or the group means.It is important to realize that the within group and between group correlations are independent of each other.  That is to say, inferring from the 'ecological correlation' (between groups) to the lower level (within group) correlation is inappropriate.  However, these between group correlations are still very meaningful, if inferences are made at the higher level.  There are actually two ways of finding the within group correlations pooled across groups.  We can find the correlations within every group, weight these by the sample size and then report this pooled value (pooled).  This is found if the cors option is set to TRUE.  It is logically  equivalent to doing a sample size weighted meta-analytic correlation.  The other way, rwg, considers the covariances, variances, and thus correlations when each subject's scores are given as deviation score from the group mean.  If finding tetrachoric, polychoric, or mixed correlations, these two estimates will differ, for the pooled value is the weighted polychoric correlation, but the rwg is the Pearson correlation. If the weights parameter is specified, the pooled correlations are found by weighting the groups by the specified weight, rather than sample size.Confidence values and significance  of  r_{xy_{wg}}, pwg, reflect the pooled number of cases within groups, while  r_{xy_{bg}} , pbg, the number of groups. These are not corrected for multiple comparisons.withinBetween is an example data set of the mixture of within and between group correlations. sim.multilevel will generate simulated data with a multilevel structure.The statsBy.boot function will randomize the grouping variable ntrials times and find the statsBy output.  This can take a long time and will produce a great deal of output.  This output can then be summarized for relevant variables using the statsBy.boot.summary function specifying the variable of interest.  These two functions are useful in order to find if the mere act of grouping leads to large between group correlations.Consider the case of the relationship between various tests of ability when the data are grouped by level of education (statsBy(sat.act,"education")) or when affect data are analyzed within and between an affect manipulation (statsBy(flat,group="Film") ). Note in this latter example, that because subjects were randomly assigned to Film condition for the pretest, that the pretest ICC1s cluster around 0. faBy uses the output of statsBy to perform a factor analysis on the correlation matrix within each group. If the free parameter is FALSE, then each solution is rotated towards the group solution (as much as possible).  The output is a list of each factor solution, as well as a summary matrix of loadings and interfactor correlations for  all groups.
Factor analysis is an attempt to approximate a correlation or covariance matrix with one of lesser rank.  The basic model is that nRn = nFk kFn' + U2 where k is much less than n. There are many ways to do factor analysis, and maximum likelihood procedures are probably the most commonly preferred (see factanal ).  The existence of uniquenesses is what distinguishes factor analysis from principal components analysis (e.g., principal). If variables are thought to represent a “true" or latent part then factor analysis provides an estimate of the correlations with the latent factor(s) representing the data.  If variables are thought to be measured without error, then principal components provides the most parsimonious description of the data.  Factor loadings will be smaller than component loadings for the later reflect unique error in each variable. The off diagonal residuals for a factor solution will be superior (smaller)  that of a component model.  Factor loadings can be thought of as the asymptotic component loadings as the number of variables loading on each factor increases.  The fa function will do factor analyses using one of six different algorithms: minimum residual (minres, aka ols, uls), principal axes,  alpha factoring, weighted least squares, minimum rank, or maximum likelihood.Principal axes factor analysis has a long history in exploratory analysis and is a straightforward procedure.  Successive eigen value decompositions are done on a correlation matrix with the diagonal replaced with  diag (FF') until ∑(diag(FF')) does not change (very much).  The current limit of max.iter =50 seems to work for most problems, but the Holzinger-Harmon 24 variable problem needs about 203 iterations to converge for a 5 factor solution.  Not all factor programs that do principal axes do iterative solutions.  The example from the SAS manual (Chapter 33) is such a case. To achieve that solution, it is necessary to specify that the max.iter = 1.  Comparing that solution to an iterated one (the default) shows that iterations improve the solution. In addition, fm="mle" produces even better solutions for this example.  Both the RMSEA and the root mean square of the residuals are smaller than the fm="pa" solution.However, simulations of multiple problem sets suggest that fm="pa" tends to produce slightly smaller residuals while having slightly larger RMSEAs than does fm="minres" or fm="mle".    That is, the sum of squared residuals for fm="pa" seem to be slightly smaller than those found using fm="minres" but the RMSEAs are slightly worse when using fm="pa".  That is to say, the "true" minimal residual is probably found by fm="pa". Following extensive correspondence with Hao Wu and Mikko Ronkko, in April, 2017 the derivative of the minres  and uls) fitting was modified.  This leads to slightly smaller residuals (appropriately enough for a method claiming to minimize them) than the prior procedure.  For  consistency with prior analyses, "old.min" was added to give these slightly larger residuals.  The differences between old.min and the newer "minres" and "ols" solutions are at the third to fourth decimal, but none the less, are worth noting. For comparison purposes, the fm="ols" uses empirical first derivatives, while uls and minres use equation based first derivatives.  The results seem to be identical, but the minres and uls solutions require fewer iterations for larger problems and are faster. Thanks to Hao Wu for some very thoughtful help.  Although usually these various algorithms produce equivalent results, there are several data sets included that show large differences between the methods. Schutz produces Heywood and super Heywood cases,   blant leads to very different solutions.  In particular, the minres solution produces smaller residuals than does the mle solution, and the factor.congruence coefficients show very different solutions.A very strong argument against using MLE is found in the chapter by MacCallum, Brown and Cai (2007) who show that OLS approaches produce equivalent solutions most of the time, and better solutions some of the time.  This particularly in the case of models with some unmodeled small factors.  (See sim.minor to generate such data.)Principal axes may be used in cases when maximum likelihood solutions fail to converge, although fm="minres" will also do that and tends to produce better (smaller RMSEA) solutions.The fm="minchi" option is a variation on the "minres" (ols) solution and minimizes the sample size weighted residuals rather than just the residuals. This was developed to handle the problem of data that Massively Missing Completely at Random (MMCAR) which a condition that happens in the SAPA project.A traditional problem in factor analysis was to find the best estimate of the original communalities in order to speed up convergence.  Using the Squared Multiple Correlation (SMC) for each variable will underestimate the original communalities, using 1s will over estimate.  By default, the SMC estimate is used.  Note that in the case of non-invertible matrices, the pseudo-inverse is found so smcs are still estimated. In either case, iterative techniques will tend to converge on a stable solution. If, however, a solution fails to be achieved, it is useful to try again using ones (SMC =FALSE).  Alternatively, a vector of starting values for the communalities may be specified by the SMC option.The iterated principal axes algorithm does not attempt to find the best (as defined by a maximum likelihood criterion) solution, but rather one that converges rapidly using successive eigen value decompositions.  The maximum likelihood criterion of fit and the associated chi square value are reported, and will be (slightly) worse than that found using maximum likelihood procedures.The minimum residual (minres) solution is an unweighted least squares solution that takes a slightly different approach.  It uses the optim function and adjusts the diagonal elements of the correlation matrix to mimimize the squared residual when the factor model is the eigen value decomposition of the reduced matrix.  MINRES and PA will both work when ML will not, for they can be used when the matrix is singular. Although before the change in the derivative,  the MINRES solution was slightly more similar to the ML solution than is the PA solution. With the change in the derivative of the minres fit, the minres, pa and uls solutions are practically identical. To a great extent, the minres and wls solutions follow ideas in the factanal function with the change in the derivative. The weighted least squares (wls) solution weights the residual matrix by 1/ diagonal of the inverse of the correlation matrix.  This has the effect of weighting items with low communalities more than those with high communalities. The generalized least squares (gls) solution weights the residual matrix by the inverse of the correlation matrix.  This has the effect of weighting those variables with low communalities even more than those with high communalities.The maximum likelihood solution takes yet another approach and finds those communality values that minimize the chi square goodness of fit test.  The fm="ml" option provides a maximum likelihood solution following the procedures used in factanal but does not provide all the extra features of that function.  It does, however, produce more expansive output.The minimum rank factor model (MRFA) roughly follows ideas by Shapiro and Ten Berge (2002) and Ten Berge and Kiers (1991).  It makes use of the glb.algebraic procedure contributed by Andreas Moltner.  MRFA attempts to extract factors such that the residual matrix is still positive semi-definite.  This version is still being tested and feedback is most welcome.Alpha factor analysis finds solutions based upon a correlation matrix corrected for communalities and then rescales these to the original correlation matrix.  This procedure is described by Kaiser and Coffey, 1965.  Test cases comparing the output to SPSS suggest that the PA algorithm matches what SPSS calls uls, and that the wls solutions are equivalent in their fits. The wls and gls solutions have slightly larger eigen values, but slightly worse fits of the off diagonal residuals than do the minres or maximum likelihood solutions.  Comparing the results to the examples in Harman (1976), the PA solution with no iterations matches what Harman calls Principal Axes (as does SAS), while the iterated PA solution matches his minres solution.  The minres solution found in psych tends to have slightly smaller off diagonal residuals (as it should) than does the iterated PA solution.   Although for items, it is typical to find factor scores by scoring the salient items (using, e.g., scoreItems) factor scores can be estimated by regression as well as several other means. There are multiple approaches that are possible (see Grice, 2001) and  one taken here was developed by tenBerge et al.(see factor.scores). The alternative, which will match factanal is to find the scores using regression –  Thurstone's least squares regression where the weights are found byW = inverse(R)S where R is the correlation matrix of the variables ans S is the structure matrix.  Then, factor scores are just Fs = X W.In the oblique case, the factor loadings are referred to as Pattern coefficients and are related to the Structure coefficients by S = P Phi and thus P = S Phi^-1.  When estimating factor scores, fa and  factanal differ in that fa finds the factors from the Structure matrix while factanal seems to do it from the Pattern matrix.  Thus, although in the orthogonal case, fa and factanal agree perfectly in their factor score estimates, they do not agree in the case of oblique factors.  Setting oblique.scores = TRUE  will produce factor score estimate that match those of factanal.It is sometimes useful to extend the factor solution to variables that were not factored.  This may be done using fa.extension. Factor extension is typically done in the case where some variables were not appropriate to factor, but factor loadings on the original factors are still desired.  Factor extension is a very powerful procedure in that it allows one to find the factor-criterion correlations without using factor scores. For dichotomous items or polytomous items, it is recommended to analyze the tetrachoric or polychoric correlations rather than the Pearson correlations. This may be done by specifying cor="poly" or cor="tet" or cor="mixed" if the data have a mixture of dichotomous, polytomous, and continous variables.  Analysis of dichotomous or polytomous data may also be done by using  irt.fa or simply setting the cor="poly" option.  In the first case,  the factor analysis results are reported in Item Response Theory (IRT) terms, although the original factor solution is returned in the results. In the later case, a typical factor loadings matrix is returned, but the tetrachoric/polychoric correlation matrix and item statistics are saved for reanalysis by irt.fa. (See also the mixed.cor function to find correlations from a mixture of continuous, dichotomous, and polytomous items.)Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The “cluster” option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional "keys" parameter, the "target" option will rotate to a target supplied as a keys matrix. (See target.rot.)oblimin is implemented in GPArotation by a call to quartimin with delta=0.  This leads to confusion when people refer to quartimin solutions. It is important to note a dirty little secret about factor rotation.  That is the problem of local minima.  Multiple restarts of the rotation algorithms are strongly encouraged (see Nguyen and Waller, N. G. 2021). Two additional target rotation options are available through calls to GPArotation.  These are the targetQ (oblique) and targetT (orthogonal) target rotations of Michael Browne.  See target.rot for more documentation. The "bifactor" rotation implements the Jennrich and Bentler (2011) bifactor rotation by calling the GPForth function in the GPArotation package and using two functions adapted from  the MatLab code of Jennrich and Bentler.  This seems to have a problem with local minima and multiple starting values should be used.There are two varimax rotation functions.  One, Varimax, in the GPArotation package does not by default apply Kaiser normalization.  The other, varimax, in the stats package, does.  It appears that the two rotation functions produce slightly different results even when normalization is set. For consistency with the other rotation functions, Varimax is probably preferred.The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by Phi = Th' ThThere are two ways to handle dichotomous or polytomous responses: fa with the cor="poly" option which will return the tetrachoric or polychoric correlation matrix, as well as the normal factor analysis output, and irt.fa which returns a two parameter irt analysis as well as the normal fa output. When factor analyzing items with dichotomous or polytomous responses, the irt.fa function provides an Item Response Theory representation of the factor output. The factor analysis results are available, however, as an object in the irt.fa output. fa.poly is deprecated, for its functioning is matched by setting cor="poly".  It will produce normal factor analysis output but also will save the polychoric matrix  (rho) and items difficulties (tau) for subsequent irt analyses.  fa.poly will,  by default, find factor scores if the data are available.  The correlations are found using either tetrachoric or polychoric and then this matrix is factored.  Weights from the factors are then applied to the original data to estimate factor scores.The function fa will repeat the analysis n.iter times on a bootstrapped sample of the data (if they exist) or of a simulated data set based upon the observed correlation matrix.  The mean estimate and standard deviation of the estimate are returned and will print the original factor analysis as well as the alpha level confidence intervals for the estimated coefficients.  The bootstrapped solutions are rotated towards the original solution using target.rot. The factor loadings are z-transformed, averaged and then back transformed. This leads to an error in the case of Heywood cases.  The probably better alternative is to just find the mean bootstrapped value and find the confidence intervals based upon the observed range of values. The default is to have n.iter =1 and thus not do bootstrapping.If using polytomous or dichotomous items, it is perhaps more useful  to find the Item Response Theory parameters equivalent to the factor loadings reported in fa.poly by using the irt.fa function.  Some correlation matrices that arise from using pairwise deletion or from tetrachoric or polychoric matrices will not be proper.  That is, they will not be positive semi-definite (all eigen values >= 0).  The cor.smooth function will adjust correlation matrices (smooth them) by making all negative eigen values slightly greater than 0, rescaling the other eigen values to sum to the number of variables, and then recreating the correlation matrix.  See cor.smooth for an example of this problem using the burt data set.One reason for this problem when using tetrachorics or polychorics seems to be the adjustment for continuity.  Setting correct=0 turns this off and seems to produce more proper matrices.For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin KMO test may be found from the correlation matrix or data matrix using the KMO function.  Similarly, the Bartlett's test of Sphericity may be found using the  cortest.bartlett function.For those who want to have an object of the variances accounted for, this is returned invisibly by the print function.  (e.g., p <- print(fa(ability))$Vaccounted ).  This is now returned by the fa function as well (e.g. p <- fa(ability)$Vaccounted ).  Just as communalities may be found by the diagonal of Pattern %*% t(Structure) so can the variance accounted for be found by diagonal ( t(Structure) %*% Pattern.  Note that referred to as SS loadings. The output from the print.psych.fa function displays the factor loadings (from the pattern matrix, the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the factor loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared factor loadings. But for an oblique solution, it is the row sum of the orthogonal factor loadings (remember, that rotations or transformations do not change the communality).  In response to a request from Asghar Minaei who wanted to combine several imputation data sets from mice, fa.pooled was added.  The separate data sets are combined into a list (datasets) which are then factored separately. Each solution is rotated towards the first set in the list.  The results are then reported in terms of pooled loadings and confidence intervals based upon the replications.  fa.sapa simulates the process of doing SAPA (Synthetic Aperture Personality Assessment).  It will do iterative solutions for successive random samples of  fractions (frac) of the data set. This allows us to find the stability of solutions for various sample sizes and various sample rates. Need to specify the number of iterations (n.iter) as well as the percent of data sampled (frac).  
The factor correlations are found using the approach discussed by Gorsuch (1983) and uses the weights matrices found by w=S R^{-1} and r = w' R w where S is the structure matrix and is   s= F Φ.  The resulting correlations may be  adjusted for the factor score variances (the diagonal of r) (the default). For factor loading vectors of F1 and F2 the measure of factor congruence, phi, is {phi = sum(F1 F2)/sqrt(sum(F1^2) sum(F2^2))}  and is also found in factor.congruence.For comparisons of factor solutions from 1 to n, use bassAckward. This function just compares two solutions from the same correlation/data matrix.  factor.congruence can be used to compare any two sets of factor loadings. Note that alternative ways of finding weights (e.g., regression, Bartlett, tenBerge) will produce somewhat different results.  tenBerge produces weights that maintain the factor correlations in the factor scores.
Find the coefficient of factor congruence between two sets of factor loadings. Factor congruences are the cosines of pairs of vectors defined by the loadings matrix and based at the origin.  Thus, for loadings that differ only by a scaler (e.g. the size of the eigen value), the factor congruences will be 1.For factor loading vectors of F1 and F2 the measure of factor congruence, phi, is phi = sum(F1 F2)/sqrt(sum(F1^2) sum(F2^2)) It is an interesting exercise to compare factor congruences with the correlations of factor loadings.  Factor congruences are based upon the raw cross products, while correlations are based upon centered cross products. That is,  correlations of factor loadings are cosines of the vectors based at the mean loading for each factor.   phi = sum((F1-a)(F2-b))/sqrt(sum((F1-a)^2) sum((F2-b)^2)) .For congruence coefficients, a = b= 0.  For correlations a=mean F1, b= mean F2.Input may either be matrices or factor analysis or principal components analyis output (which includes a loadings object), or a mixture of the two.To compare more than two solutions, x may be a list of matrices, all of which will be compared.Normally, all factor loading matrices should be complete (have no missing loadings).  In the case where some loadings are missing, if the use option is specified, then variables with missing loadings are dropped.If the data are zero centered, this is the correlation, if the data are centered around the scale midpoint (M), this is Cohen's Similarity coefficient.  See examples. If M is not specified, it is found as the midpoint of the items in x and y.cohen.profile applies the congruence function to data centered around M.  M may be specified, or found from the data. The last example is taken from Cohen (1969). distance finds the generalized distance as a function of r.  City block (r=1), Euclidean (r=2) or weighted towards maximimum (r >2).
There are probably as many fit indices as there are psychometricians.  This fit is a plausible estimate of the amount of reduction in a correlation matrix given a factor model.  Note that it is sensitive to the size of the original correlations.  That is, if the residuals are small but the original correlations are small, that is a bad fit. Let R*= R - FF'fit = 1 - sum(R*^2)/sum(R^2). The sums are taken for the off diagonal elements.
Please see the writeup for fa for all of the functionality in these older functions.
NA
Please see the writeup for fa for all of the functionality in these older functions.
Results of either a factor analysis or cluster analysis are plotted.  Each item is assigned to its highest loading factor, and then identified by variable name as well as cluster (by color). The cluster assignments can be specified to override the automatic clustering by loading.Both of these functions may be called directly or by calling the generic plot function.  (see example).
The basic factor equation is nRn = nFk kFn' + U2. Residuals are just  R* = R - F'F. The residuals should be (but in practice probably rarely are) examined to understand the adequacy of the factor analysis.  When doing Factor analysis or Principal Components analysis, one usually continues to extract factors/components until the residuals do not differ from those expected from a random matrix.
Partly meant as a demonstration of how rotation works, factor.rotate is useful for those cases that require specific rotations that are not available in more advanced packages such as GPArotation.  If the plot option is set to TRUE, then the original axes are shown as dashed lines.The rotation is in degrees counter clockwise.
Although the factor analysis model is defined at the structural level, it is undefined at the data level.  This is a well known but little discussed problem with factor analysis.  Factor scores represent estimates of common part of the variables and should not be thought of as identical to the factors themselves. If a factor is thought of as a chop stick stuck into the center of an ice cream cone and factor score estimates are represented by straws anywhere along the edge of the cone the problem of factor indeterminacy becomes clear, for depending on the shape of the cone, two straws can be negatively correlated with each other. (The imagery is taken from Niels Waller, adapted from Stanley Mulaik). In a very clear discussion of the problem of factor score indeterminacy, Grice (2001) reviews several alternative ways of estimating factor scores and considers weighting schemes that will produce uncorrelated factor score estimates as well as the effect of using coarse coded (unit weighted) factor weights.factor.scores uses four different ways of estimate factor scores.  In all cases, the factor score estimates are based upon the data matrix, X, times a weighting matrix, W, which weights the observed variables.For polytomous or dichotmous data, factor scores can be estimated using Item Response Theory techniques (e.g., using link{irt.fa} and then link{scoreIrt}.  Such scores are still just factor score estimates, for the IRT model is a latent variable model equivalent to factor analysis.   method="Thurstone" finds the regression based weights: W = R^{-1} F where R is the correlation matrix and F is the factor loading matrix.  method="tenBerge" finds weights such that the correlation between factors for an oblique solution is preserved. Note that  formula 8 in Grice has a typo in the formula for C and should be:L = F Φ^(1/2) C = R^(-1/2) L (L' R^(-1) L)^(-1/2) W = R ^(-1/2) C Φ^(1/2)  method="Anderson" finds weights such that the factor scores will be uncorrelated: W = U^{-2}F (F' U^{-2} R  U^{-2} F)^{-1/2} where U is the diagonal matrix of uniquenesses. The Anderson method works for orthogonal factors only, while the tenBerge method works for orthogonal or oblique solutions. method = "Bartlett"  finds weights given W = U^{-2}F (F' U^{-2}F)^{-1} method="Harman" finds weights based upon socalled "idealized" variables: W =  F (t(F) F)^{-1}. method="components" uses weights that are just component loadings.  

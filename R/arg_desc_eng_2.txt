x
Further options to plot
deprecated.
deprecated
 deprecated 
deprecated 
deprecated
deprecated
deprecated 
deprecated
deprecated 
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
A correlation or covariance matrix or a raw data matrix. If raw data, the correlation matrix will be found using pairwise deletion. If covariances are supplied, they will be converted to correlations unless the covar option is TRUE.
 Number of factors to extract, default is 1 
Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals.
The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.
"none", "varimax", "quartimax",  "bentlerT", "equamax", "varimin", "geominT" and "bifactor" are orthogonal rotations.  "Promax", "promax", "oblimin", "simplimax", "bentlerQ,  "geominQ" and "biquartimin" and "cluster" are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax. SPSS seems to do a Kaiser normalization before doing Promax, this is done here by the call to "promax" which does the normalization before calling Promax in GPArotation.
Number of random starts for the rotations.
Number of bootstrap interations to do in fa or fa.poly
Should the residual matrix be shown 
the default="regression" finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression ("Thurstone"), correlaton preserving ("tenBerge") as well as "Anderson" and "Bartlett" using the appropriate algorithms ( factor.scores). Although scores="tenBerge" is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  
Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. 
if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix
if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean
"median" or "mean" values are used to replace missing values
Iterate until the change in communalities is less than min.err
Maximum number of iterations for convergence 
symmetric=TRUE forces symmetry by just looking at the lower off diagonal values
Count number of (absolute) loadings less than hyper. 
warnings=TRUE => warn if number of factors is too many 
Factoring method  fm="minres" will do a minimum residual as will fm="uls".  Both of these use a first derivative.  fm="ols" differs very slightly from "minres" in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative.  This will be slower.  fm="wls" will do a weighted least squares (WLS) solution, fm="gls" does a generalized weighted least squares (GLS), fm="pa" will do the principal factor solution, fm="ml" will do a maximum likelihood factor analysis. fm="minchi" will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm ="minrank" will do a minimum rank factor analysis. "old.min" will do minimal residual the way it was done prior to April, 2017 (see discussion below).  fm="alpha" will do alpha factor analysis as described in Kaiser and Coffey (1965)
alpha level for the confidence intervals for RMSEA
if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals
When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  Now it is always false.  If you want oblique factor scores, use tenBerge.  
If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate
When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)
The fraction of data to sample n.iter times if showing stability across sample sizes
A list of replication data sets to analyse in fa.pooled. All need to have the same number of variables.
additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax
A raw data matrix (or data.frame)
 Number of factors to extract, default is 1 
If TRUE, then a small amount of random error is added to each observed variable to keep the matrix positive semi-definite.  If FALSE, then this is not done but because the matrix is non-positive semi-definite it will need to be smoothed when finding the scores and the various statistics.
Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals. Ignored.
The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.
"none", "varimax", "quartimax",  "bentlerT", "equamax", "varimin", "geominT" and "bifactor" are orthogonal rotations.  "Promax", "promax", "oblimin", "simplimax", "bentlerQ,  "geominQ" and "biquartimin" and "cluster" are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax. SPSS seems to do a Kaiser normalization before doing Promax, this is done here by the call to "promax" which does the normalization before calling Promax in GPArotation.
Number of bootstrap interations to do in fa or fa.poly
Should the residual matrix be shown 
the default="regression" finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression ("Thurstone"), correlaton preserving ("tenBerge") as well as "Anderson" and "Bartlett" using the appropriate algorithms ( factor.scores). Although scores="tenBerge" is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  
Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. 
if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix
if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean
"median" or "mean" values are used to replace missing values
Iterate until the change in communalities is less than min.err
Maximum number of iterations for convergence 
symmetric=TRUE forces symmetry by just looking at the lower off diagonal values
warnings=TRUE => warn if number of factors is too many 
Factoring method  fm="minres" will do a minimum residual as will fm="uls".  Both of these use a first derivative.  fm="ols" differs very slightly from "minres" in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative.  This will be slower.  fm="wls" will do a weighted least squares (WLS) solution, fm="gls" does a generalized weighted least squares (GLS), fm="pa" will do the principal factor solution, fm="ml" will do a maximum likelihood factor analysis. fm="minchi" will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm ="minrank" will do a minimum rank factor analysis. "old.min" will do minimal residual the way it was done prior to April, 2017 (see discussion below).
alpha level for the confidence intervals for RMSEA
if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals
When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  
If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate
additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax
The output of factor analysis, principal components analysis, or ICLUST analysis.  May also be a factor loading matrix from anywhere.
Normally not specified (it is is found in the FA, pc, or ICLUST, solution), this may be given if the input is a loadings matrix.
the results of a factor extension analysis (if any)
 If it exists, a dot representation of the graph will be stored here (fa.graph)
 Variable labels 
 Loadings with abs(loading) > cut will be shown 
extension loadings with abs(loading) > e.cut will be shown
Only the biggest loading per item is shown
Only the biggest loading per extension item is shown 
Does the factor matrix reflect a g (first) factor.  If so, then draw this to the left of the variables, with the remaining factors to the right of the variables.  It is useful to turn off the simple parameter in this case.
If drawing a cluster analysis result, should we treat it as latent variable model (ic=FALSE) or as an observed variable model (ic=TRUE) 
A correlation matrix for the het.diagram function
A list of the elements in each level
Should arrows have double heads (in het.diagram)
graph size 
sort the factor loadings before showing the diagram
include error estimates (as arrows)
size of ellipses
size of rectangles
on which side should error arrows go?
modify font size
modify the font size in arrows, defaults to cex
The gap in the arrow for the label.  Can be adjusted to compensate for variations in cex or l.cex
sets the margins to be wider than normal, returns them to the normal size upon exit
how many different positions (1-3) should be used for the numeric labels. Useful if they overlap each other.
what font should be used for nodes in fa.graph  
what font should be used for edges in fa.graph  
 parameter passed to Rgraphviz– which way to draw the graph 
 Number of digits to show as an edgelable 
 Graphic title, defaults to "factor analyis" or "factor analysis and extension" 
Should we try to use Rgraphviz for output?
 other parameters 
A correlation or covariance matrix or a raw data matrix. If raw data, the correlation matrix will be found using pairwise deletion. If covariances are supplied, they will be converted to correlations unless the covar option is TRUE.
 Number of factors to extract, default is 1 
Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals.
The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.
"none", "varimax", "quartimax",  "bentlerT", "equamax", "varimin", "geominT" and "bifactor" are orthogonal rotations.  "Promax", "promax", "oblimin", "simplimax", "bentlerQ,  "geominQ" and "biquartimin" and "cluster" are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax. SPSS seems to do a Kaiser normalization before doing Promax, this is done here by the call to "promax" which does the normalization before calling Promax in GPArotation.
Number of random starts for the rotations.
Number of bootstrap interations to do in fa or fa.poly
Should the residual matrix be shown 
the default="regression" finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression ("Thurstone"), correlaton preserving ("tenBerge") as well as "Anderson" and "Bartlett" using the appropriate algorithms ( factor.scores). Although scores="tenBerge" is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  
Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. 
if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix
if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean
"median" or "mean" values are used to replace missing values
Iterate until the change in communalities is less than min.err
Maximum number of iterations for convergence 
symmetric=TRUE forces symmetry by just looking at the lower off diagonal values
Count number of (absolute) loadings less than hyper. 
warnings=TRUE => warn if number of factors is too many 
Factoring method  fm="minres" will do a minimum residual as will fm="uls".  Both of these use a first derivative.  fm="ols" differs very slightly from "minres" in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative.  This will be slower.  fm="wls" will do a weighted least squares (WLS) solution, fm="gls" does a generalized weighted least squares (GLS), fm="pa" will do the principal factor solution, fm="ml" will do a maximum likelihood factor analysis. fm="minchi" will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm ="minrank" will do a minimum rank factor analysis. "old.min" will do minimal residual the way it was done prior to April, 2017 (see discussion below).  fm="alpha" will do alpha factor analysis as described in Kaiser and Coffey (1965)
alpha level for the confidence intervals for RMSEA
if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals
When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  Now it is always false.  If you want oblique factor scores, use tenBerge.  
If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate
When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)
The fraction of data to sample n.iter times if showing stability across sample sizes
A list of replication data sets to analyse in fa.pooled. All need to have the same number of variables.
additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax
The output from a factor analysis or principal components analysis using fa or principal.  Can also just be a matrix of loadings. 
Sort by polar coordinates of first two factors (FALSE)
The order in which to order the factors
The order in which to order the items
new factor names
Organize the factors so that they are in echelon form (variable 1 .. n on factor 1, n+1 ...n=k on factor 2, etc.) 
Flip factor loadings such that the colMean is positive.
A correlation matrix or a data frame of raw data
A factor analysis loadings matrix or the output from a factor or principal components analysis.  In which case the r matrix need not be specified.
A factor intercorrelation matrix if the factor solution was oblique.
The number of observations for the correlation matrix.  If not specified, and a correlation matrix is used, chi square will not be reported. Not needed if the input is a data matrix.
The pairwise number of subjects for each pair in the correlation matrix.  This is used for finding observed chi square.
alpha level of confidence intervals for RMSEA (twice the confidence at each tail)
flag if components are being given statistics
A data matrix of dichotomous or discrete items, or the result of tetrachoric or polychoric   
Defaults to 1 factor
If true, then correct the tetrachoric correlations for continuity.  (See tetrachoric).  
If TRUE, automatically call the plot.irt or plot.poly functions.
the subset of variables to pick from the rho and tau output of a previous irt.fa analysis to allow for further analysis.
The number of subjects used in the initial analysis if doing a second analysis of a correlation matrix.  In particular, if using the fm="minchi" option, this should be the matrix returned by count.pairwise.
The default rotation is oblimin.  See fa for the other options.
The default factor extraction is minres.  See fa for the other options.
The object returned from fa 
The object returned from polychoric or tetrachoric.  This will include both a correlation matrix and the item difficulty levels.
Should the factor loadings be sorted before preparing the item information tables.  Defaults to FALSE as this is more useful for scoring items. For tabular output it is better to have sort=TRUE.
Additional parameters to pass to the factor analysis function
A matrix or dataframe with rows for subjects, columns for variables.  One of these columns should be the values of a grouping variable.
The names or numbers of the variable in data to use as the grouping variables.
Should the results include the correlation matrix within each group?  Default is FALSE.
Type of correlation/covariance to find within groups and between groups.  The default is Pearson correlation.  To find within and between covariances, set cor="cov".  Although polychoric, tetrachoric, and mixed correlations can be found within groups, this does not make sense for the between groups or the pooled within groups.  In this case, correlations for each group will be as specified, but the between groups and pooled within will be Pearson. See the discussion below.
What kind of correlations should be found (default is Pearson product moment)
How to treat missing data.  use="pairwise" is the default
Find polychoric.tetrachoric correlations within groups if requested.
Should missing values be deleted (na.rm=TRUE) or should we assume the data clean?
The alpha level for the confidence intervals for the ICC1 and ICC2, and rwg, rbg
The minimum length to use when abbreviating the labels for confidence intervals
If specified, weight the groups by weights when finding the pooled within group correlation.  Otherwise weight by sample size.
The number of trials to run when bootstrapping statistics
Should the bootstrap be done by permuting the data (replace=FALSE) or sampling with replacement (replace=TRUE)
The results from statsBy.boot may be summarized using boot.stats
Name of the variable to be summarized from statsBy.boot
The output of statsBy
The number of factors to extract in each subgroup
The factor rotation/transformation
The factor method (see fa for details)
Allow the factor solution to be freely estimated for each individual (see note).
Report individual factor analyses for each group as well as the summary table
The minimum number of within subject cases before we factor analyze it.
Show the upper and lower quant quantile of the factor loadings in faBy
Other parameters to pass to the fa function
A correlation or covariance matrix or a raw data matrix. If raw data, the correlation matrix will be found using pairwise deletion. If covariances are supplied, they will be converted to correlations unless the covar option is TRUE.
 Number of factors to extract, default is 1 
Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics. Must be specified if using a correlaton matrix and finding confidence intervals.
The pairwise number of observations. Used if using a correlation matrix and asking for a minchi solution.
"none", "varimax", "quartimax",  "bentlerT", "equamax", "varimin", "geominT" and "bifactor" are orthogonal rotations.  "Promax", "promax", "oblimin", "simplimax", "bentlerQ,  "geominQ" and "biquartimin" and "cluster" are possible oblique transformations of the solution.  The default is to do a oblimin transformation, although  versions prior to 2009 defaulted to varimax. SPSS seems to do a Kaiser normalization before doing Promax, this is done here by the call to "promax" which does the normalization before calling Promax in GPArotation.
Number of random starts for the rotations.
Number of bootstrap interations to do in fa or fa.poly
Should the residual matrix be shown 
the default="regression" finds factor scores using regression.  Alternatives for  estimating factor scores include simple regression ("Thurstone"), correlaton preserving ("tenBerge") as well as "Anderson" and "Bartlett" using the appropriate algorithms ( factor.scores). Although scores="tenBerge" is probably preferred for most solutions, it will lead to problems with some improper correlation matrices.  
Use squared multiple correlations (SMC=TRUE) or use 1  as initial communality estimate. Try using 1 if imaginary eigen values are reported. If SMC is a vector of length the number of variables, then these values are used as starting values in the case of fm='pa'. 
if covar is TRUE, factor the covariance matrix, otherwise factor the correlation matrix
if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean
"median" or "mean" values are used to replace missing values
Iterate until the change in communalities is less than min.err
Maximum number of iterations for convergence 
symmetric=TRUE forces symmetry by just looking at the lower off diagonal values
Count number of (absolute) loadings less than hyper. 
warnings=TRUE => warn if number of factors is too many 
Factoring method  fm="minres" will do a minimum residual as will fm="uls".  Both of these use a first derivative.  fm="ols" differs very slightly from "minres" in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative.  This will be slower.  fm="wls" will do a weighted least squares (WLS) solution, fm="gls" does a generalized weighted least squares (GLS), fm="pa" will do the principal factor solution, fm="ml" will do a maximum likelihood factor analysis. fm="minchi" will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm ="minrank" will do a minimum rank factor analysis. "old.min" will do minimal residual the way it was done prior to April, 2017 (see discussion below).  fm="alpha" will do alpha factor analysis as described in Kaiser and Coffey (1965)
alpha level for the confidence intervals for RMSEA
if doing iterations to find confidence intervals, what probability values should be found for the confidence intervals
When factor scores are found, should they be based on the structure matrix (default) or the pattern matrix (oblique.scores=TRUE).  Now it is always false.  If you want oblique factor scores, use tenBerge.  
If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate
When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)
The fraction of data to sample n.iter times if showing stability across sample sizes
A list of replication data sets to analyse in fa.pooled. All need to have the same number of variables.
additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax
A correlation matrix or a data matrix suitable for factoring
Number of factors in each solution to extract
Factor method.  The default is 'minres' factoring. To compare with pca solutions, can also be (fm ="pca")
What type of rotations to apply.  The default for factors is oblimin, for pca is varimax.
What factor scoring algorithm should be used. Defaults to tenBerge for both cases.
Should the factor intercorrelations be corrected by the lack of factor deteriminancy (i.e. divide by the square root of the factor R2)
How to treat missing data.  Use='pairwise" finds pairwise complete correlations.
What kind of correlation to find.  The default is Pearson.
Should cases be weighted?  Default, no.
If finding tetrachoric or polychoric correlations, what correction should be applied to empty cells (defaults to .5)
If doing target rotations (e.g., TargetQ or TargetT), then the Target must be specified.  If TargetT, this may be a matrix, if TargetQ, this must be a list.  (Strange property of GPARotation.)
 A matrix of factor loadings or a list of matrices of factor loadings
 A second matrix of factor loadings (if x is a list, then y may be empty)
Round off to digits
If NULL, then no loading matrices may contain missing values.  If use="complete" then variables with any missing loadings are dropped (with a warning)
If TRUE, find the factor congruences based upon the Structure matrix (if available), otherwise based upon the pattern matrix.
a correlation matrix 
A factor matrix of loadings.
deprecated.
deprecated
 deprecated 
deprecated 
deprecated
deprecated
deprecated 
deprecated
deprecated 
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
 A matrix of loadings. 
A matrix of factor correlations
Should the diagonal be model by ff' (U2 = TRUE) or replaced with 1's (U2 = FALSE)
deprecated.
deprecated
 deprecated 
deprecated 
deprecated
deprecated
deprecated 
deprecated
deprecated 
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
A factor analysis or cluster analysis output including the loadings, or a matrix of item by cluster correlations. Or the output from a kmeans cluster analysis. 
 A vector of cluster membership 
 Assign items to clusters if the absolute loadings are > cut 
If row.names exist they will be added to the plot, or, if they don't, labels can be specified.  If labels =NULL, and there are no row names, then variables are labeled by row number.)
 Any title
When plotting with factor loadings that are almost identical, it is sometimes useful to "jiggle" the points by jittering them. The default is to not jiggle.
if jiggle=TRUE, then how much should the points be jittered?
factor and clusters are shown with different pch values, starting at pch+1
Position of the text for labels for two dimensional plots. 1=below, 2 = left, 3 = above, 4= right
When adding labels to the points, should we show the points as well as the labels.  For many points, better to not show them, just the labels.
Specify the factor/clusters to plot
Any title – redundant with title
Further options to plot
 A correlation matrix 
 A factor model matrix or a list of class loadings
original loading matrix or a data frame  (can be output from a factor analysis function
 angle (in degrees!) to rotate 
 column in factor matrix defining the first variable
 column in factor matrix defining the second variable 
plot the original (unrotated) and rotated factors
parameters to pass to fa.plot
Either a matrix of data if scores are to be found, or a correlation matrix if just the factor weights are to be found.
The output from the fa or irt.fa functions, or a factor loading matrix.
If a pattern matrix is provided, then what were the factor intercorrelations.  Does not need to be specified if f is the output from the  fa or irt.fa functions.
Which of four factor score estimation procedures should be used. Defaults to "Thurstone" or regression based weights.  See details below for the other four methods.
If x is a set of data and rho is specified, then find scores based upon the fa results and the correlations reported in rho.  Used when scoring fa.poly results.
By default, only complete cases are scored.  But, missing data can be imputed using "median" or "mean".  The number of missing by subject is reported.
A correlation matrix or a data frame of raw data
A factor analysis loadings matrix or the output from a factor or principal components analysis.  In which case the r matrix need not be specified.
A factor intercorrelation matrix if the factor solution was oblique.
The number of observations for the correlation matrix.  If not specified, and a correlation matrix is used, chi square will not be reported. Not needed if the input is a data matrix.
The pairwise number of subjects for each pair in the correlation matrix.  This is used for finding observed chi square.
alpha level of confidence intervals for RMSEA (twice the confidence at each tail)
flag if components are being given statistics
deprecated.
deprecated
 deprecated 
deprecated 
deprecated
deprecated
deprecated 
deprecated
deprecated 
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
deprecated
either a matrix of loadings, or the result of a factor analysis/principal components analyis with a loading component 
Extract items with absolute loadings > cut
if TRUE, Return a keys list, else return a keys matrix (old style) 
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
Factor loadings matrix from fa or pca or any N x k loadings matrix
The correlation matrix used to find the factors.  (Used to find the factor indeterminancy of the solution)
"none", "varimax", "quartimax",  "bentlerT", "equamax", "varimin", "geominT" and "bifactor" are orthogonal rotations.  "Promax", "promax", "oblimin", "simplimax", "bentlerQ,  "geominQ" and "biquartimin" and "cluster" are possible oblique transformations of the solution.  Defaults to oblimin. 
The value defining when a loading is in the “hyperplane".
The number of random restarts to use.
additional parameters, specifically, keys may be passed if using the target rotation, or delta if using geominQ, or whether to normalize if using Varimax
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
A legitimate expression in the form y ~ x1 ,  etc. (see details)
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
 a vector or data.frame
remove NA values before processing
A correlation or covariance matrix or raw data matrix.
return a vector of split half reliabilities
Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!
If brute is false, how many samples of split halves should be tried? ()
Should the covariances or correlations be used for reliability calculations
If TRUE, any item with a negative loading on the first factor will be flipped in sign
a vector of -1, 0, 1 to select or reverse key items.  See if the key vector is less than the number of variables, then item numbers to be reverse can be specified.
Should we find the correlations using "pairwise" or "complete" (see ?cor)
The alpha level to use for the confidence intervals of the split half estimates
A p * p covariance matrix. Positive definiteness is not checked.
A vector L = (l1 ... lp) of length p with lower bounds to the diagonal elements x_i.	The  default l=(0, . . . , 0) does not imply any constraint, because positive semidefiniteness of the matrix C0 + Diag(x) implies 0 ≤ xi.
A vector u =(u1, . . . , up) of length p with upper bounds to the diagonal elements xi. The default is u = v.
A correlation or covariance matrix or raw data matrix.
return a vector of split half reliabilities
Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!
If brute is false, how many samples of split halves should be tried? ()
Should the covariances or correlations be used for reliability calculations
If TRUE, any item with a negative loading on the first factor will be flipped in sign
a vector of -1, 0, 1 to select or reverse key items.  See if the key vector is less than the number of variables, then item numbers to be reverse can be specified.
Should we find the correlations using "pairwise" or "complete" (see ?cor)
The alpha level to use for the confidence intervals of the split half estimates
A correlation or covariance matrix or raw data matrix.
return a vector of split half reliabilities
Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!
If brute is false, how many samples of split halves should be tried? ()
Should the covariances or correlations be used for reliability calculations
If TRUE, any item with a negative loading on the first factor will be flipped in sign
a vector of -1, 0, 1 to select or reverse key items.  See if the key vector is less than the number of variables, then item numbers to be reverse can be specified.
Should we find the correlations using "pairwise" or "complete" (see ?cor)
The alpha level to use for the confidence intervals of the split half estimates
 a vector, matrix, or data.frame 
na.rm=TRUE remove NA values before processing
If TRUE, then if there are any zeros, return 0, else, return the harmonic mean of the non-zero elements
 A matrix or data frame or free text
The number of lines at the beginning to show
The number of lines at the end to show
Round off the data to digits
Separate the head and tail with dots (ellipsis)
The first column to show (defaults to 1)
The last column to show (defaults to the number of columns
The number of lines at the beginning to show (an alias for top)
The number of lines at the end to show (an alias for bottom)
 A matrix or data frame or free text
The number of lines at the beginning to show
The number of lines at the end to show
Round off the data to digits
Separate the head and tail with dots (ellipsis)
The first column to show (defaults to 1)
The last column to show (defaults to the number of columns
The number of lines at the beginning to show (an alias for top)
The number of lines at the end to show (an alias for bottom)
The output of factor analysis, principal components analysis, or ICLUST analysis.  May also be a factor loading matrix from anywhere.
Normally not specified (it is is found in the FA, pc, or ICLUST, solution), this may be given if the input is a loadings matrix.
the results of a factor extension analysis (if any)
 If it exists, a dot representation of the graph will be stored here (fa.graph)
 Variable labels 
 Loadings with abs(loading) > cut will be shown 
extension loadings with abs(loading) > e.cut will be shown
Only the biggest loading per item is shown
Only the biggest loading per extension item is shown 
Does the factor matrix reflect a g (first) factor.  If so, then draw this to the left of the variables, with the remaining factors to the right of the variables.  It is useful to turn off the simple parameter in this case.
If drawing a cluster analysis result, should we treat it as latent variable model (ic=FALSE) or as an observed variable model (ic=TRUE) 
A correlation matrix for the het.diagram function
A list of the elements in each level
Should arrows have double heads (in het.diagram)
graph size 
sort the factor loadings before showing the diagram
include error estimates (as arrows)
size of ellipses
size of rectangles
on which side should error arrows go?
modify font size
modify the font size in arrows, defaults to cex
The gap in the arrow for the label.  Can be adjusted to compensate for variations in cex or l.cex
sets the margins to be wider than normal, returns them to the normal size upon exit
how many different positions (1-3) should be used for the numeric labels. Useful if they overlap each other.
what font should be used for nodes in fa.graph  
what font should be used for edges in fa.graph  
 parameter passed to Rgraphviz– which way to draw the graph 
 Number of digits to show as an edgelable 
 Graphic title, defaults to "factor analyis" or "factor analysis and extension" 
Should we try to use Rgraphviz for output?
 other parameters 
 matrix or data.frame
The variable in x to plot in histBy
The name of the variable in x to use as the grouping variable
Needs to be specified if using formula input to histBy
number of rows in the plot
number of columns in the plot
density=TRUE, show the normal fits and density distributions
freq=FALSE shows probability densities and density distribution, freq=TRUE shows frequencies
Color for the bars
The color(s) for the normal and the density fits. Defaults to black. 
The line type (lty) of the normal and density fits.  (specify the optional graphic parameter lwd to change the line size)
title for each panel will be set to the column name unless specified
Specify the lower, left, upper and right hand side margin in lines – set to be tighter than normal default of c(5,4,4,2) + .1 
Label for the x variable
The number of breaks in histBy (see hist)
If TRUE, use the same x-axis for all plots
The degree of transparency of the overlapping bars in histBy
A vector of colors in histBy  (defaults to the rainbow)
additional graphic parameters (e.g., col)
a matrix or dataframe of ratings
if TRUE, remove missing data – work on complete cases only (aov only)
The alpha level for significance for finding the confidence intervals
Should we use the lmer function from lme4?  This handles missing data and gives variance components as well. TRUE by default.
If TRUE reverse those items that do not correlate with total score. This is not done by default.
 A correlation matrix or data matrix/data.frame. (If r.mat is not square i.e, a correlation matrix, the data are correlated using pairwise deletion for Pearson correlations. 
Extract clusters until nclusters remain (default will extract until the other criteria are met or 1 cluster, whichever happens first). See the discussion below for alternative techniques for specifying the number of clusters. 
 Apply the increase in alpha criterion  (0) never or for (1) the smaller, 2) the average, or 3) the greater of the separate alphas. (default = 3) 
  Apply the increase in beta criterion (0) never or for (1) the smaller, 2) the average, or 3) the greater of the separate betas. (default =1) 
 Apply the beta criterion after clusters are of beta.size  (default = 4)
 Apply the alpha criterion after clusters are of size alpha.size (default =3) 
 Correct correlations for reliability (default = TRUE) 
Correct cluster -sub cluster correlations for reliability of the sub cluster , default is TRUE))
Reverse negative keyed items (default = TRUE
 Stop clustering if the beta is not greater than beta.min (default = .5) 
 1) short, 2) medium, 3 ) long output (default =1)
vector of item content or labels. If NULL, then the colnames are used.  If FALSE, then labels are V1 .. Vn
sort cluster loadings > absolute(cut) (default = 0) 
iterate the solution n.iterations times to "purify" the clusters (default = 0)
 Precision of digits of output (default = 2) 
 Title for this run 
Should ICLUST.rgraph be called automatically for plotting (requires Rgraphviz default=TRUE)
Weight the intercluster correlation by the size of the two clusters (TRUE) or do not weight them (FALSE)
When correlating clusters with subclusters, base the correlations on the general factor (default) or general + group (cor.gen=FALSE)
When estimating cluster-item correlations, use the smcs as the estimate of an item communality (SMC=TRUE) or use the maximum correlation (SMC=FALSE).
Should clusters be defined as the original groupings (purify = FAlSE) or by the items with the highest loadings on those original clusters? (purify = TRUE)  
Should the diagonal be included in the fit statistics.  The default is not to include it.  Prior to 1.2.8, the diagonal was included.
 A correlation matrix or data matrix/data.frame. (If r.mat is not square i.e, a correlation matrix, the data are correlated using pairwise deletion for Pearson correlations. 
Extract clusters until nclusters remain (default will extract until the other criteria are met or 1 cluster, whichever happens first). See the discussion below for alternative techniques for specifying the number of clusters. 
 Apply the increase in alpha criterion  (0) never or for (1) the smaller, 2) the average, or 3) the greater of the separate alphas. (default = 3) 
  Apply the increase in beta criterion (0) never or for (1) the smaller, 2) the average, or 3) the greater of the separate betas. (default =1) 
 Apply the beta criterion after clusters are of beta.size  (default = 4)
 Apply the alpha criterion after clusters are of size alpha.size (default =3) 
 Correct correlations for reliability (default = TRUE) 
Correct cluster -sub cluster correlations for reliability of the sub cluster , default is TRUE))
Reverse negative keyed items (default = TRUE
 Stop clustering if the beta is not greater than beta.min (default = .5) 
 1) short, 2) medium, 3 ) long output (default =1)
vector of item content or labels. If NULL, then the colnames are used.  If FALSE, then labels are V1 .. Vn
sort cluster loadings > absolute(cut) (default = 0) 
iterate the solution n.iterations times to "purify" the clusters (default = 0)
 Precision of digits of output (default = 2) 
 Title for this run 
Should ICLUST.rgraph be called automatically for plotting (requires Rgraphviz default=TRUE)
Weight the intercluster correlation by the size of the two clusters (TRUE) or do not weight them (FALSE)
When correlating clusters with subclusters, base the correlations on the general factor (default) or general + group (cor.gen=FALSE)
When estimating cluster-item correlations, use the smcs as the estimate of an item communality (SMC=TRUE) or use the maximum correlation (SMC=FALSE).
Should clusters be defined as the original groupings (purify = FAlSE) or by the items with the highest loadings on those original clusters? (purify = TRUE)  
Should the diagonal be included in the fit statistics.  The default is not to include it.  Prior to 1.2.8, the diagonal was included.
 A correlation matrix
 A list of options (see ICLUST) 
passed from the main program to speed up processing
Output from ICLUST
labels for variables (if not specified as rownames in the ICLUST output
if short=TRUE, variable names are replaced with Vn
Round the path coefficients to digits accuracy
The standard graphic control parameter for font size modifications.  This can be used to make the labels bigger or smaller than the default values.
Don't provide statistics for clusters less than min.size
size of the ellipses with the cluster statistics.
postive and negative 
The main graphic title
Normally, clusters are named sequentially C1 ... Cn.  If cluster.names are specified, then these values will be used instead.
Sets the margins to be narrower than the default values. Resets them upon return
output list from ICLUST 
 name of output file (defaults to console) 
draw a smaller node (without all the information) for clusters < min.size – useful for large problems
if short==TRUE, don't use variable names
vector of text labels (contents) for the variables
size of output 
 Font to use for nodes in the graph 
 Font to use for the labels of the arrows (edges)
LR or RL  
 number of digits to show 
 any title 
 other options to pass 
output list from ICLUST 
 File name to save optional dot code. 
draw a smaller node (without all the information) for clusters < min.size – useful for large problems
if short==TRUE, don't use variable names
vector of text labels (contents) for the variables
size of output 
 Font to use for nodes in the graph 
 Font to use for the labels of the arrows (edges)
LR or TB  or RL  
 number of digits to show 
 any title 
The variable labels can be a different size than the other nodes.  This is particularly helpful if the number of variables is large or the labels are long.
 other options to pass 
 The output from a factor or principal components analysis, or from ICLUST, or a matrix of loadings.
Do not include items in clusters with absolute loadings less than cut
labels for each item.
should cluster keys be returned? Useful if clusters scales are to be scored.
TRUE will will sort the clusters by their eigenvalues
 The output from a factor or principal components analysis, or from ICLUST, or a matrix of loadings.
Do not include items in clusters with absolute loadings less than cut
labels for each item.
should cluster keys be returned? Useful if clusters scales are to be scored.
TRUE will will sort the clusters by their eigenvalues
A correlation matrix or a raw data matrix suitable for factor analysis
The variables defining set X
The variables defining set Y
The number of factors to extract for the X variables
The number of factors to extract for the Y variables
Number of observations (needed for eBIC and chi square), can be ignored.
The factor  method to use, e.g., "minres", "mle" etc.  (see fa for details)
Which rotation to use. (see fa for details)
If TRUE, draw the esem.diagram
What options for to use for correlations (see fa for details)
"pairwise" for pairwise complete data, for other options see cor
Weights to apply to cases when finding wt.cov
other parameters to pass to fa or to esem.diagram functions.
The object returned from esem and passed to esem.diagram
 Variable labels 
 Loadings with abs(loading) > cut will be shown 
Only the biggest loading per item is shown
include error estimates (as arrows)
size of ellipses (adjusted by the number of variables)
Round coefficient to digits
loadings are adjusted by factor number mod adj to decrease likelihood of overlap
 Graphic title, defaults to "Exploratory Structural Model" 
draw the graphic left to right (TRUE) or top to bottom (FALSE)
Not yet implemented
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
input vector 
quantile to estimate ( 0 < q < 1
category width
input vector for interp.qplot.by
should missing values be removed
x label
Y label
limits for the y axis
length of arrow in interp.qplot.by
plot type in interp.qplot.by
add the plot or not
additional parameters to plotting function
 A vector of item difficulties  –probably taken from irt.item.diff.rasch
A matrix of 0,1 items  nrows = number of subjects, ncols = number of items
delta is the same as diff and is the item difficulty parameter
beta is the item discrimination parameter  found in irt.discrim 
 A vector of item difficulties  –probably taken from irt.item.diff.rasch
A matrix of 0,1 items  nrows = number of subjects, ncols = number of items
delta is the same as diff and is the item difficulty parameter
beta is the item discrimination parameter  found in irt.discrim 
 A vector of item difficulties  –probably taken from irt.item.diff.rasch
A matrix of 0,1 items  nrows = number of subjects, ncols = number of items
delta is the same as diff and is the item difficulty parameter
beta is the item discrimination parameter  found in irt.discrim 
 a matrix of items 
a vector of item difficulties (found by irt.item.diff)
ability estimate from irt.person.theta
A data matrix of dichotomous or discrete items, or the result of tetrachoric or polychoric   
Defaults to 1 factor
If true, then correct the tetrachoric correlations for continuity.  (See tetrachoric).  
If TRUE, automatically call the plot.irt or plot.poly functions.
the subset of variables to pick from the rho and tau output of a previous irt.fa analysis to allow for further analysis.
The number of subjects used in the initial analysis if doing a second analysis of a correlation matrix.  In particular, if using the fm="minchi" option, this should be the matrix returned by count.pairwise.
The default rotation is oblimin.  See fa for the other options.
The default factor extraction is minres.  See fa for the other options.
The object returned from fa 
The object returned from polychoric or tetrachoric.  This will include both a correlation matrix and the item difficulty levels.
Should the factor loadings be sorted before preparing the item information tables.  Defaults to FALSE as this is more useful for scoring items. For tabular output it is better to have sort=TRUE.
Additional parameters to pass to the factor analysis function
 a matrix of items 
a vector of item difficulties (found by irt.item.diff)
ability estimate from irt.person.theta
 A vector of item difficulties  –probably taken from irt.item.diff.rasch
A matrix of 0,1 items  nrows = number of subjects, ncols = number of items
delta is the same as diff and is the item difficulty parameter
beta is the item discrimination parameter  found in irt.discrim 
The estimated latent trait (found, for example by using score.irt).
A matrix or data frame of the multiple choice item responses.
The number of levels of the theta to use to form the probability estimates. May be increased if there are enough cases. 
Show the legend
For some SAPA data sets, there are a very large number of missing responses.  In general, we do not want to show their frequency.
Choose among c("bottomright", "bottom", "bottomleft", "left", "topleft", "top", "topright", "right",  "center","none").  The default is "topleft".
if NULL, then use the default colors, otherwise, specify the color choices. The basic color palette is c("black", "blue", "red", "darkgreen", "gold2", "gray50", "cornflowerblue", "mediumorchid2").
Other parameters for plots and points
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
A data matrix of dichotomous or discrete items, or the result of tetrachoric or polychoric   
Defaults to 1 factor
If true, then correct the tetrachoric correlations for continuity.  (See tetrachoric).  
If TRUE, automatically call the plot.irt or plot.poly functions.
the subset of variables to pick from the rho and tau output of a previous irt.fa analysis to allow for further analysis.
The number of subjects used in the initial analysis if doing a second analysis of a correlation matrix.  In particular, if using the fm="minchi" option, this should be the matrix returned by count.pairwise.
The default rotation is oblimin.  See fa for the other options.
The default factor extraction is minres.  See fa for the other options.
The object returned from fa 
The object returned from polychoric or tetrachoric.  This will include both a correlation matrix and the item difficulty levels.
Should the factor loadings be sorted before preparing the item information tables.  Defaults to FALSE as this is more useful for scoring items. For tabular output it is better to have sort=TRUE.
Additional parameters to pass to the factor analysis function
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
 Number of variables to simulate 
Number of subjects to simulate 
 circum=TRUE is circumplex structure, FALSE is simple structure
simple structure or spherical structure in sim.spherical
the average loading on the first dimension 
Average loading on the second dimension 
the average loading on the third dimension in sim.spherical
Average loading on a general factor (default=0)
To introduce skew, how far off center is the first dimension 
To introduce skew on the second dimension
To introduce skew on the third dimension – if using sim.spherical
 continuous or categorical variables.  
 values less than low are forced to low (or 0 in item.dichot)
 values greater than high are forced to high (or 1 in item.dichot) 
Change all values less than cutpoint to cutpoint. 
What is the cutpoint 
number of variables for the first factor in sim.spherical
number of variables for the second and third factors in sim.spherical
a matrix or data frame
Values of old to be used as cut points when converting continuous values to categorical values
Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.
A data matrix or data frame depending upon the function.
A data matrix or data frame or a vector
Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. 
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
The word(s) to search for from a dictionary
A list of scoring keys suitable to use for make.keys
Return all values in abs(x[,c1]) > cut.
Return the n best items per factor (as long as they have their highest loading on that factor)
If provided (e.g. from scoreItems) will be added to the lookupFromKeys output
a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content.
Column names of dictionary to search, defaults to "Item" or "Content" (dictionaries have different labels for this column), can search any column specified by search.
A data frame of item means
round to digits
Should the factors be sorted first?
In lookupFromKeys, should we suppress the column labels
Show setCor regressions with probability < p
 Number of variables to simulate 
Number of subjects to simulate 
 circum=TRUE is circumplex structure, FALSE is simple structure
simple structure or spherical structure in sim.spherical
the average loading on the first dimension 
Average loading on the second dimension 
the average loading on the third dimension in sim.spherical
Average loading on a general factor (default=0)
To introduce skew, how far off center is the first dimension 
To introduce skew on the second dimension
To introduce skew on the third dimension – if using sim.spherical
 continuous or categorical variables.  
 values less than low are forced to low (or 0 in item.dichot)
 values greater than high are forced to high (or 1 in item.dichot) 
Change all values less than cutpoint to cutpoint. 
What is the cutpoint 
number of variables for the first factor in sim.spherical
number of variables for the second and third factors in sim.spherical
a matrix or data frame
Values of old to be used as cut points when converting continuous values to categorical values
Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.
A data set
Variables to predict from the scales
A  keys.list that defines the scales
If not specified, these will be found.  Otherwise, this is the output from reliability.
If not specified, the average item validities for each scale will be found. Otherwise use the output from item.validity
A factor analysis output from fa or a factor loading matrix.
Any of the standard rotations avaialable in the GPArotation package. 
a parameter to pass to Promax  
A redundant parameter, which is used to replace m in calls to Promax
A data matrix or data frame depending upon the function.
A data matrix or data frame or a vector
Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. 
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
The word(s) to search for from a dictionary
A list of scoring keys suitable to use for make.keys
Return all values in abs(x[,c1]) > cut.
Return the n best items per factor (as long as they have their highest loading on that factor)
If provided (e.g. from scoreItems) will be added to the lookupFromKeys output
a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content.
Column names of dictionary to search, defaults to "Item" or "Content" (dictionaries have different labels for this column), can search any column specified by search.
A data frame of item means
round to digits
Should the factors be sorted first?
In lookupFromKeys, should we suppress the column labels
Show setCor regressions with probability < p
Number of variables items to be scored, or the name of the data.frame/matrix to be scored
 A list of the scoring keys, one element for each scale
 Typically, just the colnames of the items data matrix. 
 Labels for the scales can be specified here, or in the key.list 
A keys matrix returned from make.keys
if TRUE, prefix negatively keyed items with - (e.g., “-E2")
A matrix/dataframe of items or a correlation/covariance matrix of items
Form parcels of size 2 or size 3
if flip=TRUE, negative correlations lead to at least one item being negatively scored
Should item correlation/covariance be adjusted for their maximum correlation
Should the correlations be converted to congruence coefficients?
Sort a matrix of keys to reflect item order as much as possible
A correlation matrix or a data matrix (correlations will be found)
 A data.frame or matrix 
 how to treat missing data 
See the discussion in describe
Plot the expected normal distribution values versus the Mahalanobis distance of the subjects.
a factor model on the x variables. 
A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. 
a factor model on the y variables (can be empty) 
The correlation matrix among the x variables
The correlation matrix among the y variables
name a file to send dot language instructions. 
variable labels if not specified as colnames for the matrices
Draw paths for values > cut 
The output from a lavaan cfa or sem
draw an error term for observerd variables 
Just draw one path per x or y variable 
Draw a regression diagram (observed variables cause Y)
Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) 
size of the ellipses in structure.diagram
main title of diagram
page size of graphic 
 font type for graph 
font type for graph 
 Which direction should the graph be oriented 
Number of digits to draw
 Title of graphic 
 other options to pass to Rgraphviz 
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
Any integer or real value 
Item difficulty or delta parameter 
The slope of the curve at x=0  is equivalent to the discrimination parameter in 2PL models or alpha parameter. Is either 1  in 1PL or  1.702 in 1PN approximations.  
Lower asymptote  =  guessing parameter in 3PL models or gamma 
The upper asymptote  — in 4PL models
Probability to be converted to logit value
The response category for the graded response model
The response thresholds
Any integer or real value 
Item difficulty or delta parameter 
The slope of the curve at x=0  is equivalent to the discrimination parameter in 2PL models or alpha parameter. Is either 1  in 1PL or  1.702 in 1PN approximations.  
Lower asymptote  =  guessing parameter in 3PL models or gamma 
The upper asymptote  — in 4PL models
Probability to be converted to logit value
The response category for the graded response model
The response thresholds
Any integer or real value 
Item difficulty or delta parameter 
The slope of the curve at x=0  is equivalent to the discrimination parameter in 2PL models or alpha parameter. Is either 1  in 1PL or  1.702 in 1PN approximations.  
Lower asymptote  =  guessing parameter in 3PL models or gamma 
The upper asymptote  — in 4PL models
Probability to be converted to logit value
The response category for the graded response model
The response thresholds
A data matrix or data frame depending upon the function.
A data matrix or data frame or a vector
Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. 
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
The word(s) to search for from a dictionary
A list of scoring keys suitable to use for make.keys
Return all values in abs(x[,c1]) > cut.
Return the n best items per factor (as long as they have their highest loading on that factor)
If provided (e.g. from scoreItems) will be added to the lookupFromKeys output
a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content.
Column names of dictionary to search, defaults to "Item" or "Content" (dictionaries have different labels for this column), can search any column specified by search.
A data frame of item means
round to digits
Should the factors be sorted first?
In lookupFromKeys, should we suppress the column labels
Show setCor regressions with probability < p
A data matrix or data frame depending upon the function.
A data matrix or data frame or a vector
Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. 
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
The word(s) to search for from a dictionary
A list of scoring keys suitable to use for make.keys
Return all values in abs(x[,c1]) > cut.
Return the n best items per factor (as long as they have their highest loading on that factor)
If provided (e.g. from scoreItems) will be added to the lookupFromKeys output
a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content.
Column names of dictionary to search, defaults to "Item" or "Content" (dictionaries have different labels for this column), can search any column specified by search.
A data frame of item means
round to digits
Should the factors be sorted first?
In lookupFromKeys, should we suppress the column labels
Show setCor regressions with probability < p
A data matrix or data frame depending upon the function.
A data matrix or data frame or a vector
Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. 
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
The word(s) to search for from a dictionary
A list of scoring keys suitable to use for make.keys
Return all values in abs(x[,c1]) > cut.
Return the n best items per factor (as long as they have their highest loading on that factor)
If provided (e.g. from scoreItems) will be added to the lookupFromKeys output
a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content.
Column names of dictionary to search, defaults to "Item" or "Content" (dictionaries have different labels for this column), can search any column specified by search.
A data frame of item means
round to digits
Should the factors be sorted first?
In lookupFromKeys, should we suppress the column labels
Show setCor regressions with probability < p
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
A square matrix
A square matrix of the same size as the first (if omitted, then the matrix is converted to two symmetric matrices).
Find the difference between the first and second matrix and put the results in the above the diagonal entries.  
A data frame or matrix  (can be specified in formula mode) 
Some dichotomous grouping variable (may be specified using formula input (see example))
Apply cohen.d for each of the subgroups defined by group2 (may be specified by formula as well)
If using formula mode and specifying a particular variable (see example)
An effect size
The amount of trimming used in finding the means and sds in d.robust
Total sample size (of groups 1 and 2)
Sample size of group 1 (if only one group)
Sample size of group 2 
Pool the two variances
Student's t statistic
1-alpha is the width of the confidence interval
Find the correlation rather covariance matrix
A correlation to be converted to an effect size
Mean of group 1
Mean of group 2
Standard deviation of group 1
Standard deviation of group 2
Should we sort (and if so, in which direction), the results of cohen.d? Directions are "decreasing" or  "increasing".
What are the items being described?
Find Mahalanobis distance in cohen.d.
A data frame or matrix  (can be specified in formula mode) 
Some dichotomous grouping variable (may be specified using formula input (see example))
Apply cohen.d for each of the subgroups defined by group2 (may be specified by formula as well)
If using formula mode and specifying a particular variable (see example)
An effect size
The amount of trimming used in finding the means and sds in d.robust
Total sample size (of groups 1 and 2)
Sample size of group 1 (if only one group)
Sample size of group 2 
Pool the two variances
Student's t statistic
1-alpha is the width of the confidence interval
Find the correlation rather covariance matrix
A correlation to be converted to an effect size
Mean of group 1
Mean of group 2
Standard deviation of group 1
Standard deviation of group 2
Should we sort (and if so, in which direction), the results of cohen.d? Directions are "decreasing" or  "increasing".
What are the items being described?
Find Mahalanobis distance in cohen.d.
How many subjects to simulate. If NULL, return the population model 
 A vector of factor loadings for the tests  
A vector of error variances – if NULL then error = 1 - loading 2
short=TRUE: Just give the test correlations, short=FALSE, report observed test scores as well as the implied pattern matrix
 continuous or categorical (discrete) variables.  
 values less than low are forced to low 
 values greater than high are forced to high  
If specified, and categorical = TRUE, will cut the resulting continuous output at the value of cuts
 Loadings of group factors on a general factor 
 Loadings of items on the group factors 
 Number of subjects to generate: N=0 => population values 
 raw=TRUE, report the raw data, raw=FALSE, report the sample  correlation matrix. 
means for the individual variables
lower cutoff for categorical data
If True, then create categorical data
Upper cuttoff for categorical data
Number of variables to simulate
A vector of loadings that will be sampled (rowwise) to define the factors
The factor loadings of ‘pure’ measures of the factor.
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
Number of variables items to be scored, or the name of the data.frame/matrix to be scored
 A list of the scoring keys, one element for each scale
 Typically, just the colnames of the items data matrix. 
 Labels for the scales can be specified here, or in the key.list 
A keys matrix returned from make.keys
if TRUE, prefix negatively keyed items with - (e.g., “-E2")
Number of variables items to be scored, or the name of the data.frame/matrix to be scored
 A list of the scoring keys, one element for each scale
 Typically, just the colnames of the items data matrix. 
 Labels for the scales can be specified here, or in the key.list 
A keys matrix returned from make.keys
if TRUE, prefix negatively keyed items with - (e.g., “-E2")
A matrix or data.frame of items or a correlation matrix.
What column names should be predicted.  If a separate file, what are the variables to predict.
a keys.list similar to that used in scoreItems  
The default is raw data, the alternative is a correlation matrix
If given a correlation matrix, and showing log.p, we need the number of observations
Should we show the absolute value of the correlations.
If NULL, will label as either correlations or log (10) of correlations
if NULL, will use the names of the keys
Should we show the correlations (log.p = FALSE) or the log of the probabilities of the correlations (TRUE)
The probability for the upper and lower confidence intervals – bonferroni adjusted
The default plot chararcter is a filled circle
The title for each criterion
Which adjustment for multiple correlations should be applied ("holm", "bonferroni", "none")
If NULL will be the min and max of the data
Round off the results to digits
A dictionary of items
Other graphic parameters
 A data.frame or matrix 
 how to treat missing data 
See the discussion in describe
Plot the expected normal distribution values versus the Mahalanobis distance of the subjects.
Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c("Flags","Addition"). See notes and examples for each.
 either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c("Cubes","PaperFormBoard").  x and y may also be set by use of the formula style of lm.
A matrix or data.frame of correlations or, if not square, of raw data
A variance/covariance matrix, or a correlation matrix
The column name or numbers of the set of mediating variables (see mediate).
the column names or numbers of the set of covariates. 
If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.
find the correlations using "pairwise" (default) or just use "complete" cases (to match the lm function)
Report standardized betas (based upon the correlations) or raw bs (based upon covariances)
z is specified should part (TRUE) or partial (default) correlations be found.
Are data from a correlation matrix or data matrix?
A vector of means for the data in matReg if giving matrix input
The title for setCor.diagram
if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.
The output of setCor may be used for drawing diagrams
How many digits should be displayed in the setCor.diagram?
Show the unweighted matrix correlation between the x and y sets?
zero center the x variables before finding the interaction terms.
p value of the confidence intervals for the beta coefficients
By default, setCor makes a plot of the results, set to FALSE to suppress the plot
Text size of boxes displaying the variables in the diagram
Text size of numbers in arrows, defaults to cex
Additional graphical parameters for setCor.diagram
The resulting object from setCor or bestScales
If using a bestScales object, which set of keys to use ("best.key","weights","optimal.key","optimal.weights")
Not yet implemented option to crossValidation
type="b" draws both lines and points
Orientation of the x labels (3 to make vertical)
When abbreviating x labels how many characters as a minimum
If not NULL, the draw a legend at the appropriate location
What plot character(s) to use in matPlot
colors to use in matPlot
line types to use in matPlot
What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)
A correlation matrix
A factor analysis output (i.e., one with a loadings matrix) or a matrix of weights
Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c("Flags","Addition"). See notes and examples for each.
 either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c("Cubes","PaperFormBoard").  x and y may also be set by use of the formula style of lm.
A matrix or data.frame of correlations or, if not square, of raw data
A variance/covariance matrix, or a correlation matrix
The column name or numbers of the set of mediating variables (see mediate).
the column names or numbers of the set of covariates. 
If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.
find the correlations using "pairwise" (default) or just use "complete" cases (to match the lm function)
Report standardized betas (based upon the correlations) or raw bs (based upon covariances)
z is specified should part (TRUE) or partial (default) correlations be found.
Are data from a correlation matrix or data matrix?
A vector of means for the data in matReg if giving matrix input
The title for setCor.diagram
if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.
The output of setCor may be used for drawing diagrams
How many digits should be displayed in the setCor.diagram?
Show the unweighted matrix correlation between the x and y sets?
zero center the x variables before finding the interaction terms.
p value of the confidence intervals for the beta coefficients
By default, setCor makes a plot of the results, set to FALSE to suppress the plot
Text size of boxes displaying the variables in the diagram
Text size of numbers in arrows, defaults to cex
Additional graphical parameters for setCor.diagram
The resulting object from setCor or bestScales
If using a bestScales object, which set of keys to use ("best.key","weights","optimal.key","optimal.weights")
Not yet implemented option to crossValidation
type="b" draws both lines and points
Orientation of the x labels (3 to make vertical)
When abbreviating x labels how many characters as a minimum
If not NULL, the draw a legend at the appropriate location
What plot character(s) to use in matPlot
colors to use in matPlot
line types to use in matPlot
What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)
Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c("Flags","Addition"). See notes and examples for each.
 either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c("Cubes","PaperFormBoard").  x and y may also be set by use of the formula style of lm.
A matrix or data.frame of correlations or, if not square, of raw data
A variance/covariance matrix, or a correlation matrix
The column name or numbers of the set of mediating variables (see mediate).
the column names or numbers of the set of covariates. 
If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.
find the correlations using "pairwise" (default) or just use "complete" cases (to match the lm function)
Report standardized betas (based upon the correlations) or raw bs (based upon covariances)
z is specified should part (TRUE) or partial (default) correlations be found.
Are data from a correlation matrix or data matrix?
A vector of means for the data in matReg if giving matrix input
The title for setCor.diagram
if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.
The output of setCor may be used for drawing diagrams
How many digits should be displayed in the setCor.diagram?
Show the unweighted matrix correlation between the x and y sets?
zero center the x variables before finding the interaction terms.
p value of the confidence intervals for the beta coefficients
By default, setCor makes a plot of the results, set to FALSE to suppress the plot
Text size of boxes displaying the variables in the diagram
Text size of numbers in arrows, defaults to cex
Additional graphical parameters for setCor.diagram
The resulting object from setCor or bestScales
If using a bestScales object, which set of keys to use ("best.key","weights","optimal.key","optimal.weights")
Not yet implemented option to crossValidation
type="b" draws both lines and points
Orientation of the x labels (3 to make vertical)
When abbreviating x labels how many characters as a minimum
If not NULL, the draw a legend at the appropriate location
What plot character(s) to use in matPlot
colors to use in matPlot
line types to use in matPlot
What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)
A correlation matrix
A factor analysis output (i.e., one with a loadings matrix) or a matrix of weights
The dependent variable (or a formula suitable for a linear model),  If a formula, then this is of the form y ~ x +(m) -z (see details)
One or more predictor variables
One (or more) mediating variables
A data frame holding the data or a correlation or covariance  matrix. 
A moderating variable, if desired
Variables to partial out, if desired
If the data are from a correlation or covariance matrix, how many observations were used. This will lead to simulated data for the bootstrap.
use="pairwise" is the default when finding correlations or covariances
Number of bootstrap resamplings to conduct
Set the width of the confidence interval to be 1 - alpha
standardize the covariances to find the standardized betas
if part=TRUE, find part correlations otherwise find partial correlations when partialling
Plot the resulting paths
By default, will zero center the data before doing moderation
The number of digits to report in the mediate.diagram.
The output from mediate may be imported into mediate.diagram
The limits for the y axis in the mediate and moderate diagram functions
The limits for the x axis.  Make the minimum more negative if the x by x correlations do not fit.
If FALSE, do not draw the c lines, just the partialed (c')  lines
The title for the mediate and moderate functions
Adjust the text size (defaults to 1)
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Additional graphical parameters to pass to mediate.diagram
The dependent variable (or a formula suitable for a linear model),  If a formula, then this is of the form y ~ x +(m) -z (see details)
One or more predictor variables
One (or more) mediating variables
A data frame holding the data or a correlation or covariance  matrix. 
A moderating variable, if desired
Variables to partial out, if desired
If the data are from a correlation or covariance matrix, how many observations were used. This will lead to simulated data for the bootstrap.
use="pairwise" is the default when finding correlations or covariances
Number of bootstrap resamplings to conduct
Set the width of the confidence interval to be 1 - alpha
standardize the covariances to find the standardized betas
if part=TRUE, find part correlations otherwise find partial correlations when partialling
Plot the resulting paths
By default, will zero center the data before doing moderation
The number of digits to report in the mediate.diagram.
The output from mediate may be imported into mediate.diagram
The limits for the y axis in the mediate and moderate diagram functions
The limits for the x axis.  Make the minimum more negative if the x by x correlations do not fit.
If FALSE, do not draw the c lines, just the partialed (c')  lines
The title for the mediate and moderate functions
Adjust the text size (defaults to 1)
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Additional graphical parameters to pass to mediate.diagram
a vector,matrix, or data.frame 
Optional second vector 
Should a new plot be created, or should it be added to?
smooth = TRUE -> draw a loess fit
lm=TRUE -> draw the linear fit
data=TRUE implies draw the data points
Should 1 or 2 ellipses be drawn 
averaging window parameter for the lowess fit
iteration parameter for lowess
color of ellipses (default is red
label for the x axis
label for the y axis
The size of ellipses in sd units (defaults to 1 and 2)
Other parameters for plotting
r=1 draws a city block, r=2 is a Euclidean circle, r > 2 tends towards a square
title to use when drawing Minkowski circles
stretch the x axis
stretch the y axis
The data set to be analyzed (either a matrix or dataframe)
The names (or locations) of the continuous variables) (may be missing)
A set of continuous variables (may be missing) or, if p and d are missing, the variables to be analyzed.
A set of polytomous items (may be missing)
A set of dichotomous items (may be missing)
If TRUE, then smooth the correlation matix if it is non-positive definite
When finding tetrachoric correlations, what value should be used to correct for continuity?
For polychorics, should the global values of the tau parameters be used, or should the pairwise values be used.  Set to local if errors are occurring.
The number of categories beyond which a variable is considered "continuous".
The various options to the cor function include "everything", "all.obs", "complete.obs", "na.or.complete", or "pairwise.complete.obs". The default here is "pairwise"
The correlation method to use for the continuous variables. "pearson" (default), "kendall", or "spearman"
If specified, this is a vector of weights (one per participant) to differentially weight participants. The NULL case is equivalent of weights of 1 for all cases. 
The data set to be analyzed (either a matrix or dataframe)
The names (or locations) of the continuous variables) (may be missing)
A set of continuous variables (may be missing) or, if p and d are missing, the variables to be analyzed.
A set of polytomous items (may be missing)
A set of dichotomous items (may be missing)
If TRUE, then smooth the correlation matix if it is non-positive definite
When finding tetrachoric correlations, what value should be used to correct for continuity?
For polychorics, should the global values of the tau parameters be used, or should the pairwise values be used.  Set to local if errors are occurring.
The number of categories beyond which a variable is considered "continuous".
The various options to the cor function include "everything", "all.obs", "complete.obs", "na.or.complete", or "pairwise.complete.obs". The default here is "pairwise"
The correlation method to use for the continuous variables. "pearson" (default), "kendall", or "spearman"
If specified, this is a vector of weights (one per participant) to differentially weight participants. The NULL case is equivalent of weights of 1 for all cases. 
A data frame with persons, time, and items.
Which variable specifies people (groups)
Which variable specifies the temporal sequence?
Which items should be scored?  Note that if there are multiple scales, just specify the items on one scale at a time.  An item to be reversed scored can be specified by a minus sign. If long format, this is the column specifying item number. 
If TRUE, report alphas for every subject (default)
If TRUE, find ICCs for each person – can take a while
if FALSE, and if icc  is FALSE, then just draw the within subject plots
Should we use the lme4 package and lmer or just do the ANOVA?  Requires thelme4 package to be installed.  Necessary to do crossed designs with missing data but takes a very long time.
If TRUE, will find the nested components of variance.  Relatively fast.
Are the data in wide (default) or long format.
If the data are in long format, which column name (number) has the values to be analyzed?
How to handle missing data.  Passed to the lme function. 
If TRUE, show a lattice plot of the data by subject
Names or locations of extra columns to include in the long output.  These will be carried over from the wide form and duplicated for all items. See example.
Color for the lines in mlPlot.  Note that items are categorical and thus drawn in alphabetical order. Order the colors appropriately.
The main title for the plot (if drawn)
Other parameters to pass to xyplot
A data frame with persons, time, and items.
Which variable specifies people (groups)
Which variable specifies the temporal sequence?
Which items should be scored?  Note that if there are multiple scales, just specify the items on one scale at a time.  An item to be reversed scored can be specified by a minus sign. If long format, this is the column specifying item number. 
If TRUE, report alphas for every subject (default)
If TRUE, find ICCs for each person – can take a while
if FALSE, and if icc  is FALSE, then just draw the within subject plots
Should we use the lme4 package and lmer or just do the ANOVA?  Requires thelme4 package to be installed.  Necessary to do crossed designs with missing data but takes a very long time.
If TRUE, will find the nested components of variance.  Relatively fast.
Are the data in wide (default) or long format.
If the data are in long format, which column name (number) has the values to be analyzed?
How to handle missing data.  Passed to the lme function. 
If TRUE, show a lattice plot of the data by subject
Names or locations of extra columns to include in the long output.  These will be carried over from the wide form and duplicated for all items. See example.
Color for the lines in mlPlot.  Note that items are categorical and thus drawn in alphabetical order. Order the colors appropriately.
The main title for the plot (if drawn)
Other parameters to pass to xyplot
A data frame with persons, time, and items.
Which variable specifies people (groups)
Which variable specifies the temporal sequence?
Which items should be scored?  Note that if there are multiple scales, just specify the items on one scale at a time.  An item to be reversed scored can be specified by a minus sign. If long format, this is the column specifying item number. 
If TRUE, report alphas for every subject (default)
If TRUE, find ICCs for each person – can take a while
if FALSE, and if icc  is FALSE, then just draw the within subject plots
Should we use the lme4 package and lmer or just do the ANOVA?  Requires thelme4 package to be installed.  Necessary to do crossed designs with missing data but takes a very long time.
If TRUE, will find the nested components of variance.  Relatively fast.
Are the data in wide (default) or long format.
If the data are in long format, which column name (number) has the values to be analyzed?
How to handle missing data.  Passed to the lme function. 
If TRUE, show a lattice plot of the data by subject
Names or locations of extra columns to include in the long output.  These will be carried over from the wide form and duplicated for all items. See example.
Color for the lines in mlPlot.  Note that items are categorical and thus drawn in alphabetical order. Order the colors appropriately.
The main title for the plot (if drawn)
Other parameters to pass to xyplot
The dependent variable (or a formula suitable for a linear model),  If a formula, then this is of the form y ~ x +(m) -z (see details)
One or more predictor variables
One (or more) mediating variables
A data frame holding the data or a correlation or covariance  matrix. 
A moderating variable, if desired
Variables to partial out, if desired
If the data are from a correlation or covariance matrix, how many observations were used. This will lead to simulated data for the bootstrap.
use="pairwise" is the default when finding correlations or covariances
Number of bootstrap resamplings to conduct
Set the width of the confidence interval to be 1 - alpha
standardize the covariances to find the standardized betas
if part=TRUE, find part correlations otherwise find partial correlations when partialling
Plot the resulting paths
By default, will zero center the data before doing moderation
The number of digits to report in the mediate.diagram.
The output from mediate may be imported into mediate.diagram
The limits for the y axis in the mediate and moderate diagram functions
The limits for the x axis.  Make the minimum more negative if the x by x correlations do not fit.
If FALSE, do not draw the c lines, just the partialed (c')  lines
The title for the mediate and moderate functions
Adjust the text size (defaults to 1)
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Additional graphical parameters to pass to mediate.diagram
a vector, data.frame or matrix
the lag to use when finding diff     
A column of the x data.frame to be used for grouping
Should missing data be removed?
How to handle missing data in autoR
The results from a factor analysis fa, components analysis  principal, omega reliability analysis, omega, cluster analysis  iclust, topdown (bassAckward) bassAckward or confirmatory factor analysis, cfa, or structural equation model,sem, using the lavaan package.
x coordinate of a rectangle or ellipse
y coordinate of a rectangle or ellipse
The size of the ellipse (scaled by the number of variables
Text to insert in rectangle, ellipse, or arrow
adjust the text size
line color  (normal meaning for plot figures)
line type
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Tweak the gap in an arrow to be allow the label to be in a gap
Where to put the label along the arrows (values are then divided by 4)
Should the arrows have arrow heads on both ends?
modifies size of rectangle and ellipse as well as the curvature of curves.  (For curvature, positive numbers are concave down and to the left
arrows and curves go from 
arrows and curves go to
where is the rectangle?
Which shape to draw
default ranges
default ranges
Draw the text box
Which side of boxes should errors appear
Angle in degrees of vectors
draw arrows for edges in dia.cone
if TRUE, plot on previous plot
if TRUE, draw curves between arrows in dia.cone
The position of the text in . Follows the text positions of 1, 2, 3, 4 or NULL
Should the direction of the curve be calculated dynamically, or set as "up" or "left"
Most graphic parameters may be passed here
list saved from dia.self
lst saved from dia.arrow
list saved from dia.curved.arrow
list saved from dia.rect
The results from a factor analysis fa, components analysis  principal, omega reliability analysis, omega, cluster analysis  iclust, topdown (bassAckward) bassAckward or confirmatory factor analysis, cfa, or structural equation model,sem, using the lavaan package.
x coordinate of a rectangle or ellipse
y coordinate of a rectangle or ellipse
The size of the ellipse (scaled by the number of variables
Text to insert in rectangle, ellipse, or arrow
adjust the text size
line color  (normal meaning for plot figures)
line type
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Tweak the gap in an arrow to be allow the label to be in a gap
Where to put the label along the arrows (values are then divided by 4)
Should the arrows have arrow heads on both ends?
modifies size of rectangle and ellipse as well as the curvature of curves.  (For curvature, positive numbers are concave down and to the left
arrows and curves go from 
arrows and curves go to
where is the rectangle?
Which shape to draw
default ranges
default ranges
Draw the text box
Which side of boxes should errors appear
Angle in degrees of vectors
draw arrows for edges in dia.cone
if TRUE, plot on previous plot
if TRUE, draw curves between arrows in dia.cone
The position of the text in . Follows the text positions of 1, 2, 3, 4 or NULL
Should the direction of the curve be calculated dynamically, or set as "up" or "left"
Most graphic parameters may be passed here
list saved from dia.self
lst saved from dia.arrow
list saved from dia.curved.arrow
list saved from dia.rect
 matrix or data.frame
The variable in x to plot in histBy
The name of the variable in x to use as the grouping variable
Needs to be specified if using formula input to histBy
number of rows in the plot
number of columns in the plot
density=TRUE, show the normal fits and density distributions
freq=FALSE shows probability densities and density distribution, freq=TRUE shows frequencies
Color for the bars
The color(s) for the normal and the density fits. Defaults to black. 
The line type (lty) of the normal and density fits.  (specify the optional graphic parameter lwd to change the line size)
title for each panel will be set to the column name unless specified
Specify the lower, left, upper and right hand side margin in lines – set to be tighter than normal default of c(5,4,4,2) + .1 
Label for the x variable
The number of breaks in histBy (see hist)
If TRUE, use the same x-axis for all plots
The degree of transparency of the overlapping bars in histBy
A vector of colors in histBy  (defaults to the rainbow)
additional graphic parameters (e.g., col)
The results from a factor analysis fa, components analysis  principal, omega reliability analysis, omega, cluster analysis  iclust, topdown (bassAckward) bassAckward or confirmatory factor analysis, cfa, or structural equation model,sem, using the lavaan package.
x coordinate of a rectangle or ellipse
y coordinate of a rectangle or ellipse
The size of the ellipse (scaled by the number of variables
Text to insert in rectangle, ellipse, or arrow
adjust the text size
line color  (normal meaning for plot figures)
line type
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Tweak the gap in an arrow to be allow the label to be in a gap
Where to put the label along the arrows (values are then divided by 4)
Should the arrows have arrow heads on both ends?
modifies size of rectangle and ellipse as well as the curvature of curves.  (For curvature, positive numbers are concave down and to the left
arrows and curves go from 
arrows and curves go to
where is the rectangle?
Which shape to draw
default ranges
default ranges
Draw the text box
Which side of boxes should errors appear
Angle in degrees of vectors
draw arrows for edges in dia.cone
if TRUE, plot on previous plot
if TRUE, draw curves between arrows in dia.cone
The position of the text in . Follows the text positions of 1, 2, 3, 4 or NULL
Should the direction of the curve be calculated dynamically, or set as "up" or "left"
Most graphic parameters may be passed here
list saved from dia.self
lst saved from dia.arrow
list saved from dia.curved.arrow
list saved from dia.rect
The results from a factor analysis fa, components analysis  principal, omega reliability analysis, omega, cluster analysis  iclust, topdown (bassAckward) bassAckward or confirmatory factor analysis, cfa, or structural equation model,sem, using the lavaan package.
x coordinate of a rectangle or ellipse
y coordinate of a rectangle or ellipse
The size of the ellipse (scaled by the number of variables
Text to insert in rectangle, ellipse, or arrow
adjust the text size
line color  (normal meaning for plot figures)
line type
Adjust the text size in arrows, defaults to cex which in turn defaults to 1
Tweak the gap in an arrow to be allow the label to be in a gap
Where to put the label along the arrows (values are then divided by 4)
Should the arrows have arrow heads on both ends?
modifies size of rectangle and ellipse as well as the curvature of curves.  (For curvature, positive numbers are concave down and to the left
arrows and curves go from 
arrows and curves go to
where is the rectangle?
Which shape to draw
default ranges
default ranges
Draw the text box
Which side of boxes should errors appear
Angle in degrees of vectors
draw arrows for edges in dia.cone
if TRUE, plot on previous plot
if TRUE, draw curves between arrows in dia.cone
The position of the text in . Follows the text positions of 1, 2, 3, 4 or NULL
Should the direction of the curve be calculated dynamically, or set as "up" or "left"
Most graphic parameters may be passed here
list saved from dia.self
lst saved from dia.arrow
list saved from dia.curved.arrow
list saved from dia.rect
A data frame with persons, time, and items.
Which variable specifies people (groups)
Which variable specifies the temporal sequence?
Which items should be scored?  Note that if there are multiple scales, just specify the items on one scale at a time.  An item to be reversed scored can be specified by a minus sign. If long format, this is the column specifying item number. 
If TRUE, report alphas for every subject (default)
If TRUE, find ICCs for each person – can take a while
if FALSE, and if icc  is FALSE, then just draw the within subject plots
Should we use the lme4 package and lmer or just do the ANOVA?  Requires thelme4 package to be installed.  Necessary to do crossed designs with missing data but takes a very long time.
If TRUE, will find the nested components of variance.  Relatively fast.
Are the data in wide (default) or long format.
If the data are in long format, which column name (number) has the values to be analyzed?
How to handle missing data.  Passed to the lme function. 
If TRUE, show a lattice plot of the data by subject
Names or locations of extra columns to include in the long output.  These will be carried over from the wide form and duplicated for all items. See example.
Color for the lines in mlPlot.  Note that items are categorical and thus drawn in alphabetical order. Order the colors appropriately.
The main title for the plot (if drawn)
Other parameters to pass to xyplot
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
 a correlation matrix or a data matrix
Number of factors to extract – should be more than hypothesized! 
 what rotation to use c("none", "varimax",  "oblimin","promax")
Should we fit the diagonal as well 
factoring method – fm="pa"  Principal Axis Factor Analysis, fm = "minres" minimum residual (OLS) factoring fm="mle"  Maximum Likelihood FA, fm="pc" Principal Components" 
Number of observations if doing a factor analysis of correlation matrix.  This value is ignored by VSS but is necessary for the ML factor analysis package.
plot=TRUE  Automatically call VSS.plot with the VSS output, otherwise don't plot
a title to be passed on to VSS.plot
the plot character for the nfactors plots
If doing covariances or Pearson R, should we use "pairwise" or "complete cases"
What kind of correlation to find, defaults to Pearson but see fa for the choices
parameters to pass to the factor analysis program The most important of these is if using a correlation matrix is covmat= xx
A correlation matrix, or a data.frame/matrix of data, or (if Phi) is specified, an oblique factor pattern matrix 
Number of factors believed to be group factors
How many replications to do in omega for bootstrapped estimates
factor method (the default is minres)  fm="pa" for principal axes, fm="minres" for a minimum residual (OLS) solution, fm="pc" for principal components (see note), or  fm="ml" for maximum likelihood.
should the correlation matrix be found using polychoric/tetrachoric or normal Pearson correlations
a vector of +/- 1s to specify the direction of scoring of items.  The default is to assume all items are positively keyed, but if some items are reversed scored, then key should be specified.
If flip is TRUE, then items are automatically flipped to have positive correlations on the general factor. Items that have been reversed are shown with a - sign.
probability of two tailed conference boundaries
if specified, round the output to digits
Title for this analysis
main for this analysis  (directSl)
Loadings greater than cut are used in directSl
If plotting the results, should the Schmid Leiman solution be shown or should the hierarchical solution be shown? (default sl=TRUE)
If plotting, what labels should be applied to the variables? If not specified, will default to the column names.
plot=TRUE (default) calls omega.diagram, plot =FALSE does not.  If Rgraphviz is available, then omega.graph may be used separately.
Number of observations - used for goodness of fit statistic
What rotation to apply? The default is oblimin, the alternatives include simplimax, Promax,  cluster and target. target will rotate to an optional keys matrix (See target.rot)
If specified, then omega is found from the pattern matrix (m) and the factor intercorrelation matrix (Phi).
In the two factor case (not recommended), should the loadings be equal, emphasize the first factor, or emphasize the second factor. See in particular the option parameter in  schmid for treating the case of two group factors.
defaults to FALSE and the correlation matrix is found (standardized variables.)  If TRUE, the do the calculations on the unstandardized variables and use covariances.
If TRUE, do not give a warning about 3 factors being required. 
if FALSE, will use John Fox's sem package to do the omegaSem.  If TRUE, will use Yves Rosseel's lavaan package. 
The fitted object from lavaan or sem. For lavaan, this includes the correlation matrix and the variable names and thus m needs not be specified.
Allows additional parameters to be passed through to the factor routines.  
The output from the omega function 
 Optional output file for off line analysis using Graphviz 
 Orthogonal clusters using the Schmid-Leiman transform (sl=TRUE) or oblique clusters 
 variable labels 
Labels for the factors (not counting g)
size of graphics window 
 What font to use for the items
What font to use for the edge labels 
 Defaults to left to right 
 Precision of labels 
control font size
Use black for positive, red for negative
The margins for the figure are set to be wider than normal by default
Adjust the location of the factor loadings to vary as factor mod 4 + 1
 Figure title 
 main figure caption 
Other options to pass into the graphics packages 
the size to draw the ellipses for the factors. This is scaled by the  number of variables.
Minimum path coefficient to draw
Minimum general factor path to draw
draw just one path per item
sort the solution before making the diagram
on which side should errors  be drawn?
show the error estimates
size of the rectangles
The output from the omega function 
 Optional output file for off line analysis using Graphviz 
 Orthogonal clusters using the Schmid-Leiman transform (sl=TRUE) or oblique clusters 
 variable labels 
Labels for the factors (not counting g)
size of graphics window 
 What font to use for the items
What font to use for the edge labels 
 Defaults to left to right 
 Precision of labels 
control font size
Use black for positive, red for negative
The margins for the figure are set to be wider than normal by default
Adjust the location of the factor loadings to vary as factor mod 4 + 1
 Figure title 
 main figure caption 
Other options to pass into the graphics packages 
the size to draw the ellipses for the factors. This is scaled by the  number of variables.
Minimum path coefficient to draw
Minimum general factor path to draw
draw just one path per item
sort the solution before making the diagram
on which side should errors  be drawn?
show the error estimates
size of the rectangles
A correlation matrix, or a data.frame/matrix of data, or (if Phi) is specified, an oblique factor pattern matrix 
Number of factors believed to be group factors
How many replications to do in omega for bootstrapped estimates
factor method (the default is minres)  fm="pa" for principal axes, fm="minres" for a minimum residual (OLS) solution, fm="pc" for principal components (see note), or  fm="ml" for maximum likelihood.
should the correlation matrix be found using polychoric/tetrachoric or normal Pearson correlations
a vector of +/- 1s to specify the direction of scoring of items.  The default is to assume all items are positively keyed, but if some items are reversed scored, then key should be specified.
If flip is TRUE, then items are automatically flipped to have positive correlations on the general factor. Items that have been reversed are shown with a - sign.
probability of two tailed conference boundaries
if specified, round the output to digits
Title for this analysis
main for this analysis  (directSl)
Loadings greater than cut are used in directSl
If plotting the results, should the Schmid Leiman solution be shown or should the hierarchical solution be shown? (default sl=TRUE)
If plotting, what labels should be applied to the variables? If not specified, will default to the column names.
plot=TRUE (default) calls omega.diagram, plot =FALSE does not.  If Rgraphviz is available, then omega.graph may be used separately.
Number of observations - used for goodness of fit statistic
What rotation to apply? The default is oblimin, the alternatives include simplimax, Promax,  cluster and target. target will rotate to an optional keys matrix (See target.rot)
If specified, then omega is found from the pattern matrix (m) and the factor intercorrelation matrix (Phi).
In the two factor case (not recommended), should the loadings be equal, emphasize the first factor, or emphasize the second factor. See in particular the option parameter in  schmid for treating the case of two group factors.
defaults to FALSE and the correlation matrix is found (standardized variables.)  If TRUE, the do the calculations on the unstandardized variables and use covariances.
If TRUE, do not give a warning about 3 factors being required. 
if FALSE, will use John Fox's sem package to do the omegaSem.  If TRUE, will use Yves Rosseel's lavaan package. 
The fitted object from lavaan or sem. For lavaan, this includes the correlation matrix and the variable names and thus m needs not be specified.
Allows additional parameters to be passed through to the factor routines.  
A correlation matrix, or a data.frame/matrix of data, or (if Phi) is specified, an oblique factor pattern matrix 
Number of factors believed to be group factors
How many replications to do in omega for bootstrapped estimates
factor method (the default is minres)  fm="pa" for principal axes, fm="minres" for a minimum residual (OLS) solution, fm="pc" for principal components (see note), or  fm="ml" for maximum likelihood.
should the correlation matrix be found using polychoric/tetrachoric or normal Pearson correlations
a vector of +/- 1s to specify the direction of scoring of items.  The default is to assume all items are positively keyed, but if some items are reversed scored, then key should be specified.
If flip is TRUE, then items are automatically flipped to have positive correlations on the general factor. Items that have been reversed are shown with a - sign.
probability of two tailed conference boundaries
if specified, round the output to digits
Title for this analysis
main for this analysis  (directSl)
Loadings greater than cut are used in directSl
If plotting the results, should the Schmid Leiman solution be shown or should the hierarchical solution be shown? (default sl=TRUE)
If plotting, what labels should be applied to the variables? If not specified, will default to the column names.
plot=TRUE (default) calls omega.diagram, plot =FALSE does not.  If Rgraphviz is available, then omega.graph may be used separately.
Number of observations - used for goodness of fit statistic
What rotation to apply? The default is oblimin, the alternatives include simplimax, Promax,  cluster and target. target will rotate to an optional keys matrix (See target.rot)
If specified, then omega is found from the pattern matrix (m) and the factor intercorrelation matrix (Phi).
In the two factor case (not recommended), should the loadings be equal, emphasize the first factor, or emphasize the second factor. See in particular the option parameter in  schmid for treating the case of two group factors.
defaults to FALSE and the correlation matrix is found (standardized variables.)  If TRUE, the do the calculations on the unstandardized variables and use covariances.
If TRUE, do not give a warning about 3 factors being required. 
if FALSE, will use John Fox's sem package to do the omegaSem.  If TRUE, will use Yves Rosseel's lavaan package. 
The fitted object from lavaan or sem. For lavaan, this includes the correlation matrix and the variable names and thus m needs not be specified.
Allows additional parameters to be passed through to the factor routines.  
A correlation matrix, or a data.frame/matrix of data, or (if Phi) is specified, an oblique factor pattern matrix 
Number of factors believed to be group factors
How many replications to do in omega for bootstrapped estimates
factor method (the default is minres)  fm="pa" for principal axes, fm="minres" for a minimum residual (OLS) solution, fm="pc" for principal components (see note), or  fm="ml" for maximum likelihood.
should the correlation matrix be found using polychoric/tetrachoric or normal Pearson correlations
a vector of +/- 1s to specify the direction of scoring of items.  The default is to assume all items are positively keyed, but if some items are reversed scored, then key should be specified.
If flip is TRUE, then items are automatically flipped to have positive correlations on the general factor. Items that have been reversed are shown with a - sign.
probability of two tailed conference boundaries
if specified, round the output to digits
Title for this analysis
main for this analysis  (directSl)
Loadings greater than cut are used in directSl
If plotting the results, should the Schmid Leiman solution be shown or should the hierarchical solution be shown? (default sl=TRUE)
If plotting, what labels should be applied to the variables? If not specified, will default to the column names.
plot=TRUE (default) calls omega.diagram, plot =FALSE does not.  If Rgraphviz is available, then omega.graph may be used separately.
Number of observations - used for goodness of fit statistic
What rotation to apply? The default is oblimin, the alternatives include simplimax, Promax,  cluster and target. target will rotate to an optional keys matrix (See target.rot)
If specified, then omega is found from the pattern matrix (m) and the factor intercorrelation matrix (Phi).
In the two factor case (not recommended), should the loadings be equal, emphasize the first factor, or emphasize the second factor. See in particular the option parameter in  schmid for treating the case of two group factors.
defaults to FALSE and the correlation matrix is found (standardized variables.)  If TRUE, the do the calculations on the unstandardized variables and use covariances.
If TRUE, do not give a warning about 3 factors being required. 
if FALSE, will use John Fox's sem package to do the omegaSem.  If TRUE, will use Yves Rosseel's lavaan package. 
The fitted object from lavaan or sem. For lavaan, this includes the correlation matrix and the variable names and thus m needs not be specified.
Allows additional parameters to be passed through to the factor routines.  
A correlation matrix, or a data.frame/matrix of data, or (if Phi) is specified, an oblique factor pattern matrix 
Number of factors believed to be group factors
How many replications to do in omega for bootstrapped estimates
factor method (the default is minres)  fm="pa" for principal axes, fm="minres" for a minimum residual (OLS) solution, fm="pc" for principal components (see note), or  fm="ml" for maximum likelihood.
should the correlation matrix be found using polychoric/tetrachoric or normal Pearson correlations
a vector of +/- 1s to specify the direction of scoring of items.  The default is to assume all items are positively keyed, but if some items are reversed scored, then key should be specified.
If flip is TRUE, then items are automatically flipped to have positive correlations on the general factor. Items that have been reversed are shown with a - sign.
probability of two tailed conference boundaries
if specified, round the output to digits
Title for this analysis
main for this analysis  (directSl)
Loadings greater than cut are used in directSl
If plotting the results, should the Schmid Leiman solution be shown or should the hierarchical solution be shown? (default sl=TRUE)
If plotting, what labels should be applied to the variables? If not specified, will default to the column names.
plot=TRUE (default) calls omega.diagram, plot =FALSE does not.  If Rgraphviz is available, then omega.graph may be used separately.
Number of observations - used for goodness of fit statistic
What rotation to apply? The default is oblimin, the alternatives include simplimax, Promax,  cluster and target. target will rotate to an optional keys matrix (See target.rot)
If specified, then omega is found from the pattern matrix (m) and the factor intercorrelation matrix (Phi).
In the two factor case (not recommended), should the loadings be equal, emphasize the first factor, or emphasize the second factor. See in particular the option parameter in  schmid for treating the case of two group factors.
defaults to FALSE and the correlation matrix is found (standardized variables.)  If TRUE, the do the calculations on the unstandardized variables and use covariances.
If TRUE, do not give a warning about 3 factors being required. 
if FALSE, will use John Fox's sem package to do the omegaSem.  If TRUE, will use Yves Rosseel's lavaan package. 
The fitted object from lavaan or sem. For lavaan, this includes the correlation matrix and the variable names and thus m needs not be specified.
Allows additional parameters to be passed through to the factor routines.  
A data matrix or data.frame
Plot the resulting QQ graph
Label the bad worst values
Should missing data be deleted
Label for x axis
Label for y axis
More graphic parameters, e.g., cex=.8
conventional probability of statistic (e.g., of F, t, or r)
The F statistic
Degrees of freedom of the t-test, or of the first group if unequal sizes
Degrees of freedom of the denominator of F or the second group in an unequal sizes t test
Correlation coefficient
Total sample size if using r 
t-statistic if doing a t-test or testing significance of a regression slope
Should a one or two tailed test be used? 
conventional probability of statistic (e.g., of F, t, or r)
The F statistic
Degrees of freedom of the t-test, or of the first group if unequal sizes
Degrees of freedom of the denominator of F or the second group in an unequal sizes t test
Correlation coefficient
Total sample size if using r 
t-statistic if doing a t-test or testing significance of a regression slope
Should a one or two tailed test be used? 
conventional probability of statistic (e.g., of F, t, or r)
The F statistic
Degrees of freedom of the t-test, or of the first group if unequal sizes
Degrees of freedom of the denominator of F or the second group in an unequal sizes t test
Correlation coefficient
Total sample size if using r 
t-statistic if doing a t-test or testing significance of a regression slope
Should a one or two tailed test be used? 
conventional probability of statistic (e.g., of F, t, or r)
The F statistic
Degrees of freedom of the t-test, or of the first group if unequal sizes
Degrees of freedom of the denominator of F or the second group in an unequal sizes t test
Correlation coefficient
Total sample size if using r 
t-statistic if doing a t-test or testing significance of a regression slope
Should a one or two tailed test be used? 
r(xy) 
r(xz) 
r(yz) 
Number of subjects for first group
Number of subjects in second group (if not equal to n)
Calculate two or one tailed probability values
a data.frame or matrix
TRUE draws loess smooths 
 TRUE scales the correlation font by the size of the absolute correlation. 
TRUE shows the density plots as well as histograms
TRUE draws correlation ellipses
Plot the linear fit rather than the LOESS smoothed fits.
 the number of digits to show
method parameter for the correlation ("pearson","spearman","kendall")
The plot character (defaults to 20 which is a '.').
If plotting regressions, should correlations be reported?
Should the points be jittered before plotting?
factor for jittering (1-5)
What color should the histogram on the diagonal be?
If FALSE, do not show the data points, just the data ellipses and smoothed functions
if TRUE (default) draw a rug under the histogram, if FALSE, don't draw the rug
If specified, allows control for the number of breaks in the histogram (see the hist function)
If this is specified, this will change the size of the text in the correlations. this allows one to also change the size of the points in the plot by specifying the normal cex values. If just specifying cex, it will change the character size, if cex.cor is specified, then cex will function to change the point size.
If specified, then weight the correlations by a weights matrix (see note for some comments)
If TRUE, then smooth.scatter the data points  – slow but pretty with lots of subjects 
For those people who like to show the significance of correlations by using magic astricks, set stars=TRUE
Draw confidence intervals for the linear model or for the loess fit, defaults to ci=FALSE. If confidence intervals are not drawn, the fitting function is lowess.
The alpha level for the confidence regions, defaults to .05
other options for pairs 
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
 An input matrix, typically a data matrix ready to be correlated. 
 An optional second input matrix 
if TRUE, then report the diagonal, else fill the diagonals with NA
Other parameters to pass to describe
Count the number of item pairs with <= min entries
Show the table of the item pairs that have entries <= min
A keys.list specifying which items belong to which scale.
A correlation matrix to be described or imputed
Report the item pairs and numbers with cell sizes less than cut
If TRUE, then replace all NA correlations with the mean correlation for that  within or between scale
Should the upper off diagonal matrix be drawn, or left blank?
if NULL, use column and row names, otherwise use labels
A legend (key) to the colors is shown on the right hand side
How many categories should be labelled in the legend?
Defaults to FALSE and  will use a grey scale. colors=TRUE use colors \from the colorRampPalette from red through white to blue
If not NULL, then the maximum number of characters to use in row/column labels
Orientation of the x axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
Orientation of the y axis labels (1 = horizontal, 0, parallel to axis, 2 perpendicular to axis)
A title. Defaults to "Relative Frequencies"
A color gradient: e.g.,  gr <- colorRampPalette(c("#B52127", "white", "#2171B5"))  will produce slightly more pleasing (to some) colors. See next to last example of corPlot.  
Should we count the number of pairwise observations using pairwiseCount, or just plot the counts for a matrix?
Sample size of the number of variables to sample in pairwiseSample
A matrix/dataframe of items or a correlation/covariance matrix of items
Form parcels of size 2 or size 3
if flip=TRUE, negative correlations lead to at least one item being negatively scored
Should item correlation/covariance be adjusted for their maximum correlation
Should the correlations be converted to congruence coefficients?
Sort a matrix of keys to reflect item order as much as possible
A data or correlation matrix
The variable names or locations associated with the X set (or formula input) 
The variable  names or locations associated with the Y set to be partialled from the X set
How should we treat missing data? The default is pairwise complete.
Which method of correlation should we use, the default is pearson.
a correlation matrix.  If a raw data matrix is used, the correlations will be found using pairwise deletions for missing values.
Number of components to extract 
 FALSE, do not show residuals, TRUE, report residuals 
"none", "varimax", "quartimax", "promax", "oblimin", "simplimax", and "cluster" are possible rotations/transformations of the solution. See fa for all rotations avaiable.
Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics.
If false, find the correlation matrix from the raw data or convert to a correlation matrix if given a square matrix as input.
If TRUE, find component scores
if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean
"median" or "mean" values are used to replace missing values
If TRUE (default), then the component scores are based upon the structure matrix.  If FALSE, upon the pattern matrix.
Which way of finding component scores should be used. The default is "regression"
If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixedCor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate
When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)
other parameters to pass to functions such as factor.scores or the various rotation functions.  
a 1 x 4 vector or a 2 x 2 matrix 
 round the result to digits 
number of cases to simulate
 correlation between latent and observed 
form dichotomized variables at the value of cuts
Number of variables in the design matrix 
A list of items included in each factor (for structure.list, or the factors that correlate with the specified factor for phi.list
prefix for parameters – needed in case of creating an X set and a Y set
Names for the factors 
Item labels 
Number of factors in the phi matrix
phi 
a vector of the selection ratio and probability of criterion.  In the case where ph is a matrix, m is a vector of the frequencies of the selected cases
When finding tetrachoric correlations, should we correct for continuity for small marginals.  See tetrachoric for a discussion.
If the marginals are given as frequencies, what was the total number of cases?
 probability of the predictor – the so called selection ratio 
probability of the criterion – the so called success rate. 
a matrix of phi or Yule  coefficients 
A vector of marginal frequencies  
phi 
a vector of the selection ratio and probability of criterion.  In the case where ph is a matrix, m is a vector of the frequencies of the selected cases
When finding tetrachoric correlations, should we correct for continuity for small marginals.  See tetrachoric for a discussion.
If the marginals are given as frequencies, what was the total number of cases?
 probability of the predictor – the so called selection ratio 
probability of the criterion – the so called success rate. 
A correlation or covariance matrix to analyze
A very small number. Reject values with eigen values less than tolerance
The object to plot 
Variable labels
Label for the x axis  – defaults to Latent Trait
Label for the y axis
The limits for the x axis
Specify the limits for the y axis
Main title for graph
"ICC" plots items, "IIC" plots item information, "test" plots test information, defaults to IIC.,"qq" does a quantile plot,"chi" plots chi square distributions,"hist" shows the histogram,"cor" does a corPlot of the residuals. 
The discrimination parameter
Only plot item responses with discrimiantion greater than cut
Used in plotting irt results from irt.fa.
ylab for test reliability, defaults to "reliability"
label the most 1.. bad items in residuals
if using the cor option in plot residuals, show the numeric values
if using the cor option in plot residuals, show the upper off diagonalvalues
if using the cor option in plot residuals, show the diagonal values
Standardize the resduals?
The color of the lines in the IRT plots.  Defaults to all being black, but it is possible to specify lncol as a vector of colors to be used.
other calls to plot
The object to plot 
Variable labels
Label for the x axis  – defaults to Latent Trait
Label for the y axis
The limits for the x axis
Specify the limits for the y axis
Main title for graph
"ICC" plots items, "IIC" plots item information, "test" plots test information, defaults to IIC.,"qq" does a quantile plot,"chi" plots chi square distributions,"hist" shows the histogram,"cor" does a corPlot of the residuals. 
The discrimination parameter
Only plot item responses with discrimiantion greater than cut
Used in plotting irt results from irt.fa.
ylab for test reliability, defaults to "reliability"
label the most 1.. bad items in residuals
if using the cor option in plot residuals, show the numeric values
if using the cor option in plot residuals, show the upper off diagonalvalues
if using the cor option in plot residuals, show the diagonal values
Standardize the resduals?
The color of the lines in the IRT plots.  Defaults to all being black, but it is possible to specify lncol as a vector of colors to be used.
other calls to plot
 A data.frame or data matrix of scores.  If the matrix is square, it is assumed to be a correlation matrix.  Otherwise, correlations (with pairwise deletion) will be found 
n.obs=0 implies a data matrix/data.frame.  Otherwise, how many cases were used to find the correlations. 
What factor method to use. (minres, ml, uls, wls, gls, pa) See  fa for details.
show the eigen values for a principal components (fa="pc") or a principal axis factor analysis (fa="fa") or both principal components and principal factors (fa="both")
The number of factors to extract when estimating the eigen values. Defaults to 1, which was the prior value used.
 a title for the analysis 
Number of simulated analyses to perform
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate.  This matches the call to fa
For tetrachoric correlations, should a correction for continuity be applied. (See tetrachoric)  If set to 0, then no correction is applied, otherwise, the default is to add .5 observations to the cell.
For continuous data, the default is to resample as well as to generate random normal data.  If sim=FALSE, then just show the resampled results. These two results are very similar. This does not make sense in the case of  correlation matrix, in which case resampling is impossible. In the case of polychoric or tetrachoric data, in addition to randomizing the real data, should we compare the solution to random simulated data.  This will double the processing time, but will basically show the same result.
Should error.bars be plotted (default = FALSE)
Should the error bars be standard errors (se.bars=TRUE) or 1 standard deviation (se.bars=FALSE, the default).  With many iterations, the standard errors are very small and some prefer to see the broader range.  The default has been changed in 1.7.8 to be se.bars=FALSE to more properly show the range.
SMC=TRUE finds eigen values after estimating communalities by using SMCs.  smc = FALSE finds eigen values after estimating communalities with the first factor.
Label for the y axis – defaults to “eigen values of factors and components", can be made empty to show many graphs
the default is to have a legend.  For multiple panel graphs, it is better to not show the legend
if nothing is specified, the empirical eigen values are compared to the mean of the resampled or simulated eigen values.  If a value (e.g., quant=.95) is specified, then the eigen values are compared against the matching quantile of the simulated data.  Clearly the larger the value of quant, the few factors/components that will be identified.
If doing polychoric analyses (fa.parallel.poly) and the number of alternatives differ across items, it is necessary to turn off the global option
additional plotting parameters, for plot.poly.parallel
By default, fa.parallel draws the eigen value plots.  If FALSE, suppresses the graphic output
The object to plot 
Variable labels
Label for the x axis  – defaults to Latent Trait
Label for the y axis
The limits for the x axis
Specify the limits for the y axis
Main title for graph
"ICC" plots items, "IIC" plots item information, "test" plots test information, defaults to IIC.,"qq" does a quantile plot,"chi" plots chi square distributions,"hist" shows the histogram,"cor" does a corPlot of the residuals. 
The discrimination parameter
Only plot item responses with discrimiantion greater than cut
Used in plotting irt results from irt.fa.
ylab for test reliability, defaults to "reliability"
label the most 1.. bad items in residuals
if using the cor option in plot residuals, show the numeric values
if using the cor option in plot residuals, show the upper off diagonalvalues
if using the cor option in plot residuals, show the diagonal values
Standardize the resduals?
The color of the lines in the IRT plots.  Defaults to all being black, but it is possible to specify lncol as a vector of colors to be used.
other calls to plot
A list of items to be scored (may be taken from a keys.list for scoreItems. This list may contain one or more keys. ) If keys are not specified, then all the items are used.
The matrix or data.frame of items to be scored.  Can be substantially greater than the items included in keys. For just those items in each key are scored.
Omega is not well defined for two factors, but for small sets of items, two is the better choice. For larger number of items per scale, 3 is probably preferable.
By default, find splitHalf reliabilities as well as the omega statistics. When plotting, split implies that raw was called in reliability.
By default, suppress the omega plots for each scale.
If TRUE, return a list of all the possible splits (up to n.samples).  Useful for graphic display. 
If TRUE then  split and raw are forced to TRUE and the histograms of the split half values are drawn.  (Otherwise, just return the values for later plotting)
Normally defaults to 10,000.  This means that for up to 16 item tests, all possible splits are found.  choose(n,n/2)/2 explodes above that, eg. for all splits of the epi E scale requires 1,352,078 splits or 23.4 seconds on a MacBook Pro with a 2.4GHZ 8 core Intel Core I9.  Can be done, but do you want to do so?  
The object returned from reliability
Add in the values of omega_h and omega_t
Show the unidimensionality value from unidim.  
Add the value of alpha
Allows us to merge this figure with other ones
Defaults to "Split half distributions + omega, alpha"
The xlim of the plot
Other graphical parameters
The object to plot 
Variable labels
Label for the x axis  – defaults to Latent Trait
Label for the y axis
The limits for the x axis
Specify the limits for the y axis
Main title for graph
"ICC" plots items, "IIC" plots item information, "test" plots test information, defaults to IIC.,"qq" does a quantile plot,"chi" plots chi square distributions,"hist" shows the histogram,"cor" does a corPlot of the residuals. 
The discrimination parameter
Only plot item responses with discrimiantion greater than cut
Used in plotting irt results from irt.fa.
ylab for test reliability, defaults to "reliability"
label the most 1.. bad items in residuals
if using the cor option in plot residuals, show the numeric values
if using the cor option in plot residuals, show the upper off diagonalvalues
if using the cor option in plot residuals, show the diagonal values
Standardize the resduals?
The color of the lines in the IRT plots.  Defaults to all being black, but it is possible to specify lncol as a vector of colors to be used.
other calls to plot
A matrix of loadings or the output from a factor or cluster analysis program
 sort=TRUE: sort items by the angle of the items on the first pair of factors.
The input may be in one of four forms:a) a data frame or matrix of dichotmous data (e.g., the lsat6 from the bock data set) or discrete numerical (i.e., not too many levels, e.g., the big 5 data set, bfi) for polychoric, or continuous for the case of biserial and polyserial. b) a 2 x 2 table of cell counts or cell frequencies (for tetrachoric) or an n x m table of cell counts  (for both tetrachoric and polychoric). c) a vector with elements corresponding to the four cell frequencies (for tetrachoric)d) a vector with elements of the two marginal frequencies (row and column) and the comorbidity (for tetrachoric)
A (matrix or dataframe) of discrete scores. In the case of tetrachoric, these should be dichotomous, for polychoric not too many levels, for biserial they should be discrete (e.g., item responses) with not too many (<10?) categories.
Correction value to use to correct for continuity in the case of zero entry cell for tetrachoric, polychoric, polybi, and mixed.cor.  See the examples for the effect of correcting versus not correcting for continuity.
if TRUE and if the tetrachoric/polychoric matrix is not positive definite, then apply a simple smoothing algorithm using cor.smooth
When finding pairwise correlations, should we use the global values of the tau parameter (which is somewhat faster), or the local values (global=FALSE)?  The local option is equivalent to the polycor solution, or to doing one correlation at a time. global=TRUE borrows information for one item pair from the other pairs using those item's frequencies.   This will make a difference in the presence of lots of missing data. With very small sample sizes with global=FALSE and correct=TRUE, the function will fail (for as yet underdetermined reasons. 
A no longer used option, kept to stop other packages from breaking.
A vector of length of the number of observations that specifies the weights to apply to each case.  The NULL case is equivalent of weights of 1 for all cases.  
 short=TRUE, just show the correlations, short=FALSE give the full hetcor output from John Fox's hetcor function if installed and if doing polychoric  Deprecated
std.err=FALSE does not report the standard errors (faster)  deprecated
Show the progress bar (if  not doing multicores)
 ML=FALSE  do a quick two step procedure, ML=TRUE, do longer maximum likelihood — very slow! Deprecated
Should missing data be deleted
Cases with no variance are deleted with a warning before proceeding.
The maximum number of categories to bother with for polychoric.  
The polytomous input to polydi
The dichotomous input to polydi
The tau values for the polytomous variables – if global=TRUE
The tau values for the dichotomous variables – if globabl = TRUE
The input may be in one of four forms:a) a data frame or matrix of dichotmous data (e.g., the lsat6 from the bock data set) or discrete numerical (i.e., not too many levels, e.g., the big 5 data set, bfi) for polychoric, or continuous for the case of biserial and polyserial. b) a 2 x 2 table of cell counts or cell frequencies (for tetrachoric) or an n x m table of cell counts  (for both tetrachoric and polychoric). c) a vector with elements corresponding to the four cell frequencies (for tetrachoric)d) a vector with elements of the two marginal frequencies (row and column) and the comorbidity (for tetrachoric)
A (matrix or dataframe) of discrete scores. In the case of tetrachoric, these should be dichotomous, for polychoric not too many levels, for biserial they should be discrete (e.g., item responses) with not too many (<10?) categories.
Correction value to use to correct for continuity in the case of zero entry cell for tetrachoric, polychoric, polybi, and mixed.cor.  See the examples for the effect of correcting versus not correcting for continuity.
if TRUE and if the tetrachoric/polychoric matrix is not positive definite, then apply a simple smoothing algorithm using cor.smooth
When finding pairwise correlations, should we use the global values of the tau parameter (which is somewhat faster), or the local values (global=FALSE)?  The local option is equivalent to the polycor solution, or to doing one correlation at a time. global=TRUE borrows information for one item pair from the other pairs using those item's frequencies.   This will make a difference in the presence of lots of missing data. With very small sample sizes with global=FALSE and correct=TRUE, the function will fail (for as yet underdetermined reasons. 
A no longer used option, kept to stop other packages from breaking.
A vector of length of the number of observations that specifies the weights to apply to each case.  The NULL case is equivalent of weights of 1 for all cases.  
 short=TRUE, just show the correlations, short=FALSE give the full hetcor output from John Fox's hetcor function if installed and if doing polychoric  Deprecated
std.err=FALSE does not report the standard errors (faster)  deprecated
Show the progress bar (if  not doing multicores)
 ML=FALSE  do a quick two step procedure, ML=TRUE, do longer maximum likelihood — very slow! Deprecated
Should missing data be deleted
Cases with no variance are deleted with a warning before proceeding.
The maximum number of categories to bother with for polychoric.  
The polytomous input to polydi
The dichotomous input to polydi
The tau values for the polytomous variables – if global=TRUE
The tau values for the dichotomous variables – if globabl = TRUE
The input may be in one of four forms:a) a data frame or matrix of dichotmous data (e.g., the lsat6 from the bock data set) or discrete numerical (i.e., not too many levels, e.g., the big 5 data set, bfi) for polychoric, or continuous for the case of biserial and polyserial. b) a 2 x 2 table of cell counts or cell frequencies (for tetrachoric) or an n x m table of cell counts  (for both tetrachoric and polychoric). c) a vector with elements corresponding to the four cell frequencies (for tetrachoric)d) a vector with elements of the two marginal frequencies (row and column) and the comorbidity (for tetrachoric)
A (matrix or dataframe) of discrete scores. In the case of tetrachoric, these should be dichotomous, for polychoric not too many levels, for biserial they should be discrete (e.g., item responses) with not too many (<10?) categories.
Correction value to use to correct for continuity in the case of zero entry cell for tetrachoric, polychoric, polybi, and mixed.cor.  See the examples for the effect of correcting versus not correcting for continuity.
if TRUE and if the tetrachoric/polychoric matrix is not positive definite, then apply a simple smoothing algorithm using cor.smooth
When finding pairwise correlations, should we use the global values of the tau parameter (which is somewhat faster), or the local values (global=FALSE)?  The local option is equivalent to the polycor solution, or to doing one correlation at a time. global=TRUE borrows information for one item pair from the other pairs using those item's frequencies.   This will make a difference in the presence of lots of missing data. With very small sample sizes with global=FALSE and correct=TRUE, the function will fail (for as yet underdetermined reasons. 
A no longer used option, kept to stop other packages from breaking.
A vector of length of the number of observations that specifies the weights to apply to each case.  The NULL case is equivalent of weights of 1 for all cases.  
 short=TRUE, just show the correlations, short=FALSE give the full hetcor output from John Fox's hetcor function if installed and if doing polychoric  Deprecated
std.err=FALSE does not report the standard errors (faster)  deprecated
Show the progress bar (if  not doing multicores)
 ML=FALSE  do a quick two step procedure, ML=TRUE, do longer maximum likelihood — very slow! Deprecated
Should missing data be deleted
Cases with no variance are deleted with a warning before proceeding.
The maximum number of categories to bother with for polychoric.  
The polytomous input to polydi
The dichotomous input to polydi
The tau values for the polytomous variables – if global=TRUE
The tau values for the dichotomous variables – if globabl = TRUE
The input may be in one of four forms:a) a data frame or matrix of dichotmous data (e.g., the lsat6 from the bock data set) or discrete numerical (i.e., not too many levels, e.g., the big 5 data set, bfi) for polychoric, or continuous for the case of biserial and polyserial. b) a 2 x 2 table of cell counts or cell frequencies (for tetrachoric) or an n x m table of cell counts  (for both tetrachoric and polychoric). c) a vector with elements corresponding to the four cell frequencies (for tetrachoric)d) a vector with elements of the two marginal frequencies (row and column) and the comorbidity (for tetrachoric)
A (matrix or dataframe) of discrete scores. In the case of tetrachoric, these should be dichotomous, for polychoric not too many levels, for biserial they should be discrete (e.g., item responses) with not too many (<10?) categories.
Correction value to use to correct for continuity in the case of zero entry cell for tetrachoric, polychoric, polybi, and mixed.cor.  See the examples for the effect of correcting versus not correcting for continuity.
if TRUE and if the tetrachoric/polychoric matrix is not positive definite, then apply a simple smoothing algorithm using cor.smooth
When finding pairwise correlations, should we use the global values of the tau parameter (which is somewhat faster), or the local values (global=FALSE)?  The local option is equivalent to the polycor solution, or to doing one correlation at a time. global=TRUE borrows information for one item pair from the other pairs using those item's frequencies.   This will make a difference in the presence of lots of missing data. With very small sample sizes with global=FALSE and correct=TRUE, the function will fail (for as yet underdetermined reasons. 
A no longer used option, kept to stop other packages from breaking.
A vector of length of the number of observations that specifies the weights to apply to each case.  The NULL case is equivalent of weights of 1 for all cases.  
 short=TRUE, just show the correlations, short=FALSE give the full hetcor output from John Fox's hetcor function if installed and if doing polychoric  Deprecated
std.err=FALSE does not report the standard errors (faster)  deprecated
Show the progress bar (if  not doing multicores)
 ML=FALSE  do a quick two step procedure, ML=TRUE, do longer maximum likelihood — very slow! Deprecated
Should missing data be deleted
Cases with no variance are deleted with a warning before proceeding.
The maximum number of categories to bother with for polychoric.  
The polytomous input to polydi
The dichotomous input to polydi
The tau values for the polytomous variables – if global=TRUE
The tau values for the dichotomous variables – if globabl = TRUE
the result of a factor analysis, principal components analysis (pca) or bestScales of data set A
Data set B, of the same number of variables as data set A.
if specified, the data set B will be standardized in terms of values from the old data.  This is probably the preferred option. This is done automatically if object is from bestScales 
scoring options for bestScales objects ("best.keys","weights","optimal.keys","optimal.weights")
If missing=FALSE, cases with missing data are given NA scores, otherwise they are given the values based upon the wts x complete data 
Should missing cases be replaced by "means", "medians" or treated as missing ("none" is the default
More options to pass to predictions 
A data set
Variables to predict from the scales
A  keys.list that defines the scales
If not specified, these will be found.  Otherwise, this is the output from reliability.
If not specified, the average item validities for each scale will be found. Otherwise use the output from item.validity
a correlation matrix.  If a raw data matrix is used, the correlations will be found using pairwise deletions for missing values.
Number of components to extract 
 FALSE, do not show residuals, TRUE, report residuals 
"none", "varimax", "quartimax", "promax", "oblimin", "simplimax", and "cluster" are possible rotations/transformations of the solution. See fa for all rotations avaiable.
Number of observations used to find the correlation matrix if using a correlation matrix.  Used for finding the goodness of fit statistics.
If false, find the correlation matrix from the raw data or convert to a correlation matrix if given a square matrix as input.
If TRUE, find component scores
if scores are TRUE, and missing=TRUE, then impute missing values using either the median or the mean
"median" or "mean" values are used to replace missing values
If TRUE (default), then the component scores are based upon the structure matrix.  If FALSE, upon the pattern matrix.
Which way of finding component scores should be used. The default is "regression"
If not NULL, a vector of length n.obs that contains weights for each observation. The NULL case is equivalent to all cases being weighted 1.
How to treat missing data, use="pairwise" is the default".  See cor for other options.
How to find the correlations: "cor" is Pearson", "cov" is covariance, "tet" is tetrachoric, "poly" is polychoric, "mixed" uses mixedCor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate
When doing tetrachoric, polycoric, or mixed cor, how should we treat empty cells.  (See the discussion in the help for tetrachoric.)
other parameters to pass to functions such as factor.scores or the various rotation functions.  
 Output from a psych function (e.g., factor.pa, omega,ICLUST, score.items, cluster.cor
Output from a psych function
items=TRUE (default) does not print the item whole correlations
Number of digits to use in printing
if all=TRUE, then the object is declassed and all output from the function is printed
Cluster loadings < cut will not be printed.  For the factor analysis functions (fa and factor.pa etc.), cut defaults to 0, for ICLUST  to .3, for omega to .2.
Cluster loadings are in sorted order
Controls how much to print
For square matrices, just print the lower half of the matrix
If not NULL,  a numeric value, show just signif number of leading digits for describe output
More options to pass to summary and print
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
 A matrix or data frame or free text
The number of lines at the beginning to show
The number of lines at the end to show
Round off the data to digits
Separate the head and tail with dots (ellipsis)
The first column to show (defaults to 1)
The last column to show (defaults to the number of columns
The number of lines at the beginning to show (an alias for top)
The number of lines at the end to show (an alias for bottom)
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
Sample size of first group 
Correlation to be tested
Test if this correlation is different from r12, if r23 is specified, but r13 is not, then r34 becomes r13  
if ra = r(12) and rb = r(13)  then test for differences of dependent correlations given r23
implies  ra =r(12) and rb =r(34)  test for difference of dependent correlations 
implies   ra =r(12) and rb =r(34) 
 ra =r(12) and rb =r(34)
n2 is specified in the case of two independent correlations. n2 defaults to n if if not specified 
use pooled estimates of correlations
 should a twotailed or one tailed test be used 
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
A data frame or matrix  (can be specified in formula mode) 
Some dichotomous grouping variable (may be specified using formula input (see example))
Apply cohen.d for each of the subgroups defined by group2 (may be specified by formula as well)
If using formula mode and specifying a particular variable (see example)
An effect size
The amount of trimming used in finding the means and sds in d.robust
Total sample size (of groups 1 and 2)
Sample size of group 1 (if only one group)
Sample size of group 2 
Pool the two variances
Student's t statistic
1-alpha is the width of the confidence interval
Find the correlation rather covariance matrix
A correlation to be converted to an effect size
Mean of group 1
Mean of group 2
Standard deviation of group 1
Standard deviation of group 2
Should we sort (and if so, in which direction), the results of cohen.d? Directions are "decreasing" or  "increasing".
What are the items being described?
Find Mahalanobis distance in cohen.d.
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
The y variables to plot.  Each y is plotted against all the x variables
The x variables defining each line.  Each y is plotted against all the x variables
A correlation matrix from which the x and y variables are selected
Labels (assumed to be colnames of the data matrix) for each x variable
If TRUE, then rescale the data to have mean 0 and sd = 1. This is used if plotting raw data rather than correlations.
if TRUE, then lines originate at the center of the plot, otherwise they start at the mid point.
if TRUE, a spider plot is drawn, if FALSE, just a radar plot
can be used to magnify the plot, to make small values appear larger.
if ncolors > 2, then positive correlations are plotted with shades of blue and negative correlations shades of red.  This is particularly useful if fill is TRUE.  ncolors should be an odd number, so that neutral values are coded as white.  
if TRUE, fill the polygons with colors scaled to size of correlation
If TRUE, plot multiple spiders on one plot, otherwise plot them as separate plots
If TRUE, add a new spider diagram to the previous one.
see lty in the par options
A label or set of labels for the plots
If a keys list is provided, then variables are grouped by the keys, with labels drawn for the key names
Rotate the entire figure angle/nvar to the left.  Useful for drawing circumplex structures
If TRUE, then just use color to show correlation size
If TRUE, show the values at the end of the radar lines if they are > cut
round the values to digits
Just show values > cut
Draw circles at .25, .5 and .75
If TRUE, do not draw circles, but rather polygons with nvar sides
If TRUE, organize the variables clockwise
How far from the ends of the lines should the values be placed (defaults to 1.05 * length of line). May be vector.
How far out should the labels be placed?  (defaults to 1.05 which is just outside of the outer circle.)
A way of passing the pos parameter that includes NULL as a value.  (See pos in graphics help) 
default values may be changed for more space for labels
default values by be changed for more space for labelssap
Additional parameters can be passed to the underlying graphics call
The observed correlation
The unrestricted standard deviation)
The restricted standard deviation
Unrestricted standard deviation for case 4
Restricted standard deviation for case 4
Which of the four Thurstone/Stauffer cases to use
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
A list of items to be scored (may be taken from a keys.list for scoreItems. This list may contain one or more keys. ) If keys are not specified, then all the items are used.
The matrix or data.frame of items to be scored.  Can be substantially greater than the items included in keys. For just those items in each key are scored.
Omega is not well defined for two factors, but for small sets of items, two is the better choice. For larger number of items per scale, 3 is probably preferable.
By default, find splitHalf reliabilities as well as the omega statistics. When plotting, split implies that raw was called in reliability.
By default, suppress the omega plots for each scale.
If TRUE, return a list of all the possible splits (up to n.samples).  Useful for graphic display. 
If TRUE then  split and raw are forced to TRUE and the histograms of the split half values are drawn.  (Otherwise, just return the values for later plotting)
Normally defaults to 10,000.  This means that for up to 16 item tests, all possible splits are found.  choose(n,n/2)/2 explodes above that, eg. for all splits of the epi E scale requires 1,352,078 splits or 23.4 seconds on a MacBook Pro with a 2.4GHZ 8 core Intel Core I9.  Can be done, but do you want to do so?  
The object returned from reliability
Add in the values of omega_h and omega_t
Show the unidimensionality value from unidim.  
Add the value of alpha
Allows us to merge this figure with other ones
Defaults to "Split half distributions + omega, alpha"
The xlim of the plot
Other graphical parameters
A matrix or data frame 
Desired mean of the rescaled scores- may be a vector
Desired standard deviation of the rescaled scores
if TRUE, returns a data frame, otherwise a matrix
The object returned by a psych function.
if FALSE, then convert the diagonal of the residuals to NA
Other parameters to be passed to residual (ignored but required by the generic function)
The object returned by a psych function.
if FALSE, then convert the diagonal of the residuals to NA
Other parameters to be passed to residual (ignored but required by the generic function)
A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using make.keys. Just using a list of scoring keys (see example) is probably more convenient.
 Matrix or dataframe of raw item scores
 if TRUE  find total scores, if FALSE (default), find average scores 
 a vector of item labels. 
missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
if delete=TRUE, automatically delete items with no variance (and issue a warning)
May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.
May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. 
If specified, the set of possible unique response categories
 Number of digits to report for mean scores 
If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.
By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.
If TRUE, report the number of items/scale answered for each subject.
A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using make.keys. Just using a list of scoring keys (see example) is probably more convenient.
 Matrix or dataframe of raw item scores
 if TRUE  find total scores, if FALSE (default), find average scores 
 a vector of item labels. 
missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
if delete=TRUE, automatically delete items with no variance (and issue a warning)
May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.
May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. 
If specified, the set of possible unique response categories
 Number of digits to report for mean scores 
If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.
By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.
If TRUE, report the number of items/scale answered for each subject.
A vector of 1s and -1s.  -1 implies reverse the item
A data set of items
if NULL, the empirical minimum for each item.  Otherwise, a vector of minima
f NULL, the empirical maximum for each item.  Otherwise, a vector of maxima
a vector, data.frame or matrix
the lag to use when finding diff     
A column of the x data.frame to be used for grouping
Should missing data be removed?
How to handle missing data in autoR
A vector of scale values 
 A matrix or dataframe of choice frequencies 
 "choice", "logistic", "normal" 
 Precision of answer 
Are the choices ordered by column over row (TRUE) or row over column False)
The X vector, or the first column of a  data.frame or matrix. Can be specified using formula input.  
The Y vector, of if X is a data.frame or matrix, the second column of X
if TRUE, then add a loess smooth to the plot
if TRUE, then show the best fitting linear fit
TRUE: Show the correlation
if using formula input, the data must be specified
TRUE: Show the estimated densities
TRUE
TRUE: draw 1 and 2 sigma ellipses and smooth
How many digits to use if showing the correlation
Which method to use for correlation ("pearson","spearman","kendall") defaults to "pearson"
if TRUE, use smoothScatter instead of plot. Nice for large N.
If using smoothScatter, show nrpoints as dots. Defaults to 0
If TRUE, show a grid for the scatter plot.
Adjustment for the size of the correlation
Adjustment for the size of the data points
Label for the x axis
Label for the y axis
Allow specification for limits of x axis, although this seems to just work for the scatter plots.
Allow specification for limits of y axis
Number of breaks to suggest to the x axis histogram.
Number of breaks to suggest to the y axis histogram.
space between bars
Space between y bars
Show frequency counts, otherwise show density counts
Show the x axis for the x histogram
Show the y axis for the y histogram
The sizes of the ellipses (in sd units).  Defaults to 1,2
Colors to use when showing groups
Amount of transparency in the density plots
Where to put a legend  c("topleft","topright","top","left","right")
Base plot character (each group is one more)
Not currently available
Label for y axis histogram.  Not currently available.
An optional title
If TRUE, show the distances between the groups
If TRUE, draw an arrow between the two centroids
optional lable for the arrow connecting the two groups for the x axis
optional lable for the arrow connecting the two groups for the y axis
cex control for the label size of the arrows.
Other parameters for graphics
The X vector, or the first column of a  data.frame or matrix. Can be specified using formula input.  
The Y vector, of if X is a data.frame or matrix, the second column of X
if TRUE, then add a loess smooth to the plot
if TRUE, then show the best fitting linear fit
TRUE: Show the correlation
if using formula input, the data must be specified
TRUE: Show the estimated densities
TRUE
TRUE: draw 1 and 2 sigma ellipses and smooth
How many digits to use if showing the correlation
Which method to use for correlation ("pearson","spearman","kendall") defaults to "pearson"
if TRUE, use smoothScatter instead of plot. Nice for large N.
If using smoothScatter, show nrpoints as dots. Defaults to 0
If TRUE, show a grid for the scatter plot.
Adjustment for the size of the correlation
Adjustment for the size of the data points
Label for the x axis
Label for the y axis
Allow specification for limits of x axis, although this seems to just work for the scatter plots.
Allow specification for limits of y axis
Number of breaks to suggest to the x axis histogram.
Number of breaks to suggest to the y axis histogram.
space between bars
Space between y bars
Show frequency counts, otherwise show density counts
Show the x axis for the x histogram
Show the y axis for the y histogram
The sizes of the ellipses (in sd units).  Defaults to 1,2
Colors to use when showing groups
Amount of transparency in the density plots
Where to put a legend  c("topleft","topright","top","left","right")
Base plot character (each group is one more)
Not currently available
Label for y axis histogram.  Not currently available.
An optional title
If TRUE, show the distances between the groups
If TRUE, draw an arrow between the two centroids
optional lable for the arrow connecting the two groups for the x axis
optional lable for the arrow connecting the two groups for the y axis
cex control for the label size of the arrows.
Other parameters for graphics
 A correlation matrix 
 Number of factors to extract 
the default is to do minres. fm="pa" for principal axes, fm="pc" for principal components, fm = "minres" for minimum residual (OLS), pc="ml" for maximum likelihood 
if digits not equal NULL, rounds to digits
The default, oblimin, produces somewhat more correlated factors than the alternative, simplimax. Other options include Promax (not Kaiser normalized) or promax (Promax with Kaiser normalization).  See fa for possible oblique rotations.
Number of observations, used to find fit statistics if specified.  Will be calculated if input is raw data
When asking for just two group factors, option can be for "equal", "first" or "second"
If Phi is specified, then the analysis is done on a pattern matrix with the associated factor intercorrelation (Phi) matrix. This allows for reanalysess of published results
Defaults to FALSE and finds correlations.  If set to TRUE, then do the calculations on the unstandardized variables.
If TRUE, do not give a warning about three factors being required.
Allows additional parameters to be passed to the factoring routines
 A matrix or dataframe of -1, 0, or 1 weights for each item on each scale 
Data frame or matrix of raw item scores 
column names for the resulting scales
Find sum scores (default) or average score
Number of digits for answer (default =2) 
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using make.keys. Just using a list of scoring keys (see example) is probably more convenient.
 Matrix or dataframe of raw item scores
 if TRUE  find total scores, if FALSE (default), find average scores 
 a vector of item labels. 
missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
if delete=TRUE, automatically delete items with no variance (and issue a warning)
May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.
May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. 
If specified, the set of possible unique response categories
 Number of digits to report for mean scores 
If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.
By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.
If TRUE, report the number of items/scale answered for each subject.
 A vector of the correct item alternatives
a matrix or data frame of items to be scored.
score=FALSE, just convert to right (1) or wrong (0).score=TRUE, find the totals or average scores and do item analysis
total=FALSE: find the average number correct total=TRUE: find the total number correct
item labels 
missing=TRUE: missing values are replaced with means or medians    missing=FALSE  missing values are not scored 
impute="median", replace missing items with the median score impute="mean": replace missing values with the item mean
 How many digits of output 
short=TRUE, just report the item statistics, short=FALSE, report item statistics and subject scores as well
Should the skews and kurtosi of the raw data be reported? Defaults to FALSE because what is the meaning of skew for a multiple choice item?
A list of scale/cluster keys, or a  matrix of cluster keys 
A correlation matrix 
Either a correlation matrix or a raw data matrix
The output from statsBy.  Needed for scoreBy.
 TRUE shows both raw and corrected for attenuation correlations
Should squared multiple correlations be used as communality estimates for the correlation matrix? 
the smcs of the items may be passed into the function for speed, or calculated if SMC=TRUE 
if TRUE, impute missing scale correlations based upon the average interitem correlation, otherwise return NA.
Should the average r be used in correcting for overlap? smcs otherwise.
By default, just find statistics for items included in the scoring keys. This allows for finding scores from matrices with bad items if they are not included in the set of scoring keys.
The minimum number of pairwise observations needed to define a correlation (in scoreBy)
If the matrices used in scoreBy are not positive semi-definite, should we smooth them?
A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using make.keys. Just using a list of scoring keys (see example) is probably more convenient.
 Matrix or dataframe of raw item scores
 if TRUE  find total scores, if FALSE (default), find average scores 
 a vector of item labels. 
missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
if delete=TRUE, automatically delete items with no variance (and issue a warning)
May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.
May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. 
If specified, the set of possible unique response categories
 Number of digits to report for mean scores 
If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.
By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.
If TRUE, report the number of items/scale answered for each subject.
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
Output from irt.fa is used for parameter estimates of location and discrimination. Stats may also be the  output from a normal factor analysis (fa). If stats is a data.frame of discrimination and thresholds from some other data set, these values will be used.  See the last example. 
The raw data, may be either dichotomous or polytomous.
a list of items to be factored and scored for each scale, can be a keys.list as used in scoreItems or scoreIrt.1pl
A list of items to be scored with keying direction  (see example)
A keys matrix of which items should be scored for each factor
Only items with discrimination values > cut will be used for scoring.
The raw data to be used to find the tau parameter in irt.tau
The lower and upper estimates for the fitting function
Should a logistic or normal model be used in estimating the scores?
What value should be used for continuity correction when finding thetetrachoric or polychoric correlations when using irt.fa
Should messages be suppressed when running multiple scales?
A single score or a vector of scores to find standard errors
The scaling function for the test information statistic used in irt.se
The difficulties for each item in a polytomous scoring
The item discrimin
A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using make.keys. Just using a list of scoring keys (see example) is probably more convenient.
 Matrix or dataframe of raw item scores
 if TRUE  find total scores, if FALSE (default), find average scores 
 a vector of item labels. 
missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
if delete=TRUE, automatically delete items with no variance (and issue a warning)
May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.
May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. 
If specified, the set of possible unique response categories
 Number of digits to report for mean scores 
If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.
By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.
If TRUE, report the number of items/scale answered for each subject.
A list of scale/cluster keys, or a  matrix of cluster keys 
A correlation matrix 
Either a correlation matrix or a raw data matrix
The output from statsBy.  Needed for scoreBy.
 TRUE shows both raw and corrected for attenuation correlations
Should squared multiple correlations be used as communality estimates for the correlation matrix? 
the smcs of the items may be passed into the function for speed, or calculated if SMC=TRUE 
if TRUE, impute missing scale correlations based upon the average interitem correlation, otherwise return NA.
Should the average r be used in correcting for overlap? smcs otherwise.
By default, just find statistics for items included in the scoring keys. This allows for finding scores from matrices with bad items if they are not included in the set of scoring keys.
The minimum number of pairwise observations needed to define a correlation (in scoreBy)
If the matrices used in scoreBy are not positive semi-definite, should we smooth them?
A list of scoring keys or a matrix or dataframe of -1, 0, or 1 weights for each item on each scale which may be created by hand, or by using make.keys. Just using a list of scoring keys (see example) is probably more convenient.
 Matrix or dataframe of raw item scores
 if TRUE  find total scores, if FALSE (default), find average scores 
 a vector of item labels. 
missing = TRUE is the normal case and data are imputed according to the impute option.  missing=FALSE, only complete cases are scored.
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
if delete=TRUE, automatically delete items with no variance (and issue a warning)
May be specified as minimum item score allowed, else will be calculated from data.  min and max should be specified if items differ in their possible minima or maxima.  See notes for details.
May be specified as maximum item score allowed, else will be calculated from data.  Alternatively, in response frequencies, it is maximum number  of alternative responses to count. 
If specified, the set of possible unique response categories
 Number of digits to report for mean scores 
If scoring from a correlation matrix, specify the number of subjects allows for the calculation of the confidence intervals for alpha.
By default, just find the statistics of those items that are included in scoring keys.  This allows scoring of data sets that have bad data for some items that are not included in the scoring keys. This also speeds up the scoring of small subsets of item from larger data sets.
If TRUE, report the number of items/scale answered for each subject.
This is just a matrix of weights to use for each item for each scale.
 Matrix or dataframe of raw item scores
if TRUE, then find weighted standard scores else just use raw data
By default, find the average item score. If sums = TRUE, then find the sum scores.  This is useful for regression with an intercept term
impute="median" replaces missing values with the item medians, impute = "mean" replaces values with the mean response. impute="none" the subject's scores are based upon the average of the keyed, but non missing scores. impute = "none" is probably more appropriate for a large number of missing cases (e.g., SAPA data).  
 a correlation matrix or a data matrix. If data, then correlations are found using pairwise deletions. 
If true, draw the scree for factors 
If true, draw the scree for components
if null, draw a horizontal line at 1, otherwise draw it at hline (make negative to not draw it)
 Title 
Should multiple plots be drawn?
a data frame or matrix
The variables to examine.  (Can be by name or by column number)
a vector of minimum values that are acceptable
a vector of maximum values that are acceptable
a vector of values to be converted to newvalue (one per variable)
a vector of values to replace those that match isvalue
The lower,and upper boundaries for recoding 
a vector, data.frame, or matrix 
na.rm is assumed to be TRUE
Number of variables items to be scored, or the name of the data.frame/matrix to be scored
 A list of the scoring keys, one element for each scale
 Typically, just the colnames of the items data matrix. 
 Labels for the scales can be specified here, or in the key.list 
A keys matrix returned from make.keys
if TRUE, prefix negatively keyed items with - (e.g., “-E2")
a factor model on the x variables. 
A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. 
a factor model on the y variables (can be empty) 
The correlation matrix among the x variables
The correlation matrix among the y variables
name a file to send dot language instructions. 
variable labels if not specified as colnames for the matrices
Draw paths for values > cut 
The output from a lavaan cfa or sem
draw an error term for observerd variables 
Just draw one path per x or y variable 
Draw a regression diagram (observed variables cause Y)
Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) 
size of the ellipses in structure.diagram
main title of diagram
page size of graphic 
 font type for graph 
font type for graph 
 Which direction should the graph be oriented 
Number of digits to draw
 Title of graphic 
 other options to pass to Rgraphviz 
a factor model on the x variables. 
A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. 
a factor model on the y variables (can be empty) 
The correlation matrix among the x variables
The correlation matrix among the y variables
name a file to send dot language instructions. 
variable labels if not specified as colnames for the matrices
Draw paths for values > cut 
The output from a lavaan cfa or sem
draw an error term for observerd variables 
Just draw one path per x or y variable 
Draw a regression diagram (observed variables cause Y)
Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) 
size of the ellipses in structure.diagram
main title of diagram
page size of graphic 
 font type for graph 
font type for graph 
 Which direction should the graph be oriented 
Number of digits to draw
 Title of graphic 
 other options to pass to Rgraphviz 
Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c("Flags","Addition"). See notes and examples for each.
 either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c("Cubes","PaperFormBoard").  x and y may also be set by use of the formula style of lm.
A matrix or data.frame of correlations or, if not square, of raw data
A variance/covariance matrix, or a correlation matrix
The column name or numbers of the set of mediating variables (see mediate).
the column names or numbers of the set of covariates. 
If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.
find the correlations using "pairwise" (default) or just use "complete" cases (to match the lm function)
Report standardized betas (based upon the correlations) or raw bs (based upon covariances)
z is specified should part (TRUE) or partial (default) correlations be found.
Are data from a correlation matrix or data matrix?
A vector of means for the data in matReg if giving matrix input
The title for setCor.diagram
if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.
The output of setCor may be used for drawing diagrams
How many digits should be displayed in the setCor.diagram?
Show the unweighted matrix correlation between the x and y sets?
zero center the x variables before finding the interaction terms.
p value of the confidence intervals for the beta coefficients
By default, setCor makes a plot of the results, set to FALSE to suppress the plot
Text size of boxes displaying the variables in the diagram
Text size of numbers in arrows, defaults to cex
Additional graphical parameters for setCor.diagram
The resulting object from setCor or bestScales
If using a bestScales object, which set of keys to use ("best.key","weights","optimal.key","optimal.weights")
Not yet implemented option to crossValidation
type="b" draws both lines and points
Orientation of the x labels (3 to make vertical)
When abbreviating x labels how many characters as a minimum
If not NULL, the draw a legend at the appropriate location
What plot character(s) to use in matPlot
colors to use in matPlot
line types to use in matPlot
What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)
Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c("Flags","Addition"). See notes and examples for each.
 either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c("Cubes","PaperFormBoard").  x and y may also be set by use of the formula style of lm.
A matrix or data.frame of correlations or, if not square, of raw data
A variance/covariance matrix, or a correlation matrix
The column name or numbers of the set of mediating variables (see mediate).
the column names or numbers of the set of covariates. 
If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.
find the correlations using "pairwise" (default) or just use "complete" cases (to match the lm function)
Report standardized betas (based upon the correlations) or raw bs (based upon covariances)
z is specified should part (TRUE) or partial (default) correlations be found.
Are data from a correlation matrix or data matrix?
A vector of means for the data in matReg if giving matrix input
The title for setCor.diagram
if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.
The output of setCor may be used for drawing diagrams
How many digits should be displayed in the setCor.diagram?
Show the unweighted matrix correlation between the x and y sets?
zero center the x variables before finding the interaction terms.
p value of the confidence intervals for the beta coefficients
By default, setCor makes a plot of the results, set to FALSE to suppress the plot
Text size of boxes displaying the variables in the diagram
Text size of numbers in arrows, defaults to cex
Additional graphical parameters for setCor.diagram
The resulting object from setCor or bestScales
If using a bestScales object, which set of keys to use ("best.key","weights","optimal.key","optimal.weights")
Not yet implemented option to crossValidation
type="b" draws both lines and points
Orientation of the x labels (3 to make vertical)
When abbreviating x labels how many characters as a minimum
If not NULL, the draw a legend at the appropriate location
What plot character(s) to use in matPlot
colors to use in matPlot
line types to use in matPlot
What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)
Three options: 'formula' form (similar to lm) or either  the column numbers of the y set  (e.g., c(2,4,6) or the column names of the y set (e.g., c("Flags","Addition"). See notes and examples for each.
 either the column numbers of the x set (e.g., c(1,3,5) or the column names of the x set (e.g. c("Cubes","PaperFormBoard").  x and y may also be set by use of the formula style of lm.
A matrix or data.frame of correlations or, if not square, of raw data
A variance/covariance matrix, or a correlation matrix
The column name or numbers of the set of mediating variables (see mediate).
the column names or numbers of the set of covariates. 
If specified, then confidence intervals, etc. are calculated, not needed if raw data are given.
find the correlations using "pairwise" (default) or just use "complete" cases (to match the lm function)
Report standardized betas (based upon the correlations) or raw bs (based upon covariances)
z is specified should part (TRUE) or partial (default) correlations be found.
Are data from a correlation matrix or data matrix?
A vector of means for the data in matReg if giving matrix input
The title for setCor.diagram
if FALSE, then square matrices are treated as correlation matrices not as data matrices. In the rare case that one has as many cases as variables, then set square=TRUE.
The output of setCor may be used for drawing diagrams
How many digits should be displayed in the setCor.diagram?
Show the unweighted matrix correlation between the x and y sets?
zero center the x variables before finding the interaction terms.
p value of the confidence intervals for the beta coefficients
By default, setCor makes a plot of the results, set to FALSE to suppress the plot
Text size of boxes displaying the variables in the diagram
Text size of numbers in arrows, defaults to cex
Additional graphical parameters for setCor.diagram
The resulting object from setCor or bestScales
If using a bestScales object, which set of keys to use ("best.key","weights","optimal.key","optimal.weights")
Not yet implemented option to crossValidation
type="b" draws both lines and points
Orientation of the x labels (3 to make vertical)
When abbreviating x labels how many characters as a minimum
If not NULL, the draw a legend at the appropriate location
What plot character(s) to use in matPlot
colors to use in matPlot
line types to use in matPlot
What labels should be supplied to the lines in matPlot.  (Defaults to colnames of the variables.)
A data matrix or data frame depending upon the function.
A data matrix or data frame or a vector
Which variables (by name or location) should be the empirical target for bestScales and bestItems.  May be a separate object. 
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
The word(s) to search for from a dictionary
A list of scoring keys suitable to use for make.keys
Return all values in abs(x[,c1]) > cut.
Return the n best items per factor (as long as they have their highest loading on that factor)
If provided (e.g. from scoreItems) will be added to the lookupFromKeys output
a data.frame with rownames corresponding to rownames in the f$loadings matrix or colnames of the data matrix or correlation matrix, and entries (may be multiple columns) of item content.
Column names of dictionary to search, defaults to "Item" or "Content" (dictionaries have different labels for this column), can search any column specified by search.
A data frame of item means
round to digits
Should the factors be sorted first?
In lookupFromKeys, should we suppress the column labels
Show setCor regressions with probability < p
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
The measurement model for x. If NULL, a 4 factor model is generated
The structure matrix of the latent variables
The measurement model for y
The means structure for the fx factors
 Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.
if raw=TRUE, raw data are returned as well.
Number of variables for a simplex structure
Number of large factors to simulate in sim.minor,number of group factors in sim.general,sim.omega
General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor
the base correlation for an autoregressive simplex
the trait component of a State Trait Autoregressive Simplex
Test reliability of a STARS simplex
Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.
if TRUE, then positive and negative loadings are generated from fbig
nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)
Effect size of IV1
Effect size of IV2
Effect size of IV3
Effect size of the IV1 x IV2 interaction
Effect size of the IV1 x IV3 interaction
Effect size of the IV2 x IV3 interaction
Effect size of the IV1 x IV2 * IV3  interaction
Effect size of the quadratric term of IV1
Effect size of the quadratric term of IV2
Effect size of the quadratric term of IV3
Sample size per cell (if all variables are categorical) or (if at least one variable is continuous), the total sample size
Number of levels of IV1 (0) if continuous
Number of levels of IV2
Number of levels of IV3
if not NULL, then within should be a vector of the means of  any repeated measures.
the correlation between the repeated measures (if they exist).  This can be thought of as the reliablility of the measures.
report the IVs as factors rather than numeric
center=TRUE provides orthogonal contrasts, center=FALSE adds the minimum value + 1 to all contrasts
Standardize the effect sizes by standardizing the IVs
 Loadings of group factors on a general factor 
 Loadings of items on the group factors 
 Number of subjects to generate: N=0 => population values 
 raw=TRUE, report the raw data, raw=FALSE, report the sample  correlation matrix. 
means for the individual variables
lower cutoff for categorical data
If True, then create categorical data
Upper cuttoff for categorical data
Number of variables to simulate
A vector of loadings that will be sampled (rowwise) to define the factors
The factor loadings of ‘pure’ measures of the factor.
 Number of variables to simulate 
Number of subjects to simulate 
 circum=TRUE is circumplex structure, FALSE is simple structure
simple structure or spherical structure in sim.spherical
the average loading on the first dimension 
Average loading on the second dimension 
the average loading on the third dimension in sim.spherical
Average loading on a general factor (default=0)
To introduce skew, how far off center is the first dimension 
To introduce skew on the second dimension
To introduce skew on the third dimension – if using sim.spherical
 continuous or categorical variables.  
 values less than low are forced to low (or 0 in item.dichot)
 values greater than high are forced to high (or 1 in item.dichot) 
Change all values less than cutpoint to cutpoint. 
What is the cutpoint 
number of variables for the first factor in sim.spherical
number of variables for the second and third factors in sim.spherical
a matrix or data frame
Values of old to be used as cut points when converting continuous values to categorical values
Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.
How many subjects to simulate. If NULL, return the population model 
 A vector of factor loadings for the tests  
A vector of error variances – if NULL then error = 1 - loading 2
short=TRUE: Just give the test correlations, short=FALSE, report observed test scores as well as the implied pattern matrix
 continuous or categorical (discrete) variables.  
 values less than low are forced to low 
 values greater than high are forced to high  
If specified, and categorical = TRUE, will cut the resulting continuous output at the value of cuts
The measurement model for x
The structure matrix of the latent variables
The measurement model for y
 The measurement model
 Number of cases to simulate.  If n=0, the population matrix is returned.
The uniquenesses if creating a covariance matrix
if raw=TRUE, raw data are returned as well for n > 0.
TRUE if simulating items, FALSE if simulating scales
Restrict the item difficulties to range from low to high
Restrict the item difficulties to range from low to high
A vector of item difficulties, if NULL will range uniformly from low to high
Number of categories when creating binary (2) or polytomous items
A vector of means, defaults to 0
The correlation matrix to reproduce
if TRUE, return the raw data, otherwise return the sample correlation matrix.
standardize the simulated data?
Defaults to none (the multivariate normal case.  Alternatives take the log, the squareroot, or the absolute value of latent or observed data )
Apply the skewing or cuts to just these variables.  If NULL, to all the variables/
Should the skewing transforms be applied to the latent variables, or the observed variables?
Either a single number or a vector length nvar.  The data will be dichotomized at quant.
 Number of variables to simulate 
Number of subjects to simulate 
 circum=TRUE is circumplex structure, FALSE is simple structure
simple structure or spherical structure in sim.spherical
the average loading on the first dimension 
Average loading on the second dimension 
the average loading on the third dimension in sim.spherical
Average loading on a general factor (default=0)
To introduce skew, how far off center is the first dimension 
To introduce skew on the second dimension
To introduce skew on the third dimension – if using sim.spherical
 continuous or categorical variables.  
 values less than low are forced to low (or 0 in item.dichot)
 values greater than high are forced to high (or 1 in item.dichot) 
Change all values less than cutpoint to cutpoint. 
What is the cutpoint 
number of variables for the first factor in sim.spherical
number of variables for the second and third factors in sim.spherical
a matrix or data frame
Values of old to be used as cut points when converting continuous values to categorical values
Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.
Number of variables
Number of group factors in sim.general, sim.omega
 Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.
General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor
Should the sim.omega function do both an EFA omega as well as a CFA omega using the sem package?
Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.
nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)
if TRUE, then positive and negative loadings are generated from fbig
Number of  factors to extract in omega
In omega, should item signs be flipped if negative
In omega, for the case of two factors, how to weight them?
Number of replications per level
 Loadings of group factors on a general factor 
 Loadings of items on the group factors 
 Number of subjects to generate: N=0 => population values 
 raw=TRUE, report the raw data, raw=FALSE, report the sample  correlation matrix. 
means for the individual variables
lower cutoff for categorical data
If True, then create categorical data
Upper cuttoff for categorical data
Number of variables to simulate
A vector of loadings that will be sampled (rowwise) to define the factors
The factor loadings of ‘pure’ measures of the factor.
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
 Number of variables to simulate 
Number of subjects to simulate 
 circum=TRUE is circumplex structure, FALSE is simple structure
simple structure or spherical structure in sim.spherical
the average loading on the first dimension 
Average loading on the second dimension 
the average loading on the third dimension in sim.spherical
Average loading on a general factor (default=0)
To introduce skew, how far off center is the first dimension 
To introduce skew on the second dimension
To introduce skew on the third dimension – if using sim.spherical
 continuous or categorical variables.  
 values less than low are forced to low (or 0 in item.dichot)
 values greater than high are forced to high (or 1 in item.dichot) 
Change all values less than cutpoint to cutpoint. 
What is the cutpoint 
number of variables for the first factor in sim.spherical
number of variables for the second and third factors in sim.spherical
a matrix or data frame
Values of old to be used as cut points when converting continuous values to categorical values
Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.
The measurement model for x. If NULL, a 4 factor model is generated
The structure matrix of the latent variables
The measurement model for y
The means structure for the fx factors
 Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.
if raw=TRUE, raw data are returned as well.
Number of variables for a simplex structure
Number of large factors to simulate in sim.minor,number of group factors in sim.general,sim.omega
General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor
the base correlation for an autoregressive simplex
the trait component of a State Trait Autoregressive Simplex
Test reliability of a STARS simplex
Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.
if TRUE, then positive and negative loadings are generated from fbig
nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)
How many subjects should be simulated.  Four allows for nice graphics, use more to examine structural properties.
How many  variables are to be simulated?
How many factors are simulated,defaults to 2
How many observations per subject (across time)
How many days do these observations reflect? This is relevant if we are adding sine and cosines to the model to model diurnal rhythms.
The grand mean for each variable across subjects
The between person standard deviation
if NULL, a two factor model is created with loadings of loading or zero in a simple structure form
If fact is NULL,  then we create a factor model with loading or zeros
The between person factor intercorrelation 
The within person factor intercorrelations
Within subject rate of change over trials
The within subject mean for each subject
the within subject standard deviation
To what extent should we diurnally vary by subject?
This will specify the within subject diurnal phase (lag)
Auto regressive value implies error at time t +1 is partly a function of error at time t.  
Factor loadings for each subject
If TRUE, create a lattice plot for each subject
The number of groups to simulate
The number of simulated cases
The within group correlational structure
The between group correlational structure
The correlation of the data with the within data
How many subjects should be simulated.  Four allows for nice graphics, use more to examine structural properties.
How many  variables are to be simulated?
How many factors are simulated,defaults to 2
How many observations per subject (across time)
How many days do these observations reflect? This is relevant if we are adding sine and cosines to the model to model diurnal rhythms.
The grand mean for each variable across subjects
The between person standard deviation
if NULL, a two factor model is created with loadings of loading or zero in a simple structure form
If fact is NULL,  then we create a factor model with loading or zeros
The between person factor intercorrelation 
The within person factor intercorrelations
Within subject rate of change over trials
The within subject mean for each subject
the within subject standard deviation
To what extent should we diurnally vary by subject?
This will specify the within subject diurnal phase (lag)
Auto regressive value implies error at time t +1 is partly a function of error at time t.  
Factor loadings for each subject
If TRUE, create a lattice plot for each subject
The number of groups to simulate
The number of simulated cases
The within group correlational structure
The between group correlational structure
The correlation of the data with the within data
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of variables
Number of group factors in sim.general, sim.omega
 Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.
General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor
Should the sim.omega function do both an EFA omega as well as a CFA omega using the sem package?
Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.
nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)
if TRUE, then positive and negative loadings are generated from fbig
Number of  factors to extract in omega
In omega, should item signs be flipped if negative
In omega, for the case of two factors, how to weight them?
Number of replications per level
Number of variables
Number of group factors in sim.general, sim.omega
 Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.
General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor
Should the sim.omega function do both an EFA omega as well as a CFA omega using the sem package?
Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.
nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)
if TRUE, then positive and negative loadings are generated from fbig
Number of  factors to extract in omega
In omega, should item signs be flipped if negative
In omega, for the case of two factors, how to weight them?
Number of replications per level
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
Number of cases to simulate
The means for the items (if not 0)
Number of variables for a simplex structure
lower difficulty for sim.rasch or sim.irt
higher difficulty for sim.rasch or sim.irt
if not specified as a vector, the descrimination parameter a = α will be set to 1.0 for all items
 if not specified as a vector, item difficulties (d = δ) will range from low to high
the gamma parameter: if not specified as a vector, the guessing asymptote is set to 0
the zeta parameter: if not specified as a vector, set to 1
the standard deviation for the underlying latent variable in the irt simulations
which IRT model to use, mod="logistic" simulates a logistic function, otherwise, a normal function
Number of categories to simulate in sim.poly.  If cat=2, then this is the same as simulating t/f items and sim.poly is functionally equivalent to sim.irt
The underlying latent trait value for each simulated subject
A correlation matrix to be simulated using the sim.poly.mat function
The matrix of marginals for all the items
The measurement model for x. If NULL, a 4 factor model is generated
The structure matrix of the latent variables
The measurement model for y
The means structure for the fx factors
 Number of cases to simulate.  If n=0 or NULL, the population matrix is returned.
if raw=TRUE, raw data are returned as well.
Number of variables for a simplex structure
Number of large factors to simulate in sim.minor,number of group factors in sim.general,sim.omega
General factor correlations in sim.general and general factor loadings in sim.omega and sim.minor
the base correlation for an autoregressive simplex
the trait component of a State Trait Autoregressive Simplex
Test reliability of a STARS simplex
Factor loadings for the main factors.  Default is a simple structure with loadings sampled from (.8,.6) for nvar/nfact variables and 0 for the remaining.  If fbig is specified, then  each factor has loadings sampled from it.
if TRUE, then positive and negative loadings are generated from fbig
nvar/2 small factors are generated with loadings sampled from (-.2,0,.2)
 Number of variables to simulate 
Number of subjects to simulate 
 circum=TRUE is circumplex structure, FALSE is simple structure
simple structure or spherical structure in sim.spherical
the average loading on the first dimension 
Average loading on the second dimension 
the average loading on the third dimension in sim.spherical
Average loading on a general factor (default=0)
To introduce skew, how far off center is the first dimension 
To introduce skew on the second dimension
To introduce skew on the third dimension – if using sim.spherical
 continuous or categorical variables.  
 values less than low are forced to low (or 0 in item.dichot)
 values greater than high are forced to high (or 1 in item.dichot) 
Change all values less than cutpoint to cutpoint. 
What is the cutpoint 
number of variables for the first factor in sim.spherical
number of variables for the second and third factors in sim.spherical
a matrix or data frame
Values of old to be used as cut points when converting continuous values to categorical values
Which columns of old should be converted to categorical variables.  If missing, then all columns are converted.
The measurement model for x
The structure matrix of the latent variables
The measurement model for y
 The measurement model
 Number of cases to simulate.  If n=0, the population matrix is returned.
The uniquenesses if creating a covariance matrix
if raw=TRUE, raw data are returned as well for n > 0.
TRUE if simulating items, FALSE if simulating scales
Restrict the item difficulties to range from low to high
Restrict the item difficulties to range from low to high
A vector of item difficulties, if NULL will range uniformly from low to high
Number of categories when creating binary (2) or polytomous items
A vector of means, defaults to 0
The correlation matrix to reproduce
if TRUE, return the raw data, otherwise return the sample correlation matrix.
standardize the simulated data?
Defaults to none (the multivariate normal case.  Alternatives take the log, the squareroot, or the absolute value of latent or observed data )
Apply the skewing or cuts to just these variables.  If NULL, to all the variables/
Should the skewing transforms be applied to the latent variables, or the observed variables?
Either a single number or a vector length nvar.  The data will be dichotomized at quant.
The measurement model for x
The structure matrix of the latent variables
The measurement model for y
 The measurement model
 Number of cases to simulate.  If n=0, the population matrix is returned.
The uniquenesses if creating a covariance matrix
if raw=TRUE, raw data are returned as well for n > 0.
TRUE if simulating items, FALSE if simulating scales
Restrict the item difficulties to range from low to high
Restrict the item difficulties to range from low to high
A vector of item difficulties, if NULL will range uniformly from low to high
Number of categories when creating binary (2) or polytomous items
A vector of means, defaults to 0
The correlation matrix to reproduce
if TRUE, return the raw data, otherwise return the sample correlation matrix.
standardize the simulated data?
Defaults to none (the multivariate normal case.  Alternatives take the log, the squareroot, or the absolute value of latent or observed data )
Apply the skewing or cuts to just these variables.  If NULL, to all the variables/
Should the skewing transforms be applied to the latent variables, or the observed variables?
Either a single number or a vector length nvar.  The data will be dichotomized at quant.
 number of simulated subjects 
 Number of variables 
 Number of factors to generate 
with a mean loading 
dichot=FALSE give continuous variables, dichot=TRUE gives dichotomous variables
if dichotomous = TRUE, then items with values > cut are assigned 1, otherwise 0.
The measurement model for x
The structure matrix of the latent variables
The measurement model for y
 The measurement model
 Number of cases to simulate.  If n=0, the population matrix is returned.
The uniquenesses if creating a covariance matrix
if raw=TRUE, raw data are returned as well for n > 0.
TRUE if simulating items, FALSE if simulating scales
Restrict the item difficulties to range from low to high
Restrict the item difficulties to range from low to high
A vector of item difficulties, if NULL will range uniformly from low to high
Number of categories when creating binary (2) or polytomous items
A vector of means, defaults to 0
The correlation matrix to reproduce
if TRUE, return the raw data, otherwise return the sample correlation matrix.
standardize the simulated data?
Defaults to none (the multivariate normal case.  Alternatives take the log, the squareroot, or the absolute value of latent or observed data )
Apply the skewing or cuts to just these variables.  If NULL, to all the variables/
Should the skewing transforms be applied to the latent variables, or the observed variables?
Either a single number or a vector length nvar.  The data will be dichotomized at quant.
a vector of sample sizes to simulate 
vector of the number of variables to simulate 
A data frame resulting from simulation.circ 
 A data.frame or matrix 
 how to treat missing data 
See the discussion in describe
Plot the expected normal distribution values versus the Mahalanobis distance of the subjects.
 A correlation matrix or a dataframe.  In the latter case, correlations are found.
if covar = TRUE and  R is either a covariance matrix or data frame, then return the smc * variance for each item
The y variables to plot.  Each y is plotted against all the x variables
The x variables defining each line.  Each y is plotted against all the x variables
A correlation matrix from which the x and y variables are selected
Labels (assumed to be colnames of the data matrix) for each x variable
If TRUE, then rescale the data to have mean 0 and sd = 1. This is used if plotting raw data rather than correlations.
if TRUE, then lines originate at the center of the plot, otherwise they start at the mid point.
if TRUE, a spider plot is drawn, if FALSE, just a radar plot
can be used to magnify the plot, to make small values appear larger.
if ncolors > 2, then positive correlations are plotted with shades of blue and negative correlations shades of red.  This is particularly useful if fill is TRUE.  ncolors should be an odd number, so that neutral values are coded as white.  
if TRUE, fill the polygons with colors scaled to size of correlation
If TRUE, plot multiple spiders on one plot, otherwise plot them as separate plots
If TRUE, add a new spider diagram to the previous one.
see lty in the par options
A label or set of labels for the plots
If a keys list is provided, then variables are grouped by the keys, with labels drawn for the key names
Rotate the entire figure angle/nvar to the left.  Useful for drawing circumplex structures
If TRUE, then just use color to show correlation size
If TRUE, show the values at the end of the radar lines if they are > cut
round the values to digits
Just show values > cut
Draw circles at .25, .5 and .75
If TRUE, do not draw circles, but rather polygons with nvar sides
If TRUE, organize the variables clockwise
How far from the ends of the lines should the values be placed (defaults to 1.05 * length of line). May be vector.
How far out should the labels be placed?  (defaults to 1.05 which is just outside of the outer circle.)
A way of passing the pos parameter that includes NULL as a value.  (See pos in graphics help) 
default values may be changed for more space for labels
default values by be changed for more space for labelssap
Additional parameters can be passed to the underlying graphics call
A correlation or covariance matrix or raw data matrix.
return a vector of split half reliabilities
Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!
If brute is false, how many samples of split halves should be tried? ()
Should the covariances or correlations be used for reliability calculations
If TRUE, any item with a negative loading on the first factor will be flipped in sign
a vector of -1, 0, 1 to select or reverse key items.  See if the key vector is less than the number of variables, then item numbers to be reverse can be specified.
Should we find the correlations using "pairwise" or "complete" (see ?cor)
The alpha level to use for the confidence intervals of the split half estimates
A matrix or dataframe with rows for subjects, columns for variables.  One of these columns should be the values of a grouping variable.
The names or numbers of the variable in data to use as the grouping variables.
Should the results include the correlation matrix within each group?  Default is FALSE.
Type of correlation/covariance to find within groups and between groups.  The default is Pearson correlation.  To find within and between covariances, set cor="cov".  Although polychoric, tetrachoric, and mixed correlations can be found within groups, this does not make sense for the between groups or the pooled within groups.  In this case, correlations for each group will be as specified, but the between groups and pooled within will be Pearson. See the discussion below.
What kind of correlations should be found (default is Pearson product moment)
How to treat missing data.  use="pairwise" is the default
Find polychoric.tetrachoric correlations within groups if requested.
Should missing values be deleted (na.rm=TRUE) or should we assume the data clean?
The alpha level for the confidence intervals for the ICC1 and ICC2, and rwg, rbg
The minimum length to use when abbreviating the labels for confidence intervals
If specified, weight the groups by weights when finding the pooled within group correlation.  Otherwise weight by sample size.
The number of trials to run when bootstrapping statistics
Should the bootstrap be done by permuting the data (replace=FALSE) or sampling with replacement (replace=TRUE)
The results from statsBy.boot may be summarized using boot.stats
Name of the variable to be summarized from statsBy.boot
The output of statsBy
The number of factors to extract in each subgroup
The factor rotation/transformation
The factor method (see fa for details)
Allow the factor solution to be freely estimated for each individual (see note).
Report individual factor analyses for each group as well as the summary table
The minimum number of within subject cases before we factor analyze it.
Show the upper and lower quant quantile of the factor loadings in faBy
Other parameters to pass to the fa function
A matrix or dataframe with rows for subjects, columns for variables.  One of these columns should be the values of a grouping variable.
The names or numbers of the variable in data to use as the grouping variables.
Should the results include the correlation matrix within each group?  Default is FALSE.
Type of correlation/covariance to find within groups and between groups.  The default is Pearson correlation.  To find within and between covariances, set cor="cov".  Although polychoric, tetrachoric, and mixed correlations can be found within groups, this does not make sense for the between groups or the pooled within groups.  In this case, correlations for each group will be as specified, but the between groups and pooled within will be Pearson. See the discussion below.
What kind of correlations should be found (default is Pearson product moment)
How to treat missing data.  use="pairwise" is the default
Find polychoric.tetrachoric correlations within groups if requested.
Should missing values be deleted (na.rm=TRUE) or should we assume the data clean?
The alpha level for the confidence intervals for the ICC1 and ICC2, and rwg, rbg
The minimum length to use when abbreviating the labels for confidence intervals
If specified, weight the groups by weights when finding the pooled within group correlation.  Otherwise weight by sample size.
The number of trials to run when bootstrapping statistics
Should the bootstrap be done by permuting the data (replace=FALSE) or sampling with replacement (replace=TRUE)
The results from statsBy.boot may be summarized using boot.stats
Name of the variable to be summarized from statsBy.boot
The output of statsBy
The number of factors to extract in each subgroup
The factor rotation/transformation
The factor method (see fa for details)
Allow the factor solution to be freely estimated for each individual (see note).
Report individual factor analyses for each group as well as the summary table
The minimum number of within subject cases before we factor analyze it.
Show the upper and lower quant quantile of the factor loadings in faBy
Other parameters to pass to the fa function
A matrix or dataframe with rows for subjects, columns for variables.  One of these columns should be the values of a grouping variable.
The names or numbers of the variable in data to use as the grouping variables.
Should the results include the correlation matrix within each group?  Default is FALSE.
Type of correlation/covariance to find within groups and between groups.  The default is Pearson correlation.  To find within and between covariances, set cor="cov".  Although polychoric, tetrachoric, and mixed correlations can be found within groups, this does not make sense for the between groups or the pooled within groups.  In this case, correlations for each group will be as specified, but the between groups and pooled within will be Pearson. See the discussion below.
What kind of correlations should be found (default is Pearson product moment)
How to treat missing data.  use="pairwise" is the default
Find polychoric.tetrachoric correlations within groups if requested.
Should missing values be deleted (na.rm=TRUE) or should we assume the data clean?
The alpha level for the confidence intervals for the ICC1 and ICC2, and rwg, rbg
The minimum length to use when abbreviating the labels for confidence intervals
If specified, weight the groups by weights when finding the pooled within group correlation.  Otherwise weight by sample size.
The number of trials to run when bootstrapping statistics
Should the bootstrap be done by permuting the data (replace=FALSE) or sampling with replacement (replace=TRUE)
The results from statsBy.boot may be summarized using boot.stats
Name of the variable to be summarized from statsBy.boot
The output of statsBy
The number of factors to extract in each subgroup
The factor rotation/transformation
The factor method (see fa for details)
Allow the factor solution to be freely estimated for each individual (see note).
Report individual factor analyses for each group as well as the summary table
The minimum number of within subject cases before we factor analyze it.
Show the upper and lower quant quantile of the factor loadings in faBy
Other parameters to pass to the fa function
a factor model on the x variables. 
A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. 
a factor model on the y variables (can be empty) 
The correlation matrix among the x variables
The correlation matrix among the y variables
name a file to send dot language instructions. 
variable labels if not specified as colnames for the matrices
Draw paths for values > cut 
The output from a lavaan cfa or sem
draw an error term for observerd variables 
Just draw one path per x or y variable 
Draw a regression diagram (observed variables cause Y)
Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) 
size of the ellipses in structure.diagram
main title of diagram
page size of graphic 
 font type for graph 
font type for graph 
 Which direction should the graph be oriented 
Number of digits to draw
 Title of graphic 
 other options to pass to Rgraphviz 
a factor model on the x variables. 
A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. 
a factor model on the y variables (can be empty) 
The correlation matrix among the x variables
The correlation matrix among the y variables
name a file to send dot language instructions. 
variable labels if not specified as colnames for the matrices
Draw paths for values > cut 
The output from a lavaan cfa or sem
draw an error term for observerd variables 
Just draw one path per x or y variable 
Draw a regression diagram (observed variables cause Y)
Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) 
size of the ellipses in structure.diagram
main title of diagram
page size of graphic 
 font type for graph 
font type for graph 
 Which direction should the graph be oriented 
Number of digits to draw
 Title of graphic 
 other options to pass to Rgraphviz 
Number of variables in the design matrix 
A list of items included in each factor (for structure.list, or the factors that correlate with the specified factor for phi.list
prefix for parameters – needed in case of creating an X set and a Y set
Names for the factors 
Item labels 
Number of factors in the phi matrix
a factor model on the x variables. 
A matrix of directed relationships.  Lower diagonal values are drawn.  If the upper diagonal values match the lower diagonal, two headed arrows are drawn.  For a single, directed path, just the value may be specified. 
a factor model on the y variables (can be empty) 
The correlation matrix among the x variables
The correlation matrix among the y variables
name a file to send dot language instructions. 
variable labels if not specified as colnames for the matrices
Draw paths for values > cut 
The output from a lavaan cfa or sem
draw an error term for observerd variables 
Just draw one path per x or y variable 
Draw a regression diagram (observed variables cause Y)
Direction of diagram is from left to right (lr=TRUE, default) or from bottom to top (lr=FALSE) 
size of the ellipses in structure.diagram
main title of diagram
page size of graphic 
 font type for graph 
font type for graph 
 Which direction should the graph be oriented 
Number of digits to draw
 Title of graphic 
 other options to pass to Rgraphviz 
 Output from a psych function (e.g., factor.pa, omega,ICLUST, score.items, cluster.cor
Output from a psych function
items=TRUE (default) does not print the item whole correlations
Number of digits to use in printing
if all=TRUE, then the object is declassed and all output from the function is printed
Cluster loadings < cut will not be printed.  For the factor analysis functions (fa and factor.pa etc.), cut defaults to 0, for ICLUST  to .3, for omega to .2.
Cluster loadings are in sorted order
Controls how much to print
For square matrices, just print the lower half of the matrix
If not NULL,  a numeric value, show just signif number of leading digits for describe output
More options to pass to summary and print
A n x m matrix or a list of such matrices, or the output object from link{scoreOverlap} 
A j x k matrix or a list of such matrices
A n x k matrix
A n x m matrix or a list of such matrices, or the output object from link{scoreOverlap} 
A j x k matrix or a list of such matrices
A n x k matrix
A n x m matrix or a list of such matrices, or the output object from link{scoreOverlap} 
A j x k matrix or a list of such matrices
A n x k matrix
A data frame or matrix  (can be specified in formula mode) 
Some dichotomous grouping variable (may be specified using formula input (see example))
Apply cohen.d for each of the subgroups defined by group2 (may be specified by formula as well)
If using formula mode and specifying a particular variable (see example)
An effect size
The amount of trimming used in finding the means and sds in d.robust
Total sample size (of groups 1 and 2)
Sample size of group 1 (if only one group)
Sample size of group 2 
Pool the two variances
Student's t statistic
1-alpha is the width of the confidence interval
Find the correlation rather covariance matrix
A correlation to be converted to an effect size
Mean of group 1
Mean of group 2
Standard deviation of group 1
Standard deviation of group 2
Should we sort (and if so, in which direction), the results of cohen.d? Directions are "decreasing" or  "increasing".
What are the items being described?
Find Mahalanobis distance in cohen.d.
 a Pearson r 
A Fisher z
Sample size for confidence intervals
degrees of freedom for t, or g
Confidence interval
Treat p as twotailed p
An effect size (Hedge's g)
A student's t value
A chi square
a vector of standard deviations to be used to convert a correlation matrix to a covariance matrix
A two dimensional table of counts with row and column names that can be converted to numeric values. 
if present, then duplicate each row count times
Labels for the rows and columns. These will be used for the names of the two columns of the resulting matrix 
A two dimensional table of counts with row and column names that can be converted to numeric values. 
if present, then duplicate each row count times
Labels for the rows and columns. These will be used for the names of the two columns of the resulting matrix 
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A correlation or covariance matrix or raw data matrix.
return a vector of split half reliabilities
Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!
If brute is false, how many samples of split halves should be tried? ()
Should the covariances or correlations be used for reliability calculations
If TRUE, any item with a negative loading on the first factor will be flipped in sign
a vector of -1, 0, 1 to select or reverse key items.  See if the key vector is less than the number of variables, then item numbers to be reverse can be specified.
Should we find the correlations using "pairwise" or "complete" (see ?cor)
The alpha level to use for the confidence intervals of the split half estimates
A rectangular matrix or data frame (probably a correlation matrix)
A data matrix or data frame or a vector depending upon the function.
A data matrix or data frame or a vector
The object returned from either a factor analysis (fa) or a principal components analysis (principal) 
round to digits
Abbreviate to minlength in lowerCor
Should pairwise deletion be done, or one of the other options to cor
Should we check for NA on the diagonal of a correlation matices
"pearson", "kendall", "spearman"
the current value of some looping variable
The maximum value the loop will achieve
what function is looping
The factor or components to be reversed keyed (by factor number)
flag=TRUE in char2numeric will flag those variables that had been numeric
Correct for the maximum possible information in this item
What is the base for the log function (default=2, e implies base = exp(1))
The name of a package (or list of packages) to be activated and then have all      the examples tested.
Find the dependencies for this package, e.g., psych
Which type of dependency to examine?
Look up the dependencies, and then test all of their examples
Do not test these dependencies
A dataframe or matrix to choose from
select from column with name from to column with name to
select from column from to column to
Any string of legitimate objects
Number of variables to create (simulate) and score
Number of simulated subjects
"logistic" or "normal" theory data are generated
"tetra" for dichotomous, "poly" for polytomous
items range from low to high
items range from low to high
Set the random number seed using some non-nul value.  Otherwise, use the existing sequence of random numbers
first=1: start with dataset first
last=5: test for datasets until last
short=TRUE - don't return any analyses
To get around a failure on certain Solaris 32 bit systems, all=FALSE is the default
if fapc=TRUE, then do a whole series of tests of factor and principal component extraction and rotations.
a data.frame or matrix for the first time of measurement.
a data.frame or matrix for the second time of measurement. May be NULL if time is specifed in t1
item names (or locations) to analyze, preface by "-" to reverse score. 
subject identification codes to match across time
The name of the time variable identifying time 1 or 2 if just one data set is supplied. 
A subset of items to analyze
If TRUE will automatically reverse items based upon their correlation with the first principal component.  Will throw a warning when doing so, but some people seem to miss this kind of message.
If TRUE, then warn when items are reverse scored
If TRUE, include the lmer variance decomposition. By default, this is true, but this can lead to long times for large data sets. 
If TRUE, the data are sorted by id and time.  This allows for random ordering of data, but will fail if ids are duplicated in different studies.  In that case, we need to add a constant to the ids for each study.  See the last example.
The input may be in one of four forms:a) a data frame or matrix of dichotmous data (e.g., the lsat6 from the bock data set) or discrete numerical (i.e., not too many levels, e.g., the big 5 data set, bfi) for polychoric, or continuous for the case of biserial and polyserial. b) a 2 x 2 table of cell counts or cell frequencies (for tetrachoric) or an n x m table of cell counts  (for both tetrachoric and polychoric). c) a vector with elements corresponding to the four cell frequencies (for tetrachoric)d) a vector with elements of the two marginal frequencies (row and column) and the comorbidity (for tetrachoric)
A (matrix or dataframe) of discrete scores. In the case of tetrachoric, these should be dichotomous, for polychoric not too many levels, for biserial they should be discrete (e.g., item responses) with not too many (<10?) categories.
Correction value to use to correct for continuity in the case of zero entry cell for tetrachoric, polychoric, polybi, and mixed.cor.  See the examples for the effect of correcting versus not correcting for continuity.
if TRUE and if the tetrachoric/polychoric matrix is not positive definite, then apply a simple smoothing algorithm using cor.smooth
When finding pairwise correlations, should we use the global values of the tau parameter (which is somewhat faster), or the local values (global=FALSE)?  The local option is equivalent to the polycor solution, or to doing one correlation at a time. global=TRUE borrows information for one item pair from the other pairs using those item's frequencies.   This will make a difference in the presence of lots of missing data. With very small sample sizes with global=FALSE and correct=TRUE, the function will fail (for as yet underdetermined reasons. 
A no longer used option, kept to stop other packages from breaking.
A vector of length of the number of observations that specifies the weights to apply to each case.  The NULL case is equivalent of weights of 1 for all cases.  
 short=TRUE, just show the correlations, short=FALSE give the full hetcor output from John Fox's hetcor function if installed and if doing polychoric  Deprecated
std.err=FALSE does not report the standard errors (faster)  deprecated
Show the progress bar (if  not doing multicores)
 ML=FALSE  do a quick two step procedure, ML=TRUE, do longer maximum likelihood — very slow! Deprecated
Should missing data be deleted
Cases with no variance are deleted with a warning before proceeding.
The maximum number of categories to bother with for polychoric.  
The polytomous input to polydi
The dichotomous input to polydi
The tau values for the polytomous variables – if global=TRUE
The tau values for the dichotomous variables – if globabl = TRUE
 A square matrix or data frame of preferences, or a rectangular data frame or matrix rank order choices. 
TRUE if rank orders are presented
number of digits in the goodness of fit
 A matrix or data frame or free text
The number of lines at the beginning to show
The number of lines at the end to show
Round off the data to digits
Separate the head and tail with dots (ellipsis)
The first column to show (defaults to 1)
The last column to show (defaults to the number of columns
The number of lines at the beginning to show (an alias for top)
The number of lines at the end to show (an alias for bottom)
A square matrix 
An input matrix or data frame.  If x is not a correlation matrix, then the correlations are found.
If specified,  then a number of scales can be tested at once. (See scoreItems for a similar procedure.)
By default, find the Pearson correlation, other options are "spearman","kendall","tet"(for tetrachoric), "poly" (for polychoric), or  "mixed"
If using "tetrachoric" or "polychoric" correlations, should we correct empty cells for continuity, and if so, by how much.  (See tetrachoric for a discussion of this correction)
If TRUE, then items will be keyed based upon their loadings on the first factor.  Automatically done if key.list is NULL.
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A loadings matrix
A loadings matrix
A loadings matrix
Which rotation should be used?
the power to which to raise the varimax loadings (for Promax)
the power to which to raise the various loadings in Promax.
An arbitrary target matrix, can be composed of  any weights, but probably -1,0, 1 weights.  If missing, the target is the independent cluster structure determined by assigning every item to it's highest loaded factor.
A matrix of values (mainly 0s, some 1s, some NAs) to which the matrix is transformed.
An initial rotation matrix
parameter passed to optimization routine (GPForth in the GPArotation package and Promax)
parameter passed to optimization routine (GPForth in the GPArotation package) 
parameter passed to optimization routine (GPForth in the GPArotation package)
Other parameters to pass (e.g. to faRotate) include a Target list or matrix
A matrix or data.frame (can be expressed in formula input)
The variable(s) to display
The grouping variable(s) 
The name of the data object if using formula input
If the grouping variable is specified, then what names should be give to the group? Defaults to 1:ngrp
The y label
The x label
Figure title
If TRUE, plot the violins vertically, otherwise, horizontonally
if TRUE, add a stripchart with the data points
If doing a stripchart, then jitter the points this much
If TRUE, add error bars or cats eyes to the violins
if TRUE and errors=TRUE, then draw cats eyes
A degree of transparency (0=transparent ... 1 not transparent)
Allows smoothing of density histograms when plotting variables like height
if TRUE, then plot frequencies (n * density)
Restrict the density to the observed max and min of the data
if not specified, will be .5 beyond the number of variables
If not specified, determined by the data
Allows overplotting
Allows for specification of colours.  The default for 2 groups is blue and red, for more group levels, rainbows.
The plot character for the mean is by default a small filled circle.  To not show the mean, use pch=NA 
If NULL, scale the widths by the square root of sample size, otherwise scale by the value supplied.
If not NULL, draw a legend at c(topleft,topright,top,left,right) 
Other graphic parameters
A matrix or data.frame (can be expressed in formula input)
The variable(s) to display
The grouping variable(s) 
The name of the data object if using formula input
If the grouping variable is specified, then what names should be give to the group? Defaults to 1:ngrp
The y label
The x label
Figure title
If TRUE, plot the violins vertically, otherwise, horizontonally
if TRUE, add a stripchart with the data points
If doing a stripchart, then jitter the points this much
If TRUE, add error bars or cats eyes to the violins
if TRUE and errors=TRUE, then draw cats eyes
A degree of transparency (0=transparent ... 1 not transparent)
Allows smoothing of density histograms when plotting variables like height
if TRUE, then plot frequencies (n * density)
Restrict the density to the observed max and min of the data
if not specified, will be .5 beyond the number of variables
If not specified, determined by the data
Allows overplotting
Allows for specification of colours.  The default for 2 groups is blue and red, for more group levels, rainbows.
The plot character for the mean is by default a small filled circle.  To not show the mean, use pch=NA 
If NULL, scale the widths by the square root of sample size, otherwise scale by the value supplied.
If not NULL, draw a legend at c(topleft,topright,top,left,right) 
Other graphic parameters
 a correlation matrix or a data matrix
Number of factors to extract – should be more than hypothesized! 
 what rotation to use c("none", "varimax",  "oblimin","promax")
Should we fit the diagonal as well 
factoring method – fm="pa"  Principal Axis Factor Analysis, fm = "minres" minimum residual (OLS) factoring fm="mle"  Maximum Likelihood FA, fm="pc" Principal Components" 
Number of observations if doing a factor analysis of correlation matrix.  This value is ignored by VSS but is necessary for the ML factor analysis package.
plot=TRUE  Automatically call VSS.plot with the VSS output, otherwise don't plot
a title to be passed on to VSS.plot
the plot character for the nfactors plots
If doing covariances or Pearson R, should we use "pairwise" or "complete cases"
What kind of correlation to find, defaults to Pearson but see fa for the choices
parameters to pass to the factor analysis program The most important of these is if using a correlation matrix is covmat= xx
 a correlation matrix or a data matrix
Number of factors to extract – should be more than hypothesized! 
 what rotation to use c("none", "varimax",  "oblimin","promax")
Should we fit the diagonal as well 
factoring method – fm="pa"  Principal Axis Factor Analysis, fm = "minres" minimum residual (OLS) factoring fm="mle"  Maximum Likelihood FA, fm="pc" Principal Components" 
Number of observations if doing a factor analysis of correlation matrix.  This value is ignored by VSS but is necessary for the ML factor analysis package.
plot=TRUE  Automatically call VSS.plot with the VSS output, otherwise don't plot
a title to be passed on to VSS.plot
the plot character for the nfactors plots
If doing covariances or Pearson R, should we use "pairwise" or "complete cases"
What kind of correlation to find, defaults to Pearson but see fa for the choices
parameters to pass to the factor analysis program The most important of these is if using a correlation matrix is covmat= xx
Number of simulated cases 
 number of simulated variables 
Show a scree plot for random data – see  omega
rotate="none" or rotate="varimax"
output from VSS 
any title 
 connect different complexities 
 a correlation matrix or a data matrix. If data, then correlations are found using pairwise deletions. 
If true, draw the scree for factors 
If true, draw the scree for components
if null, draw a horizontal line at 1, otherwise draw it at hline (make negative to not draw it)
 Title 
Should multiple plots be drawn?
 number of simulated subjects 
 Number of variables 
 Number of factors to generate 
with a mean loading 
dichot=FALSE give continuous variables, dichot=TRUE gives dichotomous variables
if dichotomous = TRUE, then items with values > cut are assigned 1, otherwise 0.
 number of simulated subjects 
 Number of variables 
 Number of factors to generate 
with a mean loading 
dichot=FALSE give continuous variables, dichot=TRUE gives dichotomous variables
if dichotomous = TRUE, then items with values > cut are assigned 1, otherwise 0.
A data vector, matrix or data frame
Percentage of data to move  from the top and bottom of the distributions
Missing data are removed 
A data vector, matrix or data frame
Percentage of data to move  from the top and bottom of the distributions
Missing data are removed 
A data vector, matrix or data frame
Percentage of data to move  from the top and bottom of the distributions
Missing data are removed 
A data vector, matrix or data frame
Percentage of data to move  from the top and bottom of the distributions
Missing data are removed 
A data vector, matrix or data frame
Percentage of data to move  from the top and bottom of the distributions
Missing data are removed 
Either a two by n data with categorical values from 1 to p or a p x p table.  If a data array, a table will be found.
A p x p matrix of weights.  If not specified, they are set to be 0 (on the diagonal) and (distance from diagonal) off the diagonal)^2. 
Number of observations (if input is a square matrix.
Probability level for confidence intervals
Specify the levels if some levels of x or y are completely missing.  See Examples
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
a matrix of phi or Yule  coefficients 
A vector of marginal frequencies  
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
a matrix of phi or Yule  coefficients 
A vector of marginal frequencies  
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
A vector of four elements or a two by two matrix, or, in the case of YuleBonett or YuleCor, this can also be a data matrix 
1 returns Yule Q, .5, Yule's Y, .75 Digby's H
If FALSE, then find Q, Y, or H, if TRUE, then find the generalized Bonett cofficient
The two tailed probability for confidence intervals
Y=TRUE return Yule's Y coefficient of colligation
Either a single Yule coefficient or a matrix of Yule coefficients
The vector c(R1,C2) or a two x two matrix of marginals or a four element vector of marginals.  The preferred form is c(R1,C1)
The number of subjects (if the marginals are given as frequencies
When finding a tetrachoric correlation, should small cell sizes be corrected for continuity.  See {link{tetrachoric} for a discussion.
a univariate or multivariate (not ccf) numeric timeseries object or a numeric vector or matrix, or an "acf" object.
maximum lag at which to calculate the acf.Default is 10*log10(N/m) where N is thenumber of observations and m the number of series.  Willbe automatically limited to one less than the number of observationsin the series.
character string giving the type of acf to be computed.Allowed values are"correlation" (the default), "covariance" or"partial".  Will be partially matched.
logical. If TRUE (the default) the acf is plotted.
function to be called to handle missingvalues. na.pass can be used.
logical.  Should the covariances be about the samplemeans?
further arguments to be passed to plot.acf.
a set of lags (time differences) to retain.
a set of series (names or numbers) to retain.
An autocorrelation or autocovariance sequence.
the terms or formula for the base model.
the terms or formula for the upper (add.scope) orlower (drop.scope) scope. If missing for drop.scope it istaken to be the null formula, so all terms (except any intercept) arecandidates to be dropped.
the "factor" attribute of the terms of the base object.
a list with one or both components drop andadd giving the "factor" attribute of the lower andupper scopes respectively.
a fitted model object.
a formula giving the terms to be considered for adding ordropping.
an estimate of the residual mean square to beused in computing Cp. Ignored if 0 or NULL.
should the results include a test statistic relative to theoriginal model?  The F test is only appropriate for lm andaov models or perhaps for glm fits withestimated dispersion.The Chisq test can be an exact test(lm models with known scale) or a likelihood-ratio test or atest of the reduction in scaled deviance depending on the method.For glm fits, you can also choose "LRT" and"Rao" for likelihood ratio tests and Rao's efficient score test.The former is synonymous with "Chisq" (although both havean asymptotic chi-square distribution).Values can be abbreviated.
the penalty constant in AIC / Cp.
if TRUE, print out progress reports.
a model matrix containing columns for the fitted model and allterms in the upper scope.  Useful if add1 is to be calledrepeatedly.  Warning: no checks are done on its validity.
(Provided for compatibility with S.)  Logical to specifywhether all columns of the design matrix should be used.  IfFALSE then non-estimable columns are dropped, but the resultis not usually statistically meaningful.
further arguments passed to or from other methods.
table or array.  The function uses the presence of the"dim" and "dimnames" attributes of A.
vector of dimensions over which to form margins.  Marginsare formed in the order in which dimensions are specified inmargin.
list of the same length as margin, eachelement of the list being either a function or a list offunctions.  In the length-1 case, can be a function instead of a listof one.  Names ofthe list elements will appear as levels in dimnames of the result.Unnamed list elements will have names constructed:  the nameof a function or a constructed name based on the position in the table.
logical which suppresses the message telling the order inwhich the margins were computed.
an R object.
a list of grouping elements, each as long as the variablesin the data frame x.  The elements are coerced to factorsbefore use.
a function to compute the summary statistics which can beapplied to all data subsets.
a logical indicating whether results should besimplified to a vector or matrix if possible.
a logical indicating whether to drop unused combinationsof grouping values.  The non-default case drop=FALSE has beenamended for R 3.5.0 to drop unused combinations.
a formula, such as y ~ x orcbind(y1, y2) ~ x1 + x2, where the y variables arenumeric data to be split into groups according to the groupingx variables (usually factors).
a data frame (or list) from which the variables in formulashould be taken.
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NA values. The default is to ignore missingvalues in the given variables.
new number of observations per unit of time; mustbe a divisor of the frequency of x.
new fraction of the sampling period betweensuccessive observations; must be a divisor of the samplinginterval of x.
tolerance used to decide if nfrequency is asub-multiple of the original frequency.
further arguments passed to or used by methods.
an R object.
a list of grouping elements, each as long as the variablesin the data frame x.  The elements are coerced to factorsbefore use.
a function to compute the summary statistics which can beapplied to all data subsets.
a logical indicating whether results should besimplified to a vector or matrix if possible.
a logical indicating whether to drop unused combinationsof grouping values.  The non-default case drop=FALSE has beenamended for R 3.5.0 to drop unused combinations.
a formula, such as y ~ x orcbind(y1, y2) ~ x1 + x2, where the y variables arenumeric data to be split into groups according to the groupingx variables (usually factors).
a data frame (or list) from which the variables in formulashould be taken.
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NA values. The default is to ignore missingvalues in the given variables.
new number of observations per unit of time; mustbe a divisor of the frequency of x.
new fraction of the sampling period betweensuccessive observations; must be a divisor of the samplinginterval of x.
tolerance used to decide if nfrequency is asub-multiple of the original frequency.
further arguments passed to or used by methods.
an R object.
a list of grouping elements, each as long as the variablesin the data frame x.  The elements are coerced to factorsbefore use.
a function to compute the summary statistics which can beapplied to all data subsets.
a logical indicating whether results should besimplified to a vector or matrix if possible.
a logical indicating whether to drop unused combinationsof grouping values.  The non-default case drop=FALSE has beenamended for R 3.5.0 to drop unused combinations.
a formula, such as y ~ x orcbind(y1, y2) ~ x1 + x2, where the y variables arenumeric data to be split into groups according to the groupingx variables (usually factors).
a data frame (or list) from which the variables in formulashould be taken.
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NA values. The default is to ignore missingvalues in the given variables.
new number of observations per unit of time; mustbe a divisor of the frequency of x.
new fraction of the sampling period betweensuccessive observations; must be a divisor of the samplinginterval of x.
tolerance used to decide if nfrequency is asub-multiple of the original frequency.
further arguments passed to or used by methods.
a fitted model object for which there exists alogLik method to extract the corresponding log-likelihood, oran object inheriting from class logLik.
optionally more fitted model objects.
numeric, the penalty per parameter to be used; thedefault k = 2 is the classical AIC.
A fitted model object, for example from lm oraov, or a formula for alias.formula.
Optionally, a data frame to search for the objectsin the formula.
Should information on complete aliasing be included?
Should information on partial aliasing be included?
Should partial aliasing be presented in aschematic way? If this is done, the results are presented in amore compact way, usually giving the deciles of the coefficients.
further arguments passed to or from other methods.
an object containing the results returned by a modelfitting function (e.g., lm or glm).
additional objects of the same type.
numeric vector of data values.
numeric vector of data values.
indicates the alternative hypothesis and must beone of "two.sided", "greater" or "less".  Youcan specify just the initial letter.
a logical indicating whether an exact p-valueshould be computed.
a logical,indicating whether a confidence intervalshould be computed.
confidence level of the interval.
a formula of the form lhs ~ rhs where lhsis a numeric variable giving the data values and rhs a factorwith two levels giving the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
A formula specifying the model.
A data frame in which the variables specified in theformula will be found. If missing, the variables are searched for inthe standard way.
Logical flag: should the projections be returned?
Logical flag: should the QR decomposition be returned?
A list of contrasts to be used for some of the factorsin the formula. These are not used for any Error term, andsupplying contrasts for factors only in the Error term will givea warning.
Arguments to be passed to lm, such as subsetor na.action.  See ‘Details’ about weights.
numeric vectors giving the coordinates of the points to beinterpolated.  Alternatively a single plotting structure can bespecified: see xy.coords.
an optional set of numeric values specifying whereinterpolation is to take place.
specifies the interpolation method to be used.  Choicesare "linear" or "constant".
If xout is not specified, interpolation takes place atn equally spaced points spanning the interval [min(x),max(x)].
the value to be returned when input x values areless than min(x). The default is defined by the valueof rule given below.
the value to be returned when input x values aregreater than max(x). The default is defined by the valueof rule given below.
an integer (of length 1 or 2) describing how interpolationis to take place outside the interval [min(x), max(x)].If rule is 1 then NAs are returned for suchpoints and if it is 2, the value at the closest data extremeis used.  Use, e.g., rule = 2:1, if the left and right sideextrapolation should differ.
for method = "constant" a number between 0 and 1inclusive, indicating a compromise between left- andright-continuous step functions. If y0 and y1 arethe values to the left and right of the point then the value isy0 if f == 0, y1 if f == 1, and y0*(1-f)+y1*f for intermediate values. In this way the result isright-continuous for f == 0 and left-continuous for f    == 1, even for non-finite y values.
handling of tied x values.  The string"ordered" or a function (or the name of a function)taking a single vector argument and returning a single numberor a list of both, e.g.,list("ordered", mean), see ‘Details’.
logical specifying how missing values (NA's)should be handled.  Setting na.rm=FALSE will propagateNA's in y to the interpolated values, also depending onthe rule set.  Note that in this case, NA's in xare invalid, see also the examples.
numeric vectors giving the coordinates of the points to beinterpolated.  Alternatively a single plotting structure can bespecified: see xy.coords.
an optional set of numeric values specifying whereinterpolation is to take place.
specifies the interpolation method to be used.  Choicesare "linear" or "constant".
If xout is not specified, interpolation takes place atn equally spaced points spanning the interval [min(x),max(x)].
the value to be returned when input x values areless than min(x). The default is defined by the valueof rule given below.
the value to be returned when input x values aregreater than max(x). The default is defined by the valueof rule given below.
an integer (of length 1 or 2) describing how interpolationis to take place outside the interval [min(x), max(x)].If rule is 1 then NAs are returned for suchpoints and if it is 2, the value at the closest data extremeis used.  Use, e.g., rule = 2:1, if the left and right sideextrapolation should differ.
for method = "constant" a number between 0 and 1inclusive, indicating a compromise between left- andright-continuous step functions. If y0 and y1 arethe values to the left and right of the point then the value isy0 if f == 0, y1 if f == 1, and y0*(1-f)+y1*f for intermediate values. In this way the result isright-continuous for f == 0 and left-continuous for f    == 1, even for non-finite y values.
handling of tied x values.  The string"ordered" or a function (or the name of a function)taking a single vector argument and returning a single numberor a list of both, e.g.,list("ordered", mean), see ‘Details’.
logical specifying how missing values (NA's)should be handled.  Setting na.rm=FALSE will propagateNA's in y to the interpolated values, also depending onthe rule set.  Note that in this case, NA's in xare invalid, see also the examples.
a univariate or multivariate time series.
logical.  If TRUE then the Akaike InformationCriterion is used to choose the order of the autoregressivemodel.  If FALSE, the model of order order.max isfitted.
maximum order (or order) of model to fit.  Defaultsto the smaller of N-1 and 10*log10(N)where N is the number of non-missing observationsexcept for method = "mle" where it is the minimum of thisquantity and 12.
character string specifying the method to fit themodel.  Must be one of the strings in the default argument(the first few characters are sufficient).  Defaults to"yule-walker".
function to be called to handle missingvalues.  Currently, via na.action = na.pass, only Yule-Walkermethod can handle missing values which must be consistent within atime point: either all variables must be missing or none.
should a mean be estimated during fitting?
names for the series.  Defaults todeparse1(substitute(x)).
the method to estimate the innovations variance(see ‘Details’).
additional arguments for specific methods.
a fit from ar().
data to which to apply the prediction.
number of steps ahead at which to predict.
logical: return estimated standard errors of theprediction error?
a univariate or multivariate time series.
logical.  If TRUE then the Akaike InformationCriterion is used to choose the order of the autoregressivemodel.  If FALSE, the model of order order.max isfitted.
maximum order (or order) of model to fit.  Defaultsto the smaller of N-1 and 10*log10(N)where N is the number of non-missing observationsexcept for method = "mle" where it is the minimum of thisquantity and 12.
character string specifying the method to fit themodel.  Must be one of the strings in the default argument(the first few characters are sufficient).  Defaults to"yule-walker".
function to be called to handle missingvalues.  Currently, via na.action = na.pass, only Yule-Walkermethod can handle missing values which must be consistent within atime point: either all variables must be missing or none.
should a mean be estimated during fitting?
names for the series.  Defaults todeparse1(substitute(x)).
the method to estimate the innovations variance(see ‘Details’).
additional arguments for specific methods.
a fit from ar().
data to which to apply the prediction.
number of steps ahead at which to predict.
logical: return estimated standard errors of theprediction error?
a univariate or multivariate time series.
logical.  If TRUE then the Akaike InformationCriterion is used to choose the order of the autoregressivemodel.  If FALSE, the model of order order.max isfitted.
maximum order (or order) of model to fit.  Defaultsto the smaller of N-1 and 10*log10(N)where N is the number of non-missing observationsexcept for method = "mle" where it is the minimum of thisquantity and 12.
character string specifying the method to fit themodel.  Must be one of the strings in the default argument(the first few characters are sufficient).  Defaults to"yule-walker".
function to be called to handle missingvalues.  Currently, via na.action = na.pass, only Yule-Walkermethod can handle missing values which must be consistent within atime point: either all variables must be missing or none.
should a mean be estimated during fitting?
names for the series.  Defaults todeparse1(substitute(x)).
the method to estimate the innovations variance(see ‘Details’).
additional arguments for specific methods.
a fit from ar().
data to which to apply the prediction.
number of steps ahead at which to predict.
logical: return estimated standard errors of theprediction error?
A univariate or multivariate time series.
Logical flag.  If TRUE then the Akaike InformationCriterion is used to choose the order of the autoregressivemodel. If FALSE, the model of order order.max isfitted.
Maximum order (or order) of model to fit. Defaultsto 10*log10(N) where N is the numberof observations.
function to be called to handle missing values.
should the AR model be for x minus its mean?
should a separate intercept term be fitted?
names for the series.  Defaults todeparse1(substitute(x)).
further arguments to be passed to or from methods.
a univariate or multivariate time series.
logical.  If TRUE then the Akaike InformationCriterion is used to choose the order of the autoregressivemodel.  If FALSE, the model of order order.max isfitted.
maximum order (or order) of model to fit.  Defaultsto the smaller of N-1 and 10*log10(N)where N is the number of non-missing observationsexcept for method = "mle" where it is the minimum of thisquantity and 12.
character string specifying the method to fit themodel.  Must be one of the strings in the default argument(the first few characters are sufficient).  Defaults to"yule-walker".
function to be called to handle missingvalues.  Currently, via na.action = na.pass, only Yule-Walkermethod can handle missing values which must be consistent within atime point: either all variables must be missing or none.
should a mean be estimated during fitting?
names for the series.  Defaults todeparse1(substitute(x)).
the method to estimate the innovations variance(see ‘Details’).
additional arguments for specific methods.
a fit from ar().
data to which to apply the prediction.
number of steps ahead at which to predict.
logical: return estimated standard errors of theprediction error?
a univariate time series
A specification of the non-seasonal part of the ARIMAmodel: the three integer components (p, d, q) are the AR order, thedegree of differencing, and the MA order.
A specification of the seasonal part of the ARIMAmodel, plus the period (which defaults to frequency(x)).This may be a list with components order andperiod, or just a numeric vector of length 3 whichspecifies the seasonal order.  In the latter case thedefault period is used.
Optionally, a vector or matrix of external regressors,which must have the same number of rows as x.
Should the ARMA model include a mean/intercept term?  Thedefault is TRUE for undifferenced series, and it is ignoredfor ARIMA models with differencing.
logical; if true, the AR parameters aretransformed to ensure that they remain in the region ofstationarity.  Not used for method = "CSS".  Formethod = "ML", it has been advantageous to settransform.pars = FALSE in some cases, see also fixed.
optional numeric vector of the same length as the totalnumber of coefficients to be estimated.  It should be of the form(phi_1, ..., phi_p, theta_1, ..., theta_p,Phi_1,..., Phi_P, Theta_1,...,Theta_Q, mu),where phi_i are the AR coefficients,theta_i are the MA coefficients,Phi_i are the seasonal AR coefficients,Theta_i are the seasonal MA coefficients andmu is the intercept term.  Note that the muentry is required if and only if include.mean is TRUE.In particular it should not be present if the model is an ARIMAmodel with differencing.The entries of the fixed vector should consist of thevalues at which the user wishes to “fix” the correspondingcoefficient, or NA if that coefficient should not befixed, but estimated.The argument transform.pars will be set to FALSE if anyAR parameters are fixed.  A warning will be given if transform.parsis set to (or left at its default) TRUE.  It may be wise to settransform.pars = FALSE even when fixing MA parameters,especially at values that cause the model to be nearly non-invertible.
optional numeric vector of initial parametervalues.  Missing values will be filled in, by zeroes except forregression coefficients.  Values already specified in fixedwill be ignored.
fitting method: maximum likelihood or minimizeconditional sum-of-squares.  The default (unless there are missingvalues) is to use conditional-sum-of-squares to find startingvalues, then maximum likelihood.  Can be abbreviated.
only used if fitting by conditional-sum-of-squares: thenumber of initial observations to ignore.  It will be ignored ifless than the maximum lag of an AR term.
a string specifying the algorithm to compute thestate-space initialization of the likelihood; seeKalmanLike for details.   Can be abbreviated.
The value passed as the method argument tooptim.
List of control parameters for optim.
the prior variance (as a multiple of the innovationsvariance) for the past observations in a differenced model.  Do notreduce this.
A list with component ar and/or ma givingthe AR and MA coefficients respectively.  Optionally a componentorder can be used.  An empty list gives an ARIMA(0, 0, 0)model, that is white noise.
length of output series, before un-differencing.  A strictlypositive integer.
optional: a function to generate the innovations.
an optional times series of innovations.  If notprovided, rand.gen is used.
length of ‘burn-in’ period.  If NA, thedefault, a reasonable value is computed.
an optional times series of innovations to be usedfor the burn-in period.  If supplied there must be at leastn.start values (and n.start is by default computedinside the function).
additional arguments for rand.gen.  Most usefully,the standard deviation of the innovations generated by rnormcan be specified by sd.
a univariate time series
A specification of the non-seasonal part of the ARIMAmodel: the three components (p, d, q) are the AR order, thedegree of differencing, and the MA order.
A specification of the seasonal part of the ARIMAmodel, plus the period (which defaults to frequency(x)).This should be a list with components order andperiod, but a specification of just a numeric vector oflength 3 will be turned into a suitable list with the specificationas the order.
Optionally, a vector or matrix of external regressors,which must have the same number of rows as x.
Should the ARIMA model includea mean term? The default is TRUE for undifferenced series,FALSE for differenced ones (where a mean would not affectthe fit nor predictions).
A value to indicate at which point ‘fastrecursions’ should be used.  See the ‘Details’ section.
Logical.  If true, the AR parameters aretransformed to ensure that they remain in the region ofstationarity.  Not used for method = "CSS".
optional numeric vector of the same length as the totalnumber of parameters.  If supplied, only NA entries infixed will be varied.  transform.pars = TRUEwill be overridden (with a warning) if any ARMA parameters arefixed.
optional numeric vector of initial parametervalues.  Missing values will be filled in, by zeroes except forregression coefficients.  Values already specified in fixedwill be ignored.
Fitting method: maximum likelihood or minimizeconditional sum-of-squares.  Can be abbreviated.
Only used if fitting by conditional-sum-of-squares: thenumber of initial observations to ignore.  It will be ignored ifless than the maximum lag of an AR term.
List of control parameters for optim.
The result of an arima0 fit.
New values of xreg to be used forprediction. Must have at least n.ahead rows.
The number of steps ahead for which prediction is required.
Logical: should standard errors of prediction be returned?
arguments passed to or from other methods.
numeric vector of AR coefficients
numeric vector of MA coefficients
integer.  Maximum lag required.  Defaults tomax(p, q+1), where p, q are the numbers of AR and MAterms respectively.
logical.  Should the partial autocorrelations be returned?
numeric vector of AR coefficients
numeric vector of MA coefficients
Largest MA(Inf) coefficient required.
any R object that can be made into one of class"dendrogram".
object(s) of class "dendrogram".
numeric scalar indicating how the height of leavesshould be computed from the heights of their parents; seeplot.hclust.
logical indicating if object should be checked forvalidity.  This check is not necessary when x is known to bevalid such as when it is the direct result of hclust().  Thedefault is check=TRUE, e.g. for protecting against memoryexplosion with invalid inputs.
type of plot.
logical; if TRUE, nodes are plotted centered withrespect to the leaves in the branch.  Otherwise (default), plot themin the middle of all direct child nodes.
logical; if true, draw an edge to the root node.
a list of plotting parameters to use for thenodes (see points) or NULL by default whichdoes not draw symbols at the nodes.  The list may contain componentsnamed pch, cex, col, xpd,and/or bg each ofwhich can have length two for specifying separate attributes forinner nodes and leaves.  Note that the default ofpch is 1:2, so you may want to use pch = NA ifyou specify nodePar.
a list of plotting parameters to use for theedge segments and labels (if there's anedgetext).  The list may contain componentsnamed col, lty and lwd (for the segments),p.col, p.lwd, and p.lty (for thepolygon around the text) and t.col for the textcolor.  As with nodePar, each can have length two fordifferentiating leaves and inner nodes.
a string specifying how leaves are labeled.  Thedefault "perpendicular" write text vertically (by default)."textlike" writes text horizontally (in a rectangle), and "none" suppresses leaf labels.
a number specifying the distance in usercoordinates between the tip of a leaf and its label.  If NULLas per default, 3/4 of a letter width or height is used.
logical indicating if the dendrogram should be drawnhorizontally or not.
logical indicating if a box around the plot shouldbe drawn, see plot.default.
height at which the tree is cut.
height at which the two dendrograms should be merged.  If notspecified (or NULL), the default is ten percent larger thanthe (larger of the) two component heights.
a string determining if the leaf values should beadjusted.  The default, "auto", checks if the (first) twodendrograms both start at 1; if they do, code"add.max" ischosen, which adds the maximum of the previous dendrogram leafvalues to each leaf of the “next” dendrogram.  Specifyingadjust to another value skips the check and hence is a tadmore efficient.
optional x- and y-limits of the plot, passed toplot.default.  The defaults for these show the fulldendrogram.
graphical parameters, or arguments forother methods.
integer specifying the precision for printing, seeprint.default.
argumentsto str, see str.default().  Note thatgive.attr = FALSE still shows height and membersattributes for each node.
strings used for str() specifying how thelast branch (at each level) should start and the stemto use for each dendrogram branch.  In some environments, usinglast.str = "'" will provide much nicer looking output, thanthe historical default last.str = "`".
a numeric matrix, data frame or "dist" object.
the distance measure to be used.  This must be one of"euclidean", "maximum", "manhattan","canberra", "binary" or "minkowski".Any unambiguous substring can be given.
logical value indicating whether the diagonal of thedistance matrix should be printed by print.dist.
logical value indicating whether the upper triangle of thedistance matrix should be printed by print.dist.
The power of the Minkowski distance.
An object with distance information to be converted to a"dist" object.  For the default method, a "dist"object, or a matrix (of distances) or an object which can be coercedto such a matrix using as.matrix().  (Only the lowertriangle of the matrix is used, the rest is ignored).
passed to format inside ofprint().
further arguments, passed to other methods.
R object, for DF2formula() a data.frame.
further arguments passed to or from other methods.
the environment to associate with the result, if notalready a formula.
logical indicating if the environment should be printedas well.
Hierarchical clustering object
further arguments passed to or from other methods.
numeric vector giving the knots or jump locations of the stepfunction for stepfun().  For the other functions, x isas object below.
numeric vector one longer than x, giving the heights ofthe function values between the x values.
a number between 0 and 1, indicating how interpolation outsidethe given x values should happen.  See approxfun.
Handling of tied x values. Either a function orthe string "ordered".  See  approxfun.
logical, indicating if the intervals should be closed onthe right (and open on the left) or vice versa.
an R object inheriting from "stepfun".
number of significant digits to use, see print.
potentially further arguments (required by the generic).
a vector or matrix of the observed time-seriesvalues. A data frame will be coerced to a numeric matrix viadata.matrix.  (See also ‘Details’.)
the time of the first observation.  Either a singlenumber or a vector of two numbers (the second of which is an integer),which specify a natural timeunit and a (1-based) number of samples into the time unit.  Seethe examples for the use of the second form.
the time of the last observation, specified in the same wayas start.
the number of observations per unit of time.
the fraction of the sampling period between successiveobservations; e.g., 1/12 for monthly data.  Only one offrequency or deltat should be provided.
time series comparison tolerance.  Frequencies areconsidered equal if their absolute difference is less thants.eps.
class to be given to the result, or none if NULLor "none".  The default is "ts" for a single series,c("mts", "ts", "matrix") for multiple series.
a character vector of names for the series in a multipleseries: defaults to the colnames of data, or Series 1,Series 2, ....
an arbitrary R object.
arguments passed to methods (unused for the default method).
a one-sided formula, an expression, a numeric value, or acharacter string.
A numeric.
Grouping variables, typically factors, all of the samelength as x.
Function to apply for each factor level combination.
the upper half of the smoothing kernel coefficients(including coefficient zero) or the name of a kernel(currently "daniell", "dirichlet", "fejer" or"modified.daniell").
the kernel dimension(s) if coef is a name.  When mhas length larger than one, it means the convolution ofkernels of dimension m[j], for j in 1:length(m).Currently this is supported only for the named "*daniell" kernels.
the name the kernel will be called.
the kernel order for a Fejer kernel.
a "tskernel" object.
arguments passed toplot.default.
a numeric vector of data values, or a list of numeric datavectors representing the respective samples, or fitted linear modelobjects (inheriting from class "lm").
a vector or factor object giving the group for thecorresponding elements of x.Ignored if x is a list.
a formula of the form lhs ~ rhs where lhsgives the data values and rhs the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
a fitted model object for which there exists alogLik method to extract the corresponding log-likelihood, oran object inheriting from class logLik.
optionally more fitted model objects.
numeric, the penalty per parameter to be used; thedefault k = 2 is the classical AIC.
number of successes, or a vector of length 2 giving thenumbers of successes and failures, respectively.
number of trials; ignored if x has length 2.
hypothesized probability of success.
indicates the alternative hypothesis and must beone of "two.sided", "greater" or "less".You can specify just the initial letter.
confidence level for the returned confidenceinterval.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
The biplot, a fitted object. For biplot.default,the first set of points (a two-column matrix), usually associatedwith observations.
The second set of points (a two-column matrix), usually associatedwith variables.
If TRUE the second set of points have arrowsrepresenting them as (unscaled) axes.
A vector of length 2 giving the colours for the first andsecond set of points respectively (and the corresponding axes). If asingle colour is specified it will be used for both sets.  Ifmissing the default colour is looked for in thepalette: if there it and the next colour as used,otherwise the first two colours of the palette are used.
The character expansion factor used for labelling thepoints. The labels can be of different sizes for the two sets bysupplying a vector of length two.
A vector of character strings to label the first set ofpoints: the default is to use the row dimname of x, or1:n if the dimname is NULL.
A vector of character strings to label the second set ofpoints: the default is to use the row dimname of y, or1:n if the dimname is NULL.
An expansion factor to apply when plotting the second setof points relative to the first. This can be used to tweak thescaling of the two sets to a physically comparable scale.
The length of the arrow heads on the axes plotted invar.axes is true. The arrow head can be suppressed byarrow.len = 0.
Limits for the x and y axes in the units of thefirst set of variables.
graphical parameters.
a numeric vector or univariate time series.
the statistic will be based on lag autocorrelationcoefficients.
test to be performed: partial matching is used.
number of degrees of freedom to be subtracted if xis a series of residuals.
numeric vector.
number of bins to use.
range over which to minimize.  The default isalmost always satisfactory.  hmax is calculated internallyfrom a normal reference bandwidth.
either "ste" ("solve-the-equation") or"dpi" ("direct plug-in").   Can be abbreviated.
for method "ste", the convergence tolerance foruniroot.  The default leads to bandwidth estimateswith only slightly more than one digit accuracy, which is sufficientfor practical density estimation, but possibly not for theoreticalsimulation studies.
numeric vector.
number of bins to use.
range over which to minimize.  The default isalmost always satisfactory.  hmax is calculated internallyfrom a normal reference bandwidth.
either "ste" ("solve-the-equation") or"dpi" ("direct plug-in").   Can be abbreviated.
for method "ste", the convergence tolerance foruniroot.  The default leads to bandwidth estimateswith only slightly more than one digit accuracy, which is sufficientfor practical density estimation, but possibly not for theoreticalsimulation studies.
numeric vector.
number of bins to use.
range over which to minimize.  The default isalmost always satisfactory.  hmax is calculated internallyfrom a normal reference bandwidth.
either "ste" ("solve-the-equation") or"dpi" ("direct plug-in").   Can be abbreviated.
for method "ste", the convergence tolerance foruniroot.  The default leads to bandwidth estimateswith only slightly more than one digit accuracy, which is sufficientfor practical density estimation, but possibly not for theoreticalsimulation studies.
numeric vector.
number of bins to use.
range over which to minimize.  The default isalmost always satisfactory.  hmax is calculated internallyfrom a normal reference bandwidth.
either "ste" ("solve-the-equation") or"dpi" ("direct plug-in").   Can be abbreviated.
for method "ste", the convergence tolerance foruniroot.  The default leads to bandwidth estimateswith only slightly more than one digit accuracy, which is sufficientfor practical density estimation, but possibly not for theoreticalsimulation studies.
numeric vector.
number of bins to use.
range over which to minimize.  The default isalmost always satisfactory.  hmax is calculated internallyfrom a normal reference bandwidth.
either "ste" ("solve-the-equation") or"dpi" ("direct plug-in").   Can be abbreviated.
for method "ste", the convergence tolerance foruniroot.  The default leads to bandwidth estimateswith only slightly more than one digit accuracy, which is sufficientfor practical density estimation, but possibly not for theoreticalsimulation studies.
a factor or ordered factor
which contrasts to use. Can be a matrix with one row foreach level of the factor or a suitable function likecontr.poly or a character string giving the name of the function
the number of contrasts to set, by default one lessthan nlevels(object).
additional arguments for the function contr.
numeric matrix (n * p1), containing thex coordinates.
numeric matrix (n * p2), containing they coordinates.
logical or numeric vector of length p1,describing any centering to be done on the x values before theanalysis.  If TRUE (default), subtract the column means.If FALSE, do not adjust the columns.  Otherwise, a vectorof values to be subtracted from the columns.
analogous to xcenter, but for the y values.
an R object, typically a fitted model.
logical; if TRUE, all names (including zero weights,...) are returned.
further arguments passed to or from other methods.
a univariate or multivariate (not ccf) numeric timeseries object or a numeric vector or matrix, or an "acf" object.
maximum lag at which to calculate the acf.Default is 10*log10(N/m) where N is thenumber of observations and m the number of series.  Willbe automatically limited to one less than the number of observationsin the series.
character string giving the type of acf to be computed.Allowed values are"correlation" (the default), "covariance" or"partial".  Will be partially matched.
logical. If TRUE (the default) the acf is plotted.
function to be called to handle missingvalues. na.pass can be used.
logical.  Should the covariances be about the samplemeans?
further arguments to be passed to plot.acf.
a set of lags (time differences) to retain.
a set of series (names or numbers) to retain.
a numeric vector or matrix. x and y can alsoboth be factors.
a numeric vector; ignored if x is a matrix.  Ifx is a factor, y should be a factor of the same length.
a logical indicating whether to apply continuitycorrection when computing the test statistic for 2 by 2 tables: onehalf is subtracted from all |O - E| differences; however, thecorrection will not be bigger than the differences themselves.  No correctionis done if simulate.p.value = TRUE.
a vector of probabilities of the same length of x.An error is given if any entry of p is negative.
a logical scalar; if TRUE then p is rescaled(if necessary) to sum to 1.  If rescale.p is FALSE, andp does not sum to 1, an error is given.
a logical indicating whether to computep-values by Monte Carlo simulation.
an integer specifying the number of replicates used in theMonte Carlo test.
a distance structure such as that returned by distor a full symmetric matrix containing the dissimilarities.
the maximum dimension of the space which the data are to berepresented in; must be in {1, 2, …, n-1}.
indicates whether eigenvalues should be returned.
logical indicating if an additive constant c* shouldbe computed, and added to the non-diagonal dissimilarities such thatthe modified dissimilarities are Euclidean.
indicates whether the doubly centred symmetric distancematrix should be returned.
logical indicating if a list should bereturned or just the n * k matrix, see ‘Value:’.
an object for which the extraction of model coefficients ismeaningful.
for the default (used for lm, etc) andaov methods:logical indicating if the full coefficient vector should be returnedalso in case of an over-determined system where some coefficientswill be set to NA, see also alias.  Notethat the default differs for lm() andaov() results.
other arguments.
an object for which the extraction of model coefficients ismeaningful.
for the default (used for lm, etc) andaov methods:logical indicating if the full coefficient vector should be returnedalso in case of an over-determined system where some coefficientswill be set to NA, see also alias.  Notethat the default differs for lm() andaov() results.
other arguments.
a sequence of vectors, matrices and data frames.
a fitted model object.
a specification of which parameters are to be givenconfidence intervals, either a vector of numbers or a vector ofnames.  If missing, all parameters are considered.
the confidence level required.
additional argument(s) for methods.
a fitted model object.
a specification of which parameters are to be givenconfidence intervals, either a vector of numbers or a vector ofnames.  If missing, all parameters are considered.
the confidence level required.
additional argument(s) for methods.
a fitted model object.
a specification of which parameters are to be givenconfidence intervals, either a vector of numbers or a vector ofnames.  If missing, all parameters are considered.
the confidence level required.
additional argument(s) for methods.
numeric (vector) starting value (of length p): mustbe in the feasible region.
function to minimise (see below).
gradient of f (a function as well),or NULL (see below).
constraint matrix (k x p), see below.
constraint vector of length k (see below).
(Small) tuning parameter.
passed to optim.
iterations of the barrier algorithm.
non-negative number; the relative convergencetolerance of the barrier algorithm.
Other named arguments to be passed to f and grad:needs to be passed through optim so should not match itsargument names.
a vector of levels for a factor, or the number of levels.
a logical indicating whether contrasts should becomputed.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
the set of values over which orthogonal polynomials areto be computed.
an integer specifying which group is considered thebaseline group. Ignored if contrasts is FALSE.
a vector of levels for a factor, or the number of levels.
a logical indicating whether contrasts should becomputed.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
the set of values over which orthogonal polynomials areto be computed.
an integer specifying which group is considered thebaseline group. Ignored if contrasts is FALSE.
a vector of levels for a factor, or the number of levels.
a logical indicating whether contrasts should becomputed.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
the set of values over which orthogonal polynomials areto be computed.
an integer specifying which group is considered thebaseline group. Ignored if contrasts is FALSE.
a vector of levels for a factor, or the number of levels.
a logical indicating whether contrasts should becomputed.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
the set of values over which orthogonal polynomials areto be computed.
an integer specifying which group is considered thebaseline group. Ignored if contrasts is FALSE.
a vector of levels for a factor, or the number of levels.
a logical indicating whether contrasts should becomputed.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
the set of values over which orthogonal polynomials areto be computed.
an integer specifying which group is considered thebaseline group. Ignored if contrasts is FALSE.
a factor or a logical variable.
logical.  See ‘Details’.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
How many contrasts should be made. Defaults to oneless than the number of levels of x.  This need not be thesame as the number of columns of value.
either a numeric matrix (or a sparse or dense matrix of aclass extending dMatrix frompackage Matrix)  whose columns give coefficients forcontrasts in the levels of x, or the (quoted) name of afunction which computes such matrices.
a factor or a logical variable.
logical.  See ‘Details’.
logical indicating if the result should be sparse(of class dgCMatrix), usingpackage Matrix.
How many contrasts should be made. Defaults to oneless than the number of levels of x.  This need not be thesame as the number of columns of value.
either a numeric matrix (or a sparse or dense matrix of aclass extending dMatrix frompackage Matrix)  whose columns give coefficients forcontrasts in the levels of x, or the (quoted) name of afunction which computes such matrices.
numeric sequences of the same length to beconvolved.
logical; if TRUE, take the complex conjugatebefore back-transforming (default, and used for usual convolution).
character; partially matched to "circular", "open","filter".  For "circular", thetwo sequences are treated as circular, i.e., periodic.For "open" and "filter", the sequences are padded with0s (from left and right) first; "filter" returns themiddle sub-vector of "open", namely, the result of running aweighted mean of x with weights y.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
an R object representing a hierarchical clustering.For the default method, an object of class "hclust" orwith a method for as.hclust() such as"agnes" in package cluster.
a numeric vector, matrix or data frame.
NULL (default) or a vector, matrix or data frame withcompatible dimensions to x.   The default is equivalent toy = x (but more efficient).
logical. Should missing values be removed?
an optional character string giving amethod for computing covariances in the presenceof missing values.  This must be (an abbreviation of) one of the strings"everything", "all.obs", "complete.obs","na.or.complete", or "pairwise.complete.obs".
a character string indicating which correlationcoefficient (or covariance) is to be computed.  One of"pearson" (default), "kendall", or "spearman":can be abbreviated.
symmetric numeric matrix, usually positive definite such as acovariance matrix.
numeric vectors of data values.  x and ymust have the same length.
indicates the alternative hypothesis and must beone of "two.sided", "greater" or "less".  Youcan specify just the initial letter.  "greater" correspondsto positive association, "less" to negative association.
a character string indicating which correlationcoefficient is to be  used for the test.  One of "pearson","kendall", or "spearman", can be abbreviated.
a logical indicating whether an exact p-value should becomputed.  Used for Kendall's tau andSpearman's rho.See ‘Details’ for the meaning of NULL (the default).
confidence level for the returned confidenceinterval.  Currently only used for the Pearson product momentcorrelation coefficient if there are at least 4 complete pairs ofobservations.
logical: if true, a continuity correction is usedfor Kendall's tau and Spearman's rho whennot computed exactly.
a formula of the form ~ u + v, where each ofu and v are numeric variables giving the data valuesfor one sample.  The samples must be of the same length.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
a numeric vector, matrix or data frame.
NULL (default) or a vector, matrix or data frame withcompatible dimensions to x.   The default is equivalent toy = x (but more efficient).
logical. Should missing values be removed?
an optional character string giving amethod for computing covariances in the presenceof missing values.  This must be (an abbreviation of) one of the strings"everything", "all.obs", "complete.obs","na.or.complete", or "pairwise.complete.obs".
a character string indicating which correlationcoefficient (or covariance) is to be computed.  One of"pearson" (default), "kendall", or "spearman":can be abbreviated.
symmetric numeric matrix, usually positive definite such as acovariance matrix.
a matrix or data frame.  As usual, rows are observations andcolumns are variables.
a non-negative and non-zero vector of weights for eachobservation.  Its length must equal the number of rows of x.
a logical indicating whether the estimated correlationweighted matrix will be returned as well.
either a logical or a numeric vector specifying thecenters to be used when computing covariances.  If TRUE, the(weighted) mean of each variable is used, if FALSE, zero isused.  If center is numeric, its length must equal the numberof columns of x.
string specifying how the result is scaled, see‘Details’ below.  Can be abbreviated.
a numeric vector, matrix or data frame.
NULL (default) or a vector, matrix or data frame withcompatible dimensions to x.   The default is equivalent toy = x (but more efficient).
logical. Should missing values be removed?
an optional character string giving amethod for computing covariances in the presenceof missing values.  This must be (an abbreviation of) one of the strings"everything", "all.obs", "complete.obs","na.or.complete", or "pairwise.complete.obs".
a character string indicating which correlationcoefficient (or covariance) is to be computed.  One of"pearson" (default), "kendall", or "spearman":can be abbreviated.
symmetric numeric matrix, usually positive definite such as acovariance matrix.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
a univariate time series
proportion tapered in forming the periodogram
main title
colour for confidence band.
a tree as produced by hclust. cutree()only expects a list with components merge, height, andlabels, of appropriate content each.
an integer scalar or vector with the desired number of groups
numeric scalar or vector with heights where the tree shouldbe cut.
a univariate or multivariate time-series, or a vector or matrix.
can be used to indicate when sampling took placein the time unit. 0 (the default) indicates the startof the unit, 0.5 the middle and 1 the end ofthe interval.
extra arguments for future methods.
a expression or call or(except D) a formula with no lhs.
character vector, giving the variable names (onlyone for D()) with respect to which derivatives will becomputed.
if specified and non-NULL, a charactervector of arguments for a function return, or a function (with emptybody) or TRUE, the latter indicating that a function withargument names namevec should be used.
character; the prefix to be used for the locally createdvariables in result.
a logical value indicating whether the second derivativesshould be calculated and incorporated in the return value.
arguments to be passed to or from methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
non-negative parameters of the Beta distribution.
non-centrality parameter.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
number of trials (zero or more).
probability of success on each trial.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (non-negative, but can be non-integer).
non-centrality parameter (non-negative).
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
A time series.
The type of seasonal component. Can be abbreviated.
A vector of filter coefficients in reverse time order (as forAR or MA coefficients), used for filtering out the seasonalcomponent.  If NULL, a moving average with symmetric window isperformed.
A terms object
character vector giving the right-hand side of amodel formula.  Cannot be zero-length.
character string, symbol or call giving the left-handside of a model formula, or NULL.
logical: should the formula have an intercept?
the environment of the formulareturned.
vector of positions of variables to drop from theright-hand side of the model.
Keep the response in the resulting object?
a univariate or multivariate time-series, or a vector or matrix.
can be used to indicate when sampling took placein the time unit. 0 (the default) indicates the startof the unit, 0.5 the middle and 1 the end ofthe interval.
extra arguments for future methods.
an object of class "dendrogram".
an R function to be applied to each dendrogram node,typically working on its attributes alone, returning analtered version of the same node.
potential further arguments passed to FUN.
the data from which the estimate is to be computed.  For thedefault method a numeric vector: long vectors are not supported.
the smoothing bandwidth to be used.  The kernels are scaledsuch that this is the standard deviation of the smoothing kernel.(Note this differs from the reference books cited below, and from S-PLUS.)bw can also be a character string giving a rule to choose thebandwidth.  See bw.nrd.  The default,"nrd0", has remained the default for historical andcompatibility reasons, rather than as a general recommendation,where e.g., "SJ" would rather fit, see also Venables andRipley (2002).The specified (or computed) value of bw is multiplied byadjust.
the bandwidth used is actually adjust*bw.This makes it easy to specify values like ‘half the default’bandwidth.
a character string giving the smoothing kernelto be used. This must partially match one of "gaussian","rectangular", "triangular", "epanechnikov","biweight", "cosine" or "optcosine", with default"gaussian", and may be abbreviated to a unique prefix (singleletter)."cosine" is smoother than "optcosine", which is theusual ‘cosine’ kernel in the literature and almost MSE-efficient.However, "cosine" is the version used by S.
numeric vector of non-negative observation weights,hence of same length as x. The default NULL isequivalent to weights = rep(1/nx, nx) where nx is thelength of (the finite entries of) x[].
this exists for compatibility with S; if given, andbw is not, will set bw to width if this is acharacter string, or to a kernel-dependent multiple of widthif this is numeric.
logical; if true, no density is estimated, andthe ‘canonical bandwidth’ of the chosen kernel is returnedinstead.
the number of equally spaced points at which the density isto be estimated.  When n > 512, it is rounded up to a powerof 2 during the calculations (as fft is used) and thefinal result is interpolated by approx.  So it almostalways makes sense to specify n as a power of two.
the left and right-most points of the grid at which thedensity is to be estimated; the defaults are cut * bw outsideof range(x).
by default, the values of from and to arecut bandwidths beyond the extremes of the data.  This allowsthe estimated density to drop to approximately zero at the extremes.
logical; if TRUE, missing values are removedfrom x. If FALSE any missing values cause an error.
further arguments for (non-default) methods.
the data from which the estimate is to be computed.  For thedefault method a numeric vector: long vectors are not supported.
the smoothing bandwidth to be used.  The kernels are scaledsuch that this is the standard deviation of the smoothing kernel.(Note this differs from the reference books cited below, and from S-PLUS.)bw can also be a character string giving a rule to choose thebandwidth.  See bw.nrd.  The default,"nrd0", has remained the default for historical andcompatibility reasons, rather than as a general recommendation,where e.g., "SJ" would rather fit, see also Venables andRipley (2002).The specified (or computed) value of bw is multiplied byadjust.
the bandwidth used is actually adjust*bw.This makes it easy to specify values like ‘half the default’bandwidth.
a character string giving the smoothing kernelto be used. This must partially match one of "gaussian","rectangular", "triangular", "epanechnikov","biweight", "cosine" or "optcosine", with default"gaussian", and may be abbreviated to a unique prefix (singleletter)."cosine" is smoother than "optcosine", which is theusual ‘cosine’ kernel in the literature and almost MSE-efficient.However, "cosine" is the version used by S.
numeric vector of non-negative observation weights,hence of same length as x. The default NULL isequivalent to weights = rep(1/nx, nx) where nx is thelength of (the finite entries of) x[].
this exists for compatibility with S; if given, andbw is not, will set bw to width if this is acharacter string, or to a kernel-dependent multiple of widthif this is numeric.
logical; if true, no density is estimated, andthe ‘canonical bandwidth’ of the chosen kernel is returnedinstead.
the number of equally spaced points at which the density isto be estimated.  When n > 512, it is rounded up to a powerof 2 during the calculations (as fft is used) and thefinal result is interpolated by approx.  So it almostalways makes sense to specify n as a power of two.
the left and right-most points of the grid at which thedensity is to be estimated; the defaults are cut * bw outsideof range(x).
by default, the values of from and to arecut bandwidths beyond the extremes of the data.  This allowsthe estimated density to drop to approximately zero at the extremes.
logical; if TRUE, missing values are removedfrom x. If FALSE any missing values cause an error.
further arguments for (non-default) methods.
a expression or call or(except D) a formula with no lhs.
character vector, giving the variable names (onlyone for D()) with respect to which derivatives will becomputed.
if specified and non-NULL, a charactervector of arguments for a function return, or a function (with emptybody) or TRUE, the latter indicating that a function withargument names namevec should be used.
character; the prefix to be used for the locally createdvariables in result.
a logical value indicating whether the second derivativesshould be calculated and incorporated in the return value.
arguments to be passed to or from methods.
a expression or call or(except D) a formula with no lhs.
character vector, giving the variable names (onlyone for D()) with respect to which derivatives will becomputed.
if specified and non-NULL, a charactervector of arguments for a function return, or a function (with emptybody) or TRUE, the latter indicating that a function withargument names namevec should be used.
character; the prefix to be used for the locally createdvariables in result.
a logical value indicating whether the second derivativesshould be calculated and incorporated in the return value.
arguments to be passed to or from methods.
an object for which the deviance is desired.
additional optional argument.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of rates.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom.  Inf is allowed.
non-centrality parameter. If omitted the central F is assumed.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
the upper half of the smoothing kernel coefficients(including coefficient zero) or the name of a kernel(currently "daniell", "dirichlet", "fejer" or"modified.daniell").
the kernel dimension(s) if coef is a name.  When mhas length larger than one, it means the convolution ofkernels of dimension m[j], for j in 1:length(m).Currently this is supported only for the named "*daniell" kernels.
the name the kernel will be called.
the kernel order for a Fejer kernel.
a "tskernel" object.
arguments passed toplot.default.
an object for which the degrees-of-freedom are desired.
additional optional arguments.
R object, for DF2formula() a data.frame.
further arguments passed to or from other methods.
the environment to associate with the result, if notalready a formula.
logical indicating if the environment should be printedas well.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
an alternative way to specify the scale.
shape and scale parameters.  Must be positive,scale strictly.
logical; if TRUE, probabilities/densities pare returned as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of failures ina sequence of Bernoulli trials before success occurs.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
probability of success in each trial. 0 < prob <= 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of white ballsdrawn without replacement from an urn which contains both black andwhite balls.
the number of white balls in the urn.
the number of black balls in the urn.
the number of balls drawn from the urn, hence must be in0,1,…, m+n.
probability, it must be between 0 and 1.
number of observations.  If length(nn) > 1, the lengthis taken to be the number required.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
a numeric vector, matrix, or time series.
a scalar lag parameter.
an integer representing the order of thedifference.
a numeric vector, matrix, or time series containing theinitial values for the integrals.  If missing, zeros are used.
arguments passed to or from other methods.
a numeric matrix, data frame or "dist" object.
the distance measure to be used.  This must be one of"euclidean", "maximum", "manhattan","canberra", "binary" or "minkowski".Any unambiguous substring can be given.
logical value indicating whether the diagonal of thedistance matrix should be printed by print.dist.
logical value indicating whether the upper triangle of thedistance matrix should be printed by print.dist.
The power of the Minkowski distance.
An object with distance information to be converted to a"dist" object.  For the default method, a "dist"object, or a matrix (of distances) or an object which can be coercedto such a matrix using as.matrix().  (Only the lowertriangle of the matrix is used, the rest is ignored).
passed to format inside ofprint().
further arguments, passed to other methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
mean and standard deviation of the distributionon the log scale with default values of 0 and 1 respectively.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of length K of integers in 0:size.
number of random vectors to draw.
integer, say N, specifying the total numberof objects that are put into K boxes in the typical multinomialexperiment. For dmultinom, it defaults to sum(x).
numeric non-negative vector of length K, specifyingthe probability for the K classes; is internally normalized tosum 1. Infinite and missing values are not allowed.
logical; if TRUE, log probabilities are computed.
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of observations.  If length(n) > 1, the lengthis taken to be the number required.
target for number of successful trials, or dispersionparameter (the shape parameter of the gamma mixing distribution).Must be strictly positive, need not be integer.
probability of success in each trial. 0 < prob <= 1.
alternative parametrization via mean: see ‘Details’.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of means.
vector of standard deviations.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x] otherwise, P[X > x].
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of random values to return.
vector of (non-negative) means.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
the terms or formula for the base model.
the terms or formula for the upper (add.scope) orlower (drop.scope) scope. If missing for drop.scope it istaken to be the null formula, so all terms (except any intercept) arecandidates to be dropped.
the "factor" attribute of the terms of the base object.
a list with one or both components drop andadd giving the "factor" attribute of the lower andupper scopes respectively.
A terms object
character vector giving the right-hand side of amodel formula.  Cannot be zero-length.
character string, symbol or call giving the left-handside of a model formula, or NULL.
logical: should the formula have an intercept?
the environment of the formulareturned.
vector of positions of variables to drop from theright-hand side of the model.
Keep the response in the resulting object?
a fitted model object.
a formula giving the terms to be considered for adding ordropping.
an estimate of the residual mean square to beused in computing Cp. Ignored if 0 or NULL.
should the results include a test statistic relative to theoriginal model?  The F test is only appropriate for lm andaov models or perhaps for glm fits withestimated dispersion.The Chisq test can be an exact test(lm models with known scale) or a likelihood-ratio test or atest of the reduction in scaled deviance depending on the method.For glm fits, you can also choose "LRT" and"Rao" for likelihood ratio tests and Rao's efficient score test.The former is synonymous with "Chisq" (although both havean asymptotic chi-square distribution).Values can be abbreviated.
the penalty constant in AIC / Cp.
if TRUE, print out progress reports.
a model matrix containing columns for the fitted model and allterms in the upper scope.  Useful if add1 is to be calledrepeatedly.  Warning: no checks are done on its validity.
(Provided for compatibility with S.)  Logical to specifywhether all columns of the design matrix should be used.  IfFALSE then non-estimable columns are dropped, but the resultis not usually statistically meaningful.
further arguments passed to or from other methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
number(s) of observations in the sample(s).  A positiveinteger, or a vector of such integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (> 0, maybe non-integer).  df      = Inf is allowed.
non-centrality parameter delta;currently except for rt(), only for abs(ncp) <= 37.62.If omitted, use the central t distribution.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
a linear model fit.
logical flag for coefficients in a singular model. Ifuse.na is true, undetermined coefficients will be missing; iffalse they will get one possible value.
arguments passed to or from other methods.
a linear model fit.
logical flag for coefficients in a singular model. Ifuse.na is true, undetermined coefficients will be missing; iffalse they will get one possible value.
arguments passed to or from other methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
lower and upper limits of the distribution.  Must be finite.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
shape and scale parameters, the latter defaulting to 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
numbers of observations in the first and second sample,respectively.  Can be vectors of positive integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
numeric vector of the observations for ecdf;  forthe methods, an object inheriting from class "ecdf".
arguments to be passed to subsequent methods, e.g.,plot.stepfun for the plot method.
label for the y-axis.
see plot.stepfun.
numeric or character specifying the color of thehorizontal lines at y = 0 and 1, see colors.
plotting character.
number of significant digits to use, seeprint.
The result of a call to aov with an Error term.
an R object; typically, the result of a model fitting functionsuch as lm.
logical. If TRUE, the sign of the effectscorresponding to coefficients in the model will be set to agree with thesigns of the corresponding coefficients, otherwise the sign isarbitrary.
arguments passed to or from other methods.
a numeric vector, matrix, or time series.
a scalar representing the embedding dimension.
a univariate or multivariate time-series, or a vector or matrix.
extra arguments for future methods.
object of class "mlm", or "SSD" inthe case of estVar.
Unused
a fitted model
one-sided formula or vector of character stringsdescribing new variables to be added
an environment to evaluate things in
logical; see below
fitted model, usually the result of a fitter likelm.
optional numeric specifying the scale parameter of themodel, see scale in step.  Currently only usedin the "lm" method, where scale specifies the estimateof the error variance, and scale = 0 indicates that it is tobe estimated by maximum likelihood.
numeric specifying the ‘weight’ of theequivalent degrees of freedom (=: edf)part in the AIC formula.
further arguments (currently unused in base R).
A formula or a numeric matrix or an object that can becoerced to a numeric matrix.
The number of factors to be fitted.
An optional data frame (or similar: seemodel.frame), used only if x is a formula.  Bydefault the variables are taken from environment(formula).
A covariance matrix, or a covariance list as returned bycov.wt.  Of course, correlation matrices are covariancematrices.
The number of observations, used if covmat is acovariance matrix.
A specification of the cases to be used, if x isused as a matrix or formula.
The na.action to be used if x isused as a formula.
NULL or a matrix of starting values, each columngiving an initial set of uniquenesses.
Type of scores to produce, if any.  The default is none,"regression" gives Thompson's scores, "Bartlett" givenBartlett's weighted least-squares scores. Partial matching allowsthese names to be abbreviated.
character. "none" or the name of a functionto be used to rotate the factors: it will be called with firstargument the loadings matrix, and should return a list with componentloadings giving the rotated loadings, or just the rotated loadings.
A list of control values,nstartThe number of starting values to be tried ifstart = NULL. Default 1.tracelogical. Output tracing information? Default FALSE.lowerThe lower bound for uniquenesses duringoptimization. Should be > 0. Default 0.005.optA list of control values to be passed tooptim's control argument.rotatea list of additional arguments for the rotation function.
Components of control can also be supplied asnamed arguments to factanal.
the terms or formula for the base model.
the terms or formula for the upper (add.scope) orlower (drop.scope) scope. If missing for drop.scope it istaken to be the null formula, so all terms (except any intercept) arecandidates to be dropped.
the "factor" attribute of the terms of the base object.
a list with one or both components drop andadd giving the "factor" attribute of the lower andupper scopes respectively.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
a real or complex array containing the values to betransformed.  Long vectors are not supported.
if TRUE, the unnormalized inverse transform iscomputed (the inverse has a + in the exponent of e,but here, we do not divide by 1/length(x)).
a univariate or multivariate time series.
a vector of filter coefficients in reverse time order(as for AR or MA coefficients).
Either "convolution" or "recursive" (andcan be abbreviated). If "convolution" a moving average isused: if "recursive" an autoregression is used.
for convolution filters only. If sides = 1 thefilter coefficients are for past values only; if sides = 2they are centred around lag 0.  In this case the length of thefilter should be odd, but if it is even, more of the filteris forward in time than backward.
for convolution filters only.  If TRUE, wrapthe filter around the ends of the series, otherwise assumeexternal values are missing (NA).
for recursive filters only. Specifies the initial valuesof the time series just prior to the start value, in reversetime order. The default is a set of zeros.
either a two-dimensional contingency table in matrix form,or a factor object.
a factor object; ignored if x is a matrix.
an integer specifying the size of the workspaceused in the network algorithm.  In units of 4 bytes.  Only used fornon-simulated p-values larger than 2 by 2 tables.Since R version 3.5.0, this also increases the internal stack sizewhich allows larger problems to be solved, however sometimes needinghours.  In such cases, simulate.p.values=TRUE may be morereasonable.
a logical. Only used for larger than 2 by 2tables, in which cases it indicates whether the exact probabilities(default) or a hybrid approximation thereof should be computed.
a numeric vector of length 3, by default describing“Cochran's conditions” for the validity of the chisquareapproximation, see ‘Details’.
a list with named components for low level algorithmcontrol.  At present the only one used is "mult", a positiveinteger ≥ 2 with default 30 used only for larger than2 by 2 tables.  This says how many times as muchspace should be allocated to paths as to keys: see file‘fexact.c’ in the sources of this package.
the hypothesized odds ratio.  Only used in the2 by 2 case.
indicates the alternative hypothesis and must beone of "two.sided", "greater" or "less".You can specify just the initial letter.  Only used in the2 by 2 case.
logical indicating if a confidence interval for theodds ratio in a 2 by 2 table should becomputed (and returned).
confidence level for the returned confidenceinterval.  Only used in the 2 by 2 case and ifconf.int = TRUE.
a logical indicating whether to computep-values by Monte Carlo simulation, in larger than 2 by 2 tables.
an integer specifying the number of replicates used in theMonte Carlo test.
an object for which the extraction of model fitted values ismeaningful.
other arguments.
an object for which the extraction of model fitted values ismeaningful.
other arguments.
numeric, maybe including NAs and+/-Infs.
logical; if TRUE, all NA andNaNs are dropped, before the statistics are computed.
a numeric vector of data values, or a list of numeric datavectors.
a vector or factor object giving the group for thecorresponding elements of x.Ignored if x is a list.
a formula of the form lhs ~ rhs where lhsgives the data values and rhs the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
R object, for DF2formula() a data.frame.
further arguments passed to or from other methods.
the environment to associate with the result, if notalready a formula.
logical indicating if the environment should be printedas well.
a univariate or multivariate time-series, or a vector or matrix.
can be used to indicate when sampling took placein the time unit. 0 (the default) indicates the startof the unit, 0.5 the middle and 1 the end ofthe interval.
extra arguments for future methods.
either a numeric vector of data values, or a data matrix.
a vector giving the group for the correspondingelements of y if this is a vector;  ignored if yis a matrix.  If not a factor object, it is coerced to one.
a vector giving the block for the correspondingelements of y if this is a vector;  ignored if yis a matrix.  If not a factor object, it is coerced to one.
a formula of the form a ~ b | c, where a,b and c give the data values and corresponding groupsand blocks, respectively.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
R objects which can be interpreted as factors (includingcharacter strings), or a list (or data frame) whose components canbe so interpreted, or a contingency table object of class"table" or "ftable".
values to use in the exclude argument of factorwhen interpreting non-factor objects.
a vector of integers giving the numbers of thevariables, or a character vector giving the names of the variablesto be used for the rows of the flat contingency table.
a vector of integers giving the numbers of thevariables, or a character vector giving the names of the variablesto be used for the columns of the flat contingency table.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
a model formula or termsobject or an R object.
a data.frame, list or environment (or objectcoercible by as.data.frame to a data.frame),containing the variables in formula.  Neither a matrix nor anarray will be accepted.
a specification of the rows to be used: defaults to allrows. This can be any valid indexing vector (see[.data.frame) for the rows of data or if that is notsupplied, a data frame made up of the variables used in formula.
how NAs are treated.  The default is first,any na.action attribute of data, seconda na.action setting of options, and thirdna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Another possible value is NULL.
should factors have unused levels dropped?Defaults to FALSE.
a named list of character vectors giving the full set of levelsto be assumed for each factor.
for model.frame methods, a mix of furtherarguments such as data, na.action, subset to passto the default method.  Any additional arguments (such asoffset and weights or other named arguments) whichreach the default method are used to create further columns in themodel frame, with parenthesised names such as "(offset)".For get_all_vars, further named columns to includein the model frame.
An existing fit from a model function such as lm,glm and many others.
Changes to the formula – see update.formula fordetails.
Additional arguments to the call, or arguments withchanged values. Use name = NULL to remove the argument name.
If true evaluate the new call else return the call.
a formula or a selfStart model that defines anonlinear regression model
a data frame in which the expressions in the formula orarguments to the selfStart model can be evaluated
optional additional arguments
an object of class "formula" (or one thatcan be coerced to that class): a symbolic description of themodel to be fitted.  The details of model specification are givenunder ‘Details’.
a description of the error distribution and linkfunction to be used in the model.  For glm this can be acharacter string naming a family function, a family function or theresult of a call to a family function.  For glm.fit only thethird option is supported.  (See family for details offamily functions.)
an optional data frame, list or environment (or objectcoercible by as.data.frame to a data frame) containingthe variables in the model.  If not found in data, thevariables are taken from environment(formula),typically the environment from which glm is called.
an optional vector of ‘prior weights’ to be usedin the fitting process.  Should be NULL or a numeric vector.
an optional vector specifying a subset of observationsto be used in the fitting process.
a function which indicates what should happenwhen the data contain NAs.  The default is set bythe na.action setting of options, and isna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Another possible value isNULL, no action.  Value na.exclude can be useful.
starting values for the parameters in the linear predictor.
starting values for the linear predictor.
starting values for the vector of means.
this can be used to specify an a priori knowncomponent to be included in the linear predictor during fitting.This should be NULL or a numeric vector of length equal tothe number of cases.  One or more offset terms can beincluded in the formula instead or as well, and if more than one isspecified their sum is used.  See model.offset.
a list of parameters for controlling the fittingprocess.  For glm.fit this is passed toglm.control.
a logical value indicating whether model frameshould be included as a component of the returned value.
the method to be used in fitting the model.  The defaultmethod "glm.fit" uses iteratively reweighted least squares(IWLS): the alternative "model.frame" returns the model frameand does no fitting.User-supplied fitting functions can be supplied either as a functionor a character string naming a function, with a function which takesthe same arguments as glm.fit.  If specified as a characterstring it is looked up from within the stats namespace.
For glm:logical values indicating whether the response vector and modelmatrix used in the fitting process should be returned as componentsof the returned value.For glm.fit: x is a design matrix of dimensionn * p, and y is a vector of observations of lengthn.
logical; if FALSE a singular fit is anerror.
an optional list. See the contrasts.argof model.matrix.default.
logical. Should an intercept be included in thenull model?
an object inheriting from class "glm".
character, partial matching allowed.  Type of weights toextract from the fitted model object.  Can be abbreviated.
For glm: arguments to be used to form the defaultcontrol argument if it is not supplied directly.For weights: further arguments passed to or from other methods.
positive convergence tolerance ε;the iterations converge when|dev - dev_{old}|/(|dev| + 0.1) < ε.
integer giving the maximal number of IWLS iterations.
logical indicating if output should be produced for eachiteration.
an object of class "formula" (or one thatcan be coerced to that class): a symbolic description of themodel to be fitted.  The details of model specification are givenunder ‘Details’.
a description of the error distribution and linkfunction to be used in the model.  For glm this can be acharacter string naming a family function, a family function or theresult of a call to a family function.  For glm.fit only thethird option is supported.  (See family for details offamily functions.)
an optional data frame, list or environment (or objectcoercible by as.data.frame to a data frame) containingthe variables in the model.  If not found in data, thevariables are taken from environment(formula),typically the environment from which glm is called.
an optional vector of ‘prior weights’ to be usedin the fitting process.  Should be NULL or a numeric vector.
an optional vector specifying a subset of observationsto be used in the fitting process.
a function which indicates what should happenwhen the data contain NAs.  The default is set bythe na.action setting of options, and isna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Another possible value isNULL, no action.  Value na.exclude can be useful.
starting values for the parameters in the linear predictor.
starting values for the linear predictor.
starting values for the vector of means.
this can be used to specify an a priori knowncomponent to be included in the linear predictor during fitting.This should be NULL or a numeric vector of length equal tothe number of cases.  One or more offset terms can beincluded in the formula instead or as well, and if more than one isspecified their sum is used.  See model.offset.
a list of parameters for controlling the fittingprocess.  For glm.fit this is passed toglm.control.
a logical value indicating whether model frameshould be included as a component of the returned value.
the method to be used in fitting the model.  The defaultmethod "glm.fit" uses iteratively reweighted least squares(IWLS): the alternative "model.frame" returns the model frameand does no fitting.User-supplied fitting functions can be supplied either as a functionor a character string naming a function, with a function which takesthe same arguments as glm.fit.  If specified as a characterstring it is looked up from within the stats namespace.
For glm:logical values indicating whether the response vector and modelmatrix used in the fitting process should be returned as componentsof the returned value.For glm.fit: x is a design matrix of dimensionn * p, and y is a vector of observations of lengthn.
logical; if FALSE a singular fit is anerror.
an optional list. See the contrasts.argof model.matrix.default.
logical. Should an intercept be included in thenull model?
an object inheriting from class "glm".
character, partial matching allowed.  Type of weights toextract from the fitted model object.  Can be abbreviated.
For glm: arguments to be used to form the defaultcontrol argument if it is not supplied directly.For weights: further arguments passed to or from other methods.
a vector or matrix or univariate or multivariate time-series.
a numeric vector of length 3 or NULL.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
a dissimilarity structure as produced by dist.
the agglomeration method to be used.  This shouldbe (an unambiguous abbreviation of) one of"ward.D", "ward.D2", "single", "complete","average" (= UPGMA), "mcquitty" (= WPGMA),"median" (= WPGMC) or "centroid" (= UPGMC).
NULL or a vector with length size ofd. See the ‘Details’ section.
an object of the type produced by hclust.
The fraction of the plot height by which labels should hangbelow the rest of the plot.A negative value will cause the labels to hang down from 0.
logical indicating if the x object should bechecked for validity.  This check is not necessary when xis known to be valid such as when it is the direct result ofhclust().  The default is check=TRUE, as invalidinputs may crash R due to memory violation in the internal Cplotting code.
A character vector of labels for the leaves of thetree.  By default the row names or row numbers of the original data areused.  If labels = FALSE no labels at all are plotted.
logical flags as in plot.default.
character strings fortitle.  sub and xlab have a non-NULLdefault when there's a tree$call.
Further graphical arguments.  E.g., cex controlsthe size of the labels (if plotted) in the same way as text.
numeric matrix of the values to be plotted. 
determines if and how the row dendrogram should becomputed and reordered.  Either a dendrogram or avector of values used to reorder the row dendrogram orNA to suppress any row dendrogram (and reordering) orby default, NULL, see ‘Details’ below.
determines if and how the column dendrogram should bereordered.  Has the same options as the Rowv argument above andadditionally when x is a square matrix, Colv =      "Rowv" means that columns should be treated identically to therows (and so if there is to be no row dendrogram there will not be acolumn one either).
function used to compute the distance (dissimilarity)between both rows and columns.  Defaults to dist.
function used to compute the hierarchical clusteringwhen Rowv or Colv are not dendrograms.  Defaults tohclust. Should take as argument a result of distfunand return an object to which as.dendrogram can be applied.
function(d, w) of dendrogram and weights forreordering the row and column dendrograms.  The default usesreorder.dendrogram.
expression that will be evaluated after the call toimage.  Can be used to add components to the plot.
logical indicating if x should be treatedsymmetrically; can only be true when x is a square matrix.
logical indicating if the column order should bereversed for plotting, such that e.g., for thesymmetric case, the symmetry axis is as usual.
character indicating if the values should be centered andscaled in either the row direction or the column direction, ornone.  The default is "row" if symm false, and"none" otherwise.
logical indicating whether NA's should be removed.
numeric vector of length 2 containing the margins(see par(mar = *)) for column and row names, respectively.
(optional) character vector of length ncol(x)containing the color names for a horizontal side bar that may be used toannotate the columns of x.
(optional) character vector of length nrow(x)containing the color names for a vertical side bar that may be used toannotate the rows of x.
positive numbers, used as cex.axis infor the row or column axis labeling.  The defaults currently onlyuse number of rows or columns, respectively.
character vectors with row and column labels touse; these default to rownames(x) or colnames(x),respectively.
main, x- and y-axis titles; defaults to none.
logical indicating if the dendrogram(s) should bekept as part of the result (when Rowv and/or Colv arenot NA).
logical indicating if information should be printed.
additional arguments passed on to image,e.g., col specifying the colors.
An object of class ts
alpha parameter of Holt-Winters Filter.
beta parameter of Holt-Winters Filter. If set toFALSE, the function will do exponential smoothing.
gamma parameter used for the seasonal component.If set to FALSE, an non-seasonal model is fitted.
Character string to select an "additive"(the default) or "multiplicative" seasonal model. The firstfew characters are sufficient. (Only takes effect ifgamma is non-zero).
Start periods used in the autodetection of startvalues. Must be at least 2.
Start value for level (a[0]).
Start value for trend (b[0]).
Vector of start values for the seasonal component(s_1[0] … s_p[0])
Vector with named components alpha,beta, and gamma containing the starting values for theoptimizer. Only the values needed must be specified.  Ignored in theone-parameter case.
Optional list with additional control parameterspassed to optim if this is used.  Ignored in theone-parameter case.
an object as returned by lm or glm.
logical indicating if the changed coefficients(see below) are desired.  These need O(n^2 p) computing time.
further arguments passed to or from other methods.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
an R function taking a numeric first argument and returninga numeric vector of the same length.  Returning a non-finite element willgenerate an error.
the limits of integration.  Can be infinite.
additional arguments to be passed to f.
the maximum number of subintervals.
relative accuracy requested.
absolute accuracy requested.
logical. If true (the default) an error stops thefunction.  If false some errors will give a result with a warning inthe message component.
unused.  For compatibility with S.
unused.  For compatibility with S.
a factor whose levels will form the x axis.
another factor whose levels will form the traces.
a numeric variable giving the response
the function to compute the summary. Should return a singlereal value.
the type of plot (see plot.default): linesor points or both.
logical. Should a legend be included?
overall label for the legend.
logical.  Should the legend be in the order of the levelsof trace.factor or in the order of the traces at their right-hand ends?
the x and y label of the plot each with a sensible default.
numeric of length 2 giving the y limits for the plot.
line type for the lines drawn, with sensible default.
the color to be used for plotting.
a vector of plotting symbols or characters, with sensibledefault.
determines clipping behaviour for the legendused, see par(xpd).  Per default, the legend isnot clipped at the figure border.
arguments passed to legend().
logical. Should tick marks be used on the x axis?
graphics parameters to be passed to the plotting routines.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
a numeric vector.
logical. Should missing values be removed?
an integer selecting one of the many quantile algorithms,see quantile.
A terms object or an object with a terms method.
any R object that can be made into one of class"dendrogram".
object(s) of class "dendrogram".
numeric scalar indicating how the height of leavesshould be computed from the heights of their parents; seeplot.hclust.
logical indicating if object should be checked forvalidity.  This check is not necessary when x is known to bevalid such as when it is the direct result of hclust().  Thedefault is check=TRUE, e.g. for protecting against memoryexplosion with invalid inputs.
type of plot.
logical; if TRUE, nodes are plotted centered withrespect to the leaves in the branch.  Otherwise (default), plot themin the middle of all direct child nodes.
logical; if true, draw an edge to the root node.
a list of plotting parameters to use for thenodes (see points) or NULL by default whichdoes not draw symbols at the nodes.  The list may contain componentsnamed pch, cex, col, xpd,and/or bg each ofwhich can have length two for specifying separate attributes forinner nodes and leaves.  Note that the default ofpch is 1:2, so you may want to use pch = NA ifyou specify nodePar.
a list of plotting parameters to use for theedge segments and labels (if there's anedgetext).  The list may contain componentsnamed col, lty and lwd (for the segments),p.col, p.lwd, and p.lty (for thepolygon around the text) and t.col for the textcolor.  As with nodePar, each can have length two fordifferentiating leaves and inner nodes.
a string specifying how leaves are labeled.  Thedefault "perpendicular" write text vertically (by default)."textlike" writes text horizontally (in a rectangle), and "none" suppresses leaf labels.
a number specifying the distance in usercoordinates between the tip of a leaf and its label.  If NULLas per default, 3/4 of a letter width or height is used.
logical indicating if the dendrogram should be drawnhorizontally or not.
logical indicating if a box around the plot shouldbe drawn, see plot.default.
height at which the tree is cut.
height at which the two dendrograms should be merged.  If notspecified (or NULL), the default is ten percent larger thanthe (larger of the) two component heights.
a string determining if the leaf values should beadjusted.  The default, "auto", checks if the (first) twodendrograms both start at 1; if they do, code"add.max" ischosen, which adds the maximum of the previous dendrogram leafvalues to each leaf of the “next” dendrogram.  Specifyingadjust to another value skips the check and hence is a tadmore efficient.
optional x- and y-limits of the plot, passed toplot.default.  The defaults for these show the fulldendrogram.
graphical parameters, or arguments forother methods.
integer specifying the precision for printing, seeprint.default.
argumentsto str, see str.default().  Note thatgive.attr = FALSE still shows height and membersattributes for each node.
strings used for str() specifying how thelast branch (at each level) should start and the stemto use for each dendrogram branch.  In some environments, usinglast.str = "'" will provide much nicer looking output, thanthe historical default last.str = "`".
a vector or matrix of the observed time-seriesvalues. A data frame will be coerced to a numeric matrix viadata.matrix.  (See also ‘Details’.)
the time of the first observation.  Either a singlenumber or a vector of two numbers (the second of which is an integer),which specify a natural timeunit and a (1-based) number of samples into the time unit.  Seethe examples for the use of the second form.
the time of the last observation, specified in the same wayas start.
the number of observations per unit of time.
the fraction of the sampling period between successiveobservations; e.g., 1/12 for monthly data.  Only one offrequency or deltat should be provided.
time series comparison tolerance.  Frequencies areconsidered equal if their absolute difference is less thants.eps.
class to be given to the result, or none if NULLor "none".  The default is "ts" for a single series,c("mts", "ts", "matrix") for multiple series.
a character vector of names for the series in a multipleseries: defaults to the colnames of data, or Series 1,Series 2, ....
an arbitrary R object.
arguments passed to methods (unused for the default method).
numeric vector giving the knots or jump locations of the stepfunction for stepfun().  For the other functions, x isas object below.
numeric vector one longer than x, giving the heights ofthe function values between the x values.
a number between 0 and 1, indicating how interpolation outsidethe given x values should happen.  See approxfun.
Handling of tied x values. Either a function orthe string "ordered".  See  approxfun.
logical, indicating if the intervals should be closed onthe right (and open on the left) or vice versa.
an R object inheriting from "stepfun".
number of significant digits to use, see print.
potentially further arguments (required by the generic).
a vector or matrix of the observed time-seriesvalues. A data frame will be coerced to a numeric matrix viadata.matrix.  (See also ‘Details’.)
the time of the first observation.  Either a singlenumber or a vector of two numbers (the second of which is an integer),which specify a natural timeunit and a (1-based) number of samples into the time unit.  Seethe examples for the use of the second form.
the time of the last observation, specified in the same wayas start.
the number of observations per unit of time.
the fraction of the sampling period between successiveobservations; e.g., 1/12 for monthly data.  Only one offrequency or deltat should be provided.
time series comparison tolerance.  Frequencies areconsidered equal if their absolute difference is less thants.eps.
class to be given to the result, or none if NULLor "none".  The default is "ts" for a single series,c("mts", "ts", "matrix") for multiple series.
a character vector of names for the series in a multipleseries: defaults to the colnames of data, or Series 1,Series 2, ....
an arbitrary R object.
arguments passed to methods (unused for the default method).
the upper half of the smoothing kernel coefficients(including coefficient zero) or the name of a kernel(currently "daniell", "dirichlet", "fejer" or"modified.daniell").
the kernel dimension(s) if coef is a name.  When mhas length larger than one, it means the convolution ofkernels of dimension m[j], for j in 1:length(m).Currently this is supported only for the named "*daniell" kernels.
the name the kernel will be called.
the kernel order for a Fejer kernel.
a "tskernel" object.
arguments passed toplot.default.
coordinate vectors of the regression points.  Alternatively a singleplotting structure can be specified: see xy.coords.
a univariate time series.
a list describing the state-space model: see ‘Details’.
the time at which the initialization is computed.nit = 0L implies that the initialization is for a one-stepprediction, so Pn should not be computed at the first step.
if TRUE the update mod object will bereturned as attribute "mod" of the result.
the number of steps ahead for which prediction isrequired.
numeric vectors of length ≥ 0 giving ARand MA parameters.
vector of differencing coefficients, so an ARMA model isfitted to y[t] - Delta[1]*y[t-1] - ....
the prior variance (as a multiple of the innovationsvariance) for the past observations in a differenced model.
a string specifying the algorithm to compute thePn part of the state-space initialization; see‘Details’.
tolerance eventually passed to solve.defaultwhen SSinit = "Rossignol2011".
a univariate time series.
a list describing the state-space model: see ‘Details’.
the time at which the initialization is computed.nit = 0L implies that the initialization is for a one-stepprediction, so Pn should not be computed at the first step.
if TRUE the update mod object will bereturned as attribute "mod" of the result.
the number of steps ahead for which prediction isrequired.
numeric vectors of length ≥ 0 giving ARand MA parameters.
vector of differencing coefficients, so an ARMA model isfitted to y[t] - Delta[1]*y[t-1] - ....
the prior variance (as a multiple of the innovationsvariance) for the past observations in a differenced model.
a string specifying the algorithm to compute thePn part of the state-space initialization; see‘Details’.
tolerance eventually passed to solve.defaultwhen SSinit = "Rossignol2011".
a univariate time series.
a list describing the state-space model: see ‘Details’.
the time at which the initialization is computed.nit = 0L implies that the initialization is for a one-stepprediction, so Pn should not be computed at the first step.
if TRUE the update mod object will bereturned as attribute "mod" of the result.
the number of steps ahead for which prediction isrequired.
numeric vectors of length ≥ 0 giving ARand MA parameters.
vector of differencing coefficients, so an ARMA model isfitted to y[t] - Delta[1]*y[t-1] - ....
the prior variance (as a multiple of the innovationsvariance) for the past observations in a differenced model.
a string specifying the algorithm to compute thePn part of the state-space initialization; see‘Details’.
tolerance eventually passed to solve.defaultwhen SSinit = "Rossignol2011".
a univariate time series.
a list describing the state-space model: see ‘Details’.
the time at which the initialization is computed.nit = 0L implies that the initialization is for a one-stepprediction, so Pn should not be computed at the first step.
if TRUE the update mod object will bereturned as attribute "mod" of the result.
the number of steps ahead for which prediction isrequired.
numeric vectors of length ≥ 0 giving ARand MA parameters.
vector of differencing coefficients, so an ARMA model isfitted to y[t] - Delta[1]*y[t-1] - ....
the prior variance (as a multiple of the innovationsvariance) for the past observations in a differenced model.
a string specifying the algorithm to compute thePn part of the state-space initialization; see‘Details’.
tolerance eventually passed to solve.defaultwhen SSinit = "Rossignol2011".
an input vector, matrix, time series or kernel to be smoothed.
smoothing "tskernel" object.
a logical indicating whether the input sequence to besmoothed is treated as circular, i.e., periodic.
arguments passed to or from other methods.
the upper half of the smoothing kernel coefficients(including coefficient zero) or the name of a kernel(currently "daniell", "dirichlet", "fejer" or"modified.daniell").
the kernel dimension(s) if coef is a name.  When mhas length larger than one, it means the convolution ofkernels of dimension m[j], for j in 1:length(m).Currently this is supported only for the named "*daniell" kernels.
the name the kernel will be called.
the kernel order for a Fejer kernel.
a "tskernel" object.
arguments passed toplot.default.
numeric matrix of data, or an object that can be coerced tosuch a matrix (such as a numeric vector or a data frame with allnumeric columns).
either the number of clusters, say k, or a set ofinitial (distinct) cluster centres.  If a number, a random set of(distinct) rows in x is chosen as the initial centres.
the maximum number of iterations allowed.
if centers is a number, how many random setsshould be chosen?
character: may be abbreviated.  Note that"Lloyd" and "Forgy" are alternative names for onealgorithm.
an R object of class "kmeans", typically theresult ob of ob <- kmeans(..).
character: may be abbreviated. "centers" causesfitted to return cluster centers (one for each input point) and"classes" causes fitted to return a vector of classassignments.
logical or integer number, currently only used in thedefault method ("Hartigan-Wong"): if positive (or true),tracing information on the progress of the algorithm isproduced.  Higher values may produce more tracing information.
not used.
numeric vector giving the knots or jump locations of the stepfunction for stepfun().  For the other functions, x isas object below.
numeric vector one longer than x, giving the heights ofthe function values between the x values.
a number between 0 and 1, indicating how interpolation outsidethe given x values should happen.  See approxfun.
Handling of tied x values. Either a function orthe string "ordered".  See  approxfun.
logical, indicating if the intervals should be closed onthe right (and open on the left) or vice versa.
an R object inheriting from "stepfun".
number of significant digits to use, see print.
potentially further arguments (required by the generic).
a numeric vector of data values, or a list of numeric datavectors.  Non-numeric elements of a list will be coerced, with awarning.
a vector or factor object giving the group for thecorresponding elements of x.  Ignored with a warning ifx is a list.
a formula of the form response ~ group whereresponse gives the data values and group a vector orfactor of the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
a numeric vector of data values.
either a numeric vector of data values, or a character stringnaming a cumulative distribution function or an actual cumulativedistribution function such as pnorm.  Only continuous CDFsare valid.
parameters of the distribution specified (as a characterstring) by y.
indicates the alternative hypothesis and must beone of "two.sided" (default), "less", or"greater".  You can specify just the initial letter of thevalue, but the argument name must be given in full.See ‘Details’ for the meanings of the possible values.
NULL or a logical indicating whether an exactp-value should be computed.  See ‘Details’ for the meaning ofNULL.  Not available in the two-sample case for a one-sidedtest or if ties are present.
input x values.  Long vectors are supported.
input y values.  Long vectors are supported.
the kernel to be used.  Can be abbreviated.
the bandwidth. The kernels are scaled so that theirquartiles (viewed as probability densities) are at+/- 0.25*bandwidth.
the range of points to be covered in the output.
the number of points at which to evaluate the fit.
points at which to evaluate the smoothed fit.  Ifmissing, n.points are chosen uniformly to coverrange.x.  Long vectors are supported.
A vector or matrix or univariate or multivariate time series
The number of lags (in units of observations).
further arguments to be passed to or from methods.
time-series (univariate or multivariate)
number of lag plots desired, see arg set.lags.
the layout of multiple plots, basically the mfrowpar() argument.  The default uses about a squarelayout (see n2mfrow) such that all plots are on one page.
vector of positive integers allowing specification ofthe set of lags used; defaults to 1:lags.
character with a main header title to be done on the topof each page.
Aspect ratio to be fixed, see plot.default.
logical indicating if the x=y diagonal should be drawn.
color to be used for the diagonal if(diag).
plot type to be used, but see plot.ts aboutits restricted meaning.
outer margins, see par.
logical or NULL; if true, the user is asked toconfirm before a new page is started.
logical indicating if lines should be drawn.
logical indicating if labels should be used.
Further arguments to plot.ts.  Severalgraphical parameters are set in this function and so cannot bechanged: these include xlab, ylab, mgp,col.lab and font.lab: this also applies to thearguments xy.labels and xy.lines.
the arguments can be any way of specifying x-y pairs.  Seexy.coords.
positive integer specifying the number of“polishing” iterations.  Note that this was hard coded to1 in R versions before 3.5.0, and more importantly that suchsimple iterations may not converge, see Siegel's 9-point example.
an object of class "formula" (or one thatcan be coerced to that class): a symbolic description of themodel to be fitted.  The details of model specification are givenunder ‘Details’.
an optional data frame, list or environment (or objectcoercible by as.data.frame to a data frame) containingthe variables in the model.  If not found in data, thevariables are taken from environment(formula),typically the environment from which lm is called.
an optional vector specifying a subset of observationsto be used in the fitting process.
an optional vector of weights to be used in the fittingprocess.  Should be NULL or a numeric vector.If non-NULL, weighted least squares is used with weightsweights (that is, minimizing sum(w*e^2)); otherwiseordinary least squares is used.  See also ‘Details’,
a function which indicates what should happenwhen the data contain NAs.  The default is set bythe na.action setting of options, and isna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Another possible value isNULL, no action.  Value na.exclude can be useful.
the method to be used; for fitting, currently onlymethod = "qr" is supported; method = "model.frame" returnsthe model frame (the same as with model = TRUE, see below).
logicals.  If TRUE the correspondingcomponents of the fit (the model frame, the model matrix, theresponse, the QR decomposition) are returned.
logical. If FALSE (the default in S butnot in R) a singular fit is an error.
an optional list. See the contrasts.argof model.matrix.default.
this can be used to specify an a priori knowncomponent to be included in the linear predictor during fitting.This should be NULL or a numeric vector or matrix of extentsmatching those of the response.  One or more offset terms can beincluded in the formula instead or as well, and if more than one arespecified their sum is used.  See model.offset.
additional arguments to be passed to the low levelregression fitting functions (see below).
design matrix of dimension n * p.
vector of observations of length n, or a matrix withn rows.
vector of weights (length n) to be used in the fittingprocess for the wfit functions.  Weighted least squares isused with weights w, i.e., sum(w * e^2) is minimized.
(numeric of length n).  This can be used tospecify an a priori known component to be included in thelinear predictor during fitting.
currently, only method = "qr" is supported.
tolerance for the qr decomposition.  Defaultis 1e-7.
logical. If FALSE, a singular model is anerror.
currently disregarded.
an object as returned by lm or glm.
logical indicating if the changed coefficients(see below) are desired.  These need O(n^2 p) computing time.
further arguments passed to or from other methods.
design matrix of dimension n * p.
vector of observations of length n, or a matrix withn rows.
vector of weights (length n) to be used in the fittingprocess for the wfit functions.  Weighted least squares isused with weights w, i.e., sum(w * e^2) is minimized.
(numeric of length n).  This can be used tospecify an a priori known component to be included in thelinear predictor during fitting.
currently, only method = "qr" is supported.
tolerance for the qr decomposition.  Defaultis 1e-7.
logical. If FALSE, a singular model is anerror.
currently disregarded.
an object of class "factanal" or"princomp" or the loadings component of such anobject.
number of decimal places to use in printing uniquenessesand loadings.
loadings smaller than this (in absolute value) are suppressed.
logical. If true, the variables are sorted by theirimportance on each factor.  Each variable with any loading largerthan 0.5 (in modulus) is assigned to the factor with the largestloading, and the variables are printed in the order of the factorthey are assigned to, then those unassigned.
further arguments for other methods,ignored for loadings.
a formula specifying the numeric response andone to four numeric predictors (best specified via an interaction,but can also be specified additively).  Will be coerced to a formulaif necessary.
an optional data frame, list or environment (or objectcoercible by as.data.frame to a data frame) containingthe variables in the model.  If not found in data, thevariables are taken from environment(formula),typically the environment from which loess is called.
optional weights for each case.
an optional specification of a subset of the data to beused.
the action to be taken with missing values in theresponse or predictors.  The default is given bygetOption("na.action").
should the model frame be returned?
the parameter α which controls the degree ofsmoothing.
an alternative way to specify span, as theapproximate equivalent number of parameters to be used.
the degree of the polynomials to be used, normally 1 or2. (Degree 0 is also allowed, but see the ‘Note’.)
should any terms be fitted globally rather thanlocally?  Terms can be specified by name, number or as a logicalvector of the same length as the number of predictors.
for fits with more than one predictor anddegree = 2, should the quadratic term be dropped for particularpredictors?  Terms are specified in the same way as forparametric.
should the predictors be normalized to a common scaleif there is more than one?  The normalization used is to set the10% trimmed standard deviation to one.  Set to false for spatialcoordinate predictors and others known to be on a common scale.
if "gaussian" fitting is by least-squares, and if"symmetric" a re-descending M estimator is used with Tukey'sbiweight function.  Can be abbreviated.
fit the model or just extract the model frame.  Can be abbreviated.
control parameters: see loess.control.
control parameters can also be supplied directly(if control is not specified).
should the fitted surface be computed exactly("direct") or via interpolation from a kd tree?  Can be abbreviated.
should the statistics be computed exactly,approximately or not at all?  Exact computation can be very slow.Can be abbreviated.
Only for the (default) case (surface =     "interpolate", statistics = "approximate"): should the trace ofthe smoother matrix be computed exactly or approximately?  It is recommended to use the approximationfor more than about 1000 data points.  Can be abbreviated.
if interpolation is used this controls the accuracy of theapproximation via the maximum number of points in a  cell in the kdtree. Cells with more than floor(n*span*cell) points are subdivided.
the number of iterations used in robust fitting,i.e. only if family is "symmetric".
logical (or integer) determining if tracinginformation during the robust iterations (iterations>= 2) is produced.
further arguments which are ignored.
the x and y arguments provide the x and ycoordinates for the plot.  Any reasonable way of defining thecoordinates is acceptable.  See the function xy.coordsfor details.
smoothness parameter for loess.
degree of local polynomial used.
if "gaussian" fitting is by least-squares, and iffamily = "symmetric" a re-descending M estimator is used.Can be abbreviated.
label for x axis.
label for y axis.
the y limits of the plot.
number of points at which to evaluate the smoothcurve.
For scatter.smooth(), graphical parameters, passedto plot() only.  For loess.smooth, control parameterspassed to loess.control.
a list of arguments to be passed tolines().
any object from which a log-likelihood value, or acontribution to a log-likelihood value, can be extracted.
some methods for this generic function require additionalarguments.
an optional logical value.  If TRUE the restrictedlog-likelihood is returned, else, if FALSE, thelog-likelihood is returned.  Defaults to FALSE.
a contingency table to be fit, typically the output fromtable.
a list of vectors with the marginal totals to be fit.(Hierarchical) log-linear models can be specified in terms of thesemarginal totals which give the ‘maximal’ factor subsets containedin the model.  For example, in a three-factor model,list(c(1, 2), c(1, 3)) specifies a model which containsparameters for the grand mean, each factor, and the 1-2 and 1-3interactions, respectively (but no 2-3 or 1-2-3 interaction), i.e.,a model where factors 2 and 3 are independent conditional on factor1 (sometimes represented as ‘[12][13]’).The names of factors (i.e., names(dimnames(table))) may beused rather than numeric indices.
a starting estimate for the fitted table.  This optionalargument is important for incomplete tables with structural zerosin table which should be preserved in the fit.  In thiscase, the corresponding entries in start should be zero andthe others can be taken as one.
a logical indicating whether the fitted values should bereturned.
maximum deviation allowed between observed and fittedmargins.
maximum number of iterations.
a logical indicating whether the parameter values shouldbe returned.
a logical.  If TRUE, the number of iterations andthe final deviation are printed.
vectors giving the coordinates of the points in the scatter plot.Alternatively a single plotting structure can be specified – seexy.coords.
the smoother span. This gives the proportion of points inthe plot which influence the smooth at each value.Larger values give more smoothness.
the number of ‘robustifying’ iterations which should beperformed.Using smaller values of iter will make lowess run faster.
See ‘Details’.  Defaults to 1/100th of the rangeof x.
Typically the result of lsfit()
Typically the result of lsfit()
The number of significant digits used for printing
a logical indicating whether the result should also beprinted
a matrix whose rows correspond to cases and whose columnscorrespond to variables.
the responses, possibly a matrix if you want to fit multipleleft hand sides.
an optional vector of weights for performing weighted least squares.
whether or not an intercept term should be used.
the tolerance to be used in the matrix decomposition.
names to be used for the response variables.
a numeric vector.
Optionally, the centre: defaults to the median.
scale factor.
if TRUE then NA values are strippedfrom x before computation takes place.
if TRUE, compute the ‘lo-median’, i.e., for evensample size, do not average the two middle values, but take thesmaller one.
if TRUE, compute the ‘hi-median’, i.e., take thelarger of the two middle values for even sample size.
vector or matrix of data with, say, p columns.
mean vector of the distribution or second data vector oflength p or recyclable to that length.  If set toFALSE, the centering step is skipped.
covariance matrix (p x p) of the distribution.
logical.  If TRUE, cov is supposed tocontain the inverse of the covariance matrix.
passed to solve for computing the inverse ofthe covariance matrix (if inverted is false).
character; one of "logit","probit", "cauchit", "cloglog", "identity","log",  "sqrt",  "1/mu^2", "inverse".
a univariate time series.
a list describing the state-space model: see ‘Details’.
the time at which the initialization is computed.nit = 0L implies that the initialization is for a one-stepprediction, so Pn should not be computed at the first step.
if TRUE the update mod object will bereturned as attribute "mod" of the result.
the number of steps ahead for which prediction isrequired.
numeric vectors of length ≥ 0 giving ARand MA parameters.
vector of differencing coefficients, so an ARMA model isfitted to y[t] - Delta[1]*y[t-1] - ....
the prior variance (as a multiple of the innovationsvariance) for the past observations in a differenced model.
a string specifying the algorithm to compute thePn part of the state-space initialization; see‘Details’.
tolerance eventually passed to solve.defaultwhen SSinit = "Rossignol2011".
A variable.
The term in the formula, as a call.
Arguments to be passed to aov.
either a 3-dimensional contingency table in array form whereeach dimension is at least 2 and the last dimension corresponds tothe strata, or a factor object with at least 2 levels.
a factor object with at least 2 levels; ignored if xis an array.
a factor object with at least 2 levels identifying to whichstratum the corresponding elements in x and y belong;ignored if x is an array.
indicates the alternative hypothesis and must beone of "two.sided", "greater" or "less".You can specify just the initial letter.Only used in the 2 by 2 by K case.
a logical indicating whether to apply continuitycorrection when computing the test statistic.Only used in the 2 by 2 by K case.
a logical indicating whether the Mantel-Haenszel test orthe exact conditional test (given the strata margins) should becomputed.Only used in the 2 by 2 by K case.
confidence level for the returned confidenceinterval.Only used in the 2 by 2 by K case.
object of class SSD or mlm.
matrix to be proportional to.
transformation matrix. By default computed from M andX.
formula or matrix describing the outer projection (see below).
formula or matrix describing the inner projection (see below).
data frame describing intra-block design.
arguments to be passed to or from other methods.
either a two-dimensional contingency table in matrix form,or a factor object.
a factor object; ignored if x is a matrix.
a logical indicating whether to apply continuitycorrection when computing the test statistic.
an object for which a method has been defined, or anumeric vector containing the values whose median is to be computed.
a logical value indicating whether NAvalues should be stripped before the computation proceeds.
potentially further arguments for methods; not used inthe default method.
an object for which a method has been defined, or anumeric vector containing the values whose median is to be computed.
a logical value indicating whether NAvalues should be stripped before the computation proceeds.
potentially further arguments for methods; not used inthe default method.
a numeric matrix.
real number greater than 0. A tolerance for convergence:see ‘Details’.
the maximum number of iterations
logical. Should progress in convergence be reported?
logical. Should missing values be removed?
A model frame.
literal character string or name. The name of acomponent to extract, such as "weights", "subset".
One of "any", "numeric", "double".Using either of latter two coerces the result to have storage mode"double".
a model formula or termsobject or an R object.
a data.frame, list or environment (or objectcoercible by as.data.frame to a data.frame),containing the variables in formula.  Neither a matrix nor anarray will be accepted.
a specification of the rows to be used: defaults to allrows. This can be any valid indexing vector (see[.data.frame) for the rows of data or if that is notsupplied, a data frame made up of the variables used in formula.
how NAs are treated.  The default is first,any na.action attribute of data, seconda na.action setting of options, and thirdna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Another possible value is NULL.
should factors have unused levels dropped?Defaults to FALSE.
a named list of character vectors giving the full set of levelsto be assumed for each factor.
for model.frame methods, a mix of furtherarguments such as data, na.action, subset to passto the default method.  Any additional arguments (such asoffset and weights or other named arguments) whichreach the default method are used to create further columns in themodel frame, with parenthesised names such as "(offset)".For get_all_vars, further named columns to includein the model frame.
a model formula or termsobject or an R object.
a data.frame, list or environment (or objectcoercible by as.data.frame to a data.frame),containing the variables in formula.  Neither a matrix nor anarray will be accepted.
a specification of the rows to be used: defaults to allrows. This can be any valid indexing vector (see[.data.frame) for the rows of data or if that is notsupplied, a data frame made up of the variables used in formula.
how NAs are treated.  The default is first,any na.action attribute of data, seconda na.action setting of options, and thirdna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Another possible value is NULL.
should factors have unused levels dropped?Defaults to FALSE.
a named list of character vectors giving the full set of levelsto be assumed for each factor.
for model.frame methods, a mix of furtherarguments such as data, na.action, subset to passto the default method.  Any additional arguments (such asoffset and weights or other named arguments) whichreach the default method are used to create further columns in themodel frame, with parenthesised names such as "(offset)".For get_all_vars, further named columns to includein the model frame.
an object of an appropriate class.  For the defaultmethod, a model formula or a terms object.
a data frame created with model.frame.  Ifanother sort of object, model.frame is called first.
a list, whose entries are values (numericmatrices, functions or character strings namingfunctions) to be usedas replacement values for the contrastsreplacement function and whose names are the names ofcolumns of data containing factors.
to be used as argument of model.frame ifdata is such that model.frame is called.
further arguments passed to or from other methods.
an object of an appropriate class.  For the defaultmethod, a model formula or a terms object.
a data frame created with model.frame.  Ifanother sort of object, model.frame is called first.
a list, whose entries are values (numericmatrices, functions or character strings namingfunctions) to be usedas replacement values for the contrastsreplacement function and whose names are the names ofcolumns of data containing factors.
to be used as argument of model.frame ifdata is such that model.frame is called.
further arguments passed to or from other methods.
an object of an appropriate class.  For the defaultmethod, a model formula or a terms object.
a data frame created with model.frame.  Ifanother sort of object, model.frame is called first.
a list, whose entries are values (numericmatrices, functions or character strings namingfunctions) to be usedas replacement values for the contrastsreplacement function and whose names are the names ofcolumns of data containing factors.
to be used as argument of model.frame ifdata is such that model.frame is called.
further arguments passed to or from other methods.
A model frame.
literal character string or name. The name of acomponent to extract, such as "weights", "subset".
One of "any", "numeric", "double".Using either of latter two coerces the result to have storage mode"double".
A model frame.
literal character string or name. The name of acomponent to extract, such as "weights", "subset".
One of "any", "numeric", "double".Using either of latter two coerces the result to have storage mode"double".
a model object, usually produced by aov
type of table: currently only "effects" and"means" are implemented.  Can be abbreviated.
should standard errors be computed?
A character vector giving the names of the terms forwhich tables should be computed. The default is all tables.
further arguments passed to or from other methods.
A model frame.
literal character string or name. The name of acomponent to extract, such as "weights", "subset".
One of "any", "numeric", "double".Using either of latter two coerces the result to have storage mode"double".
Time series or related object.
Labels to use for each ‘season’.
y label.
Time of each observation.
Indicator for each ‘season’.
Function to use for reference line for subseries.
Which series of an stl or StructTS object?
Arguments to be passed to the default method orgraphical parameters.
Should axes be drawn (ignored if add = TRUE)?
Type of plot.  The default is to join the points withlines, and "h" is for histogram-like vertical lines.
Should a box be drawn (ignored if add = TRUE)?
Should thus just add on an existing plot.
Graphics parameters for the series.
Graphics parameters for thesegments used for the reference lines.
numeric vectors of data values.
indicates the alternative hypothesis and must beone of "two.sided" (default), "greater" or"less" all of which can be abbreviated.
a formula of the form lhs ~ rhs where lhsis a numeric variable giving the data values and rhs a factorwith two levels giving the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
a real or complex array containing the values to betransformed.  Long vectors are not supported.
if TRUE, the unnormalized inverse transform iscomputed (the inverse has a + in the exponent of e,but here, we do not divide by 1/length(x)).
any object whose NA action is given.
further arguments special methods could require.
a univariate or multivariate time series.
further arguments passed to or from other methods.
an R object, typically a data frame
further arguments special methods could require.
an R object, typically a data frame
further arguments special methods could require.
an R object, typically a data frame
further arguments special methods could require.
an R object, typically a data frame
further arguments special methods could require.
an object produced by an na.action function,typically the "na.action" attribute of the result ofna.omit or na.exclude.
a vector, data frame, or matrix to be adjusted based upon themissing value information.
further arguments passed to or from other methods.
An object produced by an na.action function.
further arguments passed to or from other methods.
an object produced by an na.action function,typically the "na.action" attribute of the result ofna.omit or na.exclude.
a vector, data frame, or matrix to be adjusted based upon themissing value information.
further arguments passed to or from other methods.
a vector of integer numbers (of type "integer"or "double").
a vector of positive integer factors (at least 2and preferably relative prime, see the note).
the function to be minimized, returning a single numericvalue.  This should be a function with first argument a vector ofthe length of p followed by any other arguments specified bythe ... argument.If the function value has an attribute called gradient orboth gradient and hessian attributes, these will beused in the calculation of updated parameter values.  Otherwise,numerical derivatives are used. deriv returns afunction with suitable gradient attribute and optionally ahessian attribute.
starting parameter values for the minimization.
additional arguments to be passed to f.
if TRUE, the hessian of fat the minimum is returned.
an estimate of the size of each parameterat the minimum.
an estimate of the size of f at the minimum.
this argument determines the level of printingwhich is done during the minimization process.  The defaultvalue of 0 means that no printing occurs, a value of 1means that initial and final details are printed and a valueof 2 means that full tracing information is printed.
the number of significant digits in the function f.
a positive scalar giving the tolerance at which thescaled gradient is considered close enough to zero toterminate the algorithm.  The scaled gradient is ameasure of the relative change in f in each directionp[i] divided by the relative change in p[i].
a positive scalar which gives the maximum allowablescaled step length.  stepmax is used to prevent steps whichwould cause the optimization function to overflow, to prevent thealgorithm from leaving the area of interest in parameter space, or todetect divergence in the algorithm. stepmax would be chosensmall enough to prevent the first two of these occurrences, but shouldbe larger than any anticipated reasonable step.
A positive scalar providing the minimum allowablerelative step length.
a positive integer specifying the maximum number ofiterations to be performed before the program is terminated.
a logical scalar specifying whether theanalytic gradients and Hessians, if they are supplied, should bechecked against numerical derivatives at the initial parametervalues. This can help detect incorrectly formulated gradients orHessians.
numeric vector, initial values for the parameters to be optimized.
Function to be minimized.  Must return a scalar value.  The firstargument to objective is the vector of parameters to beoptimized, whose initial values are supplied through start.Further arguments (fixed during the course of the optimization) toobjective may be specified as well (see ...).
Optional function that takes the same arguments as objective andevaluates the gradient of objective at its first argument.  Mustreturn a vector as long as start.
Optional function that takes the same arguments as objective andevaluates the hessian of objective at its first argument.  Mustreturn a square matrix of order length(start).  Only thelower triangle is used.
Further arguments to be supplied to objective.
See PORT documentation (or leave alone).
A list of control parameters. See below for details.
vectors of lower and upper bounds, replicated to be as long asstart.  If unspecified, all parameters are assumed to beunconstrained.
a nonlinear model formula including variables andparameters.  Will be coerced to a formula if necessary.
an optional data frame in which to evaluate the variables informula and weights.  Can also be a list or anenvironment, but not a matrix.
a named list or named numeric vector of startingestimates.  When start is missing (and formula is nota self-starting model, see selfStart), a very cheapguess for start is tried (if algorithm != "plinear").
an optional list of control settings.  Seenls.control for the names of the settable controlvalues and their effect.
character string specifying the algorithm to use.The default algorithm is a Gauss-Newton algorithm.  Other possiblevalues are "plinear" for the Golub-Pereyra algorithm forpartially linear least-squares models and "port" for the‘nl2sol’ algorithm from the Port library – see thereferences.  Can be abbreviated.
logical value indicating if a trace of the iterationprogress should be printed.  Default is FALSE.  IfTRUE the residual (weighted) sum-of-squares, the convergencecriterion and the parameter values are printed at the conclusion ofeach iteration.  Note that format() is used, so thesemostly depend on getOption("digits").When the "plinear" algorithm is used, the conditionalestimates of the linear parameters are printed after the nonlinearparameters.  When the "port" algorithm is used theobjective function value printed is half the residual (weighted)sum-of-squares.
an optional vector specifying a subset of observationsto be used in the fitting process.
an optional numeric vector of (fixed) weights.  Whenpresent, the objective function is weighted least squares.
a function which indicates what should happenwhen the data contain NAs.  The default is set bythe na.action setting of options, and isna.fail if that is unset.  The ‘factory-fresh’default is na.omit.  Value na.excludecan be useful.
logical.  If true, the model frame is returned as part ofthe object. Default is FALSE.
vectors of lower and upper bounds, replicated tobe as long as start.  If unspecified, all parameters areassumed to be unconstrained.  Bounds can only be used with the"port" algorithm.  They are ignored, with a warning, if givenfor other algorithms.
Additional optional arguments.  None are used at present.
A positive integer specifying the maximum number ofiterations allowed.
A positive numeric value specifying the tolerance level forthe relative offset convergence criterion.
A positive numeric value specifying the minimumstep-size factor allowed on any step in the iteration.  Theincrement is calculated with a Gauss-Newton algorithm andsuccessively halved until the residual sum of squares has beendecreased or until the step-size factor has been reduced below thislimit.
a logical specifying whether the number of evaluations(steps in the gradient direction taken each iteration) is printed.
a logical specifying whether nls() shouldreturn instead of signalling an error in the case of terminationbefore convergence.Termination before convergence happens upon completion of maxiteriterations, in the case of a singular gradient, and in the case that thestep-size factor is reduced below minFactor.
a constant to be added to the denominator of the relativeoffset convergence criterion calculation to avoid a zero divide in the casewhere the fit of a model to data is very close.  The default value of0 keeps the legacy behaviour of nls().  A value such as1 seems to work for problems of reasonable scale with very smallresiduals.
only when numerical derivatives are used:logical indicating if central differencesshould be employed, i.e., numericDeriv(*, central=TRUE)be used.
a sortedXyData object
a sortedXyData object
a numeric value on the y scale
a sortedXyData object
a sortedXyData object
A fitted model object.
logical: should fallback methods be used to try toguess the value?
Further arguments to be passed to methods.
expression or call to bedifferentiated.  Should evaluate to a numeric vector.
character vector of names of numeric variablesused in expr.
environment containing all the variables needed toevaluate expr.
numeric vector of directions, typically with values in-1, 1 to use for the finite differences;will be recycled to the length of theta.
a positive number, to be used as unit step size h forthe approximate numerical derivative  (f(x+h)-f(x))/h  or thecentral version, see central.
logical indicating if central divided differencesshould be computed, i.e.,  (f(x+h) - f(x-h)) / 2h .  These aretypically more accurate but need more evaluations of f().
An offset to be included in a model frame
a formula of the form lhs ~ rhs where lhsgives the sample values and rhs the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
a logical variable indicating whether to treat thevariances in the samples as equal.  If TRUE, then a simple Ftest for the equality of means in a one-way analysis of variance isperformed.  If FALSE, an approximate method of Welch (1951)is used, which generalizes the commonly known 2-sample Welch test tothe case of arbitrarily many samples.
Initial values for the parameters to be optimized over.
A function to be minimized (or maximized), with firstargument the vector of parameters over which minimization is to takeplace.  It should return a scalar result.
A function to return the gradient for the "BFGS","CG" and "L-BFGS-B" methods.  If it is NULL, afinite-difference approximation will be used.For the "SANN" method it specifies a function to generate a newcandidate point.  If it is NULL a default Gaussian Markovkernel is used.
Further arguments to be passed to fn and gr.
The method to be used. See ‘Details’.  Can be abbreviated.
Bounds on the variables for the "L-BFGS-B"method, or bounds in which to search for method "Brent".
a list of control parameters.  See ‘Details’.
Logical. Should a numerically differentiated Hessianmatrix be returned?
Initial values for the parameters to be optimized over.
A function to be minimized (or maximized), with firstargument the vector of parameters over which minimization is to takeplace.  It should return a scalar result.
A function to return the gradient for the "BFGS","CG" and "L-BFGS-B" methods.  If it is NULL, afinite-difference approximation will be used.For the "SANN" method it specifies a function to generate a newcandidate point.  If it is NULL a default Gaussian Markovkernel is used.
Further arguments to be passed to fn and gr.
The method to be used. See ‘Details’.  Can be abbreviated.
Bounds on the variables for the "L-BFGS-B"method, or bounds in which to search for method "Brent".
a list of control parameters.  See ‘Details’.
Logical. Should a numerically differentiated Hessianmatrix be returned?
the function to be optimized.  The function iseither minimized or maximized over its first argumentdepending on the value of maximum.
a vector containing the end-points of the intervalto be searched for the minimum.
additional named or unnamed arguments to be passedto f.
the lower end point of the intervalto be searched.
the upper end point of the intervalto be searched.
logical.  Should we maximize or minimize (the default)?
the desired accuracy.
the function to be optimized.  The function iseither minimized or maximized over its first argumentdepending on the value of maximum.
a vector containing the end-points of the intervalto be searched for the minimum.
additional named or unnamed arguments to be passedto f.
the lower end point of the intervalto be searched.
the upper end point of the intervalto be searched.
logical.  Should we maximize or minimize (the default)?
the desired accuracy.
a dendrogram (see as.dendrogram).
additional arguments
numeric vector of p-values (possibly withNAs).  Any other R object is coerced byas.numeric.
correction method, a character string.Can be abbreviated.
number of comparisons, must be at least length(p);only set this (to non-default) when you know what you are doing!
numeric vector of p-values (possibly withNAs).  Any other R object is coerced byas.numeric.
correction method, a character string.Can be abbreviated.
number of comparisons, must be at least length(p);only set this (to non-default) when you know what you are doing!
a univariate or multivariate (not ccf) numeric timeseries object or a numeric vector or matrix, or an "acf" object.
maximum lag at which to calculate the acf.Default is 10*log10(N/m) where N is thenumber of observations and m the number of series.  Willbe automatically limited to one less than the number of observationsin the series.
character string giving the type of acf to be computed.Allowed values are"correlation" (the default), "covariance" or"partial".  Will be partially matched.
logical. If TRUE (the default) the acf is plotted.
function to be called to handle missingvalues. na.pass can be used.
logical.  Should the covariances be about the samplemeans?
further arguments to be passed to plot.acf.
a set of lags (time differences) to retain.
a set of series (names or numbers) to retain.
Vector. 1st element of pair.
Vector. 2nd element of pair. Should be same length as x
 Vector of counts of successes or a matrix with 2 columnsgiving the counts of successes and failures, respectively. 
 Vector of counts of trials; ignored if x is a matrix.
Method for adjusting p values(see p.adjust).  Can be abbreviated. 
 Additional arguments to pass to prop.test 
 response vector. 
 grouping vector or factor. 
 Method for adjusting p values (see p.adjust). 
 switch to allow/disallow the use of a pooled SD 
 a logical indicating whether you want pairedt-tests. 
 a character string specifying the alternativehypothesis, must be one of "two.sided" (default),"greater" or "less".  Can be abbreviated.
 additional arguments to pass to t.test. 
a function to compute (raw) p valuegiven indices i and j.
names of the group levels
a character string specifying the method formultiple testing adjustment; almost always one ofp.adjust.methods.  Can be abbreviated.
 response vector. 
 grouping vector or factor. 
 method for adjusting p values (seep.adjust). Can be abbreviated.
a logical indicating whether you want a paired test.
additional arguments to pass to wilcox.test.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
non-negative parameters of the Beta distribution.
non-centrality parameter.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
number of trials (zero or more).
probability of success on each trial.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
How many distinct categories the people could fall into
The desired probability of coincidence
The number of people
The number of people to fall in the same category
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (non-negative, but can be non-integer).
non-centrality parameter (non-negative).
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of rates.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom.  Inf is allowed.
non-centrality parameter. If omitted the central F is assumed.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
an alternative way to specify the scale.
shape and scale parameters.  Must be positive,scale strictly.
logical; if TRUE, probabilities/densities pare returned as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of failures ina sequence of Bernoulli trials before success occurs.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
probability of success in each trial. 0 < prob <= 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of white ballsdrawn without replacement from an urn which contains both black andwhite balls.
the number of white balls in the urn.
the number of black balls in the urn.
the number of balls drawn from the urn, hence must be in0,1,…, m+n.
probability, it must be between 0 and 1.
number of observations.  If length(nn) > 1, the lengthis taken to be the number required.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
mean and standard deviation of the distributionon the log scale with default values of 0 and 1 respectively.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
numeric vector of the observations for ecdf;  forthe methods, an object inheriting from class "ecdf".
arguments to be passed to subsequent methods, e.g.,plot.stepfun for the plot method.
label for the y-axis.
see plot.stepfun.
numeric or character specifying the color of thehorizontal lines at y = 0 and 1, see colors.
plotting character.
number of significant digits to use, seeprint.
an object of class "spec".
logical.  If TRUE, add to already existing plot.Only valid for plot.type = "marginal".
coverage probability for confidence interval.  Plotting ofthe confidence bar/limits is omitted unless ci is strictlypositive.
If "dB", plot on log10 (decibel) scale (as S-PLUS),otherwise use conventional log scale or linear scale.  Logicalvalues are also accepted.  The default is "yes" unlessoptions(ts.S.compat = TRUE) has been set, when it is"dB".  Only valid for plot.type = "marginal".
the x label of the plot.
the y label of the plot.  If missing a suitable label willbe constructed.
the type of plot to be drawn, defaults to lines.
colour for plotting confidence bar or confidenceintervals for coherency and phase.
line type for confidence intervals for coherency andphase.
overall title for the plot. If missing, a suitable titleis constructed.
a sub title for the plot.  Only used for plot.type =      "marginal".  If missing, a description of the smoothing is used.
For multivariate time series, the type of plotrequired.  Only the first character is needed.
Graphical parameters.
an object of class "spec".
logical.  If TRUE, add to already existing plot.Only valid for plot.type = "marginal".
coverage probability for confidence interval.  Plotting ofthe confidence bar/limits is omitted unless ci is strictlypositive.
If "dB", plot on log10 (decibel) scale (as S-PLUS),otherwise use conventional log scale or linear scale.  Logicalvalues are also accepted.  The default is "yes" unlessoptions(ts.S.compat = TRUE) has been set, when it is"dB".  Only valid for plot.type = "marginal".
the x label of the plot.
the y label of the plot.  If missing a suitable label willbe constructed.
the type of plot to be drawn, defaults to lines.
colour for plotting confidence bar or confidenceintervals for coherency and phase.
line type for confidence intervals for coherency andphase.
overall title for the plot. If missing, a suitable titleis constructed.
a sub title for the plot.  Only used for plot.type =      "marginal".  If missing, a description of the smoothing is used.
For multivariate time series, the type of plotrequired.  Only the first character is needed.
Graphical parameters.
an R object inheriting from "stepfun".
numeric vector of abscissa values at which to evaluatex.  Defaults to knots(x) restricted to xlim.
limits for the plot region: seeplot.window.  Both have sensible defaults if omitted.
labels for x and y axis.
main title.
logical; if TRUE only add to an existing plot.
logical;  if TRUE, draw vertical lines at steps.
logical;  if TRUE, also draw points at the(xlim restricted) knot locations.  Default is true, forsample size < 1000.
character; point character if do.points.
default color of all points and lines.
character or integer code; color of points ifdo.points.
numeric; character expansion factor if do.points.
color of horizontal lines.
color of vertical lines.
line type and thickness for all lines.
further arguments of plot(.), or if(add)segments(.).
time series objects, usually inheriting from class "ts".
for multivariate time series, should the series byplotted separately (with a common time axis) or on a single plot?Can be abbreviated.
logical, indicating if text() labelsshould be used for an x-y plot, or character, supplying avector of labels to be used.  The default is to label for up to 150points, and not for more.
logical, indicating if linesshould be drawn for an x-y plot.  Defaults to the value ofxy.labels if that is logical, otherwise to TRUE.
a function(x, col, bg, pch, type, ...) which gives theaction to be carried out in each panel of the display forplot.type = "multiple".  The default is lines.
the number of columns to use when type = "multiple".Defaults to 1 for up to 4 series, otherwise to 2.
logical indicating if the y-axis (ticks and numbering)should flip from side 2 (left) to 4 (right) from series to serieswhen type = "multiple".
the (default) par settingsfor plot.type = "multiple".  Modify with care!
logical indicating if x- and y- axes should be drawn.
additional graphical arguments, see plot,plot.default and par.
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of observations.  If length(n) > 1, the lengthis taken to be the number required.
target for number of successful trials, or dispersionparameter (the shape parameter of the gamma mixing distribution).Must be strictly positive, need not be integer.
probability of success in each trial. 0 < prob <= 1.
alternative parametrization via mean: see ‘Details’.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of means.
vector of standard deviations.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x] otherwise, P[X > x].
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
number of events. A vector of length one or two.
time base for event count. A vector of length one or two. 
hypothesized rate or rate ratio
indicates the alternative hypothesis and must beone of "two.sided", "greater" or "less".You can specify just the initial letter.
confidence level for the returned confidenceinterval.
a numeric vector at which to evaluate thepolynomial. x can also be a matrix.  Missing values are notallowed in x.
the degree of the polynomial.  Must be less than thenumber of unique points when raw is false, as by default.
for prediction, coefficients from a previous fit.
if true, use raw and not orthogonal polynomials.
logical indicating if a simple matrix (with no furtherattributes but dimnames) should bereturned.  For speedup only.
an object inheriting from class "poly", normallythe result of a call to poly with a single vector argument.
poly, polym: further vectors.predict.poly: arguments to be passed to or from other methods.
a numeric vector at which to evaluate thepolynomial. x can also be a matrix.  Missing values are notallowed in x.
the degree of the polynomial.  Must be less than thenumber of unique points when raw is false, as by default.
for prediction, coefficients from a previous fit.
if true, use raw and not orthogonal polynomials.
logical indicating if a simple matrix (with no furtherattributes but dimnames) should bereturned.  For speedup only.
an object inheriting from class "poly", normallythe result of a call to poly with a single vector argument.
poly, polym: further vectors.predict.poly: arguments to be passed to or from other methods.
a real number.
Number of groups
Number of observations (per group)
Between group variance
Within group variance
Significance level (Type I error probability)
Power of test (1 minus Type II error probability)
number of observations (per group)
probability in one group
probability in other group
significance level (Type I error probability)
power of test (1 minus Type II error probability)
one- or two-sided test.  Can be abbreviated.
use strict interpretation in two-sided case
numerical tolerance used in root finding, the defaultproviding (at least) four significant digits.
number of observations (per group)
true difference in means
standard deviation
significance level (Type I error probability)
power of test (1 minus Type II error probability)
string specifying the type of t test.  Can be abbreviated.
one- or two-sided test.  Can be abbreviated.
use strict interpretation in two-sided case
numerical tolerance used in root finding, the defaultproviding (at least) four significant digits.
a numeric vector or univariate time series.
a logical indicating whether the short or long versionof the truncation lag parameter is used.
either the number of points generated or a vector ofobservations.
the offset fraction to be used; typically in (0,1).
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of random values to return.
vector of (non-negative) means.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
a formula specifying one or more numeric response variables and theexplanatory variables.
numeric matrix of explanatory variables.  Rows represent observations, andcolumns represent variables.  Missing values are not accepted.
numeric matrix of response variables.  Rows represent observations, andcolumns represent variables.  Missing values are not accepted.
number of terms to include in the final model.
a data frame (or similar: see model.frame) from whichvariables specified in formula are preferentially to be taken.
a vector of weights w_i for each case.
a vector of weights for each response, so the fit criterion isthe sum over case i and responses j ofw_i ww_j (y_ij - fit_ij)^2 divided by the sum of w_i.
an index vector specifying the cases to be used in the trainingsample.  (NOTE: If given, this argument must be named.)
a function to specify the action to be taken if NAs arefound. The default action is given by getOption("na.action").(NOTE: If given, this argument must be named.)
the contrasts to be used when any factor explanatory variables are coded.
maximum number of terms to choose from when building the model.
integer from 0 to 3 which determines the thoroughness of anoptimization routine in the SMART program. See the ‘Details’section.
the method used for smoothing the ridge functions.  The default isto use Friedman's super smoother supsmu.  Thealternatives are to use the smoothing spline code underlyingsmooth.spline, either with a specified (equivalent)degrees of freedom for each ridge functions, or to allow thesmoothness to be chosen by GCV.Can be abbreviated.
super smoother bass tone control used with automatic span selection(see supsmu); the range of values is 0 to 10, with larger valuesresulting in increased smoothing.
super smoother span control (see supsmu).  The default, 0,results in automatic span selection by local cross validation. spancan also take a value in (0, 1].
if sm.method is "spline" specifies the smoothness ofeach ridge term via the requested equivalent degrees of freedom.
if sm.method is "gcvspline" this is the penalty usedin the GCV selection for each degree of freedom used.
logical indicating if each spline fit should producediagnostic output (about lambda and df), and thesupsmu fit about its steps.
arguments to be passed to or from other methods.
logical.  If true, the model frame is returned.
a formula with no response variable, referring only tonumeric variables.
an optional data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector used to select rows (observations) of thedata matrix x.
a function which indicates what should happenwhen the data contain NAs.  The default is set bythe na.action setting of options, and isna.fail if that is unset. The ‘factory-fresh’default is na.omit.
arguments passed to or from other methods.  If x isa formula one might specify scale. or tol.
a numeric or complex matrix (or data frame) which providesthe data for the principal components analysis.
a logical value indicating whether the rotated variablesshould be returned.
a logical value indicating whether the variablesshould be shifted to be zero centered. Alternately, a vector oflength equal the number of columns of x can be supplied.The value is passed to scale.
a logical value indicating whether the variables shouldbe scaled to have unit variance before the analysis takesplace.  The default is FALSE for consistency with S, butin general scaling is advisable.  Alternatively, a vector of lengthequal the number of columns of x can be supplied.  Thevalue is passed to scale.
a value indicating the magnitude below which componentsshould be omitted. (Components are omitted if theirstandard deviations are less than or equal to tol times thestandard deviation of the first component.)  With the default nullsetting, no components are omitted (unless rank. is specifiedless than min(dim(x)).).  Other settings for tol could betol = 0 or tol = sqrt(.Machine$double.eps), whichwould omit essentially constant components.
optionally, a number specifying the maximal rank, i.e.,maximal number of principal components to be used.  Can be set asalternative or in addition to tol, useful notably when thedesired rank is considerably smaller than the dimensions of the matrix.
object of class inheriting from "prcomp"
An optional data frame or matrix in which to look forvariables with which to predict.  If omitted, the scores are used.If the original fit used a formula or a data frame or a matrix withcolumn names, newdata must contain columns with the samenames. Otherwise it must contain the same number of columns, to beused in the same order.
a model object for which prediction is desired.
additional arguments affecting the predictions produced.
a fitted object of class inheriting from "glm".
optionally, a data frame in which to look for variables withwhich to predict.  If omitted, the fitted linear predictors are used.
the type of prediction required.  The default is on thescale of the linear predictors; the alternative "response"is on the scale of the response variable.  Thus for a defaultbinomial model the default predictions are of log-odds (probabilitieson logit scale) and type = "response" gives the predictedprobabilities.  The "terms" option returns a matrix giving thefitted values of each term in the model formula on the linear predictorscale.The value of this argument can be abbreviated.
logical switch indicating if standard errors are required.
the dispersion of the GLM fit to be assumed incomputing the standard errors.  If omitted, that returned bysummary applied to the object is used.
with type = "terms" by default all terms are returned.A character vector specifies which terms are to be returned
function determining what should be done with missingvalues in newdata.  The default is to predict NA.
further arguments passed to or from other methods.
Object of class inheriting from "lm"
An optional data frame in which to look for variables withwhich to predict.  If omitted, the fitted values are used.
A switch indicating if standard errors are required.
Scale parameter for std.err. calculation.
Degrees of freedom for scale.
Type of interval calculation.  Can be abbreviated.
Tolerance/confidence level.
Type of prediction (response or model term).  Can be abbreviated.
If type = "terms", which terms (default is allterms), a character vector.
function determining what should be done with missingvalues in newdata.  The default is to predict NA.
the variance(s) for future observations to be assumedfor prediction intervals.  See ‘Details’.
variance weights for prediction.  This can be a numericvector or a one-sided model formula.  In the latter case, it isinterpreted as an expression evaluated in newdata.
further arguments passed to or from other methods.
a fitted model object.
additional arguments for specific methods.
a formula with no response variable, referring only tonumeric variables.
an optional data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector used to select rows (observations) of thedata matrix x.
a function which indicates what should happenwhen the data contain NAs.  The default is set bythe na.action setting of options, and isna.fail if that is unset. The ‘factory-fresh’default is na.omit.
a numeric matrix or data frame which provides the data for theprincipal components analysis.
a logical value indicating whether the calculation shoulduse the correlation matrix or the covariance matrix.  (Thecorrelation matrix can only be used if there are no constant variables.)
a logical value indicating whether the score on eachprincipal component should be calculated.
a covariance matrix, or a covariance list as returned bycov.wt (and cov.mve orcov.mcd from package MASS).If supplied, this is used rather than the covariance matrix ofx.
Should the signs of the loadings and scores be chosenso that the first element of each loading is non-negative?
arguments passed to or from other methods. If x isa formula one might specify cor or scores.
Object of class inheriting from "princomp".
An optional data frame or matrix in which to look forvariables with which to predict.  If omitted, the scores are used.If the original fit used a formula or a data frame or a matrix withcolumn names, newdata must contain columns with the samenames. Otherwise it must contain the same number of columns, to beused in the same order.
a numeric matrix like object, to be printed.
minimum number of significant digits to be used formost numbers.
logical; if TRUE, P-values are additionallyencoded visually as ‘significance stars’ in order to help scanningof long coefficient tables.  It defaults to theshow.signif.stars slot of options.
logical; if TRUE, a legend for the‘significance stars’ is printed provided signif.stars =      TRUE.
minimum number of significant digits for the test statistics,see tst.ind.
indices (integer) of column numbers which are (like)coefficients and standard errors to be formattedtogether.
indices (integer) of column numbers for teststatistics.
indices (integer) of column numbers which should beformatted by zapsmall, i.e., by ‘zapping’ valuesclose to 0.
logical or NULL; if TRUE, the lastcolumn of x is formatted by format.pval as Pvalues.  If P.values = NULL, the default, it is set toTRUE only if options("show.coef.Pvalue") isTRUE and x has at least 4 columns andthe last column name of x starts with "Pr(".
logical; if TRUE, the last column of xcontains P values; in that case, it is printed if and only ifP.values (above) is true.
number, ..
a character string to code NA values inprinted output.
further arguments passed toprint.default.
the original fitted model object.
additional parameters. See documentation on individualmethods.
An object of class "lm" or a class inheriting fromit, or an object with a similar structure including in particularcomponents qr and effects.
A logical flag. If TRUE, a projection is returned for allthe columns of the model matrix. If FALSE, the single-columnprojections are collapsed by terms of the model (as represented inthe analysis of variance table).
If the fit producing object usedweights, this determines if the projections correspond to weighted orunweighted observations.
Swallow and ignore any other arguments.
A loadings matrix, with p rows and k < p columns
The power used the target for promax.  Values of 2 to4 are recommended.
logical. Should Kaiser normalization be performed?If so the rows of x are re-scaled to unit length beforerotation, and scaled back afterwards.
The tolerance for stopping: the relative change in the sumof singular values.
a vector of counts of successes, a one-dimensional table withtwo entries, or a two-dimensional table (or matrix) with 2 columns,giving the counts of successes and failures, respectively.
a vector of counts of trials; ignored if x is amatrix or a table.
a vector of probabilities of success.  The length ofp must be the same as the number of groups specified byx, and its elements must be greater than 0 and less than 1.
a character string specifying the alternativehypothesis, must be one of "two.sided" (default),"greater" or "less".  You can specify just the initialletter.  Only used for testing the null that a single proportionequals a given value, or that two proportions are equal; ignoredotherwise.
confidence level of the returned confidenceinterval.  Must be a single number between 0 and 1.  Only usedwhen testing the null that a single proportion equals a givenvalue, or that two proportions are equal; ignored otherwise.
a logical indicating whether Yates' continuitycorrection should be applied where possible.
 Number of events 
 Number of trials 
 Group score 
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
number(s) of observations in the sample(s).  A positiveinteger, or a vector of such integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (> 0, maybe non-integer).  df      = Inf is allowed.
non-centrality parameter delta;currently except for rt(), only for abs(ncp) <= 37.62.If omitted, use the central t distribution.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
sample size for range (same for each group).
degrees of freedom for s (see below).
number of groups whose maximum range isconsidered.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
lower and upper limits of the distribution.  Must be finite.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
shape and scale parameters, the latter defaulting to 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
numbers of observations in the first and second sample,respectively.  Can be vectors of positive integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
non-negative parameters of the Beta distribution.
non-centrality parameter.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
number of trials (zero or more).
probability of success on each trial.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
How many distinct categories the people could fall into
The desired probability of coincidence
The number of people
The number of people to fall in the same category
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (non-negative, but can be non-integer).
non-centrality parameter (non-negative).
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of rates.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom.  Inf is allowed.
non-centrality parameter. If omitted the central F is assumed.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
an alternative way to specify the scale.
shape and scale parameters.  Must be positive,scale strictly.
logical; if TRUE, probabilities/densities pare returned as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of failures ina sequence of Bernoulli trials before success occurs.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
probability of success in each trial. 0 < prob <= 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of white ballsdrawn without replacement from an urn which contains both black andwhite balls.
the number of white balls in the urn.
the number of black balls in the urn.
the number of balls drawn from the urn, hence must be in0,1,…, m+n.
probability, it must be between 0 and 1.
number of observations.  If length(nn) > 1, the lengthis taken to be the number required.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
mean and standard deviation of the distributionon the log scale with default values of 0 and 1 respectively.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of observations.  If length(n) > 1, the lengthis taken to be the number required.
target for number of successful trials, or dispersionparameter (the shape parameter of the gamma mixing distribution).Must be strictly positive, need not be integer.
probability of success in each trial. 0 < prob <= 1.
alternative parametrization via mean: see ‘Details’.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of means.
vector of standard deviations.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x] otherwise, P[X > x].
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of random values to return.
vector of (non-negative) means.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
The first sample for qqplot.
The second or only data sample.
plot labels.  The xlab and ylabrefer to the y and x axes respectively if datax = TRUE.
logical. Should the result be plotted?
logical. Should data values be on the x-axis?
quantile function for reference theoretical distribution.
numeric vector of length two, representing probabilities.Corresponding quantile pairs define the line drawn.
the type of quantile computation used in quantile.
graphical parameters.
The first sample for qqplot.
The second or only data sample.
plot labels.  The xlab and ylabrefer to the y and x axes respectively if datax = TRUE.
logical. Should the result be plotted?
logical. Should data values be on the x-axis?
quantile function for reference theoretical distribution.
numeric vector of length two, representing probabilities.Corresponding quantile pairs define the line drawn.
the type of quantile computation used in quantile.
graphical parameters.
The first sample for qqplot.
The second or only data sample.
plot labels.  The xlab and ylabrefer to the y and x axes respectively if datax = TRUE.
logical. Should the result be plotted?
logical. Should data values be on the x-axis?
quantile function for reference theoretical distribution.
numeric vector of length two, representing probabilities.Corresponding quantile pairs define the line drawn.
the type of quantile computation used in quantile.
graphical parameters.
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
number(s) of observations in the sample(s).  A positiveinteger, or a vector of such integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (> 0, maybe non-integer).  df      = Inf is allowed.
non-centrality parameter delta;currently except for rt(), only for abs(ncp) <= 37.62.If omitted, use the central t distribution.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
sample size for range (same for each group).
degrees of freedom for s (see below).
number of groups whose maximum range isconsidered.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
either a numeric vector of data values, or a data matrix.
a vector giving the group for the corresponding elementsof y if this is a vector;  ignored if y is a matrix.If not a factor object, it is coerced to one.
a vector giving the block for the corresponding elementsof y if this is a vector;  ignored if y is a matrix.If not a factor object, it is coerced to one.
a formula of the form a ~ b | c, where a,b and c give the data values and corresponding groupsand blocks, respectively.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
numeric vector whose sample quantiles are wanted, or anobject of a class for which a method has been defined (see also‘details’). NA and NaN values are notallowed in numeric vectors unless na.rm is TRUE.
numeric vector of probabilities with values in[0,1].  (Values up to 2e-14 outside thatrange are accepted and moved to the nearby endpoint.)
logical; if true, any NA and NaN'sare removed from x before the quantiles are computed.
logical; if true, the result has a namesattribute.  Set to FALSE for speedup with many probs.
an integer between 1 and 9 selecting one of thenine quantile algorithms detailed below to be used.
used only when names is true: the precision to usewhen formatting the percentages.  In R versions up to 4.0.x, this hadbeen set to max(2, getOption("digits")), internally.
further arguments passed to or from other methods.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
a specification for the model link function.  This can bea name/expression, a literal character string, a length-one charactervector, or an object of class"link-glm" (such as generated bymake.link) provided it is not specifiedvia one of the standard names given next.The gaussian family accepts the links (as names)identity, log and inverse;the binomial family the links logit,probit, cauchit, (corresponding to logistic,normal and Cauchy CDFs respectively) log andcloglog (complementary log-log);the Gamma family the links inverse, identityand log;the poisson family the links log, identity,and sqrt; and the inverse.gaussian family the links1/mu^2, inverse, identityand log.The quasi family accepts the links logit, probit,cloglog,  identity, inverse,log, 1/mu^2 and sqrt, andthe function power can be used to create apower link function.
for all families other than quasi, the variancefunction is determined by the family.  The quasi family willaccept the literal character string (or unquoted as a name/expression)specifications "constant", "mu(1-mu)", "mu","mu^2" and "mu^3", a length-one character vectortaking one of those values, or a list containing componentsvarfun, validmu, dev.resids, initializeand name.
the function family accesses the familyobjects which are stored within objects created by modellingfunctions (e.g., glm).
further arguments passed to methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
lower and upper limits of the distribution.  Must be finite.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
shape and scale parameters, the latter defaulting to 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
numbers of observations in the first and second sample,respectively.  Can be vectors of positive integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
a non-negative numeric giving the number of tables to bedrawn.
a non-negative vector of length at least 2 giving the rowtotals, to be coerced to integer.  Must sum to the same asc.
a non-negative vector of length at least 2 giving the columntotals, to be coerced to integer.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
non-negative parameters of the Beta distribution.
non-centrality parameter.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
number of trials (zero or more).
probability of success on each trial.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (non-negative, but can be non-integer).
non-centrality parameter (non-negative).
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
either a character string naming a file or a connectionwhich the data are to be read from or written to.  ""indicates input from the console for reading and output to theconsole for writing.
the field separator string.  Values on each line of the fileare separated by this string.
a character string giving the set of quoting charactersfor read.ftable; to disable quoting altogether, usequote="".  For write.table, a logical indicatingwhether strings in the data will be surrounded by double quotes.
a character vector with the names of the rowvariables, in case these cannot be determined automatically.
a list giving the names and levels of the columnvariables, in case these cannot be determined automatically.
the number of lines of the data file to skip beforebeginning to read data.
an object of class "ftable".
logical.  If TRUE and file is the name ofa file (and not a connection or "|cmd"), the output fromwrite.ftable is appended to the file.  If FALSE,the contents of file will be overwritten.
an integer giving the number of significant digits touse for (the cell entries of) x.
string specifying how the "ftable" object is formatted(and printed if used as in write.ftable() or the printmethod).  Can be abbreviated.  Available methods are (see the examples):"non.compact"the default representation of an"ftable" object."row.compact"a row-compact version without empty cellsbelow the column labels."col.compact"a column-compact version without empty cellsto the right of the row labels."compact"a row- and column-compact version.  This may implya row and a column label sharing the same cell.  They are thenseparated by the string lsep.
only for method = "compact", the separation stringfor row and column labels.
character vector of length (one or) two,specifying how string justification should happen in format(..),first for the labels, then the table entries.
further arguments to be passed to orfrom methods; for write() and print(), notablyarguments such as method, passed to format().
an object of the type produced by hclust.
Scalar. Cut the dendrogram such that either exactlyk clusters are produced or by cutting at height h.
A vector selecting the clusters around which arectangle should be drawn. which selects clusters by number(from left to right in the tree), x selects clusterscontaining the respective horizontal coordinates. Default iswhich = 1:k.
Vector with border colors for the rectangles.
Optional vector with cluster memberships as returned bycutree(hclust.obj, k = k), can be specified for efficiency ifalready computed.
A terms object
character vector giving the right-hand side of amodel formula.  Cannot be zero-length.
character string, symbol or call giving the left-handside of a model formula, or NULL.
logical: should the formula have an intercept?
the environment of the formulareturned.
vector of positions of variables to drop from theright-hand side of the model.
Keep the response in the resulting object?
an unordered factor.
the reference level, typically a string.
additional arguments for future methods.
an atomic vector, usually a factor (possiblyordered).  The vector is treated as a categorical variable whoselevels will be reordered.  If x is not a factor, its uniquevalues will be used as the implicit levels.
 a vector of the same length as x, whose subsetof values for each unique level of x determines theeventual order of that level.
a function whose first argument is a vectorand returns a scalar, to be applied to each subset of Xdetermined by the levels of x.
 optional: extra arguments supplied to FUN
 logical, whether return value will be an ordered factorrather than a factor.
a formula or a terms object or a data frame.
a data frame used  to  find  the  objects in formula.
function for handling missing values.  Defaults toa na.action attribute of data, thena setting of the option na.action, or na.fail if thatis not set.
a data frame
names of sets of variables in the wide format thatcorrespond to single variables in long format(‘time-varying’).  This is canonically a list of vectors ofvariable names, but it can optionally be a matrix of names, or asingle vector of names.  In each case, when direction =    "long", the names can be replaced by indices which are interpretedas referring to names(data).  See ‘Details’ for moredetails and options.
names of variables in the long format that correspondto multiple variables in the wide format.  See ‘Details’.
the variable in long format that differentiates multiplerecords from the same group or individual.  If more than one recordmatches, the first will be taken (with a warning). 
Names of one or more variables in long format thatidentify multiple records from the same group/individual.  Thesevariables may also be present in wide format.
the values to use for a newly created idvarvariable in long format.
the values to use for a newly created timevarvariable in long format.  See ‘Details’.
a vector of names of variables to drop before reshaping.
character string, partially matched to either"wide" to reshape to wide format, or "long" to reshapeto long format.
character or NULL: a non-null value will beused for the row names of the result.
A character vector of length 1, indicating a separatingcharacter in the variable names in the wide format.  This is used forguessing v.names and times arguments based on thenames in varying.  If sep == "", the split is just beforethe first numeral that follows an alphabetic character.  This isalso used to create variable names when reshaping to wide format.
A list with three components, regexp,include, and (optionally) fixed.  This allows anextended interface to variable name splitting.  See ‘Details’.
an object for which the extraction of model residuals ismeaningful.
other arguments.
an object for which the extraction of model residuals ismeaningful.
other arguments.
an object of class glm, typically the result ofa call to glm.
the type of residuals which should be returned.The alternatives are: "deviance" (default), "pearson","working", "response", and "partial".Can be abbreviated.
further arguments passed to or from other methods.
an object inheriting from class lm, usuallythe result of a call to lm or aov.
further arguments passed to or from other methods.
the type of residuals which should be returned.  Can be abbreviated.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of rates.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom.  Inf is allowed.
non-centrality parameter. If omitted the central F is assumed.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
an alternative way to specify the scale.
shape and scale parameters.  Must be positive,scale strictly.
logical; if TRUE, probabilities/densities pare returned as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of failures ina sequence of Bernoulli trials before success occurs.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
probability of success in each trial. 0 < prob <= 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles representing the number of white ballsdrawn without replacement from an urn which contains both black andwhite balls.
the number of white balls in the urn.
the number of black balls in the urn.
the number of balls drawn from the urn, hence must be in0,1,…, m+n.
probability, it must be between 0 and 1.
number of observations.  If length(nn) > 1, the lengthis taken to be the number required.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
mean and standard deviation of the distributionon the log scale with default values of 0 and 1 respectively.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
location and scale parameters.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of length K of integers in 0:size.
number of random vectors to draw.
integer, say N, specifying the total numberof objects that are put into K boxes in the typical multinomialexperiment. For dmultinom, it defaults to sum(x).
numeric non-negative vector of length K, specifyingthe probability for the K classes; is internally normalized tosum 1. Infinite and missing values are not allowed.
logical; if TRUE, log probabilities are computed.
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of observations.  If length(n) > 1, the lengthis taken to be the number required.
target for number of successful trials, or dispersionparameter (the shape parameter of the gamma mixing distribution).Must be strictly positive, need not be integer.
probability of success in each trial. 0 < prob <= 1.
alternative parametrization via mean: see ‘Details’.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
vector of means.
vector of standard deviations.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x] otherwise, P[X > x].
vector of (non-negative integer) quantiles.
vector of quantiles.
vector of probabilities.
number of random values to return.
vector of (non-negative) means.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
number(s) of observations in the sample(s).  A positiveinteger, or a vector of such integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
an R object, typically returned by lm orglm.
influence structure as returned bylm.influence or influence (the latteronly for the glm method of rstudent andcooks.distance).
(possibly weighted) residuals, with proper default.
standard deviation to use, see default.
dispersion (for glm objects) to use,see default.
hat values H[i,i], see default.
type of residuals for rstandard, with differentoptions and meanings for lm and glm.  Can beabbreviated.
the X or design matrix.
should an intercept column be prepended to x?
further arguments passed to or from other methods.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
degrees of freedom (> 0, maybe non-integer).  df      = Inf is allowed.
non-centrality parameter delta;currently except for rt(), only for abs(ncp) <= 37.62.If omitted, use the central t distribution.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
lower and upper limits of the distribution.  Must be finite.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
numeric vector, the ‘dependent’ variable to besmoothed.
integer width of median window; must be odd.  Turlach had adefault of k <- 1 + 2 * min((n-1)%/% 2, ceiling(0.1*n)).Use k = 3 for ‘minimal’ robust smoothing eliminatingisolated outliers.
character string indicating how the values at thebeginning and the end (of the data) should be treated.Can be abbreviated.  Possible values are:"keep"keeps the first and last k2 valuesat both ends, where k2 is the half-bandwidthk2 = k %/% 2,i.e., y[j] = x[j] for j = 1, …, k2 and (n-k2+1), …, n;"constant"copies median(y[1:k2]) to the firstvalues and analogously for the last ones making the smoothed endsconstant;"median"the default, smooths the ends by usingsymmetrical medians of subsequently smaller bandwidth, but forthe very first and last value where Tukey's robust end-pointrule is applied, see smoothEnds.
character string (partially matching "Turlach" or"Stuetzle") or the default NULL, specifying which algorithmshould be applied.  The default choice depends on n = length(x)and k where "Turlach" will be used for larger problems.
character string determining the behavior in the case ofNA or NaN in x, (partially matching)one of"+Big_alternate"Here, all the NAs in x arefirst replaced by alternating +/- B where B is a“Big” number (with 2B < M*, whereM*=.Machine $ double.xmax).  The replacementvalues are “from left” (+B, -B, +B, …),i.e. start with "+"."-Big_alternate"almost the same as"+Big_alternate", just starting with -B ("-Big...")."na.omit"the result is the same asrunmed(x[!is.na(x)], k, ..)."fail"the presence of NAs in x will raise an error.
integer, indicating verboseness of algorithm;should rarely be changed by average users.
vector of quantiles.
vector of probabilities.
number of observations. If length(n) > 1, the lengthis taken to be the number required.
shape and scale parameters, the latter defaulting to 1.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
vector of quantiles.
vector of probabilities.
number of observations. If length(nn) > 1, the lengthis taken to be the number required.
numbers of observations in the first and second sample,respectively.  Can be vectors of positive integers.
logical; if TRUE, probabilities p are given as log(p).
logical; if TRUE (default), probabilities areP[X ≤ x], otherwise, P[X > x].
integer sample size.
numeric parameter, “degrees of freedom”.
positive definite (p * p) “scale”matrix, the matrix parameter of the distribution.
the x and y arguments provide the x and ycoordinates for the plot.  Any reasonable way of defining thecoordinates is acceptable.  See the function xy.coordsfor details.
smoothness parameter for loess.
degree of local polynomial used.
if "gaussian" fitting is by least-squares, and iffamily = "symmetric" a re-descending M estimator is used.Can be abbreviated.
label for x axis.
label for y axis.
the y limits of the plot.
number of points at which to evaluate the smoothcurve.
For scatter.smooth(), graphical parameters, passedto plot() only.  For loess.smooth, control parameterspassed to loess.control.
a list of arguments to be passed tolines().
an object containing a sdev component, such as thatreturned by princomp() and prcomp().
the number of components to be plotted.
the type of plot.  Can be abbreviated.
graphics parameters.
a numeric vector or an R object but not afactor coercible to numeric by as.double(x).
logical.  Should missing values be removed?
A suitable fit, usually from aov.
The contrasts for which standard errors arerequested.  This can be specified via a list or via a matrix.  Asingle contrast can be specified by a list of logical vectors givingthe cells to be contrasted.  Multiple contrasts should be specifiedby a matrix, each column of which is a numerical contrast vector(summing to zero).
used when contrast.obj is a list; it should be avector of the same length as the list with zero sum.  The defaultvalue is the first Helmert contrast, which contrasts the first andsecond cell means specified by the list.
The data frame used to evaluate contrast.obj.
further arguments passed to or from other methods.
a function object defining a nonlinear model ora nonlinear formula object of the form ~ expression.
a function object, taking arguments mCall,data, and LHS, and ..., representing,respectively, a matched call to the function model, a data frame inwhich to interpret the variables in mCall, and the expressionfrom the left-hand side of the model formula in the call to nls.This function should return initial values for the parameters inmodel.  The ... is used by nls() to pass itscontrol and trace arguments for the cases whereinitial() itself calls nls() as it does for the tenself-starting nonlinear models in R's stats package.
a character vector specifying the terms on the righthand side of model for which initial estimates should becalculated.  Passed as the namevec argument to thederiv function.
an optional prototype for the calling sequence of thereturned object, passed as the function.arg argument to thederiv function.  By default, a template is generated with thecovariates in model coming first and the parameters inmodel coming last in the calling sequence.
an object for which a names attribute will be meaningful 
a character vector of names to assign to the object
a numeric vector of data values. Missing values are allowed,but the number of non-missing values must be between 3 and 5000.
an R object, typically resulting from a model fittingfunction such as lm.
logical, passed to nobs.
potentially further arguments passed to and frommethods.  Passed to deviance(*, ...) for the default method.
an object representing a fitted model.
number of response vectors to simulate.  Defaults to 1.
an object specifying if and how the random numbergenerator should be initialized (‘seeded’).For the "lm" method, either NULL or an integer that will beused in a call to set.seed before simulating the responsevectors.  If set, the value is saved as the "seed" attributeof the returned value.  The default, NULL will not change therandom generator state, and return .Random.seed as the"seed" attribute, see ‘Value’.
additional optional arguments.
a vector or time series
a character string indicating the kind of smoother required;defaults to "3RS3R".
logical, indicating if the result should be ‘twiced’.Twicing a smoother S(y) means S(y) + S(y - S(y)), i.e.,adding smoothed residuals to the smoothed values.  This decreasesbias (increasing variance).
a character string indicating the rule for smoothing at theboundary.  Either "Tukey" (default) or "copy".
logical, indicating if the 3-splitting of ties shouldalso happen at the boundaries (ends).  This is only used forkind = "S".
a vector giving the values of the predictor variable, or  alist or a two-column matrix specifying x and y. 
responses. If y is missing or NULL, the responsesare assumed to be specified by x, with x the indexvector.
optional vector of weights of the same length as x;defaults to all 1.
the desired equivalent number of degrees of freedom (trace ofthe smoother matrix).  Must be in (1,nx],nx the number of unique x values, see below.
smoothing parameter, typically (but not necessarily) in(0,1].  When spar is specified, the coefficientλ of the integral of the squared second derivative in thefit (penalized log likelihood) criterion is a monotone function ofspar, see the details below.  Alternatively lambda maybe specified instead of the scale free spar=s.
if desired, the internal (design-dependent) smoothingparameter λ can be specified instead of spar.This may be desirable for resampling algorithms such as crossvalidation or the bootstrap.
ordinary leave-one-out (TRUE) or ‘generalized’cross-validation (GCV) when FALSE; is used for smoothingparameter computation only when both spar and df arenot specified; it is used however to determine cv.crit in theresult.  Setting it to NA for speedup skips the evaluation ofleverages and any score.
if TRUE, all distinct points in x are usedas knots.  If FALSE (default), a subset of x[] is used,specifically x[j] where the nknots indices are evenlyspaced in 1:n, see also the next argument nknots.Alternatively, a strictly increasing numeric vectorspecifying “all the knots” to be used; must be rescaledto [0, 1] already such that it corresponds to theans $ fit$knots sequence returned, not repeating the boundaryknots.
integer or function giving the number ofknots to use when all.knots = FALSE.  If a function (as bydefault), the number of knots is nknots(nx).  By default fornx > 49 this is less than nx, the numberof unique x values, see the Note.
logical specifying if the input data should be keptin the result.  If TRUE (as per default), fitted values andresiduals are available from the result.
allows the degrees of freedom to be increased bydf.offset in the GCV criterion.
the coefficient of the penalty for degrees of freedomin the GCV criterion.
optional list with named components controlling theroot finding when the smoothing parameter spar is computed,i.e., missing or NULL, see below.Note that this is partly experimental and may changewith general spar computation improvements!low:lower bound for spar; defaults to -1.5 (used toimplicitly default to 0 in R versions earlier than 1.4).high:upper bound for spar; defaults to +1.5.tol:the absolute precision (tolerance) used; defaultsto 1e-4 (formerly 1e-3).eps:the relative precision used; defaults to 2e-8 (formerly0.00244).trace:logical indicating if iterations should be traced.maxit:integer giving the maximal number of iterations;defaults to 500.Note that spar is only searched for in the interval[low, high].
a tolerance for same-ness or uniqueness of the xvalues.  The values are binned into bins of size tol andvalues which fall into the same bin are regarded as the same.  Mustbe strictly positive (and finite).
an experimental logical indicating ifthe result should keep extras from the internal computations.  Shouldallow to reconstruct the X matrix and more.
dependent variable to be smoothed (vector).
width of largest median window; must be odd.
 a numeric vector or an expression that will evaluate indata to a numeric vector 
 a numeric vector or an expression that will evaluate indata to a numeric vector 
 an optional data frame in which to evaluate expressionsfor x and y, if they are given as expressions 
A univariate (not yet:or multivariate) time series or theresult of a fit by ar.
The number of points at which to plot.
The order of the AR model to be fitted.  If omitted,the order is chosen by AIC.
Plot the periodogram?
NA action function.
method for ar fit.
Graphical arguments passed to plot.spec.
univariate or multivariate time series.
vector of odd integers giving the widths of modifiedDaniell smoothers to be used to smooth the periodogram.
alternatively, a kernel smoother of class"tskernel".
specifies the proportion of data to taper.  A splitcosine bell taper is applied to this proportion of the data at thebeginning and end of the series.
proportion of data to pad. Zeros are added to the end ofthe series to increase its length by the proportion pad.
logical; if TRUE, pad the series to a highly compositelength.
logical. If TRUE, subtract the mean of theseries.
logical. If TRUE, remove a linear trend fromthe series. This will also remove the mean.
plot the periodogram?
NA action function.
graphical arguments passed to plot.spec.
A univariate or multivariate time series
The proportion to be tapered at each end of the series,either a scalar (giving the proportion for all series)or a vector of the length of the number of series (giving theproportion for each series).
A univariate or multivariate time series.
String specifying the method used to estimate thespectral density.  Allowed methods are "pgram" (the default)and "ar".  Can be abbreviated.
Further arguments to specific spec methods orplot.spec.
vectors giving the coordinates of the points to beinterpolated.  Alternatively a single plotting structure can bespecified: see xy.coords.y must be increasing or decreasing for method = "hyman".
(for splinefunH()): vector of slopesm[i] at the points (x[i],y[i]); thesetogether determine the Hermite “spline” which ispiecewise cubic, (only) once differentiable continuously.
specifies the type of spline to be used.  Possiblevalues are "fmm", "natural", "periodic","monoH.FC" and "hyman".  Can be abbreviated.
if xout is left unspecified, interpolation takes placeat n equally spaced points spanning the interval[xmin, xmax].
left-hand and right-hand endpoint of theinterpolation interval (when xout is unspecified).
an optional set of values specifying where interpolationis to take place.
handling of tied x values.  The string"ordered" or a function (or the name of a function) taking asingle vector argument and returning a single number or a length-2list of both, see approx and its‘Details’ section, and the example below.
vectors giving the coordinates of the points to beinterpolated.  Alternatively a single plotting structure can bespecified: see xy.coords.y must be increasing or decreasing for method = "hyman".
(for splinefunH()): vector of slopesm[i] at the points (x[i],y[i]); thesetogether determine the Hermite “spline” which ispiecewise cubic, (only) once differentiable continuously.
specifies the type of spline to be used.  Possiblevalues are "fmm", "natural", "periodic","monoH.FC" and "hyman".  Can be abbreviated.
if xout is left unspecified, interpolation takes placeat n equally spaced points spanning the interval[xmin, xmax].
left-hand and right-hand endpoint of theinterpolation interval (when xout is unspecified).
an optional set of values specifying where interpolationis to take place.
handling of tied x values.  The string"ordered" or a function (or the name of a function) taking asingle vector argument and returning a single number or a length-2list of both, see approx and its‘Details’ section, and the example below.
vectors giving the coordinates of the points to beinterpolated.  Alternatively a single plotting structure can bespecified: see xy.coords.y must be increasing or decreasing for method = "hyman".
(for splinefunH()): vector of slopesm[i] at the points (x[i],y[i]); thesetogether determine the Hermite “spline” which ispiecewise cubic, (only) once differentiable continuously.
specifies the type of spline to be used.  Possiblevalues are "fmm", "natural", "periodic","monoH.FC" and "hyman".  Can be abbreviated.
if xout is left unspecified, interpolation takes placeat n equally spaced points spanning the interval[xmin, xmax].
left-hand and right-hand endpoint of theinterpolation interval (when xout is unspecified).
an optional set of values specifying where interpolationis to take place.
handling of tied x values.  The string"ordered" or a function (or the name of a function) taking asingle vector argument and returning a single number or a length-2list of both, see approx and its‘Details’ section, and the example below.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the horizontal asymptote onthe right side (very large values of input).
a numeric parameter representing the response wheninput is zero.
a numeric parameter representing the natural logarithm ofthe rate constant.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the horizontal asymptote onthe right side (very large values of input).
a numeric parameter representing the natural logarithm ofthe rate constant.
a numeric parameter representing the input for which theresponse is zero.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the horizontal asymptote.
a numeric parameter representing the natural logarithm ofthe rate constant.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the multiplier of the firstexponential.
a numeric parameter representing the natural logarithm ofthe rate constant of the first exponential.
a numeric parameter representing the multiplier of the secondexponential.
a numeric parameter representing the natural logarithm ofthe rate constant of the second exponential.
object of class "mlm", or "SSD" inthe case of estVar.
Unused
a numeric value representing the initial dose.
a numeric vector at which to evaluate the model.
a numeric parameter representing the natural logarithm ofthe elimination rate constant.
a numeric parameter representing the natural logarithm ofthe absorption rate constant.
a numeric parameter representing the natural logarithm ofthe clearance.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the horizontal asymptote onthe left side (very small values of input).
a numeric parameter representing the horizontal asymptote onthe right side (very large values of input).
a numeric parameter representing the input value at theinflection point of the curve.  The value of SSfpl will bemidway between A and B at xmid.
a numeric scale parameter on the input axis.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the asymptote.
a numeric parameter related to the value of the function atx = 0
a numeric parameter related to the scale the x axis.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the asymptote.
a numeric parameter representing the x value at theinflection point of the curve.  The value of SSlogis will beAsym/2 at xmid.
a numeric scale parameter on the input axis.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the maximum value of the response.
a numeric parameter representing the input value atwhich half the maximum response is attained.  In the field of enzymekinetics this is called the Michaelis parameter.
a numeric vector of values at which to evaluate the model.
a numeric parameter representing the horizontal asymptote onthe right side (very small values of x).
a numeric parameter representing the change fromAsym to the y intercept.
a numeric parameter representing the natural logarithm ofthe rate constant.
a numeric parameter representing the power to which xis raised.
a univariate or multivariate time-series, or a vector or matrix.
extra arguments for future methods.
numeric matrix as results fromanova.glm(..., test = NULL).
a character string, partially matching one of "Rao","LRT", "Chisq", "F" or "Cp".
a residual mean square or other scale estimate to be usedas the denominator in an F test.
degrees of freedom corresponding to scale.
number of observations.
an object representing a model of an appropriate class (mainly"lm" and "glm").This is used as the initial model in the stepwise search.
defines the range of models examined in the stepwise search.This should be either a single formula, or a list containingcomponents upper and lower, both formulae.  See thedetails for how to specify the formulae and how they are used.
used in the definition of the AIC statistic for selecting the models,currently only for lm, aov andglm models.  The default value, 0, indicatesthe scale should be estimated: see extractAIC.
the mode of stepwise search, can be one of "both","backward", or "forward", with a default of "both".If the scope argument is missing the default fordirection is "backward".  Values can be abbreviated.
if positive, information is printed during the running of step.Larger values may give more detailed information.
a filter function whose input is a fitted model object and theassociated AIC statistic, and whose output is arbitrary.Typically keep will select a subset of the components ofthe object and return them. The default is not to keep anything.
the maximum number of steps to be considered.  The default is 1000(essentially as many as required).  It is typically used to stop theprocess early.
the multiple of the number of degrees of freedom used for the penalty.Only k = 2 gives the genuine AIC: k = log(n) is sometimesreferred to as BIC or SBC.
any additional arguments to extractAIC.
numeric vector giving the knots or jump locations of the stepfunction for stepfun().  For the other functions, x isas object below.
numeric vector one longer than x, giving the heights ofthe function values between the x values.
a number between 0 and 1, indicating how interpolation outsidethe given x values should happen.  See approxfun.
Handling of tied x values. Either a function orthe string "ordered".  See  approxfun.
logical, indicating if the intervals should be closed onthe right (and open on the left) or vice versa.
an R object inheriting from "stepfun".
number of significant digits to use, see print.
potentially further arguments (required by the generic).
univariate time series to be decomposed.This should be an object of class "ts" with a frequencygreater than one.
either the character string "periodic" or the span (inlags) of the loess window for seasonal extraction, which shouldbe odd and at least 7, according to Cleveland et al.  This has no default.
degree of locally-fitted polynomial in seasonalextraction.  Should be zero or one.
the span (in lags) of the loess window for trendextraction, which should be odd.  If NULL, the default,nextodd(ceiling((1.5*period) / (1-(1.5/s.window)))), is taken.
degree of locally-fitted polynomial in trendextraction.  Should be zero or one.
the span (in lags) of the loess window of the low-passfilter used for each subseries.  Defaults to the smallest oddinteger greater than or equal to frequency(x) which isrecommended since it prevents competition between the trend andseasonal components.  If not an odd integer its given value isincreased to the next odd one.
degree of locally-fitted polynomial for the subserieslow-pass filter.  Must be 0 or 1.
integers at least one to increase speed ofthe respective smoother.  Linear interpolation happens between every*.jumpth value.
logical indicating if robust fitting be used in theloess procedure.
integer; the number of ‘inner’ (backfitting)iterations; usually very few (2) iterations suffice.
integer; the number of ‘outer’ robustnessiterations.
action on missing values.
a univariate numeric time series. Missing values are allowed.
the class of structural model.  If omitted, a BSM is usedfor a time series with frequency(x) > 1, and a local trendmodel otherwise.  Can be abbreviated.
initial values of the variance parameters.
optional numeric vector of the same length as the totalnumber of parameters.  If supplied, only NA entries infixed will be varied.  Probably most useful for settingvariances to zero.
List of control parameters foroptim.  Method "L-BFGS-B" is used.
An object of class "aov" or "aovlist".
logical: should intercept terms be included?
an optional named list, with names corresponding to termsin the model.  Each component is itself a list with integercomponents giving contrasts whose contributions are to be summed.
logical: should the split apply also tointeractions involving the factor?
logical: should terms with no degrees of freedombe included?
Arguments to be passed to or from other methods,for summary.aovlist including those for summary.aov.
an object of class "glm", usually, a result of acall to glm.
an object of class "summary.glm", usually, a result of acall to summary.glm.
the dispersion parameter for the family used.Either a single numerical value or NULL (the default), whenit is inferred from object (see ‘Details’).
logical; if TRUE, the correlation matrix ofthe estimated parameters is returned and printed.
the number of significant digits to use when printing.
logical. If TRUE, print the correlations ina symbolic form (see symnum) rather than as numbers.
logical. If TRUE, ‘significance stars’are printed for each coefficient.
further arguments passed to or from other methods.
an object of class "lm", usually, a result of acall to lm.
an object of class "summary.lm", usually, a result of acall to summary.lm.
logical; if TRUE, the correlation matrix ofthe estimated parameters is returned and printed.
the number of significant digits to use when printing.
logical. If TRUE, print the correlations ina symbolic form (see symnum) rather than as numbers.
logical. If TRUE, ‘significance stars’are printed for each coefficient.
further arguments passed to or from other methods.
An object of class "manova" or an aovobject with multiple responses.
The name of the test statistic to be used.  Partialmatching is used so the name can be abbreviated.
logical.  If TRUE, the intercept term isincluded in the table.
tolerance to be used in deciding if the residuals arerank-deficient: see qr.
further arguments passed to or from other methods.
numeric vector giving the knots or jump locations of the stepfunction for stepfun().  For the other functions, x isas object below.
numeric vector one longer than x, giving the heights ofthe function values between the x values.
a number between 0 and 1, indicating how interpolation outsidethe given x values should happen.  See approxfun.
Handling of tied x values. Either a function orthe string "ordered".  See  approxfun.
logical, indicating if the intervals should be closed onthe right (and open on the left) or vice versa.
an R object inheriting from "stepfun".
number of significant digits to use, see print.
potentially further arguments (required by the generic).
x values for smoothing
y values for smoothing
case weights, by default all equal
the fraction of the observations in the span of the runninglines smoother, or "cv" to choose this by leave-one-outcross-validation.
if TRUE, the x values are assumed to be in[0, 1] and of period 1.
controls the smoothness of the fitted curve.  Values of upto 10 indicate increasing smoothness.
logical, if true, prints one line of info “perspar”, notably useful for "cv".
numeric or logical vector or array.
numeric vector whose values cutpoints[j] == c[j]  (after augmentation, see corrbelow) are used for intervals.
character vector, one shorter than (theaugmented, see corr below) cutpoints.symbols[j] == s[j] are used as ‘code’ forthe (half open) interval (c[j], c[j+1]].When numeric.x is FALSE, i.e., by default whenargument x is logical, the default isc(".","|") (graphical 0 / 1 s).
logical indicating if a "legend" attribute isdesired.
character or logical. How NAs are coded.  Ifna == FALSE, NAs are coded invisibly, includingthe "legend" attribute below, which otherwise mentions NAcoding.
absolute precision to be used at left and right boundary.
logical indicating if x should be treated as numbers,otherwise as logical.
logical.  If TRUE, x contains correlations.The cutpoints are augmented by 0 and 1 andabs(x) is coded.
if TRUE, or of mode character, themaximal cutpoint is coded especially.
if TRUE, or of mode character, theminimal cutpoint is coded especially.
logical, integer or NULL indicating howcolumn names should be abbreviated (if they are); if NULL(or FALSE and x has no column names),the column names will all be empty, i.e., ""; otherwise ifabbr.colnames is false, they are left unchanged.  IfTRUE or integer, existing column names will be abbreviated toabbreviate(*, minlength = abbr.colnames).
logical.  If TRUE and x is amatrix, only the lower triangular part of the matrix is codedas non-blank.
logical.  If lower.triangular andthis are TRUE, the diagonal part of the matrix isshown.
a (non-empty) numeric vector of data values.
an optional (non-empty) numeric vector of data values.
a character string specifying the alternativehypothesis, must be one of "two.sided" (default),"greater" or "less".  You can specify just the initialletter.
a number indicating the true value of the mean (ordifference in means if you are performing a two sample test).
a logical indicating whether you want a pairedt-test.
a logical variable indicating whether to treat thetwo variances as being equal. If TRUE then the pooledvariance is used to estimate the variance otherwise the Welch(or Satterthwaite) approximation to the degrees of freedom is used.
confidence level of the interval.
a formula of the form lhs ~ rhs where lhsis a numeric variable giving the data values and rhs either 1 for a one-sample or paired test or a factorwith two levels giving the corresponding groups. If lhs is of class "Pair" and rhs is 1, a paired test is done
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
fitted model object
data frame in which variables in model can befound
environment in which variables in model can be found
logical; should partial residuals be plotted?
add rugplots (jittered 1-d histograms) to the axes?
which terms to plot (default NULL means allterms); a vector passed topredict(.., type = "terms", terms = *).
plot pointwise standard errors?
vector of labels for the x axes
vector of labels for the y axes
logical, or vector of main titles;  if TRUE, themodel's call is taken as main title, NULL or FALSE meanno titles.
color and line width for the ‘term curve’,see lines.
color, line type and line width for the‘twice-standard-error curve’ when se = TRUE.
color, plotting character expansion and typefor partial residuals, when partial.resid = TRUE, seepoints.
logical; if TRUE, the user is asked beforeeach plot, see par(ask=.).
Should x-axis ticks use factor levels ornumbers for factor terms?
NULL or a function with the same arguments aspanel.smooth to draw a smooth through the partialresiduals for non-factor terms
Passed to smooth
an optional range for the y axis, or "common" whena range sufficient for all the plot will be computed, or"free" when limits are computed for each plot.
if set to FALSE plots are not produced: instead alist is returned containing the data that would have been plotted.
logical vector; if an element (recycled as necessary)is TRUE, partial residuals for the corresponding term areplotted against transformed values.The model response is then a straight line, allowing aready comparison against the data or against the curveobtained from smooth-panel.smooth.
other graphical parameters.
object used to select a method to dispatch.
further arguments passed to or from other methods.
a formula.
which functions in the formula should be marked asspecial in the terms object?  A character vector or NULL.
Not implemented in R.
a data frame from which the meaning of the special symbol. can be inferred.  It is unused if there is no . inthe formula.
Not implemented in R.
a logical value indicating whether the terms shouldkeep their positions.  If FALSE the terms are reordered sothat main effects come first, followed by the interactions,all second-order, all third-order and so on.  Effects of a givenorder are kept in the order specified.
should the formula be expanded and simplified, thepre-1.7.0 behaviour?
further arguments passed to or from other methods.
normally . in a formula refers to theremaining variables contained in data.  Exceptionally,. can be treated as a name for non-standard uses of formulae.
a univariate or multivariate time-series, or a vector or matrix.
can be used to indicate when sampling took placein the time unit. 0 (the default) indicates the startof the unit, 0.5 the middle and 1 the end ofthe interval.
extra arguments for future methods.
the first row to form the Toeplitz matrix.
a vector or matrix of the observed time-seriesvalues. A data frame will be coerced to a numeric matrix viadata.matrix.  (See also ‘Details’.)
the time of the first observation.  Either a singlenumber or a vector of two numbers (the second of which is an integer),which specify a natural timeunit and a (1-based) number of samples into the time unit.  Seethe examples for the use of the second form.
the time of the last observation, specified in the same wayas start.
the number of observations per unit of time.
the fraction of the sampling period between successiveobservations; e.g., 1/12 for monthly data.  Only one offrequency or deltat should be provided.
time series comparison tolerance.  Frequencies areconsidered equal if their absolute difference is less thants.eps.
class to be given to the result, or none if NULLor "none".  The default is "ts" for a single series,c("mts", "ts", "matrix") for multiple series.
a character vector of names for the series in a multipleseries: defaults to the colnames of data, or Series 1,Series 2, ....
an arbitrary R object.
arguments passed to methods (unused for the default method).
two or more univariate or multivariate time series, orobjects which can coerced to time series.
logical; if TRUE return the result as a dataframe.
one or more univariate or multivariate time series.
list of named graphics parameters to be passed to theplotting functions.  Those commonly used can be supplied directly in....
two or more univariate or multivariate time series, orobjects which can coerced to time series.
logical; if TRUE return the result as a dataframe.
a fitted time-series model
the maximum number of lags for a Portmanteaugoodness-of-fit test
further arguments to be passed to particular methods
a vector or matrix or univariate or multivariate time-series.
a numeric vector of length 3 or NULL.
a vector or matrix or univariate or multivariate time-series.
a numeric vector of length 3 or NULL.
a time-series fit.  Currently only class"StructTS" is supported
possible arguments for future methods.
A fitted model object, usually an aov fit.
A character vector listing terms in the fitted model forwhich the intervals should be calculated.  Defaults to all theterms.
A logical value indicating if the levels of the factorshould be ordered according to increasing average in the samplebefore taking differences.  If ordered is true thenthe calculated differences in the means will all be positive.  Thesignificant differences will be those for which the lwr endpoint is positive.
A numeric value between zero and one giving thefamily-wise confidence level to use.
Optional additional arguments.  None are used at present.
the function for which the root is sought.
a vector containing the end-points of the intervalto be searched for the root.
additional named or unnamed arguments to be passedto f
the lower and upper end points of the interval tobe searched.
the same as f(upper) andf(lower), respectively.  Passing these values from the callerwhere they are often known is more economical as soon as f()contains non-trivial computations.
character string specifying if the intervalc(lower,upper) should be extended or directly produce an errorwhen f() does not have differing signs at the endpoints.  Thedefault, "no", keeps the search interval and hence producesan error.  Can be abbreviated.
logical indicating whether a convergence warning of theunderlying uniroot should be caught as an error and ifnon-convergence in maxiter iterations should be an errorinstead of a warning.
the desired accuracy (convergence tolerance).
the maximum number of iterations.
integer number; if positive, tracing information isproduced.  Higher values giving more details.
An existing fit from a model function such as lm,glm and many others.
Changes to the formula – see update.formula fordetails.
Additional arguments to the call, or arguments withchanged values. Use name = NULL to remove the argument name.
If true evaluate the new call else return the call.
An existing fit from a model function such as lm,glm and many others.
Changes to the formula – see update.formula fordetails.
Additional arguments to the call, or arguments withchanged values. Use name = NULL to remove the argument name.
If true evaluate the new call else return the call.
a model formula to be updated.
a formula giving a template which specifies how to update.
further arguments passed to or from other methods.
a numeric vector, matrix or data frame.
NULL (default) or a vector, matrix or data frame withcompatible dimensions to x.   The default is equivalent toy = x (but more efficient).
logical. Should missing values be removed?
an optional character string giving amethod for computing covariances in the presenceof missing values.  This must be (an abbreviation of) one of the strings"everything", "all.obs", "complete.obs","na.or.complete", or "pairwise.complete.obs".
a character string indicating which correlationcoefficient (or covariance) is to be computed.  One of"pearson" (default), "kendall", or "spearman":can be abbreviated.
symmetric numeric matrix, usually positive definite such as acovariance matrix.
numeric vectors of data values, or fitted linear modelobjects (inheriting from class "lm").
the hypothesized ratio of the population variances ofx and y.
a character string specifying the alternativehypothesis, must be one of "two.sided" (default),"greater" or "less".  You can specify just the initialletter.
confidence level for the returned confidenceinterval.
a formula of the form lhs ~ rhs where lhsis a numeric variable giving the data values and rhs a factorwith two levels giving the corresponding groups.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
an R object, typically a fitted model.
logical; if TRUE, all names (including zero weights,...) are returned.
further arguments passed to or from other methods.
A loadings matrix, with p rows and k < p columns
The power used the target for promax.  Values of 2 to4 are recommended.
logical. Should Kaiser normalization be performed?If so the rows of x are re-scaled to unit length beforerotation, and scaled back afterwards.
The tolerance for stopping: the relative change in the sumof singular values.
a fitted model object, typically.  Sometimes also asummary() object of such a fitted model.
for the aov, lm, glm, mlm, and whereapplicable summary.lm etc methods: logical indicating if thefull variance-covariance matrix should be returned also in case ofan over-determined system where some coefficients are undefined andcoef(.) contains NAs correspondingly.   Whencomplete = TRUE,  vcov() is compatible withcoef() also in this singular case.
additional arguments for method functions.  For theglm method this can be used to pass adispersion parameter.
a logical vector typically identical tois.na(coef(.)) indicating which coefficients are ‘aliased’.
a variance-covariance matrix, typically “incomplete”,i.e., with no rows and columns for aliased coefficients.
an object containing the values whose weighted mean is to becomputed.
a numerical vector of weights the same length as x givingthe weights to use for elements of x.
arguments to be passed to or from methods.
a logical value indicating whether NAvalues in x should be stripped before the computation proceeds.
R object, typically of class lm orglm.
logical.  If TRUE, drop all cases withweights == 0.
an object for which the extraction of model weights ismeaningful.
other arguments passed to methods.
numeric vector of data values.  Non-finite (e.g., infinite ormissing) values will be omitted.
an optional numeric vector of data values: as with xnon-finite values will be omitted.
a character string specifying the alternativehypothesis, must be one of "two.sided" (default),"greater" or "less".  You can specify just the initialletter.
a number specifying an optional parameter used to form thenull hypothesis.  See ‘Details’.
a logical indicating whether you want a paired test.
a logical indicating whether an exact p-valueshould be computed.
a logical indicating whether to apply continuitycorrection in the normal approximation for the p-value.
a logical indicating whether a confidence intervalshould be computed.
confidence level of the interval.
(when conf.int is true:) a positive numerictolerance, used in uniroot(*, tol=tol.root) calls.
a number; if finite, rank(signif(r, digits.rank))will be used to compute ranks for the test statistic instead of (thedefault) rank(r).
a formula of the form lhs ~ rhs where lhsis a numeric variable giving the data values and rhs either1 for a one-sample or paired test or a factorwith two levels giving the corresponding groups. If lhs is ofclass "Pair" and rhs is 1, a paired test is done
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
a function which indicates what should happen whenthe data contain NAs.  Defaults togetOption("na.action").
further arguments to be passed to or from methods.
a time-series (or other object if not replacing values).
the start time of the period of interest.
the end time of the period of interest.
the new frequency can be specified by either(or both if they are consistent).
logical.  If true, the start and end valuesare allowed to extend the series.  If false, attempts to extend theseries give a warning and are ignored.
time series comparison tolerance.  Frequencies areconsidered equal if their absolute difference is less thants.eps.
further arguments passed to or from other methods.
replacement values.
a time-series (or other object if not replacing values).
the start time of the period of interest.
the end time of the period of interest.
the new frequency can be specified by either(or both if they are consistent).
logical.  If true, the start and end valuesare allowed to extend the series.  If false, attempts to extend theseries give a warning and are ignored.
time series comparison tolerance.  Frequencies areconsidered equal if their absolute difference is less thants.eps.
further arguments passed to or from other methods.
replacement values.
either a character string naming a file or a connectionwhich the data are to be read from or written to.  ""indicates input from the console for reading and output to theconsole for writing.
the field separator string.  Values on each line of the fileare separated by this string.
a character string giving the set of quoting charactersfor read.ftable; to disable quoting altogether, usequote="".  For write.table, a logical indicatingwhether strings in the data will be surrounded by double quotes.
a character vector with the names of the rowvariables, in case these cannot be determined automatically.
a list giving the names and levels of the columnvariables, in case these cannot be determined automatically.
the number of lines of the data file to skip beforebeginning to read data.
an object of class "ftable".
logical.  If TRUE and file is the name ofa file (and not a connection or "|cmd"), the output fromwrite.ftable is appended to the file.  If FALSE,the contents of file will be overwritten.
an integer giving the number of significant digits touse for (the cell entries of) x.
string specifying how the "ftable" object is formatted(and printed if used as in write.ftable() or the printmethod).  Can be abbreviated.  Available methods are (see the examples):"non.compact"the default representation of an"ftable" object."row.compact"a row-compact version without empty cellsbelow the column labels."col.compact"a column-compact version without empty cellsto the right of the row labels."compact"a row- and column-compact version.  This may implya row and a column label sharing the same cell.  They are thenseparated by the string lsep.
only for method = "compact", the separation stringfor row and column labels.
character vector of length (one or) two,specifying how string justification should happen in format(..),first for the labels, then the table entries.
further arguments to be passed to orfrom methods; for write() and print(), notablyarguments such as method, passed to format().
a formula object with the cross-classifying variables(separated by +) on the right hand side (or an object whichcan be coerced to a formula).  Interactions are not allowed.  On theleft hand side, one may optionally give a vector or a matrix ofcounts; in the latter case, the columns are interpreted ascorresponding to the levels of a variable.  This is useful if thedata have already been tabulated, see the examples below.
an optional matrix or data frame (or similar: seemodel.frame) containing the variables in theformula formula.  By default the variables are taken fromenvironment(formula).
an optional vector specifying a subset of observationsto be used.
logical specifying if the result should be asparse matrix, i.e., inheriting fromsparseMatrixOnly works for two factors (since thereare no higher-order sparse array classes yet).
a function which indicates what should happen whenthe data contain NAs.  If unspecified, andaddNA is true, this is set to na.pass.  When itis na.omit and formula has a left hand side (withcounts), sum(*, na.rm = TRUE) is used instead ofsum(*) for the counts.
logical indicating if NAs should get a separatelevel and be counted, using addNA(*, ifany=TRUE) andsetting the default for na.action to na.pass.
a vector of values to be excluded when forming theset of levels of the classifying factors.
a logical indicating whether to drop unusedlevels in the classifying factors.  If this is FALSE andthere are unused levels, the table will contain zero marginals, anda subsequent chi-squared test for independence of the factors willnot work.
an object of class "xtabs".
character string (or NULL) indicating howNA are printed.  The default ("") does not showNAs clearly, and na.print = "NA" maybe advisableinstead.
further arguments passed to or from other methods.

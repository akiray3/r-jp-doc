x
NA
kruskal.test performs a Kruskal-Wallis rank sum test of thenull that the location parameters of the distribution of xare the same in each group (sample).  The alternative is that theydiffer in at least one.If x is a list, its elements are taken as the samples to becompared, and hence have to be numeric data vectors.  In this case,g is ignored, and one can simply use kruskal.test(x)to perform the test.  If the samples are not yet contained in alist, use kruskal.test(list(x, ...)).Otherwise, x must be a numeric data vector, and g mustbe a vector or factor object of the same length as x givingthe group for the corresponding elements of x.
If y is numeric, a two-sample test of the null hypothesisthat x and y were drawn from the same continuousdistribution is performed.Alternatively, y can be a character string naming a continuous(cumulative) distribution function, or such a function.  In this case,a one-sample test is carried out of the null that the distributionfunction which generated x is distribution y withparameters specified by ....The presence of ties always generates a warning, since continuousdistributions do not generate them.  If the ties arose from roundingthe tests may be approximately valid, but even modest amounts ofrounding can have a significant effect on the calculated statistic.Missing values are silently omitted from x and (in thetwo-sample case) y.The possible values "two.sided", "less" and"greater" of alternative specify the null hypothesisthat the true distribution function of x is equal to, not lessthan or not greater than the hypothesized distribution function(one-sample case) or the distribution function of y (two-samplecase), respectively.  This is a comparison of cumulative distributionfunctions, and the test statistic is the maximum difference in value,with the statistic in the "greater" alternative beingD^+ = max[F_x(u) - F_y(u)].Thus in the two-sample case alternative = "greater" includesdistributions for which x is stochastically smaller thany (the CDF of x lies above and hence to the left of thatfor y), in contrast to t.test orwilcox.test.Exact p-values are not available for the two-sample case if one-sidedor in the presence of ties.  If exact = NULL (the default), anexact p-value is computed if the sample size is less than 100 in theone-sample case and there are no ties, and if the product ofthe sample sizes is less than 10000 in the two-sample case.Otherwise, asymptotic distributions are used whose approximations maybe inaccurate in small samples.  In the one-sample two-sided case,exact p-values are obtained as described in Marsaglia, Tsang & Wang(2003) (but not using the optional approximation in the right tail, sothis can be slow for small p-values).  The formula of Birnbaum &Tingey (1951) is used for the one-sample one-sided case.If a single-sample test is used, the parameters specified in... must be pre-specified and not estimated from the data.There is some more refined distribution theory for the KS test withestimated parameters (see Durbin, 1973), but that is not implementedin ks.test.
NA
Vector or matrix arguments x are given a tsp attributevia hasTsp.
If just one plot is produced, this is a conventional plot.  If morethan one plot is to be produced, par(mfrow) and several othergraphics parameters will be set, so it is not (easily) possible to mixsuch lag plots with other plots on the same page.If ask = NULL, par(ask = TRUE) will be called if more thanone page of plots is to be produced and the device is interactive.
Cases with missing values are omitted.Contrary to the references where the data is split in three (almost)equally sized groups with symmetric sizes depending on n andn %% 3 and computes medians inside each group, theline() code splits into three groups using all observationswith x[.] <= q1 and x[.] >= q2, where q1, q2 are(a kind of) quantiles for probabilities p = 1/3 and p = 2/3of the form (x[j1]+x[j2])/2 where j1 = floor(p*(n-1))and j2 = ceiling(p(n-1)), n = length(x).Long vectors are not supported yet.
Models for lm are specified symbolically.  A typical model hasthe form response ~ terms where response is the (numeric)response vector and terms is a series of terms which specifies alinear predictor for response.  A terms specification of the formfirst + second indicates all the terms in first togetherwith all the terms in second with duplicates removed.  Aspecification of the form first:second indicates the set ofterms obtained by taking the interactions of all terms in firstwith all terms in second.  The specification first*secondindicates the cross of first and second.  This isthe same as first + second + first:second.If the formula includes an offset, this is evaluated andsubtracted from the response.If response is a matrix a linear model is fitted separately byleast-squares to each column of the matrix.See model.matrix for some further details.  The terms inthe formula will be re-ordered so that main effects come first,followed by the interactions, all second-order, all third-order and soon: to avoid this pass a terms object as the formula (seeaov and demo(glm.vr) for an example).A formula has an implied intercept term.  To remove this use eithery ~ x - 1 or y ~ 0 + x.  See formula formore details of allowed formulae.Non-NULL weights can be used to indicate thatdifferent observations have different variances (with the values inweights being inversely proportional to the variances); orequivalently, when the elements of weights are positiveintegers w_i, that each response y_i is the mean ofw_i unit-weight observations (including the case that thereare w_i observations equal to y_i and the data have beensummarized). However, in the latter case, notice that within-groupvariation is not used.  Therefore, the sigma estimate and residualdegrees of freedom may be suboptimal; in the case of replicationweights, even wrong. Hence, standard errors and analysis of variancetables should be treated with care.lm calls the lower level functions lm.fit, etc,see below, for the actual numerical computations.  For programmingonly, you may consider doing likewise.All of weights, subset and offset are evaluatedin the same way as variables in formula, that is first indata and then in the environment of formula.
NA
The influence.measures() and other functions listed inSee Also provide a more user oriented way of computing avariety of regression diagnostics.  These all build onlm.influence.  Note that for GLMs (other than the Gaussianfamily with identity link) these are based on one-step approximationswhich may be inadequate if a case has high influence.An attempt is made to ensure that computed hat values that areprobably one are treated as one, and the corresponding rows insigma and coefficients are NaN.  (Dropping such acase would normally result in a variable being dropped, so it is notpossible to give simple drop-one diagnostics.)naresid is applied to the results and so will fill inwith NAs it the fit had na.action = na.exclude.
NA
‘Loadings’ is a term from factor analysis, but becausefactor analysis and principal component analysis (PCA) are oftenconflated in the social science literature, it was used for PCA bySPSS and hence by princomp in S-PLUS to help SPSS users.Small loadings are conventionally not printed (replaced by spaces), todraw the eye to the pattern of the larger loadings.The print method for class "factanal" calls the"loadings" method to print the loadings, and so passes downarguments such as cutoff and sort.The signs of the loadings vectors are arbitrary for both factoranalysis and PCA.
Fitting is done locally.  That is, for the fit at point x, thefit is made using points in a neighbourhood of x, weighted bytheir distance from x (with differences in ‘parametric’variables being ignored when computing the distance).  The size of theneighbourhood is controlled by α (set by span orenp.target).  For α < 1, theneighbourhood includes proportion α of the points,and these have tricubic weighting (proportional to (1 - (dist/maxdist)^3)^3).  Forα > 1, all points are used, with the‘maximum distance’ assumed to be α^(1/p)times the actual maximum distance for p explanatory variables.For the default family, fitting is by (weighted) least squares.  Forfamily="symmetric" a few iterations of an M-estimationprocedure with Tukey's biweight are used.  Be aware that as the initialvalue is the least-squares fit, this need not be a very resistant fit.It can be important to tune the control list to achieve acceptablespeed.  See loess.control for details.
NA
loess.smooth is an auxiliary function which evaluates theloess smooth at evaluation equally spaced pointscovering the range of x.
logLik is most commonly used for a model fitted by maximumlikelihood, and some uses, e.g. by AIC, assumethis.  So care is needed where other fit criteria have been used, forexample REML (the default for "lme").For a "glm" fit the family does not have tospecify how to calculate the log-likelihood, so this is based on usingthe family's aic() function to compute the AIC.  For thegaussian, Gamma andinverse.gaussian families it assumed that the dispersionof the GLM is estimated and has been counted as a parameter in the AICvalue, and for all other families it is assumed that the dispersion isknown.  Note that this procedure does not give the maximizedlikelihood for "glm" fits from the Gamma and inverse gaussianfamilies, as the estimate of dispersion used is not the MLE.For "lm" fits it is assumed that the scale has been estimated(by maximum likelihood or REML), and all the constants in thelog-likelihood are included.  That method is only applicable tosingle-response fits.
The Iterative Proportional Fitting algorithm as presented inHaberman (1972) is used for fitting the model.  At most iteriterations are performed, convergence is taken to occur when themaximum deviation between observed and fitted margins is less thaneps.  All internal computations are done in double precision;there is no limit on the number of factors (the dimension of thetable) in the model.Assuming that there are no structural zeros, both the LikelihoodRatio Test and Pearson test statistics have an asymptotic chi-squareddistribution with df degrees of freedom.Note that the IPF steps are applied to the factors in the order givenin margin.  Hence if the model is decomposable and the ordergiven in margin is a running intersection property orderingthen IPF will converge in one iteration.Package MASS contains loglm, a front-end tologlin which allows the log-linear model to be specified andfitted in a formula-based manner similar to that of other fittingfunctions such as lm or glm.
lowess is defined by a complex algorithm, the Ratfor originalof which (by W. S. Cleveland) can be found in the R sources as file‘src/library/stats/src/lowess.doc’.  Normally a local linear polynomial fit isused, but under some circumstances (see the file) a local constant fitcan be used.  ‘Local’ is defined by the distance to thefloor(f*n)th nearest neighbour, and tricubic weighting is usedfor x which fall within the neighbourhood.The initial fit is done using weighted least squares.  Ifiter > 0, further weighted fits are done using the product ofthe weights from the proximity of the x values and case weightsderived from the residuals at the previous iteration.  Specifically,the case weight is Tukey's biweight, with cutoff 6 times the MAD of theresiduals.  (The current R implementation differs from the originalin stopping iteration if the MAD is effectively zero since thealgorithm is highly unstable in that case.)delta is used to speed up computation: instead of computing thelocal polynomial fit at each data point it is not computed for pointswithin delta of the last computed point, and linearinterpolation is used to fill in the fitted values for the skippedpoints.
NA
NA
If weights are specified then a weighted least squares is performedwith the weight given to the jth case specified by the jthentry in wt.If any observation has a missing value in any field, that observationis removed before the analysis is carried out.This can be quite inefficient if there is a lot of missing data.The implementation is via a modification of the LINPACK subroutineswhich allow for multiple left-hand sides.
The actual value calculated is constant * cMedian(abs(x - center))with the default value of center being median(x), andcMedian being the usual, the ‘low’ or ‘high’ median, seethe arguments description for low and high above.In the case of n = 1 non-missing values and default center, theresult is 0, consistent with “no deviation from the center”.The default constant = 1.4826 (approximately1/ Φ^(-1)(3/4) = 1/qnorm(3/4))ensures consistency, i.e.,E[mad(X_1,…,X_n)] = σfor X_i distributed as N(μ, σ^2)and large n.If na.rm is TRUE then NAvalues are stripped from x before computation takes place.If this is not done then an NA value inx will cause mad to return NA.
NA
NA
These functions work with a general univariate state-space modelwith state vector a, transitions a <- T a + R e,e ~ N(0, kappa Q) and observationequation y = Z'a + eta,eta ~ N(0, kappa h).The likelihood is a profile likelihood after estimation ofkappa.The model is specified as a list with at least componentsthe transition matrixthe observation coefficientsthe observation varianceRQR'the current state estimatethe current estimate of the state uncertainty matrix Qthe estimate at time t-1 of the stateuncertainty matrix Q (not updated by KalmanForecast).KalmanSmooth is the workhorse function for tsSmooth.makeARIMA constructs the state-space model for an ARIMA model,see also arima.The state-space initialization has used Gardner et al's method(SSinit = "Gardner1980"), as only method for years.  However,that suffers sometimes from deficiencies when close to non-stationarity.For this reason, it may be replaced as default in the future and onlykept for reproducibility reasons.  Explicit specification ofSSinit is therefore recommended, notably also inarima().The "Rossignol2011" method has been proposed and partlydocumented by Raphael Rossignol, Univ. Grenoble, on 2011-09-20 (seePR#14682, below), and later been ported to C by Matwey V. Kornilov.It computes the covariance matrix of(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})by the method of difference equations (page 93 of Brockwell and Davis),apparently suggested by a referee of  Gardner et al (see p.314 oftheir paper).
This is a generic function with methods for poly, bs andns: the default method handles scale.  Ifmodel.frame.default encounters such a term whencreating a model frame, it modifies the predvars attribute ofthe terms supplied by replacing the term with one which will work forpredicting new data.  For example makepredictcall.ns addsarguments for the knots and intercept.To make use of this, have your model-fitting function return theterms attribute of the model frame, or copy the predvarsattribute of the terms attribute of the model frame to yourterms object.To extend this, make sure the term creates variables with a class,and write a suitable method for that class.
Class "manova" differs from class "aov" in selecting adifferent summary method.  Function manova callsaov and then add class "manova" to the resultobject for each stratum.
If x is an array, each dimension must be at least 2, andthe entries should be nonnegative integers.  NA's are notallowed.  Otherwise, x, y and z must have thesame length.  Triples containing NA's are removed.  Allvariables must take at least two different values.
Mauchly's test test for whether a covariance matrix can be assumed tobe proportional to a given matrix.This is a generic function with methods for classes "mlm" and"SSD".The basic method is for objects ofclass SSD the method for mlm objects just extracts theSSD matrix and invokes the corresponding method with the same optionsand arguments.The T argument is used to transform the observations prior totesting. This typically involves transformation to intra-blockdifferences, but more complicated within-block designs can beencountered, making more elaborate transformations necessary. Amatrix T can be given directly or specified asthe difference between two projections onto the spaces spanned byM and X, which in turn can be given as matrices or asmodel formulas with respect to idata (the tests will beinvariant to parametrization of the quotient space M/X).The common use of this test is in repeated measurements designs, withX = ~1. This is almost, but not quite the same as testing forcompound symmetry in the untransformed covariance matrix.Notice that the defaults involve p, which is calculatedinternally as the dimension of the SSD matrix, and a couple of hiddenfunctions in the stats namespace, namely proj whichcalculates projection matrices from design matrices or model formulasand Thin.row which removes linearly dependent rows from amatrix until it has full row rank.
The null is that the probabilities of being classified into cells[i,j] and [j,i] are the same.If x is a matrix, it is taken as a two-dimensional contingencytable, and hence its entries should be nonnegative integers.Otherwise, both x and y must be vectors or factors of thesame length.  Incomplete cases are removed, vectors are coerced intofactors, and the contingency table is computed from these.Continuity correction is only used in the 2-by-2 case ifcorrect is TRUE.
This is a generic function for which methods can be written.  However,the default method makes use of is.na, sort andmean from package base all of which are generic, and sothe default method will work for most classes(e.g., "Date") for which a median is a reasonableconcept.
This is a generic function for which methods can be written.  However,the default method makes use of is.na, sort andmean from package base all of which are generic, and sothe default method will work for most classes(e.g., "Date") for which a median is a reasonableconcept.
The model fitted is additive (constant + rows + columns). Thealgorithm works by alternately removing the row and column medians,and continues until the proportional reduction in the sumof absolute residuals is less than epsor until there have been maxiter iterations.The sum of absolute residuals is printed ateach iteration of the fitting process, if trace.iter is TRUE.If na.rm is FALSE the presence of any NA value inx will cause an error, otherwise NA values are ignored.medpolish returns an object of class medpolish (see below).There are printing and plotting methods for thisclass, which are invoked via by the genericsprint and plot.
model.extract is provided for compatibility with S, which doesnot have the more specific functions.  It is also useful to extracte.g. the etastart and mustart components of aglm fit.model.extract(m, "offset") and model.extract(m, "response")are equivalent to model.offset(m) and model.response(m)respectively.  model.offset sums any terms specified byoffset terms in the formula or by offset argumentsin the call producing the model frame: it does check that the offsetis numeric.model.weights is slightly different frommodel.extract(, "weights") in not naming the vector it returns.
Exactly what happens depends on the class and attributes of the objectformula.  If this is an object of fitted-model class such as"lm", the method will either return the saved model frameused when fitting the model (if any, often selected by argumentmodel = TRUE) or pass the call used when fitting on to thedefault method.  The default method itself can cope with ratherstandard model objects such as those of class"lqs" from package MASS if no otherarguments are supplied.The rest of this section applies only to the default method.If either formula or data is already a model frame (adata frame with a "terms" attribute) and the other is missing,the model frame is returned.  Unless formula is a terms object,as.formula and then terms is called on it.  (If you wishto use the keep.order argument of terms.formula, pass aterms object rather than a formula.)Row names for the model frame are taken from the data argumentif present, then from the names of the response in the formula (orrownames if it is a matrix), if there is one.All the variables in formula, subset and in ...are looked for first in data and then in the environment offormula (see the help for formula() for furtherdetails) and collected into a data frame.  Then the subsetexpression is evaluated, and it is used as a row index to the dataframe.  Then the na.action function is applied to the data frame(and may well add attributes).  The levels of any factors in the dataframe are adjusted according to the drop.unused.levels andxlev arguments: if xlev specifies a factor and acharacter variable is found, it is converted to a factor (as from R2.10.0).Unless na.action = NULL, time-series attributes will be removedfrom the variables found (since they will be wrong if NAs areremoved).Note that all the variables in the formula are included in thedata frame, even those preceded by -.Only variables whose type is raw, logical, integer, real, complex orcharacter can be included in a model frame: this includes classedvariables such as factors (whose underlying type is integer), butexcludes lists.get_all_vars returns a data.frame containing thevariables used in formula plus those specified in ...which are recycled to the number of data frame rows.Unlike model.frame.default, it returns the input variables andnot those resulting from function calls in formula.
Exactly what happens depends on the class and attributes of the objectformula.  If this is an object of fitted-model class such as"lm", the method will either return the saved model frameused when fitting the model (if any, often selected by argumentmodel = TRUE) or pass the call used when fitting on to thedefault method.  The default method itself can cope with ratherstandard model objects such as those of class"lqs" from package MASS if no otherarguments are supplied.The rest of this section applies only to the default method.If either formula or data is already a model frame (adata frame with a "terms" attribute) and the other is missing,the model frame is returned.  Unless formula is a terms object,as.formula and then terms is called on it.  (If you wishto use the keep.order argument of terms.formula, pass aterms object rather than a formula.)Row names for the model frame are taken from the data argumentif present, then from the names of the response in the formula (orrownames if it is a matrix), if there is one.All the variables in formula, subset and in ...are looked for first in data and then in the environment offormula (see the help for formula() for furtherdetails) and collected into a data frame.  Then the subsetexpression is evaluated, and it is used as a row index to the dataframe.  Then the na.action function is applied to the data frame(and may well add attributes).  The levels of any factors in the dataframe are adjusted according to the drop.unused.levels andxlev arguments: if xlev specifies a factor and acharacter variable is found, it is converted to a factor (as from R2.10.0).Unless na.action = NULL, time-series attributes will be removedfrom the variables found (since they will be wrong if NAs areremoved).Note that all the variables in the formula are included in thedata frame, even those preceded by -.Only variables whose type is raw, logical, integer, real, complex orcharacter can be included in a model frame: this includes classedvariables such as factors (whose underlying type is integer), butexcludes lists.get_all_vars returns a data.frame containing thevariables used in formula plus those specified in ...which are recycled to the number of data frame rows.Unlike model.frame.default, it returns the input variables andnot those resulting from function calls in formula.
model.matrix creates a design matrix from the descriptiongiven in terms(object), using the data in data whichmust supply variables with the same names as would be created by acall to model.frame(object) or, more precisely, by evaluatingattr(terms(object), "variables").  If data is a dataframe, there may be other columns and the order of columns is notimportant.  Any character variables are coerced to factors.  Aftercoercion, all the variables used on the right-hand side of theformula must be logical, integer, numeric or factor.If contrasts.arg is specified for a factor it overrides thedefault factor coding for that variable and any "contrasts"attribute set by C or contrasts.Whereas invalid contrasts.args have been ignored always, they arewarned about since R version 3.6.0.In an interaction term, the variable whose levels vary fastest is thefirst one to appear in the formula (and not in the term), so in~ a + b + b:a the interaction will have a varyingfastest.By convention, if the response variable also appears on theright-hand side of the formula it is dropped (with a warning),although interactions involving the term are retained.
model.matrix creates a design matrix from the descriptiongiven in terms(object), using the data in data whichmust supply variables with the same names as would be created by acall to model.frame(object) or, more precisely, by evaluatingattr(terms(object), "variables").  If data is a dataframe, there may be other columns and the order of columns is notimportant.  Any character variables are coerced to factors.  Aftercoercion, all the variables used on the right-hand side of theformula must be logical, integer, numeric or factor.If contrasts.arg is specified for a factor it overrides thedefault factor coding for that variable and any "contrasts"attribute set by C or contrasts.Whereas invalid contrasts.args have been ignored always, they arewarned about since R version 3.6.0.In an interaction term, the variable whose levels vary fastest is thefirst one to appear in the formula (and not in the term), so in~ a + b + b:a the interaction will have a varyingfastest.By convention, if the response variable also appears on theright-hand side of the formula it is dropped (with a warning),although interactions involving the term are retained.
model.matrix creates a design matrix from the descriptiongiven in terms(object), using the data in data whichmust supply variables with the same names as would be created by acall to model.frame(object) or, more precisely, by evaluatingattr(terms(object), "variables").  If data is a dataframe, there may be other columns and the order of columns is notimportant.  Any character variables are coerced to factors.  Aftercoercion, all the variables used on the right-hand side of theformula must be logical, integer, numeric or factor.If contrasts.arg is specified for a factor it overrides thedefault factor coding for that variable and any "contrasts"attribute set by C or contrasts.Whereas invalid contrasts.args have been ignored always, they arewarned about since R version 3.6.0.In an interaction term, the variable whose levels vary fastest is thefirst one to appear in the formula (and not in the term), so in~ a + b + b:a the interaction will have a varyingfastest.By convention, if the response variable also appears on theright-hand side of the formula it is dropped (with a warning),although interactions involving the term are retained.
model.extract is provided for compatibility with S, which doesnot have the more specific functions.  It is also useful to extracte.g. the etastart and mustart components of aglm fit.model.extract(m, "offset") and model.extract(m, "response")are equivalent to model.offset(m) and model.response(m)respectively.  model.offset sums any terms specified byoffset terms in the formula or by offset argumentsin the call producing the model frame: it does check that the offsetis numeric.model.weights is slightly different frommodel.extract(, "weights") in not naming the vector it returns.
model.extract is provided for compatibility with S, which doesnot have the more specific functions.  It is also useful to extracte.g. the etastart and mustart components of aglm fit.model.extract(m, "offset") and model.extract(m, "response")are equivalent to model.offset(m) and model.response(m)respectively.  model.offset sums any terms specified byoffset terms in the formula or by offset argumentsin the call producing the model frame: it does check that the offsetis numeric.model.weights is slightly different frommodel.extract(, "weights") in not naming the vector it returns.
For type = "effects" give tables of the coefficients for eachterm, optionally with standard errors.For type = "means" give tables of the mean response for eachcombinations of levels of the factors in a term.The "aov" method cannot be applied to components of a"aovlist" fit.
model.extract is provided for compatibility with S, which doesnot have the more specific functions.  It is also useful to extracte.g. the etastart and mustart components of aglm fit.model.extract(m, "offset") and model.extract(m, "response")are equivalent to model.offset(m) and model.response(m)respectively.  model.offset sums any terms specified byoffset terms in the formula or by offset argumentsin the call producing the model frame: it does check that the offsetis numeric.model.weights is slightly different frommodel.extract(, "weights") in not naming the vector it returns.
These functions extract subseries from a time series and plot themall in one frame.  The ts, stl, andStructTS methods use the internally recorded frequency andstart and finish times to set the scale and the seasons.  The defaultmethod assumes observations come in groups of 12 (though this can bechanged).If the labels are not given but the phase is given, thenthe labels default to the unique values of the phase.  Ifboth are given, then the phase values are assumed to be indicesinto the labels array, i.e., they should be in the rangefrom 1 to length(labels).
The underlying model is that the two samples are drawn fromf(x-l) and f((x-l)/s)/s, respectively, where l is acommon location parameter and s is a scale parameter.The null hypothesis is s = 1.There are more useful tests for this problem.In the case of ties, the formulation of Mielke (1967) is employed.
NA
na.action is a generic function, and na.action.default itsdefault method.  The latter extracts the "na.action" componentof a list if present, otherwise the "na.action" attribute.When model.frame is called, it records any informationon NA handling in a "na.action" attribute.  Mostmodel-fitting functions return this as a component of their result.
NA
At present these will handle vectors, matrices and data framescomprising vectors and matrices (only).If na.omit removes cases, the row numbers of the cases form the"na.action" attribute of the result, of class "omit".na.exclude differs from na.omit only in the class of the"na.action" attribute of the result, which is"exclude".  This gives different behaviour in functions makinguse of naresid and napredict: whenna.exclude is used the residuals and predictions are padded tothe correct length by inserting NAs for cases omitted byna.exclude.
At present these will handle vectors, matrices and data framescomprising vectors and matrices (only).If na.omit removes cases, the row numbers of the cases form the"na.action" attribute of the result, of class "omit".na.exclude differs from na.omit only in the class of the"na.action" attribute of the result, which is"exclude".  This gives different behaviour in functions makinguse of naresid and napredict: whenna.exclude is used the residuals and predictions are padded tothe correct length by inserting NAs for cases omitted byna.exclude.
At present these will handle vectors, matrices and data framescomprising vectors and matrices (only).If na.omit removes cases, the row numbers of the cases form the"na.action" attribute of the result, of class "omit".na.exclude differs from na.omit only in the class of the"na.action" attribute of the result, which is"exclude".  This gives different behaviour in functions makinguse of naresid and napredict: whenna.exclude is used the residuals and predictions are padded tothe correct length by inserting NAs for cases omitted byna.exclude.
At present these will handle vectors, matrices and data framescomprising vectors and matrices (only).If na.omit removes cases, the row numbers of the cases form the"na.action" attribute of the result, of class "omit".na.exclude differs from na.omit only in the class of the"na.action" attribute of the result, which is"exclude".  This gives different behaviour in functions makinguse of naresid and napredict: whenna.exclude is used the residuals and predictions are padded tothe correct length by inserting NAs for cases omitted byna.exclude.
These are utility functions used to allow predict,fitted and residuals methods for modellingfunctions to compensate for the removal of NAs in the fittingprocess.  They are used by the default, "lm", "glm" and"nls" methods, and by further methods in packages MASS,rpart and survival.  Also used for the scores returned byfactanal, prcomp and princomp.The default methods do nothing.  The default method for the na.excludeaction is to pad the object with NAs in the correct positions tohave the same number of rows as the original data frame.Currently naresid and napredict are identical, butfuture methods need not be.  naresid is used for residuals, andnapredict for fitted values, predictions and weights.
This is a generic function, and the exact information differs bymethod. naprint.omit reports the number of rows omitted:naprint.default reports an empty string.
These are utility functions used to allow predict,fitted and residuals methods for modellingfunctions to compensate for the removal of NAs in the fittingprocess.  They are used by the default, "lm", "glm" and"nls" methods, and by further methods in packages MASS,rpart and survival.  Also used for the scores returned byfactanal, prcomp and princomp.The default methods do nothing.  The default method for the na.excludeaction is to pad the object with NAs in the correct positions tohave the same number of rows as the original data frame.Currently naresid and napredict are identical, butfuture methods need not be.  naresid is used for residuals, andnapredict for fitted values, predictions and weights.
NA
Note that arguments after ... must be matched exactly.If a gradient or hessian is supplied but evaluates to the wrong modeor length, it will be ignored if check.analyticals = TRUE (thedefault) with a warning.  The hessian is not even checked unless thegradient is present and passes the sanity checks.The C code for the “perturbed” cholesky, choldc() hashad a bug in all R versions before 3.4.1.From the three methods available in the original source, we always usemethod “1” which is line search.The functions supplied should always return finite (including notNA and not NaN) values: for the function value itselfnon-finite values are replaced by the maximum positive value with a warning.
Any names of start are passed on to objective and whereapplicable, gradient and hessian.  The parameter vectorwill be coerced to double.If any of the functions returns NA or NaN this is anerror for the gradient and Hessian, and such values for functionevaluation are replaced by +Inf with a warning.
An nls object is a type of fitted model object.  It has methodsfor the generic functions anova, coef,confint, deviance,df.residual, fitted,formula, logLik, predict,print, profile, residuals,summary, vcov and weights.Variables in formula (and weights if not missing) arelooked for first in data, then the environment offormula and finally along the search path.  Functions informula are searched for first in the environment offormula and then along the search path.Arguments subset and na.action are supported only whenall the variables in the formula taken from data are of thesame length: other cases give a warning.Note that the anova method does not check that themodels are nested: this cannot easily be done automatically, so usewith care.
NA
NA
NA
NA
NA
This is a generic function, with an S4 generic in package stats4.There are methods in this package for objects of classes"lm", "glm", "nls" and"logLik", as well as a default method (which throws anerror, unless use.fallback = TRUE when it looks forweights and residuals components – use with care!).The main usage is in determining the appropriate penalty for BIC, butnobs is also used by the stepwise fitting methodsstep, add1 and drop1 as aquick check that different fits have been fitted to the same set ofdata (and not, say, that further rows have been dropped because of NAsin the new predictors).For lm, glm and nls fits, observations with zeroweight are not included.
This is a front end to the C function numeric_deriv, which isdescribed in Writing R Extensions.The numeric variables must be of type double and not integer.
There can be more than one offset in a model formula, but - isnot supported for offset terms (and is equivalent to +).
If the right-hand side of the formula contains more than one term,their interaction is taken to form the grouping.
Note that arguments after ... must be matched exactly.By default optim performs minimization, but it will maximizeif control$fnscale is negative.  optimHess is anauxiliary function to compute the Hessian at a later stage ifhessian = TRUE was forgotten.The default method is an implementation of that of Nelder and Mead(1965), that uses only function values and is robust but relativelyslow.  It will work reasonably well for non-differentiable functions.Method "BFGS" is a quasi-Newton method (also known as a variablemetric algorithm), specifically that published simultaneously in 1970by Broyden, Fletcher, Goldfarb and Shanno.  This uses function valuesand gradients to build up a picture of the surface to be optimized.Method "CG" is a conjugate gradients method based on that byFletcher and Reeves (1964) (but with the option of Polak–Ribiere orBeale–Sorenson updates).  Conjugate gradient methods will generallybe more fragile than the BFGS method, but as they do not store amatrix they may be successful in much larger optimization problems.Method "L-BFGS-B" is that of Byrd et. al. (1995) whichallows box constraints, that is each variable can be given a lowerand/or upper bound. The initial value must satisfy the constraints.This uses a limited-memory modification of the BFGS quasi-Newtonmethod.  If non-trivial bounds are supplied, this method will beselected, with a warning.Nocedal and Wright (1999) is a comprehensive reference for theprevious three methods.Method "SANN" is by default a variant of simulated annealinggiven in Belisle (1992). Simulated-annealing belongs to the class ofstochastic global optimization methods. It uses only function valuesbut is relatively slow. It will also work for non-differentiablefunctions. This implementation uses the Metropolis function for theacceptance probability. By default the next candidate point isgenerated from a Gaussian Markov kernel with scale proportional to theactual temperature. If a function to generate a new candidate point isgiven, method "SANN" can also be used to solve combinatorialoptimization problems. Temperatures are decreased according to thelogarithmic cooling schedule as given in Belisle (1992, p. 890);specifically, the temperature is set totemp / log(((t-1) %/% tmax)*tmax + exp(1)), where t isthe current iteration step and temp and tmax arespecifiable via control, see below.  Note that the"SANN" method depends critically on the settings of the controlparameters. It is not a general-purpose method but can be very usefulin getting to a good value on a very rough surface.Method "Brent" is for one-dimensional problems only, usingoptimize().  It can be useful in cases whereoptim() is used inside other functions where only methodcan be specified, such as in mle from package stats4.Function fn can return NA or Inf if the functioncannot be evaluated at the supplied value, but the initial value musthave a computable finite value of fn.(Except for method "L-BFGS-B" where the values should always befinite.)optim can be used recursively, and for a single parameteras well as many.  It also accepts a zero-length par, and justevaluates the function with that argument.The control argument is a list that can supply any of thefollowing components:Non-negative integer. If positive,tracing information on theprogress of the optimization is produced. Higher values mayproduce more tracing information: for method "L-BFGS-B"there are six levels of tracing.  (To understand exactly whatthese do see the source code: higher levels give more detail.)An overall scaling to be applied to the valueof fn and gr during optimization. If negative,turns the problem into a maximization problem. Optimization isperformed on fn(par)/fnscale.A vector of scaling values for the parameters.Optimization is performed on par/parscale and these should becomparable in the sense that a unit change in any element producesabout a unit change in the scaled value.  Not used (nor needed)for method = "Brent".A vector of step sizes for the finite-differenceapproximation to the gradient, on par/parscalescale. Defaults to 1e-3.The maximum number of iterations. Defaults to100 for the derivative-based methods, and500 for "Nelder-Mead".For "SANN" maxit gives the total number of functionevaluations: there is no other stopping criterion. Defaults to10000.The absolute convergence tolerance. Onlyuseful for non-negative functions, as a tolerance for reaching zero.Relative convergence tolerance.  The algorithmstops if it is unable to reduce the value by a factor ofreltol * (abs(val) + reltol) at a step.  Defaults tosqrt(.Machine$double.eps), typically about 1e-8.Scaling parametersfor the "Nelder-Mead" method. alpha is the reflectionfactor (default 1.0), beta the contraction factor (0.5) andgamma the expansion factor (2.0).The frequency of reports for the "BFGS","L-BFGS-B" and "SANN" methods if control$traceis positive. Defaults to every 10 iterations for "BFGS" and"L-BFGS-B", or every 100 temperatures for "SANN".a logical indicatingif the (default) "Nelder-Mean" method should signal awarning when used for one-dimensional minimization.  As thewarning is sometimes inappropriate, you can suppress it by settingthis option to false.for the conjugate-gradients method.  Takes value1 for the Fletcher–Reeves update, 2 forPolak–Ribiere and 3 for Beale–Sorenson.is an integer giving the number of BFGS updatesretained in the "L-BFGS-B" method, It defaults to 5.controls the convergence of the "L-BFGS-B"method. Convergence occurs when the reduction in the objective iswithin this factor of the machine tolerance. Default is 1e7,that is a tolerance of about 1e-8.helps control the convergence of the "L-BFGS-B"method. It is a tolerance on the projected gradient in the currentsearch direction. This defaults to zero, when the check issuppressed.controls the "SANN" method. It is thestarting temperature for the cooling schedule. Defaults to10.is the number of function evaluations at eachtemperature for the "SANN" method. Defaults to 10.Any names given to par will be copied to the vectors passed tofn and gr.  Note that no other attributes of parare copied over.The parameter vector passed to fn has special semantics and maybe shared between calls: the function should not change or copy it.
Note that arguments after ... must be matched exactly.By default optim performs minimization, but it will maximizeif control$fnscale is negative.  optimHess is anauxiliary function to compute the Hessian at a later stage ifhessian = TRUE was forgotten.The default method is an implementation of that of Nelder and Mead(1965), that uses only function values and is robust but relativelyslow.  It will work reasonably well for non-differentiable functions.Method "BFGS" is a quasi-Newton method (also known as a variablemetric algorithm), specifically that published simultaneously in 1970by Broyden, Fletcher, Goldfarb and Shanno.  This uses function valuesand gradients to build up a picture of the surface to be optimized.Method "CG" is a conjugate gradients method based on that byFletcher and Reeves (1964) (but with the option of Polak–Ribiere orBeale–Sorenson updates).  Conjugate gradient methods will generallybe more fragile than the BFGS method, but as they do not store amatrix they may be successful in much larger optimization problems.Method "L-BFGS-B" is that of Byrd et. al. (1995) whichallows box constraints, that is each variable can be given a lowerand/or upper bound. The initial value must satisfy the constraints.This uses a limited-memory modification of the BFGS quasi-Newtonmethod.  If non-trivial bounds are supplied, this method will beselected, with a warning.Nocedal and Wright (1999) is a comprehensive reference for theprevious three methods.Method "SANN" is by default a variant of simulated annealinggiven in Belisle (1992). Simulated-annealing belongs to the class ofstochastic global optimization methods. It uses only function valuesbut is relatively slow. It will also work for non-differentiablefunctions. This implementation uses the Metropolis function for theacceptance probability. By default the next candidate point isgenerated from a Gaussian Markov kernel with scale proportional to theactual temperature. If a function to generate a new candidate point isgiven, method "SANN" can also be used to solve combinatorialoptimization problems. Temperatures are decreased according to thelogarithmic cooling schedule as given in Belisle (1992, p. 890);specifically, the temperature is set totemp / log(((t-1) %/% tmax)*tmax + exp(1)), where t isthe current iteration step and temp and tmax arespecifiable via control, see below.  Note that the"SANN" method depends critically on the settings of the controlparameters. It is not a general-purpose method but can be very usefulin getting to a good value on a very rough surface.Method "Brent" is for one-dimensional problems only, usingoptimize().  It can be useful in cases whereoptim() is used inside other functions where only methodcan be specified, such as in mle from package stats4.Function fn can return NA or Inf if the functioncannot be evaluated at the supplied value, but the initial value musthave a computable finite value of fn.(Except for method "L-BFGS-B" where the values should always befinite.)optim can be used recursively, and for a single parameteras well as many.  It also accepts a zero-length par, and justevaluates the function with that argument.The control argument is a list that can supply any of thefollowing components:Non-negative integer. If positive,tracing information on theprogress of the optimization is produced. Higher values mayproduce more tracing information: for method "L-BFGS-B"there are six levels of tracing.  (To understand exactly whatthese do see the source code: higher levels give more detail.)An overall scaling to be applied to the valueof fn and gr during optimization. If negative,turns the problem into a maximization problem. Optimization isperformed on fn(par)/fnscale.A vector of scaling values for the parameters.Optimization is performed on par/parscale and these should becomparable in the sense that a unit change in any element producesabout a unit change in the scaled value.  Not used (nor needed)for method = "Brent".A vector of step sizes for the finite-differenceapproximation to the gradient, on par/parscalescale. Defaults to 1e-3.The maximum number of iterations. Defaults to100 for the derivative-based methods, and500 for "Nelder-Mead".For "SANN" maxit gives the total number of functionevaluations: there is no other stopping criterion. Defaults to10000.The absolute convergence tolerance. Onlyuseful for non-negative functions, as a tolerance for reaching zero.Relative convergence tolerance.  The algorithmstops if it is unable to reduce the value by a factor ofreltol * (abs(val) + reltol) at a step.  Defaults tosqrt(.Machine$double.eps), typically about 1e-8.Scaling parametersfor the "Nelder-Mead" method. alpha is the reflectionfactor (default 1.0), beta the contraction factor (0.5) andgamma the expansion factor (2.0).The frequency of reports for the "BFGS","L-BFGS-B" and "SANN" methods if control$traceis positive. Defaults to every 10 iterations for "BFGS" and"L-BFGS-B", or every 100 temperatures for "SANN".a logical indicatingif the (default) "Nelder-Mean" method should signal awarning when used for one-dimensional minimization.  As thewarning is sometimes inappropriate, you can suppress it by settingthis option to false.for the conjugate-gradients method.  Takes value1 for the Fletcher–Reeves update, 2 forPolak–Ribiere and 3 for Beale–Sorenson.is an integer giving the number of BFGS updatesretained in the "L-BFGS-B" method, It defaults to 5.controls the convergence of the "L-BFGS-B"method. Convergence occurs when the reduction in the objective iswithin this factor of the machine tolerance. Default is 1e7,that is a tolerance of about 1e-8.helps control the convergence of the "L-BFGS-B"method. It is a tolerance on the projected gradient in the currentsearch direction. This defaults to zero, when the check issuppressed.controls the "SANN" method. It is thestarting temperature for the cooling schedule. Defaults to10.is the number of function evaluations at eachtemperature for the "SANN" method. Defaults to 10.Any names given to par will be copied to the vectors passed tofn and gr.  Note that no other attributes of parare copied over.The parameter vector passed to fn has special semantics and maybe shared between calls: the function should not change or copy it.
Note that arguments after ... must be matched exactly.The method used is a combination of golden section search andsuccessive parabolic interpolation, and was designed for use withcontinuous functions.  Convergence is never much slowerthan that for a Fibonacci search.  If f has a continuous secondderivative which is positive at the minimum (which is not at lower orupper), then convergence is superlinear, and usually of theorder of about 1.324.The function f is never evaluated at two points closer togetherthan eps * |x_0| + (tol/3), whereeps is approximately sqrt(.Machine$double.eps)and x_0 is the final abscissa optimize()$minimum.If f is a unimodal function and the computed values of fare always unimodal when separated by at least eps * |x| + (tol/3), then x_0 approximates the abscissa of theglobal minimum of f on the interval lower,upper with anerror less than eps * |x_0|+ tol.If f is not unimodal, then optimize() may approximate alocal, but perhaps non-global, minimum to the same accuracy.The first evaluation of f is always atx_1 = a + (1-φ)(b-a) where (a,b) = (lower, upper) andphi = (sqrt(5) - 1)/2 = 0.61803..is the golden section ratio.Almost always, the second evaluation is atx_2 = a + phi(b-a).Note that a local minimum inside [x_1,x_2] will be found assolution, even when f is constant in there, see the lastexample.f will be called as f(x, ...) for a numeric valueof x.The argument passed to f has special semantics and used to beshared between calls.  The function should not copy it.
Note that arguments after ... must be matched exactly.The method used is a combination of golden section search andsuccessive parabolic interpolation, and was designed for use withcontinuous functions.  Convergence is never much slowerthan that for a Fibonacci search.  If f has a continuous secondderivative which is positive at the minimum (which is not at lower orupper), then convergence is superlinear, and usually of theorder of about 1.324.The function f is never evaluated at two points closer togetherthan eps * |x_0| + (tol/3), whereeps is approximately sqrt(.Machine$double.eps)and x_0 is the final abscissa optimize()$minimum.If f is a unimodal function and the computed values of fare always unimodal when separated by at least eps * |x| + (tol/3), then x_0 approximates the abscissa of theglobal minimum of f on the interval lower,upper with anerror less than eps * |x_0|+ tol.If f is not unimodal, then optimize() may approximate alocal, but perhaps non-global, minimum to the same accuracy.The first evaluation of f is always atx_1 = a + (1-φ)(b-a) where (a,b) = (lower, upper) andphi = (sqrt(5) - 1)/2 = 0.61803..is the golden section ratio.Almost always, the second evaluation is atx_2 = a + phi(b-a).Note that a local minimum inside [x_1,x_2] will be found assolution, even when f is constant in there, see the lastexample.f will be called as f(x, ...) for a numeric valueof x.The argument passed to f has special semantics and used to beshared between calls.  The function should not copy it.
The indices or labels for the leaves in left to right order are retrieved.
The adjustment methods include the Bonferroni correction("bonferroni") in which the p-values are multiplied by thenumber of comparisons.  Less conservative corrections are alsoincluded by Holm (1979) ("holm"), Hochberg (1988)("hochberg"), Hommel (1988) ("hommel"), Benjamini &Hochberg (1995) ("BH" or its alias "fdr"), and Benjamini &Yekutieli (2001) ("BY"), respectively.A pass-through option ("none") is also included.The set of methods are contained in the p.adjust.methods vectorfor the benefit of methods that need to have the method as an optionand pass it on to p.adjust.The first four methods are designed to give strong control of thefamily-wise error rate.  There seems no reason to use the unmodifiedBonferroni correction because it is dominated by Holm's method, whichis also valid under arbitrary assumptions.Hochberg's and Hommel's methods are valid when the hypothesis testsare independent or when they are non-negatively associated (Sarkar,1998; Sarkar and Chang, 1997).  Hommel's method is more powerful thanHochberg's, but the difference is usually small and the Hochbergp-values are faster to compute.The "BH" (aka "fdr") and "BY" methods ofBenjamini, Hochberg, and Yekutieli control the false discovery rate,the expected proportion of false discoveries amongst the rejectedhypotheses.  The false discovery rate is a less stringent conditionthan the family-wise error rate, so these methods are more powerfulthan the others.Note that you can set n larger than length(p) whichmeans the unobserved p-values are assumed to be greater than all theobserved p for "bonferroni" and "holm" methods and equalto 1 for the other methods.
The adjustment methods include the Bonferroni correction("bonferroni") in which the p-values are multiplied by thenumber of comparisons.  Less conservative corrections are alsoincluded by Holm (1979) ("holm"), Hochberg (1988)("hochberg"), Hommel (1988) ("hommel"), Benjamini &Hochberg (1995) ("BH" or its alias "fdr"), and Benjamini &Yekutieli (2001) ("BY"), respectively.A pass-through option ("none") is also included.The set of methods are contained in the p.adjust.methods vectorfor the benefit of methods that need to have the method as an optionand pass it on to p.adjust.The first four methods are designed to give strong control of thefamily-wise error rate.  There seems no reason to use the unmodifiedBonferroni correction because it is dominated by Holm's method, whichis also valid under arbitrary assumptions.Hochberg's and Hommel's methods are valid when the hypothesis testsare independent or when they are non-negatively associated (Sarkar,1998; Sarkar and Chang, 1997).  Hommel's method is more powerful thanHochberg's, but the difference is usually small and the Hochbergp-values are faster to compute.The "BH" (aka "fdr") and "BY" methods ofBenjamini, Hochberg, and Yekutieli control the false discovery rate,the expected proportion of false discoveries amongst the rejectedhypotheses.  The false discovery rate is a less stringent conditionthan the family-wise error rate, so these methods are more powerfulthan the others.Note that you can set n larger than length(p) whichmeans the unobserved p-values are assumed to be greater than all theobserved p for "bonferroni" and "holm" methods and equalto 1 for the other methods.
For type = "correlation" and "covariance", theestimates are based on the sample covariance. (The lag 0 autocorrelationis fixed at 1 by convention.)By default, no missing values are allowed.  If the na.actionfunction passes through missing values (as na.pass does), thecovariances are computed from the complete cases.  This means that theestimate computed may well not be a valid autocorrelation sequence,and may contain missing values.  Missing values are not allowed whencomputing the PACF of a multivariate time series.The partial correlation coefficient is estimated by fittingautoregressive models of successively higher orders up tolag.max.The generic function plot has a method for objects of class"acf".The lag is returned and plotted in units of time, and not numbers ofobservations.There are print and subsetting methods for objects of class"acf".
NA
NA
 The pool.sd switch calculates a common SD for allgroups and uses that for all comparisons (this can be useful if somegroups are small). This method does not actually call t.test,so extra arguments are ignored. Pooling does not generalize to paired testsso pool.sd and paired cannot both be TRUE.Only the lower triangle of the matrix of possible comparisons is beingcalculated, so setting alternative to anything other than"two.sided" requires that the levels of g are orderedsensibly.
Functions that do multiple group comparisons create separatecompare.levels functions (assumed to be symmetrical in iand j) and passes them to this function.
Extra arguments that are passed on to wilcox.test may or maynot be sensible in this context. In particular,only the lower triangle of the matrix of possible comparisons is beingcalculated, so setting alternative to anything other than"two.sided" requires that the levels of g are orderedsensibly.
The Beta distribution with parameters shape1 = a andshape2 = b has densityΓ(a+b)/(Γ(a)Γ(b))x^(a-1)(1-x)^(b-1)for a > 0, b > 0 and 0 ≤ x ≤ 1where the boundary values at x=0 or x=1 are defined asby continuity (as limits).The mean is a/(a+b) and the variance is ab/((a+b)^2 (a+b+1)).These moments and all distributional properties can be defined aslimits (leading to point masses at 0, 1/2, or 1) when a orb are zero or infinite, and the corresponding[dpqr]beta() functions are defined correspondingly.pbeta is closely related to the incomplete beta function.  Asdefined by Abramowitz and Stegun 6.6.1B_x(a,b) =    integral_0^x t^(a-1) (1-t)^(b-1) dt,and 6.6.2 I_x(a,b) = B_x(a,b) / B(a,b) whereB(a,b) = B_1(a,b) is the Beta function (beta).I_x(a,b) is pbeta(x, a, b).The noncentral Beta distribution (with ncp  = λ)is defined (Johnson et al, 1995, pp. 502) as the distribution ofX/(X+Y) where X ~ chi^2_2a(λ)and Y ~ chi^2_2b.
The binomial distribution with size = n andprob = p has density    p(x) = choose(n, x) p^x (1-p)^(n-x)for x = 0, …, n.Note that binomial coefficients can be computed bychoose in R.If an element of x is not integer, the result of dbinomis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference below.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
The birthday paradox is that a very small number of people, 23,suffices to have a 50–50 chance that two or more of them have the samebirthday.  This function generalises the calculation to probabilitiesother than 0.5, numbers of coincident events other than 2, and numbersof classes other than 365.The formula used is approximate for coincident > 2.  Theapproximation is very good for moderate values of prob but lessgood for very small probabilities.
If location or scale are not specified, they assumethe default values of 0 and 1 respectively.The Cauchy distribution with location l and scale s hasdensityf(x) = 1 / (π s (1 + ((x-l)/s)^2))for all x.
The chi-squared distribution with df= n ≥ 0degrees of freedom has densityf_n(x) = 1 / (2^(n/2) Γ(n/2))  x^(n/2-1) e^(-x/2)for x > 0, where f_0(x) := \lim_{n \to 0} f_n(x) =  δ_0(x), a point mass at zero, is not a density function proper, buta “δ distribution”.The mean and variance are n and 2n.The non-central chi-squared distribution with df= ndegrees of freedom and non-centrality parameter ncp= λ has densityf(x) = exp(-λ/2) SUM_{r=0}^∞ ((λ/2)^r / r!) dchisq(x, df + 2r)  for x ≥ 0.  For integer n, this is the distribution ofthe sum of squares of n normals each with variance one,λ being the sum of squares of the normal means; further,E(X) = n + λ, Var(X) = 2(n + 2*λ), andE((X - E(X))^3) = 8(n + 3*λ).Note that the degrees of freedom df= n, can benon-integer, and also n = 0 which is relevant fornon-centrality λ > 0,see Johnson et al (1995, chapter 29).In that (noncentral, zero df) case, the distribution is a mixture of apoint mass at x = 0 (of size pchisq(0, df=0, ncp=ncp)) anda continuous part, and dchisq() is not a density withrespect to that mixture measure but rather the limit of the densityfor df -> 0.Note that ncp values larger than about 1e5 (and even smaller)  may give inaccurateresults with many warnings for pchisq and qchisq.
If rate is not specified, it assumes the default value of1.The exponential distribution with rate λ has densityf(x) = λ {e}^{- λ x} for x ≥ 0.
The F distribution with df1 = n1 and df2 =n2 degrees of freedom has densityf(x) = Γ((n1 + n2)/2) / (Γ(n1/2) Γ(n2/2))    (n1/n2)^(n1/2) x^(n1/2 - 1)    (1 + (n1/n2) x)^-(n1 + n2)/2for x > 0.It is the distribution of the ratio of the mean squares ofn1 and n2 independent standard normals, and henceof the ratio of two independent chi-squared variates each divided by itsdegrees of freedom.  Since the ratio of a normal and the rootmean-square of m independent normals has a Student's t_mdistribution, the square of a t_m variate has a F distribution on1 and m degrees of freedom.The non-central F distribution is again the ratio of mean squares ofindependent normals of unit variance, but those in the numerator areallowed to have non-zero means and ncp is the sum of squares ofthe means.  See Chisquare for further details onnon-central distributions.
If scale is omitted, it assumes the default value of 1.The Gamma distribution with parameters shape = aand scale = s has densityf(x)= 1/(s^a Gamma(a)) x^(a-1) e^-(x/s)for x ≥ 0, a > 0 and s > 0.(Here Gamma(a) is the function implemented by R'sgamma() and defined in its help.  Note that a = 0corresponds to the trivial distribution with all mass at point 0.)The mean and variance areE(X) = a*s andVar(X) = a*s^2.The cumulative hazard H(t) = - log(1 - F(t))isNote that for smallish values of shape (and moderatescale) a large parts of the mass of the Gamma distribution ison values of x so near zero that they will be represented aszero in computer arithmetic.  So rgamma may well return valueswhich will be represented as zero.  (This will also happen for verylarge values of scale since the actual generation is done forscale = 1.)
The geometric distribution with prob = p has densityp(x) = p (1-p)^xfor x = 0, 1, 2, …, 0 < p ≤ 1.If an element of x is not integer, the result of dgeomis zero, with a warning.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
The hypergeometric distribution is used for sampling withoutreplacement.  The density of this distribution with parametersm, n and k (named Np, N-Np, andn, respectively in the reference below, where N := m+n is also usedin other references) is given byp(x) =      choose(m, x) choose(n, k-x) / choose(m+n, k)for x = 0, …, k.Note that p(x) is non-zero only formax(0, k-n) <= x <= min(k, m).With p := m/(m+n) (hence Np = N \times p in thereference's notation), the first two moments are meanE[X] = μ = k p and variance               Var(X) = k p (1 - p) * (m+n-k)/(m+n-1),which shows the closeness to the Binomial(k,p) (where thehypergeometric has smaller variance unless k = 1).The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.In rhyper(), if one of m, n, k exceeds .Machine$integer.max,currently the equivalent of qhyper(runif(nn), m,n,k) is usedwhich is comparably slow while instead a binomial approximation may beconsiderably more efficient.
The *chisq() functions now take an optional non-centralityargument, so the *nchisq() functions are no longer needed.reshape*, which were experimental, are replaced byreshape.  This has a different syntax and allowsmultiple time-varying variables.arima0.diag has been replaced by tsdiag.arima0.plot.mts has been removed, as plot.ts now has thesame functionality.print.coefmat was an older name for printCoefmatwith a different default for na.print.anovalist.lm was replaced by anova.lmlist inR 1.2.0.lm.fit.null and lm.wfit.null are superseded bylm.fit and lm.wfit which handle null models now.Similarly, glm.fit.null is superseded by glm.fit.mauchley.test was a misspelling of Mauchly's name, correctedby the introduction of mauchly.test.clearNames had been introduced at about the same time asunname, but is less general and has been used rarely.plclust has been drawing dendrograms (“cluster - trees”) and been replaced by theplot() method for class "dendrogram".
The log normal distribution has densityf(x) = 1/(√(2 π) σ x) e^-((log x - μ)^2 / (2 σ^2))where μ and σ are the mean and standarddeviation of the logarithm.The mean is E(X) = exp(μ + 1/2 σ^2),the median is med(X) = exp(μ), and the varianceVar(X) = exp(2*μ + σ^2)*(exp(σ^2) - 1)and hence the coefficient of variation issqrt(exp(σ^2) - 1) which isapproximately σ when that is small (e.g., σ < 1/2).
If location or scale are omitted, they assume thedefault values of 0 and 1 respectively.The Logistic distribution with location = m andscale = s has distribution functionF(x) = 1 / (1 + exp(-(x-m)/s))  and densityf(x) = 1/s exp((x-m)/s) (1 + exp((x-m)/s))^-2.It is a long-tailed distribution with mean m and varianceπ^2 /3 s^2.
The e.c.d.f. (empirical cumulative distribution function)Fn is a step function with jumps i/n atobservation values, where i is the number of tied observationsat that value.  Missing values are ignored.For observationsx= (x1,x2, ... xn),Fn is the fraction of observations less or equal to t,i.e.,    Fn(t) = #{xi <= t}/n  =  1/n sum(i=1,n) Indicator(xi <= t).The function plot.ecdf which implements the plotmethod for ecdf objects, is implemented via a call toplot.stepfun; see its documentation.
NA
NA
NA
If y is missing, this function creates a time seriesplot, for multivariate series of one of two kinds depending onplot.type.If y is present, both x and y must be univariate,and a scatter plot y ~ x will be drawn, enhanced byusing text if xy.labels isTRUE or character, and lines ifxy.lines is TRUE.
The negative binomial distribution with size = n andprob = p has density    Γ(x+n)/(Γ(n) x!) p^n (1-p)^xfor x = 0, 1, 2, …, n > 0 and 0 < p ≤ 1.This represents the number of failures which occur in a sequence ofBernoulli trials before a target number of successes is reached.The mean is μ = n(1-p)/p and variance n(1-p)/p^2.A negative binomial distribution can also arise as a mixture ofPoisson distributions with mean distributed as a gamma distribution(see pgamma) with scale parameter (1 - prob)/proband shape parameter size.  (This definition allows non-integervalues of size.)An alternative parametrization (often used in ecology) is by themean mu (see above), and size, the dispersionparameter, where prob = size/(size+mu).  The varianceis mu + mu^2/size in this parametrization.If an element of x is not integer, the result of dnbinomis zero, with a warning.The case size == 0 is the distribution concentrated at zero.This is the limiting distribution for size approaching zero,even if mu rather than prob is held constant.  Noticethough, that the mean of the limit distribution is 0, whatever thevalue of mu.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
If mean or sd are not specified they assume the defaultvalues of 0 and 1, respectively.The normal distribution has density    f(x) = 1/(√(2 π) σ) e^-((x - μ)^2/(2 σ^2))  where μ is the mean of the distribution andσ the standard deviation.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
Confidence intervals are computed similarly to those ofbinom.test in the one-sample case, and usingbinom.test in the two sample case.
Although formally degree should be named (as it follows...), an unnamed second argument of length 1 will beinterpreted as the degree, such that poly(x, 3) can be used informulas.The orthogonal polynomial is summarized by the coefficients, which canbe used to evaluate it via the three-term recursion given in Kennedy& Gentle (1980, pp. 343–4), and used in the predict part ofthe code.poly using ... is just a convenience wrapper forpolym: coef is ignored.  Conversely, if polym iscalled with a single argument in ... it is a wrapper forpoly.
Although formally degree should be named (as it follows...), an unnamed second argument of length 1 will beinterpreted as the degree, such that poly(x, 3) can be used informulas.The orthogonal polynomial is summarized by the coefficients, which canbe used to evaluate it via the three-term recursion given in Kennedy& Gentle (1980, pp. 343–4), and used in the predict part ofthe code.poly using ... is just a convenience wrapper forpolym: coef is ignored.  Conversely, if polym iscalled with a single argument in ... it is a wrapper forpoly.
If lambda is non-positive, it is taken as zero, and the loglink is obtained.  The default lambda = 1 gives the identitylink.
Exactly one of the parameters groups, n, between.var,power, within.var, and sig.level must be passed as NULL,and that parameter is determined from the others. Notice thatsig.level has non-NULL default so NULL must be explicitlypassed if you want it computed.
Exactly one of the parameters n, p1, p2,power, and sig.level must be passed as NULL, and thatparameter is determined from the others.  Notice that sig.levelhas a non-NULL default so NULL must be explicitly passed if youwant it computed.If strict = TRUE is used, the power will include the probability ofrejection in the opposite direction of the true effect, in the two-sidedcase.  Without this the power will be half the significance level if thetrue difference is zero.Note that not all conditions can be satisfied, e.g., for there is no proportion p2 between p1 = 0.9 and 1, asyou'd need a sample size of at least n = 74 to yield thedesired power for (p1,p2) = (0.9, 1).For these impossible conditions, currently a warning(warning) is signalled which may become an error(stop) in the future.
Exactly one of the parameters n, delta, power,sd, and sig.level must be passed as NULL, and thatparameter is determined from the others.  Notice that the last two havenon-NULL defaults, so NULL must be explicitly passed if you want tocompute them.If strict = TRUE is used, the power will include the probability ofrejection in the opposite direction of the true effect, in the two-sidedcase.  Without this the power will be half the significance level if thetrue difference is zero.
The general regression equation which incorporates a constant and alinear trend is used and the corrected t-statistic for a first orderautoregressive coefficient equals one is computed.  To estimatesigma^2 the Newey-West estimator is used.  If lshortis TRUE, then the truncation lag parameter is set totrunc(4*(n/100)^0.25), otherwisetrunc(12*(n/100)^0.25) is used.  The p-values areinterpolated from Table 4.2, page 103 of Banerjee et al(1993).Missing values are not handled.
If 0 < a < 1, the resulting values are within (0,1)(excluding boundaries).In any case, the resulting sequence is symmetric in [0,1], i.e.,p + rev(p) == 1.ppoints() is used in qqplot and qqnorm to generatethe set of probabilities at which to evaluate the inverse distribution.The choice of a follows the documentation of the function of thesame name in Becker et al (1988), and appears to have beenmotivated by results from Blom (1958) on approximations to expect normalorder statistics (see also quantile).The probability points for the continuous sample quantile types 5 to 9(see quantile) can be obtained by taking a as,respectively, 1/2, 0, 1, 1/3, and 3/8.
The Poisson distribution has densityp(x) = λ^x exp(-λ)/x!for x = 0, 1, 2, … .The mean and variance are E(X) = Var(X) = λ.Note that λ = 0 is really a limit case (setting0^0 = 1) resulting in a point mass at 0, see also the example.If an element of x is not integer, the result of dpoisis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference indbinom.The quantile is right continuous: qpois(p, lambda) is the smallestinteger x such that P(X ≤ x) ≥ p.Setting lower.tail = FALSE allows to get much more preciseresults when the default, lower.tail = TRUE would return 1, seethe example below.
The basic method is given by Friedman (1984), and is essentially thesame code used by S-PLUS's ppreg.  This code is extremelysensitive to the compiler used.The algorithm first adds up to max.terms ridge terms one at atime; it will use less if it is unable to find a term to add that makessufficient difference.  It then removes the leastimportant term at each step until nterms termsare left.The levels of optimization (argument optlevel)differ in how thoroughly the models are refitted during this process.At level 0 the existing ridge terms are not refitted.  At level 1the projection directions are not refitted, but the ridgefunctions and the regression coefficients are.Levels 2 and 3 refit all the terms and are equivalent for oneresponse; level 3 is more careful to re-balance the contributionsfrom each regressor at each step and so is a little less likely toconverge to a saddle point of the sum of squares criterion.
The calculation is done by a singular value decomposition of the(centered and possibly scaled) data matrix, not by usingeigen on the covariance matrix.  Thisis generally the preferred method for numerical accuracy.  Theprint method for these objects prints the results in a niceformat and the plot method produces a scree plot.Unlike princomp, variances are computed with the usualdivisor N - 1.Note that scale = TRUE cannot be used if there are zero orconstant (for center = TRUE) variables.
Most prediction methods which are similar to those for linear modelshave an argument newdata specifying the first place to look forexplanatory variables to be used for prediction.  Some considerableattempts are made to match up the columns in newdata to thoseused for fitting, for example that they are of comparable types andthat any factors have the same level set in the same order (or can betransformed to be so).Time series prediction methods in package stats have an argumentn.ahead specifying how many time steps ahead to predict.Many methods have a logical argument se.fit saying if standarderrors are to returned.
If newdata is omitted the predictions are based on the dataused for the fit.  In that case how cases with missing values in theoriginal fit is determined by the na.action argument of thatfit.  If na.action = na.omit omitted cases will not appear inthe residuals, whereas if na.action = na.exclude they willappear (in predictions and standard errors), with residual valueNA.  See also napredict.
predict.lm produces predicted values, obtained by evaluatingthe regression function in the frame newdata (which defaults tomodel.frame(object)).  If the logical se.fit isTRUE, standard errors of the predictions are calculated.  Ifthe numeric argument scale is set (with optional df), itis used as the residual standard deviation in the computation of thestandard errors, otherwise this is extracted from the model fit.Setting intervals specifies computation of confidence orprediction (tolerance) intervals at the specified level, sometimesreferred to as narrow vs. wide intervals.If the fit is rank-deficient, some of the columns of the design matrixwill have been dropped.  Prediction from such a fit only makes senseif newdata is contained in the same subspace as the originaldata.  That cannot be checked accurately, so a warning is issued.If newdata is omitted the predictions are based on the dataused for the fit.  In that case how cases with missing values in theoriginal fit are handled is determined by the na.action argument of thatfit.  If na.action = na.omit omitted cases will not appear inthe predictions, whereas if na.action = na.exclude they willappear (in predictions, standard errors or interval limits),with value NA.  See also napredict.The prediction intervals are for a single observation at each case innewdata (or by default, the data used for the fit) with errorvariance(s) pred.var.  This can be a multiple of res.var,the estimated value of σ^2: the default is to assume thatfuture observations have the same error variance as thoseused for fitting.  If weights is supplied, the inverse of thisis used as a scale factor.  For a weighted fit, if the predictionis for the original data frame, weights defaults to the weightsused for the  model fit, with a warning since it might not be theintended result.  If the fit was weighted and newdata is given, thedefault is to assume constant prediction variance, with a warning.
Only the generic function is currently provided in base R, but someadd-on packages have methods. Principally here for S compatibility.
princomp is a generic function with "formula" and"default" methods.The calculation is done using eigen on the correlation orcovariance matrix, as determined by cor.  This is done forcompatibility with the S-PLUS result.  A preferred method ofcalculation is to use svd on x, as is done inprcomp.Note that the default calculation uses divisor N for thecovariance matrix.The print method for these objects prints theresults in a nice format and the plot method producesa scree plot (screeplot).  There is also abiplot method.If x is a formula then the standard NA-handling is applied tothe scores (if requested): see napredict.princomp only handles so-called R-mode PCA, that is featureextraction of variables.  If a data matrix is supplied (possibly via aformula) it is required that there are at least as many units asvariables.  For Q-mode PCA use prcomp.
NA
NA
A projection is given for each stratum of the object, so for aovmodels with an Error term the result is a list of projections.
These seek a ‘rotation’ of the factors x %*% T thataims to clarify the structure of the loadings matrix.  The matrixT is a rotation (possibly with reflection) for varimax,but a general linear transformation for promax, with thevariance of the factors being preserved.
Only groups with finite numbers of successes and failures are used.Counts of successes and failures must be nonnegative and hence notgreater than the corresponding numbers of trials which must bepositive.  All finite counts should be integers.If p is NULL and there is more than one group, the nulltested is that the proportions in each group are the same.  If thereare two groups, the alternatives are that the probability of successin the first group is less than, not equal to, or greater than theprobability of success in the second group, as specified byalternative.  A confidence interval for the difference ofproportions with confidence level as specified by conf.leveland clipped to [-1,1] is returned.  Continuity correction isused only if it does not exceed the difference of the sampleproportions in absolute value.  Otherwise, if there are more than 2groups, the alternative is always "two.sided", the returnedconfidence interval is NULL, and continuity correction is neverused.If there is only one group, then the null tested is that theunderlying probability of success is p, or .5 if p isnot given.  The alternative is that the probability of success is lessthan, not equal to, or greater than p or 0.5, respectively, asspecified by alternative.  A confidence interval for theunderlying proportion with confidence level as specified byconf.level and clipped to [0,1] is returned.  Continuitycorrection is used only if it does not exceed the difference betweensample and null proportions in absolute value. The confidence intervalis computed by inverting the score test.Finally, if p is given and there are more than 2 groups, thenull tested is that the underlying probabilities of success are thosegiven by p.  The alternative is always "two.sided", thereturned confidence interval is NULL, and continuity correctionis never used.
NA
This distribution is obtained as follows.  Let x be a sample ofsize n from a continuous distribution symmetric about theorigin.  Then the Wilcoxon signed rank statistic is the sum of theranks of the absolute values x[i] for which x[i] ispositive.  This statistic takes values between 0 andn(n+1)/2, and its mean and variance are n(n+1)/4 andn(n+1)(2n+1)/24, respectively.If either of the first two arguments is a vector, the recycling rule isused to do the calculations for all combinations of the two up tothe length of the longer vector.
The t distribution with df = n degrees offreedom has densityf(x) = Γ((n+1)/2) / (√(n π) Γ(n/2)) (1 + x^2/n)^-((n+1)/2)for all real x.It has mean 0 (for n > 1) andvariance n/(n-2) (for n > 2).The general non-central twith parameters (df, Del) = (df, ncp)is defined as the distribution ofT(df, Del) := (U + Del) / √(V/df) where U and V  are independent randomvariables, U ~ N(0,1) andV ~ χ^2(df) (see Chisquare).The most used applications are power calculations for t-tests:Let T= (mX - m0) / (S/sqrt(n))wheremX is the mean and S the sample standarddeviation (sd) of X_1, X_2, …, X_n which arei.i.d. N(μ, σ^2)Then T is distributed as non-central t withdf= n - 1degrees of freedom and non-centrality parameterncp = (μ - m0) * sqrt(n)/σ.
If ng =nranges is greater than one, R isthe maximum of ng groups of nmeansobservations each.
If min or max are not specified they assume the defaultvalues of 0 and 1 respectively.The uniform distribution has densityf(x) = 1/(max-min)for min ≤ x ≤ max.For the case of u := min == max, the limit case ofX == u is assumed, although there is no density inthat case and dunif will return NaN (the error condition).runif will not generate either of the extreme values unlessmax = min or max-min is small compared to min,and in particular not for the default arguments.
The Weibull distribution with shape parameter a andscale parameter b has density given byf(x) = (a/b) (x/b)^(a-1) exp(- (x/b)^a) for x > 0.The cumulative distribution function isF(x) = 1 - exp(- (x/b)^a)on x > 0, themean is E(X) = b Γ(1 + 1/a), andthe Var(X) = b^2 * (Γ(1 + 2/a) - (Γ(1 + 1/a))^2).
This distribution is obtained as follows.  Let x and ybe two random, independent samples of size m and n.Then the Wilcoxon rank sum statistic is the number of all pairs(x[i], y[j]) for which y[j] is not greater thanx[i].  This statistic takes values between 0 andm * n, and its mean and variance are m * n / 2 andm * n * (m + n + 1) / 12, respectively.If any of the first three arguments are vectors, the recycling rule isused to do the calculations for all combinations of the three up tothe length of the longest vector.
The Beta distribution with parameters shape1 = a andshape2 = b has densityΓ(a+b)/(Γ(a)Γ(b))x^(a-1)(1-x)^(b-1)for a > 0, b > 0 and 0 ≤ x ≤ 1where the boundary values at x=0 or x=1 are defined asby continuity (as limits).The mean is a/(a+b) and the variance is ab/((a+b)^2 (a+b+1)).These moments and all distributional properties can be defined aslimits (leading to point masses at 0, 1/2, or 1) when a orb are zero or infinite, and the corresponding[dpqr]beta() functions are defined correspondingly.pbeta is closely related to the incomplete beta function.  Asdefined by Abramowitz and Stegun 6.6.1B_x(a,b) =    integral_0^x t^(a-1) (1-t)^(b-1) dt,and 6.6.2 I_x(a,b) = B_x(a,b) / B(a,b) whereB(a,b) = B_1(a,b) is the Beta function (beta).I_x(a,b) is pbeta(x, a, b).The noncentral Beta distribution (with ncp  = λ)is defined (Johnson et al, 1995, pp. 502) as the distribution ofX/(X+Y) where X ~ chi^2_2a(λ)and Y ~ chi^2_2b.
The binomial distribution with size = n andprob = p has density    p(x) = choose(n, x) p^x (1-p)^(n-x)for x = 0, …, n.Note that binomial coefficients can be computed bychoose in R.If an element of x is not integer, the result of dbinomis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference below.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
The birthday paradox is that a very small number of people, 23,suffices to have a 50–50 chance that two or more of them have the samebirthday.  This function generalises the calculation to probabilitiesother than 0.5, numbers of coincident events other than 2, and numbersof classes other than 365.The formula used is approximate for coincident > 2.  Theapproximation is very good for moderate values of prob but lessgood for very small probabilities.
If location or scale are not specified, they assumethe default values of 0 and 1 respectively.The Cauchy distribution with location l and scale s hasdensityf(x) = 1 / (π s (1 + ((x-l)/s)^2))for all x.
The chi-squared distribution with df= n ≥ 0degrees of freedom has densityf_n(x) = 1 / (2^(n/2) Γ(n/2))  x^(n/2-1) e^(-x/2)for x > 0, where f_0(x) := \lim_{n \to 0} f_n(x) =  δ_0(x), a point mass at zero, is not a density function proper, buta “δ distribution”.The mean and variance are n and 2n.The non-central chi-squared distribution with df= ndegrees of freedom and non-centrality parameter ncp= λ has densityf(x) = exp(-λ/2) SUM_{r=0}^∞ ((λ/2)^r / r!) dchisq(x, df + 2r)  for x ≥ 0.  For integer n, this is the distribution ofthe sum of squares of n normals each with variance one,λ being the sum of squares of the normal means; further,E(X) = n + λ, Var(X) = 2(n + 2*λ), andE((X - E(X))^3) = 8(n + 3*λ).Note that the degrees of freedom df= n, can benon-integer, and also n = 0 which is relevant fornon-centrality λ > 0,see Johnson et al (1995, chapter 29).In that (noncentral, zero df) case, the distribution is a mixture of apoint mass at x = 0 (of size pchisq(0, df=0, ncp=ncp)) anda continuous part, and dchisq() is not a density withrespect to that mixture measure but rather the limit of the densityfor df -> 0.Note that ncp values larger than about 1e5 (and even smaller)  may give inaccurateresults with many warnings for pchisq and qchisq.
If rate is not specified, it assumes the default value of1.The exponential distribution with rate λ has densityf(x) = λ {e}^{- λ x} for x ≥ 0.
The F distribution with df1 = n1 and df2 =n2 degrees of freedom has densityf(x) = Γ((n1 + n2)/2) / (Γ(n1/2) Γ(n2/2))    (n1/n2)^(n1/2) x^(n1/2 - 1)    (1 + (n1/n2) x)^-(n1 + n2)/2for x > 0.It is the distribution of the ratio of the mean squares ofn1 and n2 independent standard normals, and henceof the ratio of two independent chi-squared variates each divided by itsdegrees of freedom.  Since the ratio of a normal and the rootmean-square of m independent normals has a Student's t_mdistribution, the square of a t_m variate has a F distribution on1 and m degrees of freedom.The non-central F distribution is again the ratio of mean squares ofindependent normals of unit variance, but those in the numerator areallowed to have non-zero means and ncp is the sum of squares ofthe means.  See Chisquare for further details onnon-central distributions.
If scale is omitted, it assumes the default value of 1.The Gamma distribution with parameters shape = aand scale = s has densityf(x)= 1/(s^a Gamma(a)) x^(a-1) e^-(x/s)for x ≥ 0, a > 0 and s > 0.(Here Gamma(a) is the function implemented by R'sgamma() and defined in its help.  Note that a = 0corresponds to the trivial distribution with all mass at point 0.)The mean and variance areE(X) = a*s andVar(X) = a*s^2.The cumulative hazard H(t) = - log(1 - F(t))isNote that for smallish values of shape (and moderatescale) a large parts of the mass of the Gamma distribution ison values of x so near zero that they will be represented aszero in computer arithmetic.  So rgamma may well return valueswhich will be represented as zero.  (This will also happen for verylarge values of scale since the actual generation is done forscale = 1.)
The geometric distribution with prob = p has densityp(x) = p (1-p)^xfor x = 0, 1, 2, …, 0 < p ≤ 1.If an element of x is not integer, the result of dgeomis zero, with a warning.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
The hypergeometric distribution is used for sampling withoutreplacement.  The density of this distribution with parametersm, n and k (named Np, N-Np, andn, respectively in the reference below, where N := m+n is also usedin other references) is given byp(x) =      choose(m, x) choose(n, k-x) / choose(m+n, k)for x = 0, …, k.Note that p(x) is non-zero only formax(0, k-n) <= x <= min(k, m).With p := m/(m+n) (hence Np = N \times p in thereference's notation), the first two moments are meanE[X] = μ = k p and variance               Var(X) = k p (1 - p) * (m+n-k)/(m+n-1),which shows the closeness to the Binomial(k,p) (where thehypergeometric has smaller variance unless k = 1).The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.In rhyper(), if one of m, n, k exceeds .Machine$integer.max,currently the equivalent of qhyper(runif(nn), m,n,k) is usedwhich is comparably slow while instead a binomial approximation may beconsiderably more efficient.
The log normal distribution has densityf(x) = 1/(√(2 π) σ x) e^-((log x - μ)^2 / (2 σ^2))where μ and σ are the mean and standarddeviation of the logarithm.The mean is E(X) = exp(μ + 1/2 σ^2),the median is med(X) = exp(μ), and the varianceVar(X) = exp(2*μ + σ^2)*(exp(σ^2) - 1)and hence the coefficient of variation issqrt(exp(σ^2) - 1) which isapproximately σ when that is small (e.g., σ < 1/2).
If location or scale are omitted, they assume thedefault values of 0 and 1 respectively.The Logistic distribution with location = m andscale = s has distribution functionF(x) = 1 / (1 + exp(-(x-m)/s))  and densityf(x) = 1/s exp((x-m)/s) (1 + exp((x-m)/s))^-2.It is a long-tailed distribution with mean m and varianceπ^2 /3 s^2.
The negative binomial distribution with size = n andprob = p has density    Γ(x+n)/(Γ(n) x!) p^n (1-p)^xfor x = 0, 1, 2, …, n > 0 and 0 < p ≤ 1.This represents the number of failures which occur in a sequence ofBernoulli trials before a target number of successes is reached.The mean is μ = n(1-p)/p and variance n(1-p)/p^2.A negative binomial distribution can also arise as a mixture ofPoisson distributions with mean distributed as a gamma distribution(see pgamma) with scale parameter (1 - prob)/proband shape parameter size.  (This definition allows non-integervalues of size.)An alternative parametrization (often used in ecology) is by themean mu (see above), and size, the dispersionparameter, where prob = size/(size+mu).  The varianceis mu + mu^2/size in this parametrization.If an element of x is not integer, the result of dnbinomis zero, with a warning.The case size == 0 is the distribution concentrated at zero.This is the limiting distribution for size approaching zero,even if mu rather than prob is held constant.  Noticethough, that the mean of the limit distribution is 0, whatever thevalue of mu.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
If mean or sd are not specified they assume the defaultvalues of 0 and 1, respectively.The normal distribution has density    f(x) = 1/(√(2 π) σ) e^-((x - μ)^2/(2 σ^2))  where μ is the mean of the distribution andσ the standard deviation.
The Poisson distribution has densityp(x) = λ^x exp(-λ)/x!for x = 0, 1, 2, … .The mean and variance are E(X) = Var(X) = λ.Note that λ = 0 is really a limit case (setting0^0 = 1) resulting in a point mass at 0, see also the example.If an element of x is not integer, the result of dpoisis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference indbinom.The quantile is right continuous: qpois(p, lambda) is the smallestinteger x such that P(X ≤ x) ≥ p.Setting lower.tail = FALSE allows to get much more preciseresults when the default, lower.tail = TRUE would return 1, seethe example below.
NA
NA
NA
This distribution is obtained as follows.  Let x be a sample ofsize n from a continuous distribution symmetric about theorigin.  Then the Wilcoxon signed rank statistic is the sum of theranks of the absolute values x[i] for which x[i] ispositive.  This statistic takes values between 0 andn(n+1)/2, and its mean and variance are n(n+1)/4 andn(n+1)(2n+1)/24, respectively.If either of the first two arguments is a vector, the recycling rule isused to do the calculations for all combinations of the two up tothe length of the longer vector.
The t distribution with df = n degrees offreedom has densityf(x) = Γ((n+1)/2) / (√(n π) Γ(n/2)) (1 + x^2/n)^-((n+1)/2)for all real x.It has mean 0 (for n > 1) andvariance n/(n-2) (for n > 2).The general non-central twith parameters (df, Del) = (df, ncp)is defined as the distribution ofT(df, Del) := (U + Del) / √(V/df) where U and V  are independent randomvariables, U ~ N(0,1) andV ~ χ^2(df) (see Chisquare).The most used applications are power calculations for t-tests:Let T= (mX - m0) / (S/sqrt(n))wheremX is the mean and S the sample standarddeviation (sd) of X_1, X_2, …, X_n which arei.i.d. N(μ, σ^2)Then T is distributed as non-central t withdf= n - 1degrees of freedom and non-centrality parameterncp = (μ - m0) * sqrt(n)/σ.
If ng =nranges is greater than one, R isthe maximum of ng groups of nmeansobservations each.
quade.test can be used for analyzing unreplicated completeblock designs (i.e., there is exactly one observation in yfor each combination of levels of groups and blocks)where the normality assumption may be violated.The null hypothesis is that apart from an effect of blocks,the location parameter of y is the same in each of thegroups.If y is a matrix, groups and blocks are obtainedfrom the column and row indices, respectively.  NA's are notallowed in groups or blocks;  if y containsNA's, corresponding blocks are removed.
A vector of length length(probs) is returned;if names = TRUE, it has a names attribute.NA and NaN values in probs arepropagated to the result.The default method works with classed objects sufficiently likenumeric vectors that sort and (not needed by types 1 and 3)addition of elements and multiplication by a number work correctly.Note that as this is in a namespace, the copy of sort inbase will be used, not some S4 generic of that name.  Also notethat that is no check on the ‘correctly’, and soe.g. quantile can be applied to complex vectors which (apartfrom ties) will be ordered on their real parts.There is a method for the date-time classes (see"POSIXt").  Types 1 and 3 can be used for class"Date" and for ordered factors.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
If min or max are not specified they assume the defaultvalues of 0 and 1 respectively.The uniform distribution has densityf(x) = 1/(max-min)for min ≤ x ≤ max.For the case of u := min == max, the limit case ofX == u is assumed, although there is no density inthat case and dunif will return NaN (the error condition).runif will not generate either of the extreme values unlessmax = min or max-min is small compared to min,and in particular not for the default arguments.
The Weibull distribution with shape parameter a andscale parameter b has density given byf(x) = (a/b) (x/b)^(a-1) exp(- (x/b)^a) for x > 0.The cumulative distribution function isF(x) = 1 - exp(- (x/b)^a)on x > 0, themean is E(X) = b Γ(1 + 1/a), andthe Var(X) = b^2 * (Γ(1 + 2/a) - (Γ(1 + 1/a))^2).
This distribution is obtained as follows.  Let x and ybe two random, independent samples of size m and n.Then the Wilcoxon rank sum statistic is the number of all pairs(x[i], y[j]) for which y[j] is not greater thanx[i].  This statistic takes values between 0 andm * n, and its mean and variance are m * n / 2 andm * n * (m + n + 1) / 12, respectively.If any of the first three arguments are vectors, the recycling rule isused to do the calculations for all combinations of the three up tothe length of the longest vector.
NA
The Beta distribution with parameters shape1 = a andshape2 = b has densityΓ(a+b)/(Γ(a)Γ(b))x^(a-1)(1-x)^(b-1)for a > 0, b > 0 and 0 ≤ x ≤ 1where the boundary values at x=0 or x=1 are defined asby continuity (as limits).The mean is a/(a+b) and the variance is ab/((a+b)^2 (a+b+1)).These moments and all distributional properties can be defined aslimits (leading to point masses at 0, 1/2, or 1) when a orb are zero or infinite, and the corresponding[dpqr]beta() functions are defined correspondingly.pbeta is closely related to the incomplete beta function.  Asdefined by Abramowitz and Stegun 6.6.1B_x(a,b) =    integral_0^x t^(a-1) (1-t)^(b-1) dt,and 6.6.2 I_x(a,b) = B_x(a,b) / B(a,b) whereB(a,b) = B_1(a,b) is the Beta function (beta).I_x(a,b) is pbeta(x, a, b).The noncentral Beta distribution (with ncp  = λ)is defined (Johnson et al, 1995, pp. 502) as the distribution ofX/(X+Y) where X ~ chi^2_2a(λ)and Y ~ chi^2_2b.
The binomial distribution with size = n andprob = p has density    p(x) = choose(n, x) p^x (1-p)^(n-x)for x = 0, …, n.Note that binomial coefficients can be computed bychoose in R.If an element of x is not integer, the result of dbinomis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference below.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
If location or scale are not specified, they assumethe default values of 0 and 1 respectively.The Cauchy distribution with location l and scale s hasdensityf(x) = 1 / (π s (1 + ((x-l)/s)^2))for all x.
The chi-squared distribution with df= n ≥ 0degrees of freedom has densityf_n(x) = 1 / (2^(n/2) Γ(n/2))  x^(n/2-1) e^(-x/2)for x > 0, where f_0(x) := \lim_{n \to 0} f_n(x) =  δ_0(x), a point mass at zero, is not a density function proper, buta “δ distribution”.The mean and variance are n and 2n.The non-central chi-squared distribution with df= ndegrees of freedom and non-centrality parameter ncp= λ has densityf(x) = exp(-λ/2) SUM_{r=0}^∞ ((λ/2)^r / r!) dchisq(x, df + 2r)  for x ≥ 0.  For integer n, this is the distribution ofthe sum of squares of n normals each with variance one,λ being the sum of squares of the normal means; further,E(X) = n + λ, Var(X) = 2(n + 2*λ), andE((X - E(X))^3) = 8(n + 3*λ).Note that the degrees of freedom df= n, can benon-integer, and also n = 0 which is relevant fornon-centrality λ > 0,see Johnson et al (1995, chapter 29).In that (noncentral, zero df) case, the distribution is a mixture of apoint mass at x = 0 (of size pchisq(0, df=0, ncp=ncp)) anda continuous part, and dchisq() is not a density withrespect to that mixture measure but rather the limit of the densityfor df -> 0.Note that ncp values larger than about 1e5 (and even smaller)  may give inaccurateresults with many warnings for pchisq and qchisq.
read.ftable reads in a flat-like contingency table from afile.  If the file contains the written representation of a flattable (more precisely, a header with all information on names andlevels of column variables, followed by a line with the names of therow variables), no further arguments are needed.  Similarly, flattables with only one column variable the name of which is the onlyentry in the first line are handled automatically.  Other variants canbe dealt with by skipping all header information using skip,and providing the names of the row variables and the names and levelsof the column variable using row.var.names and col.vars,respectively.  See the examples below.Note that flat tables are characterized by their ‘ragged’display of row (and maybe also column) labels.  If the full grid oflevels of the row variables is given, one should instead useread.table to read in the data, and create thecontingency table from this using xtabs.write.ftable writes a flat table to a file, which is useful forgenerating ‘pretty’ ASCII representations of contingencytables.  Different versions are available via the methodargument, which may be useful, for example, for constructing LaTeX tables.
NA
NA
This, as reorder(), is a special case of simply callingfactor(x, levels = levels(x)[....]).
This, as relevel(), is a special case of simply callingfactor(x, levels = levels(x)[....]).
If formula is a data frame and data is missing,formula is used for data with the formula ~ ..Any character vectors in the formula are coerced to factors.
Although reshape() can be used in a variety of contexts, themotivating application is data from longitudinal studies, and thearguments of this function are named and described in those terms. Alongitudinal study is characterized by repeated measurements of thesame variable(s), e.g., height and weight, on each unit being studied(e.g., individual persons) at different time points (which are assumedto be the same for all units). These variables are called time-varyingvariables. The study may include other variables that are measuredonly once for each unit and do not vary with time (e.g., gender andrace); these are called time-constant variables.A ‘wide’ format representation of a longitudinal dataset willhave one record (row) for each unit, typically with some time-constantvariables that occupy single columns, and some time-varying variablesthat occupy multiple columns (one column for each time point).  A‘long’ format representation of the same dataset will havemultiple records (rows) for each individual, with the time-constantvariables being constant across these records and the time-varyingvariables varying across the records.  The ‘long’ formatdataset will have two additional variables: a ‘time’ variableidentifying which time point each record comes from, and an‘id’ variable showing which records refer to the same unit.The type of conversion (long to wide or wide to long) is determined bythe direction argument, which is mandatory unless thedata argument is the result of a previous call toreshape.  In that case, the operation can be reversed simplyusing reshape(data) (the other arguments are stored asattributes on the data frame).Conversion from long to wide format with direction = "wide" isthe simpler operation, and is mainly useful in the context ofmultivariate analysis where data is often expected as a wide-formatmatrix. In this case, the time variable timevar and id variableidvar must be specified. All other variables are assumed to betime-varying, unless the time-varying variables are explicitlyspecified via the v.names argument.  A warning is issued iftime-constant variables are not actually constant.Each time-varying variable is expanded into multiple variables in thewide format.  The names of these expanded variables are generatedautomatically, unless they are specified as the varyingargument in the form of a list (or matrix) with one component (or row)for each time-varying variable. If varying is a vector ofnames, it is implicitly converted into a matrix, with one row for eachtime-varying variable. Use this option with care if there are multipletime-varying variables, as the ordering (by column, the default in thematrix constructor) may be unintuitive, whereas theexplicit list or matrix form is unambiguous.Conversion from wide to long with direction = "long" is themore common operation as most (univariate) statistical modelingfunctions expect data in the long format. In the simpler case wherethere is only one time-varying variable, the corresponding columns inthe wide format input can be specified as the varying argument,which can be either a vector of column names or the correspondingcolumn indices. The name of the corresponding variable in the longformat output combining these columns can be optionally specified asthe v.names argument, and the name of the time variables as thetimevar argument. The values to use as the time valuescorresponding to the different columns in the wide format can bespecified as the times argument.  If v.names isunspecified, the function will attempt to guess v.names andtimes from varying (an explicitly specified timesargument is unused in that case).  The default expects variable nameslike x.1, x.2, where sep = "."  specifies tosplit at the dot and drop it from the name.  To have alphabeticfollowed by numeric times use sep = "".Multiple time-varying variables can be specified in two ways, eitherwith varying as an atomic vector as above, or as a list (or amatrix). The first form is useful (and mandatory) if the automaticvariable name splitting as described above is used; this requires thenames of all time-varying variables to be suitably formatted in thesame manner, and v.names to be unspecified. If varyingis a list (with one component for each time-varying variable) or amatrix (one row for each time-varying variable), variable namesplitting is not attempted, and v.names and times willgenerally need to be specified, although they will default to,respectively, the first variable name in each set, and sequentialtimes.Also, guessing is not attempted if v.names is given explicitly,even if varying is an atomic vector. In that case, the numberof time-varying variables is taken to be the length of v.names,and varying is implicitly converted into a matrix, with one rowfor each time-varying variable. As in the case of long to wideconversion, the matrix is filled up by column, so careful attention needsto be paid to the order of variable names (or indices) invarying, which is taken to be like x.1, y.1,x.2, y.2 (i.e., variables corresponding to the same timepoint need to be grouped together).The split argument should not usually be necessary.  Thesplit$regexp component is passed to eitherstrsplit or regexpr, where the latter isused if split$include is TRUE, in which case thesplitting occurs after the first character of the matched string.  Inthe strsplit case, the separator is not included in theresult, and it is possible to specify fixed-string matching usingsplit$fixed.
NA
NA
The references define the types of residuals: Davison & Snell is agood reference for the usages of each.The partial residuals are a matrix of working residuals, with eachcolumn formed by omitting a term from the model.How residuals treats cases with missing values in the originalfit is determined by the na.action argument of that fit.If na.action = na.omit omitted cases will not appear in theresiduals, whereas if na.action = na.exclude they will appear,with residual value NA.  See also naresid.For fits done with y = FALSE the response values are computedfrom other components.
The generic accessor functions coef, effects,fitted and residuals can be used to extractvarious useful features of the value returned by lm.The working and response residuals are ‘observed - fitted’.  Thedeviance and pearson residuals are weighted residuals, scaled by thesquare root of the weights used in fitting.  The partial residualsare a matrix with each column formed by omitting a term from themodel.  In all these, zero weight cases are never omitted (as opposedto the standardized rstudent residuals, and theweighted.residuals).How residuals treats cases with missing values in the originalfit is determined by the na.action argument of that fit.If na.action = na.omit omitted cases will not appear in theresiduals, whereas if na.action = na.exclude they will appear,with residual value NA.  See also naresid.The "lm" method for generic labels returns theterm labels for estimable terms, that is the names of the terms withan least one estimable coefficient.
If rate is not specified, it assumes the default value of1.The exponential distribution with rate λ has densityf(x) = λ {e}^{- λ x} for x ≥ 0.
The F distribution with df1 = n1 and df2 =n2 degrees of freedom has densityf(x) = Γ((n1 + n2)/2) / (Γ(n1/2) Γ(n2/2))    (n1/n2)^(n1/2) x^(n1/2 - 1)    (1 + (n1/n2) x)^-(n1 + n2)/2for x > 0.It is the distribution of the ratio of the mean squares ofn1 and n2 independent standard normals, and henceof the ratio of two independent chi-squared variates each divided by itsdegrees of freedom.  Since the ratio of a normal and the rootmean-square of m independent normals has a Student's t_mdistribution, the square of a t_m variate has a F distribution on1 and m degrees of freedom.The non-central F distribution is again the ratio of mean squares ofindependent normals of unit variance, but those in the numerator areallowed to have non-zero means and ncp is the sum of squares ofthe means.  See Chisquare for further details onnon-central distributions.
If scale is omitted, it assumes the default value of 1.The Gamma distribution with parameters shape = aand scale = s has densityf(x)= 1/(s^a Gamma(a)) x^(a-1) e^-(x/s)for x ≥ 0, a > 0 and s > 0.(Here Gamma(a) is the function implemented by R'sgamma() and defined in its help.  Note that a = 0corresponds to the trivial distribution with all mass at point 0.)The mean and variance areE(X) = a*s andVar(X) = a*s^2.The cumulative hazard H(t) = - log(1 - F(t))isNote that for smallish values of shape (and moderatescale) a large parts of the mass of the Gamma distribution ison values of x so near zero that they will be represented aszero in computer arithmetic.  So rgamma may well return valueswhich will be represented as zero.  (This will also happen for verylarge values of scale since the actual generation is done forscale = 1.)
The geometric distribution with prob = p has densityp(x) = p (1-p)^xfor x = 0, 1, 2, …, 0 < p ≤ 1.If an element of x is not integer, the result of dgeomis zero, with a warning.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
The hypergeometric distribution is used for sampling withoutreplacement.  The density of this distribution with parametersm, n and k (named Np, N-Np, andn, respectively in the reference below, where N := m+n is also usedin other references) is given byp(x) =      choose(m, x) choose(n, k-x) / choose(m+n, k)for x = 0, …, k.Note that p(x) is non-zero only formax(0, k-n) <= x <= min(k, m).With p := m/(m+n) (hence Np = N \times p in thereference's notation), the first two moments are meanE[X] = μ = k p and variance               Var(X) = k p (1 - p) * (m+n-k)/(m+n-1),which shows the closeness to the Binomial(k,p) (where thehypergeometric has smaller variance unless k = 1).The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.In rhyper(), if one of m, n, k exceeds .Machine$integer.max,currently the equivalent of qhyper(runif(nn), m,n,k) is usedwhich is comparably slow while instead a binomial approximation may beconsiderably more efficient.
The log normal distribution has densityf(x) = 1/(√(2 π) σ x) e^-((log x - μ)^2 / (2 σ^2))where μ and σ are the mean and standarddeviation of the logarithm.The mean is E(X) = exp(μ + 1/2 σ^2),the median is med(X) = exp(μ), and the varianceVar(X) = exp(2*μ + σ^2)*(exp(σ^2) - 1)and hence the coefficient of variation issqrt(exp(σ^2) - 1) which isapproximately σ when that is small (e.g., σ < 1/2).
If location or scale are omitted, they assume thedefault values of 0 and 1 respectively.The Logistic distribution with location = m andscale = s has distribution functionF(x) = 1 / (1 + exp(-(x-m)/s))  and densityf(x) = 1/s exp((x-m)/s) (1 + exp((x-m)/s))^-2.It is a long-tailed distribution with mean m and varianceπ^2 /3 s^2.
If x is a K-component vector, dmultinom(x, prob)is the probabilityP(X[1]=x[1], … , X[K]=x[k]) = C * prod(j=1 , …, K) p[j]^x[j]where C is the ‘multinomial coefficient’C = N! / (x[1]! * … * x[K]!)and N = sum(j=1, …, K) x[j].By definition, each component X[j] is binomially distributed asBin(size, prob[j]) for j = 1, …, K.The rmultinom() algorithm draws binomials X[j] fromBin(n[j], P[j]) sequentially, wheren[1] = N (N := size),P[1] = p[1] (p is prob scaled to sum 1),and for j ≥ 2, recursively,n[j] = N - sum(k=1, …, j-1) X[k]andP[j] = p[j] / (1 - sum(p[1:(j-1)])).
The negative binomial distribution with size = n andprob = p has density    Γ(x+n)/(Γ(n) x!) p^n (1-p)^xfor x = 0, 1, 2, …, n > 0 and 0 < p ≤ 1.This represents the number of failures which occur in a sequence ofBernoulli trials before a target number of successes is reached.The mean is μ = n(1-p)/p and variance n(1-p)/p^2.A negative binomial distribution can also arise as a mixture ofPoisson distributions with mean distributed as a gamma distribution(see pgamma) with scale parameter (1 - prob)/proband shape parameter size.  (This definition allows non-integervalues of size.)An alternative parametrization (often used in ecology) is by themean mu (see above), and size, the dispersionparameter, where prob = size/(size+mu).  The varianceis mu + mu^2/size in this parametrization.If an element of x is not integer, the result of dnbinomis zero, with a warning.The case size == 0 is the distribution concentrated at zero.This is the limiting distribution for size approaching zero,even if mu rather than prob is held constant.  Noticethough, that the mean of the limit distribution is 0, whatever thevalue of mu.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
If mean or sd are not specified they assume the defaultvalues of 0 and 1, respectively.The normal distribution has density    f(x) = 1/(√(2 π) σ) e^-((x - μ)^2/(2 σ^2))  where μ is the mean of the distribution andσ the standard deviation.
The Poisson distribution has densityp(x) = λ^x exp(-λ)/x!for x = 0, 1, 2, … .The mean and variance are E(X) = Var(X) = λ.Note that λ = 0 is really a limit case (setting0^0 = 1) resulting in a point mass at 0, see also the example.If an element of x is not integer, the result of dpoisis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference indbinom.The quantile is right continuous: qpois(p, lambda) is the smallestinteger x such that P(X ≤ x) ≥ p.Setting lower.tail = FALSE allows to get much more preciseresults when the default, lower.tail = TRUE would return 1, seethe example below.
This distribution is obtained as follows.  Let x be a sample ofsize n from a continuous distribution symmetric about theorigin.  Then the Wilcoxon signed rank statistic is the sum of theranks of the absolute values x[i] for which x[i] ispositive.  This statistic takes values between 0 andn(n+1)/2, and its mean and variance are n(n+1)/4 andn(n+1)(2n+1)/24, respectively.If either of the first two arguments is a vector, the recycling rule isused to do the calculations for all combinations of the two up tothe length of the longer vector.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
The t distribution with df = n degrees offreedom has densityf(x) = Γ((n+1)/2) / (√(n π) Γ(n/2)) (1 + x^2/n)^-((n+1)/2)for all real x.It has mean 0 (for n > 1) andvariance n/(n-2) (for n > 2).The general non-central twith parameters (df, Del) = (df, ncp)is defined as the distribution ofT(df, Del) := (U + Del) / √(V/df) where U and V  are independent randomvariables, U ~ N(0,1) andV ~ χ^2(df) (see Chisquare).The most used applications are power calculations for t-tests:Let T= (mX - m0) / (S/sqrt(n))wheremX is the mean and S the sample standarddeviation (sd) of X_1, X_2, …, X_n which arei.i.d. N(μ, σ^2)Then T is distributed as non-central t withdf= n - 1degrees of freedom and non-centrality parameterncp = (μ - m0) * sqrt(n)/σ.
If min or max are not specified they assume the defaultvalues of 0 and 1 respectively.The uniform distribution has densityf(x) = 1/(max-min)for min ≤ x ≤ max.For the case of u := min == max, the limit case ofX == u is assumed, although there is no density inthat case and dunif will return NaN (the error condition).runif will not generate either of the extreme values unlessmax = min or max-min is small compared to min,and in particular not for the default arguments.
Apart from the end values, the result y = runmed(x, k) simply hasy[j] = median(x[(j-k2):(j+k2)]) (k = 2*k2+1), computed veryefficiently.The two algorithms are internally entirely different:is the Härdle–Steigeralgorithm (see Ref.) as implemented by Berwin Turlach.A tree algorithm is used, ensuring performance O(n * log(k)) where n = length(x) which isasymptotically optimal.is the (older) Stuetzle–Friedman implementationwhich makes use of median updating when one observationenters and one leaves the smoothing window.  While this performs asO(n * k) which is slower asymptotically, it isconsiderably faster for small k or n.Note that, both algorithms (and the smoothEnds() utility)now “work” also when x contains non-finite entries(+/-Inf, NaN, andNA):.......currently simply works by applying theunderlying math library (‘libm’) arithmetic for the non-finitenumbers; this may optionally change in the future.Currently long vectors are only supported for algorithm = "Stuetzle".
The Weibull distribution with shape parameter a andscale parameter b has density given byf(x) = (a/b) (x/b)^(a-1) exp(- (x/b)^a) for x > 0.The cumulative distribution function isF(x) = 1 - exp(- (x/b)^a)on x > 0, themean is E(X) = b Γ(1 + 1/a), andthe Var(X) = b^2 * (Γ(1 + 2/a) - (Γ(1 + 1/a))^2).
This distribution is obtained as follows.  Let x and ybe two random, independent samples of size m and n.Then the Wilcoxon rank sum statistic is the number of all pairs(x[i], y[j]) for which y[j] is not greater thanx[i].  This statistic takes values between 0 andm * n, and its mean and variance are m * n / 2 andm * n * (m + n + 1) / 12, respectively.If any of the first three arguments are vectors, the recycling rule isused to do the calculations for all combinations of the three up tothe length of the longest vector.
If X1,...,Xm, Xi in R^p isa sample of m independent multivariate Gaussians with mean (vector) 0, andcovariance matrix Σ, the distribution ofM = X'X is W_p(Σ, m).Consequently, the expectation of M isE[M] = m * Sigma.Further, if Sigma is scalar (p = 1), the Wishartdistribution is a scaled chi-squared (chi^2)distribution with df degrees of freedom,W_1(sigma^2, m) = sigma^2 chi[m]^2.The component wise variance is    Var(M[i,j]) = m*(S[i,j]^2 + S[i,i] * S[j,j]), where S=Sigma.
loess.smooth is an auxiliary function which evaluates theloess smooth at evaluation equally spaced pointscovering the range of x.
NA
Like var this uses denominator n - 1.The standard deviation of a length-one or zero-length vector is NA.
Contrasts are usually used to test if certain means aresignificantly different; it can be easier to use se.contrastthan compute them directly from the coefficients.In multistratum models, the contrasts can appear in more than onestratum, in which case the standard errors are computed in the loweststratum and adjusted for efficiencies and comparisons betweenstrata. (See the comments in the note in the help foraov about using orthogonal contrasts.)  Such standarderrors are often conservative.Suitable matrices for use with coef can be found bycalling contrasts and indexing the columns by a factor.
nls() calls getInitial and theinitial function for these self-starting models.This function is generic; methods functions can be written to handlespecific classes of objects.
NA
NA
The stats package provides the S3 generic and a default method.The latter is correct typically for (asymptotically / approximately)generalized gaussian (“least squares”) problems, since it isdefined aswhere NN <- nobs(object, use.fallback = use.fallback)and PP <- sum(!is.na(coef(object))) – where in older Rversions this was length(coef(object)) which is too large incase of undetermined coefficients, e.g., for rank deficient model fits.
This is a generic function.  Consult the individual modeling functionsfor details on how to use this function.Package stats has a method for "lm" objects which is usedfor lm and glm fits.  There is a methodfor fits from glm.nb in package MASS, and hence thecase of negative binomial families is not covered by the "lm"method.The methods for linear models fitted by lm or glm(family  = "gaussian") assume that any weights which have been supplied areinversely proportional to the error variance.  For other GLMs the(optional) simulate component of the familyobject is used—there is no appropriate simulation method for‘quasi’ models as they are specified only up to two moments.For binomial and Poisson GLMs the dispersion is fixed at one.  Integerprior weights w_i can be interpreted as meaning thatobservation i is an average of w_i observations, which isnatural for binomials specified as proportions but less so for aPoisson, for which prior weights are ignored with a warning.For a gamma GLM the shape parameter is estimated by maximum likelihood(using function gamma.shape in packageMASS).  The interpretation of weights is as multipliers to abasic shape parameter, since dispersion is inversely proportional toshape.For an inverse gaussian GLM the model assumed isIG(μ_i, λ w_i) (seehttps://en.wikipedia.org/wiki/Inverse_Gaussian_distribution)where λ is estimated by the inverse of the dispersionestimate for the fit.  The variance isμ_i^3/(λ w_i) andhence inversely proportional to the prior weights.  The simulation isdone by function rinvGauss from theSuppDists package, which must be installed.
3 is Tukey's short notation for running mediansof length 3,3R stands for Repeated 3 untilconvergence, andS for Splitting of horizontal stretches of length 2 or 3.Hence, 3RS3R is a concatenation of 3R, Sand 3R, 3RSS similarly,whereas 3RSR means first 3Rand then (S and 3) Repeated until convergence – whichcan be bad.
Neither x nor y are allowed to containing missing orinfinite values.The x vector should contain at least four distinct values.‘Distinct’ here is controlled by tol: values which areregarded as the same are replaced by the first of their values and thecorresponding y and w are pooled accordingly.Unless lambda has been specified instead of spar,the computational λ used (as a function of\code{spar}) isλ = r * 256^(3*spar - 1)wherer = tr(X' W X) / tr(Σ),Σ is the matrix given byΣ[i,j] = Integral B''[i](t) B''[j](t) dt,X is given by X[i,j] = B[j](x[i]),W is the diagonal matrix of weights (scaled such thatits trace is n, the original number of observations)and B[k](.) is the k-th B-spline.Note that with these definitions, f_i = f(x_i), and the B-splinebasis representation f = X c (i.e., c isthe vector of spline coefficients), the penalized log likelihood isL = (y - f)' W (y - f) + λ c' Σ c, and hencec is the solution of the (ridge regression)(X' W X + λ Σ) c = X' W y.If spar and lambda are missing or NULL, the valueof df is used to determine the degree of smoothing.  Ifdf is missing as well, leave-one-out cross-validation (ordinaryor ‘generalized’ as determined by cv) is used todetermine λ.Note that from the above relation,spar is spar = s0 + 0.0601 * log(λ),which is intentionally different from the S-PLUS implementationof smooth.spline (where spar is proportional toλ).  In R's (log λ) scale, it makes moresense to vary spar linearly.Note however that currently the results may become very unreliablefor spar values smaller than about -1 or -2.  The same mayhappen for values larger than 2 or so.  Don't think of settingspar or the controls low and high outside such asafe range, unless you know what you are doing!Similarly, specifying lambda instead of spar isdelicate, notably as the range of “safe” values forlambda is not scale-invariant and hence entirely data dependent.The ‘generalized’ cross-validation method GCV will work correctly whenthere are duplicated points in x.  However, it is ambiguous whatleave-one-out cross-validation means with duplicated points, and theinternal code uses an approximation that involves leaving out groupsof duplicated points.  cv = TRUE is best avoided in that case.
smoothEnds is used to only do the ‘end point smoothing’,i.e., change at most the observations closer to the beginning/endthan half the window k.  The first and last value are computed usingTukey's end point rule, i.e.,sm[1] = median(y[1], sm[2], 3*sm[2] - 2*sm[3], na.rm=TRUE).In R versions 3.6.0 and earlier, missing values (NA)in y typically lead to an error, whereas now the equivalent ofmedian(*, na.rm=TRUE) is used.
NA
NA
The raw periodogram is not a consistent estimator of the spectral density,but adjacent values are asymptotically independent. Hence a consistentestimator can be derived by smoothing the raw periodogram, assuming thatthe spectral density is smooth.The series will be automatically padded with zeros until the serieslength is a highly composite number in order to help the Fast FourierTransform. This is controlled by the fast and not the padargument.The periodogram at zero is in theory zero as the mean of the seriesis removed (but this may be affected by tapering): it is replaced byan interpolation of adjacent values during smoothing, and no valueis returned for that frequency.
The cosine-bell taper is applied to the first and last p[i]observations of time series x[, i].
spectrum is a wrapper function which calls the methodsspec.pgram and spec.ar.The spectrum here is defined with scaling 1/frequency(x),following S-PLUS.  This makes the spectral density a density over therange (-frequency(x)/2, +frequency(x)/2], whereas a more commonscaling is 2pi and range (-0.5, 0.5] (e.g., Bloomfield)or 1 and range (-pi, pi].If available, a confidence interval will be plotted byplot.spec: this is asymmetric, and the width of the centremark indicates the equivalent bandwidth.
The inputs can contain missing values which are deleted, so at leastone complete (x, y) pair is required.If method = "fmm", the spline used is that of Forsythe, Malcolmand Moler (an exact cubic is fitted through the four points at eachend of the data, and this is used to determine the end conditions).Natural splines are used when method = "natural", and periodicsplines when method = "periodic".The method "monoH.FC" computes a monotone Hermite splineaccording to the method of Fritsch and Carlson.  It does so bydetermining slopes such that the Hermite spline, determined by(x[i],y[i],m[i]), is monotone (increasing ordecreasing) iff the data are.Method "hyman" computes a monotone cubic spline usingHyman filtering of an method = "fmm" fit for strictly monotonicinputs.These interpolation splines can also be used for extrapolation, that isprediction at points outside the range of x.  Extrapolationmakes little sense for method = "fmm"; for natural splines itis linear using the slope of the interpolating curve at the nearestdata point.
The inputs can contain missing values which are deleted, so at leastone complete (x, y) pair is required.If method = "fmm", the spline used is that of Forsythe, Malcolmand Moler (an exact cubic is fitted through the four points at eachend of the data, and this is used to determine the end conditions).Natural splines are used when method = "natural", and periodicsplines when method = "periodic".The method "monoH.FC" computes a monotone Hermite splineaccording to the method of Fritsch and Carlson.  It does so bydetermining slopes such that the Hermite spline, determined by(x[i],y[i],m[i]), is monotone (increasing ordecreasing) iff the data are.Method "hyman" computes a monotone cubic spline usingHyman filtering of an method = "fmm" fit for strictly monotonicinputs.These interpolation splines can also be used for extrapolation, that isprediction at points outside the range of x.  Extrapolationmakes little sense for method = "fmm"; for natural splines itis linear using the slope of the interpolating curve at the nearestdata point.
The inputs can contain missing values which are deleted, so at leastone complete (x, y) pair is required.If method = "fmm", the spline used is that of Forsythe, Malcolmand Moler (an exact cubic is fitted through the four points at eachend of the data, and this is used to determine the end conditions).Natural splines are used when method = "natural", and periodicsplines when method = "periodic".The method "monoH.FC" computes a monotone Hermite splineaccording to the method of Fritsch and Carlson.  It does so bydetermining slopes such that the Hermite spline, determined by(x[i],y[i],m[i]), is monotone (increasing ordecreasing) iff the data are.Method "hyman" computes a monotone cubic spline usingHyman filtering of an method = "fmm" fit for strictly monotonicinputs.These interpolation splines can also be used for extrapolation, that isprediction at points outside the range of x.  Extrapolationmakes little sense for method = "fmm"; for natural splines itis linear using the slope of the interpolating curve at the nearestdata point.
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
This model is a generalization of the SSasymp model inthat it reduces to SSasymp when pwr is unity.
These are generic functions, which will use thetsp attribute of x if it exists.Their default methods decode the start time from the original timeunits, so that for a monthly series 1995.5 is representedas c(1995, 7). For a series of frequency f, timen+i/f is presented as c(n, i+1) (even for i = 0and f = 1).
NA
step uses add1 and drop1repeatedly; it will work for any method for which they work, and thatis determined by having a valid method for extractAIC.When the additive constant can be chosen so that AIC is equal toMallows' Cp, this is done and the tables are labelledappropriately.The set of models searched is determined by the scope argument.The right-hand-side of its lower component is always includedin the model, and right-hand-side of the model is included in theupper component.  If scope is a single formula, itspecifies the upper component, and the lower model isempty.  If scope is missing, the initial model is used as theupper model.Models specified by scope can be templates to updateobject as used by update.formula.  So using. in a scope formula means ‘what isalready there’, with .^2 indicating all interactions ofexisting terms.There is a potential problem in using glm fits with avariable scale, as in that case the deviance is not simplyrelated to the maximized log-likelihood.  The "glm" method forfunction extractAIC makes theappropriate adjustment for a gaussian family, but may need to beamended for other cases.  (The binomial and poissonfamilies have fixed scale by default and do not correspondto a particular maximum-likelihood problem for variable scale.)
NA
The seasonal component is found by loess smoothing theseasonal sub-series (the series of all January values, ...); ifs.window = "periodic" smoothing is effectively replaced bytaking the mean. The seasonal values are removed, and the remaindersmoothed to find the trend. The overall level is removed from theseasonal component and added to the trend component. This process isiterated a few times.  The remainder component is theresiduals from the seasonal plus trend fit.Several methods for the resulting class "stl" objects, see,plot.stl.
Structural time series models are (linear Gaussian) state-spacemodels for (univariate) time series based on a decomposition of theseries into a number of components. They are specified by a set oferror variances, some of which may be zero.The simplest model is the local level model specified bytype = "level".  This has an underlying level m[t] whichevolves bym[t+1] = m[t] + xi[t], xi[t] ~ N(0, σ^2_ξ)The observations arex[t] = m[t] + eps[t], eps[t] ~  N(0, σ^2_\eps)There are two parameters, σ^2_ξand σ^2_eps.  It is an ARIMA(0,1,1) model,but with restrictions on the parameter set.The local linear trend model, type = "trend", has the samemeasurement equation, but with a time-varying slope in the dynamics form[t], given bym[t+1] = m[t] + n[t] + xi[t], xi[t] ~ N(0, σ^2_ξ)n[t+1] = n[t] + ζ[t],  ζ[t] ~ N(0, σ^2_ζ)with three variance parameters.  It is not uncommon to findσ^2_ζ = 0 (which reduces to the locallevel model) or σ^2_ξ = 0, which ensures asmooth trend.  This is a restricted ARIMA(0,2,2) model.The basic structural model, type = "BSM", is a localtrend model with an additional seasonal component. Thus the measurementequation isx[t] = m[t] + s[t] + eps[t], eps[t] ~  N(0, σ^2_eps)where s[t] is a seasonal component with dynamicss[t+1] = -s[t] - … - s[t - s + 2] + w[t],  w[t] ~ N(0, σ^2_w)The boundary case σ^2_w = 0 correspondsto a deterministic (but arbitrary) seasonal pattern.  (This issometimes known as the ‘dummy variable’ version of the BSM.)
NA
print.summary.glm tries to be smart about formatting thecoefficients, standard errors, etc. and additionally gives‘significance stars’ if signif.stars is TRUE.The coefficients component of the result gives the estimatedcoefficients and their estimated standard errors, together with theirratio.  This third column is labelled t ratio if thedispersion is estimated, and z ratio if the dispersion is known(or fixed by the family).  A fourth column gives the two-tailedp-value corresponding to the t or z ratio based on a Student t orNormal reference distribution.  (It is possible that the dispersion isnot known and there are no residual degrees of freedom from which toestimate it.  In that case the estimate is NaN.)Aliased coefficients are omitted in the returned object but restoredby the print method.Correlations are printed to two decimal places (or symbolically): tosee the actual correlations print summary(object)$correlationdirectly.The dispersion of a GLM is not used in the fitting process, but it isneeded to find standard errors.If dispersion is not supplied or NULL,the dispersion is taken as 1 for the binomial andPoisson families, and otherwise estimated by the residualChisquared statistic (calculated from cases with non-zero weights)divided by the residual degrees of freedom.summary can be used with Gaussian glm fits to handle thecase of a linear regression with known error variance, something nothandled by summary.lm.
print.summary.lm tries to be smart about formatting thecoefficients, standard errors, etc. and additionally gives‘significance stars’ if signif.stars is TRUE.Aliased coefficients are omitted in the returned object but restoredby the print method.Correlations are printed to two decimal places (or symbolically): tosee the actual correlations print summary(object)$correlationdirectly.
The summary.manova method uses a multivariate test statisticfor the summary table.  Wilks' statistic is most popular in theliterature, but the default Pillai–Bartlett statistic is recommendedby Hand and Taylor (1987).The table gives a transformation of the test statistic which hasapproximately an F distribution.  The approximations used followS-PLUS and SAS (the latter apart from some cases of theHotelling–Lawley statistic), but many other distributionalapproximations exist: see Anderson (1984) and Krzanowski and Marriott(1994) for further references.  All four approximate F statistics arethe same when the term being tested has one degree of freedom, but inother cases that for the Roy statistic is an upper bound.The tolerance tol is applied to the QR decomposition of theresidual correlation matrix (unless some response has essentially zeroresiduals, when it is unscaled).  Thus the default value guardsagainst very highly correlated responses: it can be reduced but doingso will allow rather inaccurate results and it will normally be betterto transform the responses to remove the high correlation.
NA
supsmu is a running lines smoother which chooses between threespans for the lines. The running lines smoothers are symmetric, withk/2 data points each side of the predicted point, and values ofk as 0.5 * n, 0.2 * n and 0.05 * n, wheren is the number of data points.  If span is specified,a single smoother with span span * n is used.The best of the three smoothers is chosen by cross-validation for eachprediction. The best spans are then smoothed by a running linessmoother and the final prediction chosen by linear interpolation.The FORTRAN code says: “For small samples (n < 40) or ifthere are substantial serial correlations between observations closein x-value, then a pre-specified fixed span smoother (span >      0) should be used.  Reasonable span values are 0.2 to 0.4.”Cases with non-finite values of x, y or wt aredropped, with a warning.
NA
alternative = "greater" is the alternative that x has alarger mean than y. For the one-sample case: that the mean is positive.If paired is TRUE then both x and y mustbe specified and they must be the same length.  Missing values aresilently removed (in pairs if paired is TRUE).  Ifvar.equal is TRUE then the pooled estimate of thevariance is used.  By default, if var.equal is FALSEthen the variance is estimated separately for both groups and theWelch modification to the degrees of freedom is used.If the input data are effectively constant (compared to the larger of thetwo means) an error is generated.
The model object must have a predict method that acceptstype = "terms", e.g., glm in the stats package,coxph and survreg inthe survival package.For the partial.resid = TRUE option model must have aresiduals method that accepts type = "partial",which lm and glm do.The data argument should rarely be needed, but in some casestermplot may be unable to reconstruct the original dataframe. Using na.action=na.exclude makes these problems less likely.Nothing sensible happens for interaction terms, and they may cause errors.The plot = FALSE option is useful when some special action is needed,e.g. to overlay the results of two different models or to plotconfidence bands.
There are methods for classes "aovlist", and "terms""formula" (see terms.formula):the default method just extracts the terms component of theobject, or failing that a "terms" attribute (as used bymodel.frame).There are print and labels methods forclass "terms": the latter prints the term labels (seeterms.object).
Not all of the options work in the same way that they do in S and notall are implemented.
These are all generic functions, which will use thetsp attribute of x if it exists. timeand cycle have methods for class ts that coercethe result to that class.
NA
The function ts is used to create time-series objects.  Theseare vectors or matrices with class of "ts" (and additionalattributes) which represent data which has been sampled at equispacedpoints in time.  In the matrix case, each column of the matrixdata is assumed to contain a single (univariate) time series.Time series must have at least one observation, and although they neednot be numeric there is very limited support for non-numeric series.Class "ts" has a number of methods.  In particular arithmeticwill attempt to align time axes, and subsetting to extract subsets ofseries can be used (e.g., EuStockMarkets[, "DAX"]).  However,subsetting the first (or only) dimension will return a matrix orvector, as will matrix subsetting.  Subassignment can be used toreplace values but not to extend a series (see window).There is a method for t that transposes the series as amatrix (a one-column matrix if a vector) and hence returns a resultthat does not inherit from class "ts".Argument frequency indicates the sampling frequency of thetime series, with the default value 1 indicating one sample ineach unit time interval.  Forexample, one could use a value of 7 for frequency whenthe data are sampled daily, and the natural time period is a week, or12 when the data are sampled monthly and the natural timeperiod is a year.  Values of 4 and 12 are assumed in(e.g.) print methods to imply a quarterly and monthly seriesrespectively.  As from R 4.0.0, frequency need not be a wholenumber.  For example, frequency = 0.2 would imply samplingonce every five time units.as.ts is generic.  Its default method will use thetsp attribute of the object if it has one to set thestart and end times and frequency.is.ts tests if an object is a time series.  It is generic: youcan write methods to handle specific classes of objects,see InternalMethods.
As a special case, ... can contain vectors or matrices of thesame length as the combined time series of the time series present, aswell as those of a single row.
NA
As a special case, ... can contain vectors or matrices of thesame length as the combined time series of the time series present, aswell as those of a single row.
This is a generic function. It will generally plot the residuals,often standardized, the autocorrelation function of the residuals, andthe p-values of a Portmanteau test for all lags up to gof.lag.The methods for arima and StructTS objectsplots residuals scaled by the estimate of their (individual) variance,and use the Ljung–Box version of the portmanteau test.
The tsp attribute gives the start time in time units,the end time and the frequency (the number of observations per unit oftime, e.g. 12 for a monthly series).Assignments are checked for consistency.Assigning NULL which removes the tsp attributeand any "ts" (or "mts") class of x.
The tsp attribute gives the start time in time units,the end time and the frequency (the number of observations per unit oftime, e.g. 12 for a monthly series).Assignments are checked for consistency.Assigning NULL which removes the tsp attributeand any "ts" (or "mts") class of x.
NA
This is a generic function: the description here applies to the methodfor fits of class "aov".When comparing the means for the levels of a factor in an analysis ofvariance, a simple comparison using t-tests will inflate theprobability of declaring a significant difference when it is not infact present.  This because the intervals are calculated with agiven coverage probability for each interval but the interpretation ofthe coverage is usually with respect to the entire family ofintervals.John Tukey introduced intervals based on the range of thesample means rather than the individual differences.  The intervalsreturned by this function are based on this Studentized rangestatistics.The intervals constructed in this way would only apply exactly tobalanced designs where there are the same number of observations madeat each level of the factor.  This function incorporates an adjustmentfor sample size that produces sensible intervals for mildly unbalanceddesigns.If which specifies non-factor terms these will be dropped witha warning: if no terms are left this is an error.
Note that arguments after ... must be matched exactly.Either interval or both lower and upper must bespecified: the upper endpoint must be strictly larger than the lowerendpoint.The function values at the endpoints must be of opposite signs (orzero), for extendInt="no", the default.  Otherwise, ifextendInt="yes", the interval is extended on both sides, insearch of a sign change, i.e., until the search interval [l,u]satisfies f(l) * f(u) <= 0.If it is known how f changes sign at the rootx0, that is, if the function is increasing or decreasing there,extendInt can (and typically should) be specified as"upX" (for “upward crossing”) or "downX",respectively.  Equivalently, define S:= +/- 1, torequire S = sign(f(x0 +    eps)) at the solution.  In that case, the search interval [l,u]possibly is extended to be such that     S * f(l) <= 0 and S * f(u) >= 0.uniroot() uses Fortran subroutine ‘"zeroin"’ (from Netlib)based on algorithms given in the reference below.  They assume acontinuous function (which then is known to have at least one root inthe interval).Convergence is declared either if f(x) == 0 or the change inx for one step of the algorithm is less than tol (plus anallowance for representation error in x).If the algorithm does not converge in maxiter steps, a warningis printed and the current approximation is returned.f will be called as f(x, ...) for a numeric valueof x.The argument passed to f has special semantics and used to beshared between calls.  The function should not copy it.
NA
NA
Either or both of old and new can be objects such aslength-one character vectors which can be coerced to a formula viaas.formula.The function works by first identifying the left-hand sideand right-hand side of the old formula.It then examines the new formula and substitutesthe lhs of the old formula for any occurrenceof ‘.’ on the left of new, and substitutesthe rhs of the old formula for any occurrenceof ‘.’ on the right of new.  The result is thensimplified via terms.formula(simplify = TRUE).
For cov and cor one must either give a matrix ordata frame for x or give both x and y.The inputs must be numeric (as determined by is.numeric:logical values are also allowed for historical compatibility): the"kendall" and "spearman" methods make sense for orderedinputs but xtfrm can be used to find a suitable priortransformation to numbers.var is just another interface to cov, wherena.rm is used to determine the default for use when thatis unspecified.  If na.rm is TRUE then the completeobservations (rows) are used (use = "na.or.complete") tocompute the variance.  Otherwise, by default use = "everything".If use is "everything", NAs willpropagate conceptually, i.e., a resulting value will be NAwhenever one of its contributing observations is NA.If use is "all.obs", then the presence of missingobservations will produce an error.  If use is"complete.obs" then missing values are handled by casewisedeletion (and if there are no complete cases, that gives an error)."na.or.complete" is the same unless there are no completecases, that gives NA.Finally, if use has the value "pairwise.complete.obs"then the correlation or covariance between each pair of variables iscomputed using all complete pairs of observations on those variables.This can result in covariance or correlation matrices which are not positivesemi-definite, as well as NA entries if there are no completepairs for that pair of variables.   For cov and var,"pairwise.complete.obs" only works with the "pearson"method.Note that (the equivalent of) var(double(0), use = *) givesNA for use = "everything" and "na.or.complete",and gives an error in the other cases.The denominator n - 1 is used which gives an unbiased estimatorof the (co)variance for i.i.d. observations.These functions return NA when there is only oneobservation (whereas S-PLUS has been returning NaN).For cor(), if method is "kendall" or"spearman", Kendall's tau or Spearman'srho statistic is used to estimate a rank-based measure ofassociation.  These are more robust and have been recommended if thedata do not necessarily come from a bivariate normal distribution.For cov(), a non-Pearson method is unusual but available forthe sake of completeness.  Note that "spearman" basicallycomputes cor(R(x), R(y)) (or cov(., .)) where R(u)  := rank(u, na.last = "keep"). In the case of missing values, theranks are calculated depending on the value of use, eitherbased on complete observations, or based on pairwise completeness withreranking for each pair.When there are ties, Kendall's tau_b is computed, asproposed by Kendall (1945).Scaling a covariance matrix into a correlation one can be achieved inmany ways, mathematically most appealing by multiplication with adiagonal matrix from left and right, or more efficiently by usingsweep(.., FUN = "/") twice.  The cov2cor functionis even a bit more efficient, and provided mostly for didacticalreasons.
The null hypothesis is that the ratio of the variances of thepopulations from which x and y were drawn, or in thedata to which the linear models x and y were fitted, isequal to ratio.
NA
These seek a ‘rotation’ of the factors x %*% T thataims to clarify the structure of the loadings matrix.  The matrixT is a rotation (possibly with reflection) for varimax,but a general linear transformation for promax, with thevariance of the factors being preserved.
vcov() is a generic function and functions with names beginningin vcov. will be methods for this function.Classes with methods for this function include:lm, mlm, glm, nls,summary.lm, summary.glm,negbin, polr, rlm (in package MASS),multinom (in package nnet)gls, lme (in package nlme),coxph and survreg (in package survival).(vcov() methods for summary objects allow moreefficient and still encapsulated access when bothsummary(mod) and vcov(mod) are needed.).vcov.aliased() is an auxiliary function useful forvcov method implementations which have to deal with singularmodel fits encoded via NA coefficients: It augments a vcov–matrixvc by NA rows and columns where needed, i.e., whensome entries of aliased are true and vc is of smaller dimensionthan length(aliased).
This is a generic function and methods can be defined for the firstargument x: apart from the default methods there are methodsfor the date-time classes "POSIXct", "POSIXlt","difftime" and "Date".  The default method will work forany numeric-like object for which [, multiplication, divisionand sum have suitable methods, including complex vectors.If w is missing then all elements of x are given thesame weight, otherwise the weights coerced to numeric byas.numeric and normalized to sum to one (if possible: iftheir sum is zero or infinite the value is likely to be NaN).Missing values in w are not handled specially and so give amissing value as the result.  However, zero weights are handledspecially and the corresponding x values are omitted from thesum.
Weighted residuals are based on the deviance residuals, which fora lm fit are the raw residuals Rimultiplied by wi^0.5, where wi are theweights as specified in lm's call.Dropping cases with weights zero is compatible withinfluence and related functions.
NA
The formula interface is only applicable for the 2-sample tests.If only x is given, or if both x and y are givenand paired is TRUE, a Wilcoxon signed rank test of thenull that the distribution of x (in the one sample case) or ofx - y (in the paired two sample case) is symmetric aboutmu is performed.Otherwise, if both x and y are given and pairedis FALSE, a Wilcoxon rank sum test (equivalent to theMann-Whitney test: see the Note) is carried out.  In this case, thenull hypothesis is that the distributions of x and ydiffer by a location shift of mu and the alternative is thatthey differ by some other location shift (and the one-sidedalternative "greater" is that x is shifted to the rightof y).By default (if exact is not specified), an exact p-valueis computed if the samples contain less than 50 finite values andthere are no ties.  Otherwise, a normal approximation is used.For stability reasons, it may be advisable to use rounded data or to setdigits.rank = 7, say, such that determination of ties does notdepend on very small numeric differences (see the example).Optionally (if argument conf.int is true), a nonparametricconfidence interval and an estimator for the pseudomedian (one-samplecase) or for the difference of the location parameters x-y iscomputed.  (The pseudomedian of a distribution F is the medianof the distribution of (u+v)/2, where u and v areindependent, each with distribution F.  If F is symmetric,then the pseudomedian and median coincide.  See Hollander & Wolfe(1973), page 34.)  Note that in the two-sample case the estimator forthe difference in location parameters does not estimate thedifference in medians (a common misconception) but rather the medianof the difference between a sample from x and a sample fromy.If exact p-values are available, an exact confidence interval isobtained by the algorithm described in Bauer (1972), and theHodges-Lehmann estimator is employed.  Otherwise, the returnedconfidence interval and point estimate are based on normalapproximations.  These are continuity-corrected for the interval butnot the estimate (as the correction depends on thealternative).With small samples it may not be possible to achieve very highconfidence interval coverages. If this happens a warning will be givenand an interval with lower coverage will be substituted.When x (and y if applicable) are valid, the function nowalways returns, also in the conf.int = TRUE case when aconfidence interval cannot be computed, in which case the intervalboundaries and sometimes the estimate now containNaN.
The start and end times can be specified as for ts. Ifthere is no observation at the new start or end,the immediately following (start) or preceding (end)observation time is used.The replacement function has a method for ts objects, andis allowed to extend the series (with a warning).  There is no defaultmethod.
The start and end times can be specified as for ts. Ifthere is no observation at the new start or end,the immediately following (start) or preceding (end)observation time is used.The replacement function has a method for ts objects, andis allowed to extend the series (with a warning).  There is no defaultmethod.
read.ftable reads in a flat-like contingency table from afile.  If the file contains the written representation of a flattable (more precisely, a header with all information on names andlevels of column variables, followed by a line with the names of therow variables), no further arguments are needed.  Similarly, flattables with only one column variable the name of which is the onlyentry in the first line are handled automatically.  Other variants canbe dealt with by skipping all header information using skip,and providing the names of the row variables and the names and levelsof the column variable using row.var.names and col.vars,respectively.  See the examples below.Note that flat tables are characterized by their ‘ragged’display of row (and maybe also column) labels.  If the full grid oflevels of the row variables is given, one should instead useread.table to read in the data, and create thecontingency table from this using xtabs.write.ftable writes a flat table to a file, which is useful forgenerating ‘pretty’ ASCII representations of contingencytables.  Different versions are available via the methodargument, which may be useful, for example, for constructing LaTeX tables.
There is a summary method for contingency table objects createdby table or xtabs(*, sparse = FALSE), which gives basicinformation and performs a chi-squared test for independence offactors (note that the function chisq.test currentlyonly handles 2-d tables).If a left hand side is given in formula, its entries are simplysummed over the cells corresponding to the right hand side; this alsoworks if the lhs does not give counts.For variables in formula which are factors, excludemust be specified explicitly; the default exclusions will not be used.In R versions before 3.4.0, e.g., when na.action = na.pass,sometimes zeroes (0) were returned instead of NAs.Note that when addNA is false as by default, and na.actionis not specified (or set to NULL), in effect na.action =    getOption("na.action", default=na.omit) is used; see also the examples.

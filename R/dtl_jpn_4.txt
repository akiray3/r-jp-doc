x
fa と principal で使われる適合度検定を1つの関数にまとめる。  行列が特異な場合、適合関数を求める前に相関行列を平滑化します。SEM関数と同様に、RMSEA（近似の二乗平均平方根誤差）とアルファ信頼区間を求めます。  カイ2乗は2つの方法で求められます。  1つ目（STATISTIC）は、最尤目的関数（下記参照）から適合度検定を適用します。  これは多変量正規性を仮定する。  もう1つは、観察された残差相関行列と各相関の観察された標本サイズに基づく経験的カイ2乗です。  これは、残差相関の2乗と標本サイズを合計することによって求められます。  
これらの古い関数のすべての機能については、faの記述を参照してください。
因子/主成分分析ローディング行列は、各項目が1つだけのクラスタに割り当てられるクラスタ(-1,0,1)定義行列に変換されます。  これは、クラスタ合成を形成するために単位重み付けされる項目を抽出する高速な方法です。  これらの複合スコアの相関を求めるには、cluster.corと組み合わせてこの関数を使用します。SAPAプロジェクトにおける典型的な使用法は、クラスタリングまたはファクタリング（ICLUST, principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することである。  入力は、項目の負荷量の行列、または負荷量行列を含む因子分析の出力です。
これらの関数の中で最も有用なものは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminの2つであろう。もう1つはTargetQで、これはターゲットの欠損NA値を許容します。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、最も負荷の高いグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（例えば、すべての変数にg負荷とグループ因子がある純粋なバイファクターモデル）を扱うことはできません。
NguyenとWallerは、因子分析における局所極小の問題をレビューしています。  faRotateは、異なる開始値でn.rotationを生成し、複数の開始値を用いて元の負荷量に指定された回転を適用します。  超平面数と複雑度指数が各開始行列について報告され、最も高い超平面数と最も低い複雑度を持つものが返される。
NA
NA
DV1 + DV2 ~ IV1 + IV2 + (IV3) + I(IV4^2) - IV5 のように与えられた基本的な数式入力は、2つのDV (1と2)、2つの通常のIV (1と2)、調停子 (IV3)、2次関数 (IV4)、および部分化される変数 (IV5) を返すように解析されます。setCor と mediate の様々な例を参照してください。
lowerCor は，対角行列の下側を丸めたものを出力し，列名は桁数 + 3 文字に省略されます．  デフォルトでは、変数のペアワイズ削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
NA
差別待遇に抗議する女性に対する女性の反応は、Garciaら(2010)によって報告された実験で調べられた。129人の女性に、職場での性差別（男性弁護士が、明らかに有能な女性弁護士よりも昇進した）に関する記述が与えられた。  被験者はその後、対象の弁護士がその決定を不当だと感じていることを読んだ。  その後、被験者は無作為に3つの条件に割り当てられた：コントロール（抗議なし）、個人的抗議（「彼らは私を不当に扱っている」）、集団的抗議（「事務所は女性を不当に扱っている」）である。その後、参加者は、対象がどの程度好きか（liking）、対象に対してどの程度怒っているか（angry）、対象の反応の妥当性を評価するか（respappr）を尋ねられた。  Garciaら（2010）は、多くの交互作用（媒介効果）だけでなく、媒介効果（moderated-mediation effect）も報告しています。このデータ集合は、媒介効果（moderated-mediation effect）の例としてHayes（2013）で使用されています。  このデータ集合は、回帰でのモデレーション（交互作用項）のやり方（setCorを参照）、モデレートされたメディエーションのやり方（mediateを参照）、交互作用グラフの描き方（helpを参照）を示すために、ここで使用されています。
関数の書き方を教えるのに便利で，中心傾向を推定するさまざまな方法を示すのにも役立つ．
驚くことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するための複数のアプローチが存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。16項目以下のテストにsplitHalfを使用すると、すべての可能な分割をかなり簡単に見つけることができます。  17項目以上のテストでは、n.sample分割がランダムに発見されます。したがって、16 個以下の項目の場合、上限値と下限値は正確です。  17項目以上の場合、それらは近いですが、おそらく最高信頼度をわずかに過小評価し、最低信頼度を過大評価するでしょう。  guttman関数は、Guttman (1945)が議論した6つの推定値、Berge and Zergers (1978)の10個のうちの4つ、およびsplitHalfを使ったRevelleのβ (1979)を含みます。コンパニオン関数の ω は、 ω hierarchical (ω_h) と ω total (ω_t) を計算します。Guttmanの最初の推定 λ_1 は，項目の分散がすべて誤差であると仮定します：λ 1= 1-tr(Vx)/Vxこれは明らかに過小評価です.2番目の境界 λ_2 は，対角を，対角から外れた要素の平方和の平方根の関数に置き換えます．  C_2 = ￢vec{1}( ￢vec{V}-diag(￢vec{V})^2 ￢vec{1}' とすると、λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x )事実上、これは対角線をn * 対角線外要素の平均二乗の平方根で置き換えています。  Guttmanの第3の下界、λ_3もλ_1を修正し、項目間の平均共分散として各項目の真の分散を推定し、もちろんCronbachのαと同じです。 λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx) = ((n)/(n-1))(Vx-tr(Vx)/Vx = αこれは対角要素を平均対角外要素に置き換えただけです。  λ_2 ≥ λ_3 で、共分散が同一でなければ λ_2 > λ_3 である。λ_3 と λ_2 はともに λ_1 の補正であり、この補正は連続的な改良の無限集合として一般化できる。(Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)ここで、p_h = sum(σ^2h, h = 0, 1, 2, ... r-1 andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978)。  μ_0＝λ_3＝α、μ_1＝λ_2であることは明らかである。μ_r≧μ_{r-1}≧・・・μ_1≧μ_0であるが、この系列は最初の2ステップ以降はあまり改善されない。Guttmanの第4の下界、λ_4はもともと任意のスピットハーフ信頼性として提案されたが、最大のスプリットハーフ信頼性と解釈されている。X}を2つの部分、相関r_{ab}で、 \vec{X}_a と \vec{X}_b に分割すると、λ 4 = 4rab/(Va + Vb + 2rabVaVb)となり、これは通常の分割半信頼性ですが、この場合は最も類似した分割の信頼性です。16以下の項目の場合、これはすべての可能な分割を試すことによって求められる。  Guttmanの第5の下界であるλ_5は、対角値を項目間共分散2乗和の最大値（項目全体）の平方根の2倍に置き換えますλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.).λ_1より優れているが、λ_5は対角の補正を過小評価する。  より良い推定は、λ_3で使用された補正に類似している：λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman の最終的な境界は、他のすべての項目の線形回帰で説明できる各項目の分散の量（2乗重相関またはsmc）を考慮します、より正確には，誤差の分散 e_j^2 で，λ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx である．smcは全ての項目から求められる。  score.items関数で報告されるGuttmanのλ_6, λ_6*の修正は、選択された尺度の項目だけでなく、与えられた項目のプール全体からsmcを求めることです。  Guttmanのλ_4は最大のスプリットハーフ信頼性です。  もともとは3つの異なるアプローチからの出力を組み合わせることで求めていましたが、現在ではsplitHalfを使用して（16項目以下の場合）総当たりで最大値を求めるか、相当数のランダムな分割を行うことで置き換えられています。以前から試みられていたアルゴリズムには次のようなものがあります： a) 逆相関行列のICLUSTを行う。  ICLUSTは通常、最も明確なクラスタを形成する。  相関を逆にすることで、最も関連性の高いクラスターを見つける傾向がある。  b) あるいは、相関のkmeansクラスタリング（対角線を0に置き換えて擬似距離を作る）により、2つの類似したクラスタを生成することができる。c) 第1主因子上の順序に基づいて項目を2つのクラスタに割り当てることにより、クラスタを識別。  (Highest to cluster 1, next 2 to cluster 2, etc.) これら3つの手順は，項目を2つの分割に割り当てるためのキーベクトルを生成する．  スプリット・ハーフの信頼性の最大値は、これら3つのアプローチの最大値をとることで求められる。  ブルートフォース法とサンプリング法は、より安定で大きな推定値を提供するようである。splitHalf に実装されているもう1つの手続きは、実際に可能なすべてのスプリットハーフ（n項目 <= 16の場合）を形成するか、テスト長で補正された10,000（またはそれ以上）のスプリットハーフをサンプリングします。  この関数は、最良分割と最悪分割を項目キーとして返します。  24項目までは妥当な時間で処理できますが、24項目を超えるとかなり遅くなります。24 個の項目のすべての可能な分割を行うには、 1,352,078 個の分割を考える。  2.4GHzの8コアを搭載したMacProで24項目の問題を実行した場合のタイミングは、デフォルトの10,000サンプルで0.24秒、30,000サンプルで0.678秒、すべての可能性で22.58秒である。  これらのサンプルサイズでの最大分割の値は、デフォルトのサンプルサイズ10000の3回の複製で.799,/.804と.800、30,000の2セットで.805と.806、網羅的探索で.812でした。  1つは、glb で、最大のスプリット・ハーフ信頼度 λ_4 を求めます。これはテストを項目の集合とみなし、項目をどのように分割するのが最適かを検討する。他の2つ、glb.faとglb.algebraicは、行列の対角の重みづけの代替方法です。glb.faは、因子の数が正の固有値を持つ数である因子モデルから変数の共通性を推定します。  そして、信頼性は、glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/Vxで求められます。この推定は、Rcsdpパッケージのcsdpの呼び出しを使用するAndreas Moeltnerによって書かれたglb.algebraicによって求められるものとは少し異なります。彼のアルゴリズムは、JacksonとWoodhouseによるglbの記述により近いものであるが、サンプルサイズが小さい場合、正のバイアス（すなわち、いくつかの項目の信頼性を過剰に推定する；それらは=1と言われる）を持つようである。  この2つのアルゴリズムについては、現在さらに調査中である。glb.algebraicと比較すると、glb.faはサンプルサイズが小さい（n < 500）場合は（正の）バイアスが少ないようですが、サンプルサイズが大きい（> 1000）場合は大きくなります。これは変数の数と相互作用するので、等しいバイアスのサンプルサイズは変数の数の関数として異なります。  しかし，その差は小さい．サンプルサイズが大きくなると、glb.algebraicは母集団の値に収束するようであるが、glb.faは正のバイアスを持つ。
Cをp * p共分散行列、v = diag(C)をその対角行列（すなわち分散v_i = c_{ii}のベクトル）、C0 = C - Diag(v)を対角に0を代入した共分散行列、x = ベクトル(x1, ... , xp)とすると、教育テスト問題は次のようになる（例えば、Al-Homidan 2008）（Sum i = 1 to p xi） -> min s.t. C0 + Diag(x) >= 0（すなわち正半正定値）かつ xi ≤ vi, i = 1 ..., p. これは、C0 + Diag(x)が正半正定値かつ xi ≤ vi であるとき、対称行列C0 + Diag(x)のトレースを最小化することと同じである。信頼性に対する最大の下界は、(sum cij (i \ne j) + sum xi )/ sum cijAdditionally, function glb.algebraic allows the user to change the upper bounds xi ≤ vi toxi ≤ ui and add lower bounds li ≤ xi.信頼性に対する最大の下界は、非均質な項目を持つテストに適用できる。これは，テストスコア全体の信頼性に対する鋭い下界を与える．glb.algebraicは厳密な共分散行列に対して厳密な下界を与えるが，経験的行列からの推定値は，サンプルサイズが小さい場合や中程度の場合，上方へ強く偏る可能性がある．glb.algebraicはRcsdpパッケージの関数csdpの呼び出しのラッパーである（そのドキュメントを参照）．Covが、信頼性の下界relが既知（例えばCronbachs α）の下位テスト/項目の共分散行列である場合、LoBoundsは、LoBounds <- rel*diag(Cov)と設定することで信頼性の下界を改善するために使用することができます。UpBoundsを変更することで、制約xi ≤ viを緩和したり、LoBounds[i] < -z; UpBounds[i] <- zと設定することでxiの値を固定するために使用することができます。
驚くことに、Spearman (1904)が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するための複数のアプローチが存在する。非常に一般的ですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。16項目以下のテストにsplitHalfを使用すると、すべての可能な分割をかなり簡単に見つけることができます。  17項目以上のテストでは、n.sample分割がランダムに発見されます。したがって、16 個以下の項目の場合、上限値と下限値は正確です。  17項目以上の場合、それらは近いですが、おそらく最高信頼度をわずかに過小評価し、最低信頼度を過大評価するでしょう。  guttman関数は、Guttman (1945)が議論した6つの推定値、Berge and Zergers (1978)の10個のうちの4つ、およびsplitHalfを使ったRevelleのβ (1979)を含みます。コンパニオン関数の ω は、 ω hierarchical (ω_h) と ω total (ω_t) を計算します。Guttmanの最初の推定 λ_1 は，項目の分散がすべて誤差であると仮定します：λ 1= 1-tr(Vx)/Vxこれは明らかに過小評価です.2番目の境界 λ_2 は，対角を，対角から外れた要素の平方和の平方根の関数に置き換えます．  C_2 = ￢vec{1}( ￢vec{V}-diag(￢vec{V})^2 ￢vec{1}' とすると、λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x )事実上、これは対角線をn * 対角線外要素の平均二乗の平方根で置き換えています。  Guttmanの第3の下界、λ_3もλ_1を修正し、項目間の平均共分散として各項目の真の分散を推定し、もちろんCronbachのαと同じです。 λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx) = ((n)/(n-1))(Vx-tr(Vx)/Vx = αこれは対角要素を平均対角外要素に置き換えただけです。  λ_2 ≥ λ_3 で、共分散が同一でなければ λ_2 > λ_3 である。λ_3 と λ_2 はともに λ_1 の補正であり、この補正は連続的な改良の無限集合として一般化できる。(Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)ここで、p_h = sum(σ^2h, h = 0, 1, 2, ... r-1 andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978)。  μ_0＝λ_3＝α、μ_1＝λ_2であることは明らかである。μ_r≧μ_{r-1}≧・・・μ_1≧μ_0であるが、この系列は最初の2ステップ以降はあまり改善されない。Guttmanの第4の下界、λ_4はもともと任意のスピットハーフ信頼性として提案されたが、最大のスプリットハーフ信頼性と解釈されている。X}を2つの部分、相関r_{ab}で、 \vec{X}_a と \vec{X}_b に分割すると、λ 4 = 4rab/(Va + Vb + 2rabVaVb)となり、これは通常の分割半信頼性ですが、この場合は最も類似した分割の信頼性です。16以下の項目の場合、これはすべての可能な分割を試すことによって求められる。  Guttmanの第5の下界であるλ_5は、対角値を、項目間共分散λ_5 = λ_1 +2/sqrt(average(C_2)/V_X.) の二乗和の最大値（項目全体）の平方根の2倍に置き換えます。λ_1より優れているが、λ_5は対角の補正を過小評価する。  より良い推定は、λ_3で使用された補正に類似している：λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman の最終的な境界は、他のすべての項目の線形回帰で説明できる各項目の分散の量（2乗重相関またはsmc）を考慮します、より正確には，誤差の分散 e_j^2 で，λ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx である．smcは全ての項目から求められる。  score.items関数で報告されるGuttmanのλ_6, λ_6*の修正は、選択された尺度の項目だけでなく、与えられた項目のプール全体からsmcを求めることです。  Guttmanのλ_4は最大のスプリットハーフ信頼性です。  もともとは3つの異なるアプローチからの出力を組み合わせることで発見されましたが、現在ではsplitHalfを使用して（16項目以下の場合）総当たりで最大値を求めるか、相当数のランダムな分割を行うことで置き換えられています。以前から試みられていたアルゴリズムには次のようなものがあります： a) 逆相関行列のICLUSTを行う。  ICLUSTは通常、最も明確なクラスタを形成する。  相関を逆にすることで、最も関連性の高いクラスターを見つける傾向がある。  b) あるいは、相関のkmeansクラスタリング（対角線を0に置き換えて擬似距離を作る）により、2つの類似したクラスタを生成することができる。c) 第1主因子の順序に基づいて項目を2つのクラスタに割り当てることにより、クラスタを識別。  (Highest to cluster 1, next 2 to cluster 2, etc.) これら3つの手順は，項目を2つの分割に割り当てるためのキーベクトルを生成する．  スプリット・ハーフの信頼性の最大値は、これら3つのアプローチの最大値をとることで求められる。  ブルートフォース法とサンプリング法は、より安定で大きな推定値を提供するようである。splitHalf に実装されているもう1つの手続きは、可能なすべてのスプリットハーフ(n items <= 16の場合)を実際に形成するか、テスト長で補正された10,000(またはそれ以上)のスプリットハーフをサンプリングします。  この関数は、最良分割と最悪分割を項目キーとして返します。  24項目までは妥当な時間で処理できますが、24項目を超えるとかなり遅くなります。24 個の項目のすべての可能な分割を行うには、 1,352,078 個の分割を考える。  2.4GHzの8コアを搭載したMacProで24項目の問題を実行した場合のタイミングは、デフォルトの10,000サンプルで0.24秒、30,000サンプルで0.678秒、すべての可能性で22.58秒である。  これらのサンプルサイズでの最大分割の値は、デフォルトのサンプルサイズ10000の3回の複製で.799,/.804と.800、30,000の2セットで.805と.806、網羅的探索で.812でした。  1つは、glb で、最大のスプリット・ハーフ信頼度 λ_4 を求めます。これはテストを項目の集合とみなし、項目をどのように分割するのが最適かを検討する。他の2つ、glb.faとglb.algebraicは、行列の対角の重みづけの代替方法です。glb.faは、因子の数が正の固有値を持つ数である因子モデルから変数の共通性を推定します。  そして、信頼性は、glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/Vxで求められます。この推定は、Rcsdpパッケージのcsdpの呼び出しを使用するAndreas Moeltnerによって書かれたglb.algebraicによって求められるものとは少し異なります。彼のアルゴリズムは、JacksonとWoodhouseによるglbの記述により近いものであるが、サンプルサイズが小さい場合、正のバイアス（すなわち、いくつかの項目の信頼性を過剰に推定する；それらは=1と言われる）を持つようである。  この2つのアルゴリズムについては、現在さらに調査中である。glb.algebraicと比較すると、glb.faはサンプルサイズが小さい（n < 500）場合は（正の）バイアスが少ないようですが、サンプルサイズが大きい（> 1000）場合は大きくなります。これは変数の数と相互作用するので、等しいバイアスのサンプルサイズは変数の数の関数として異なります。  しかし，その差は小さい．標本サイズが大きくなると、glb.algebraicは母集団の値に収束するようであるが、glb.faは正のバイアスを持つ。
一般化可能性理論は、信頼性の分析に分散成分のアプローチを適用したものです。  G研究（一般化可能性）で成分が推定され、D研究（決定）で使用されます。  特定のD研究に適したさまざまな比率が形成される。
Gorsuc (1997) は，因子拡張の代替モデルを提案した．  彼の手法は，反復変数の場合に適している．  これは，correct=FALSEのlink{fa.extension}で扱われます
驚くことに、Spearman (1904)が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するための複数のアプローチがあります。Cronbachのα (1951) は非常によく使われていますが、テストの信頼性を過小評価し、第1因子の飽和を過大評価します。16項目以下のテストにsplitHalfを使用すると、すべての可能な分割をかなり簡単に見つけることができます。  17項目以上のテストでは、n.sample分割がランダムに発見されます。したがって、16 個以下の項目の場合、上限値と下限値は正確です。  17項目以上の場合、それらは近いですが、おそらく最高信頼度をわずかに過小評価し、最低信頼度を過大評価するでしょう。  guttman関数は、Guttman (1945)が議論した6つの推定値、Berge and Zergers (1978)の10個のうちの4つ、およびsplitHalfを使ったRevelleのβ (1979)を含みます。コンパニオン関数の ω は、 ω hierarchical (ω_h) と ω total (ω_t) を計算します。Guttmanの最初の推定 λ_1 は，項目の分散がすべて誤差であると仮定します：λ 1= 1-tr(Vx)/Vxこれは明らかに過小評価です.2番目の境界 λ_2 は，対角を，対角から外れた要素の平方和の平方根の関数に置き換えます．  C_2 = ￢vec{1}( ￢vec{V}-diag(￢vec{V})^2 ￢vec{1}' とすると、λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x )事実上、これは対角線をn * 対角線外要素の平均二乗の平方根で置き換えています。  Guttmanの第3の下界、λ_3もλ_1を修正し、項目間の平均共分散として各項目の真の分散を推定し、もちろんCronbachのαと同じです。 λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx) = ((n)/(n-1))(Vx-tr(Vx)/Vx = αこれは対角要素を平均対角外要素に置き換えただけです。  λ_2 ≥ λ_3 で、共分散が同一でなければ λ_2 > λ_3 である。λ_3 と λ_2 はともに λ_1 の補正であり、この補正は連続的な改良の無限集合として一般化できる。(Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)ここで、p_h = sum(σ^2h, h = 0, 1, 2, ... r-1 andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978)。  μ_0＝λ_3＝α、μ_1＝λ_2であることは明らかである。μ_r≧μ_{r-1}≧・・・μ_1≧μ_0であるが、この系列は最初の2ステップ以降はあまり改善されない。Guttmanの第4の下界、λ_4はもともと任意のスピット・ハーフ信頼性として提案されたが、最大のスプリット・ハーフ信頼性と解釈されている。X}を2つの部分、相関r_{ab}で、 \vec{X}_a と \vec{X}_b に分割すると、λ 4 = 4rab/(Va + Vb + 2rabVaVb)となり、これは通常の分割半信頼性ですが、この場合は最も類似した分割の信頼性です。16以下の項目の場合、これはすべての可能な分割を試すことによって求められる。  Guttmanの第5の下界であるλ_5は、対角値を項目間共分散2乗和の最大値（項目全体）の平方根の2倍に置き換えますλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.).λ_1より優れているが、λ_5は対角の補正を過小評価する。  より良い推定は、λ_3で使用される補正に類似している：λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman の最終的な境界は、他のすべての項目の線形回帰で説明できる各項目の分散の量（2乗重相関またはsmc）を考慮します、より正確には，誤差の分散 e_j^2 で，λ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx である．smcは全ての項目から求められる。  score.items関数で報告されるGuttmanのλ_6, λ_6*の修正は、選択された尺度の項目だけでなく、与えられた項目のプール全体からsmcを求めることです。  Guttmanのλ_4は最大のスプリットハーフ信頼性です。  もともとは3つの異なるアプローチからの出力を組み合わせることで求めていましたが、現在ではsplitHalfを使用して（16項目以下の場合）総当たりで最大値を求めるか、相当数のランダムな分割を行うことで置き換えられています。以前から試みられていたアルゴリズムには次のようなものがあります： a) 逆相関行列のICLUSTを行う。  ICLUSTは通常、最も明確なクラスタを形成する。  相関を逆にすることで、最も関連性の高いクラスターを見つける傾向がある。  b) あるいは、相関のkmeansクラスタリング（対角線を0に置き換えて擬似距離を作る）により、2つの類似したクラスタを生成することができる。c) 第1主因子上の順序に基づいて項目を2つのクラスタに割り当てることにより、クラスタを識別。  (Highest to cluster 1, next 2 to cluster 2, etc.) これら3つの手順は，項目を2つの分割に割り当てるためのキーベクトルを生成する．  スプリット・ハーフの信頼性の最大値は、これら3つのアプローチの最大値をとることで求められる。  ブルートフォース法とサンプリング法は、より安定で大きな推定値を提供するようである。splitHalf に実装されているもう1つの手続きは、可能なすべてのスプリットハーフ(n items <= 16の場合)を実際に形成するか、テスト長で補正された10,000(またはそれ以上)のスプリットハーフをサンプリングします。  この関数は、最良分割と最悪分割を項目キーとして返します。  24項目までは妥当な時間で処理できますが、24項目を超えるとかなり遅くなります。24 個の項目のすべての可能な分割を行うには、 1,352,078 個の分割を考える。  2.4GHzの8コアを搭載したMacProで24項目の問題を実行した場合のタイミングは、デフォルトの10,000サンプルで0.24秒、30,000サンプルで0.678秒、すべての可能性で22.58秒である。  これらのサンプルサイズでの最大分割の値は、デフォルトのサンプルサイズ10000の3回の複製で.799,/.804と.800、30,000の2セットで.805と.806、網羅的探索で.812でした。  1つは、glb で、最大のスプリット・ハーフ信頼度 λ_4 を求めます。これはテストを項目の集合とみなし、項目をどのように分割するのが最適かを検討する。他の2つ、glb.faとglb.algebraicは、行列の対角の重みづけの代替方法です。glb.faは、因子の数が正の固有値を持つ数である因子モデルから変数の共通性を推定します。  そして、信頼性は、glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/Vxで求められます。この推定は、Rcsdpパッケージのcsdpの呼び出しを使用するAndreas Moeltnerによって書かれたglb.algebraicによって求められるものとは少し異なります。彼のアルゴリズムは、JacksonとWoodhouseによるglbの記述により近いものであるが、サンプルサイズが小さい場合、正のバイアス（すなわち、いくつかの項目の信頼性を過剰に推定する；それらは=1と言われる）を持つようである。  この2つのアルゴリズムについては、現在さらに調査中である。glb.algebraicと比較すると、glb.faはサンプルサイズが小さい（n < 500）場合は（正の）バイアスが少ないようですが、サンプルサイズが大きい（> 1000）場合は大きくなります。これは変数の数と相互作用するので、等しいバイアスのサンプルサイズは変数の数の関数として異なります。  しかし，その差は小さい．サンプルサイズが大きくなると、glb.algebraicは母集団の値に収束するようであるが、glb.faは正のバイアスを持つ。
 Harman.Holzinger: 能力検査の9 x 9相関行列, N = 696.Harman.Burt: "感情的 "項目の8 x 8相関行列。N = 172 Harman.5：5つの社会経済的データについて12の国勢調査区（Harman p 14） Harman.political：p166。Harman.8 8つの身体的尺度Harman.Holzinger.Harman (1967, p 244)の9つの心理学的変数は、K.J.Holzingerの696人の参加者との未発表の授業ノートから取られたものである。  これは、4つの因子を持つ12のテストのサブセットです。これは、二因子解のもう1つの良い例である。  Bentler (2007)は、信頼性分析について議論するためにこのデータセットを使用しています。このデータは明確な2因子構造を示し、ω関数に含まれる信頼性のさまざまな推定値の良い例です。bifactorのHolzingerまたはHolzinger.9データセットと混同しないでください。holzinger.swinefordの26変数で301人の被験者からなるHolzinger-Swinefordデータセットも参照してください。これらのデータはKeith Widamanによって提供された。「Holzinger and Swineford (1939)のデータは、多くの研究者によってモデルデータセットとして使用されてきました。例えば、Harman (1976)は、多因子分析に関する彼の権威あるテキストの中で、"24の心理学的変数 "の例を顕著に使用し、このルーブリックの下で提示されたデータは、Grant-White学派（N = 145）の変数の24で構成されていた。Meredith (1964a, 1964b)は，Holzinger and Swinefordの研究からいくつかの変数を，選択下での要因不変性の研究で使用した．Joreskog (1971) は、Holzinger and Swinefordのデータを用いた多群確認的因子分析の彼の仕事に基づいて、データを4つのグループにサブセットしました。Rの "lavaan "パッケージを開発したRosseelは、lavaanパッケージをダウンロードすると、Holzinger and Swineford (1939)からの顕在変数の9つを "常駐 "データ集合として含みます。この "常駐 "データ集合には、9つの心理テスト（データ集合ではx1～x9と命名されている）に加えて、いくつかの背景変数が含まれています。これらのデータを分析したところ、変数の分布（平均、SD）が元の論文の標本統計と一致しないことがわかりました。例えば、lavaanの "resident "データセットでは、すべての顕在変数のスコアは0から10の間、サンプル平均は3から6の間、サンプルSDは1.0から1.5の間であった。オリジナルのデータセットでは、スコアの範囲はテストによってかなり異なっており、スコアが0から20の範囲にある変数もあれば、スコアが50から300以上の範囲にある顕在変数もあった。8つの "情動 "変数は、Harman (1967, p 164)から引用され、Burt (1939)から引用された。  これらは、9歳から12歳の172人の健常児のものである。  Harmanが指摘したように、この相関行列は特異であり、2乗多重相関＞1である。  (たとえば、omegaは、fm="minres "またはfm="pa "に対して警告メッセージを出しますが、fm="ml "に対して失敗します。)8つの身体変数の問題は、Harman (1976)から取られたもので、305人の女の子の8つの身体変数の間の相関を表します。  2つの相関クラスターは、"lankiness "の4つの尺度と "stockiness "の4つの尺度を表します。  元のデータは、Mullen (1939)の未発表の学位論文で報告された17の変数から選ばれたものである。変数6（"Bitrochanteric diamter"）は、お尻の外側の点間の距離である。  主軸のfa解（fm="pa"）は、fm="minres "と同様に、報告されているminres解と一致する。様々な身体測定を使った教育例に興味のある方は、gclusパッケージの身体データセットを参照してください。  Burtデータセットは、おそらく元の相関行列にタイプミスがあります。  Sorrow-Tendernessの相関を.87から.81に変更すると、相関が正定値になります。Jan DeLeeuwが指摘したように、Burtデータ集合は、Burtが1915年に報告したオリジナルの11変数から8変数のサブセットです。  この行列にも同じ問題があります。burtを参照。因子分析の有用なデモンストレーションとなる他のデータセットの例は、Bechtoldtの7つの2因子の例と、Harman74.corの24の能力測定です。psychパッケージ（つまり、Harman.8）、datasetaパッケージ、GPArotationパッケージにも、いくつかのHarmanの例があります。  Harman 24 mental tests問題はHarman74.corの基本データセットパッケージにある。他のHarmanデータセットとしては、John LoehlinがEFAの例として使った12の国勢調査区の5つの社会経済変数Harman.5がある。  多くのHarman (1967)データセットのもう1つは、Harman.politicalです。  これは、147の選挙区で取られた8つの政治変数を含みます。  SMCを共分散性とする主因子法は、表8.18のものと一致します。  このデータは，DziubianとShirkeyによって，因子妥当性のKaiser-Meyer-Olkin検定の例として使用されている．
 Harman.Holzinger：能力検査の9×9相関行列、N=696。Harman.Burt：「感情」項目の8×8の相関行列。N = 172 Harman.5：5つの社会経済的データについて12の国勢調査区（Harman p 14） Harman.political：p166。Harman.8 8つの身体的尺度Harman.Holzinger.Harman (1967, p 244)の9つの心理学的変数は、K.J.Holzingerの696人の参加者との未発表の授業ノートから取られたものである。  これは、4つの因子を持つ12のテストのサブセットです。これは、二因子解のもう1つの良い例である。  Bentler (2007)は、信頼性分析について議論するためにこのデータセットを使用しています。このデータは明確な2因子構造を示し、ω関数に含まれる信頼性のさまざまな推定値の良い例です。bifactorのHolzingerまたはHolzinger.9データセットと混同しないでください。holzinger.swinefordの26変数で301人の被験者からなるHolzinger-Swinefordデータセットも参照してください。これらのデータはKeith Widamanによって提供された。「Holzinger and Swineford (1939)のデータは、多くの研究者によってモデルデータセットとして使用されてきました。例えば、Harman (1976)は、多因子分析に関する彼の権威あるテキストの中で、"24の心理学的変数 "の例を顕著に使用し、このルーブリックの下で提示されたデータは、Grant-White学派（N = 145）の変数の24から構成されていた。Meredith (1964a, 1964b)は，Holzinger and Swinefordの研究からいくつかの変数を，選択下での要因不変性の研究で使用した．Joreskog (1971) は、Holzinger and Swinefordのデータを用いた多群確認的因子分析の彼の仕事に基づいて、データを4つのグループにサブセットしました。Rの "lavaan "パッケージを開発したRosseelは、lavaanパッケージをダウンロードすると、Holzinger and Swineford (1939)からの顕在変数の9つを "常駐 "データ集合として含みます。この "常駐 "データ集合には、9つの心理テスト（データ集合ではx1～x9と命名されている）に加えて、いくつかの背景変数が含まれています。これらのデータを分析したところ、変数の分布（平均、SD）が元の論文の標本統計と一致しないことがわかりました。例えば、lavaanの "resident "データセットでは、すべての顕在変数のスコアは0から10の間、サンプル平均は3から6の間、サンプルSDは1.0から1.5の間であった。オリジナルのデータセットでは、スコアの範囲はテストによってかなり異なっており、スコアが0から20の範囲にある変数もあれば、スコアが50から300を超える顕在変数もあった。8つの "情動 "変数は、Harman (1967, p 164)から引用され、Burt (1939)から引用された。  これらは、9歳から12歳の172人の健常児のものである。  Harmanが指摘したように、この相関行列は特異であり、2乗多重相関＞1である。  (たとえば、omegaは、fm="minres "またはfm="pa "に対して警告メッセージを出しますが、fm="ml "に対して失敗します。)8つの身体変数の問題は、Harman (1976)から取られたもので、305人の女の子の8つの身体変数の間の相関を表します。  2つの相関クラスターは、"lankiness "の4つの尺度と "stockiness "の4つの尺度を表します。  元のデータは、Mullen (1939)の未発表の学位論文で報告された17の変数から選ばれたものである。変数6（"Bitrochanteric diamter"）は、お尻の外側の点間の距離である。  主軸のfa解（fm="pa"）は、fm="minres "と同様に、報告されたminres解と一致する。様々な身体測定を使った教育例に興味のある方は、gclusパッケージの身体データセットを参照してください。  Burtデータセットは、おそらく元の相関行列にタイプミスがあります。  Sorrow-Tendernessの相関を.87から.81に変更すると、相関が正定値になります。Jan DeLeeuwが指摘したように、Burtデータ集合は、Burtが1915年に報告したオリジナルの11変数から8変数のサブセットです。  この行列にも同じ問題があります。burtを参照。因子分析の有用なデモンストレーションとなる他のデータセットの例は、Bechtoldtの7つの2因子の例と、Harman74.corの24の能力測定です。psychパッケージ（つまり、Harman.8）、datasetaパッケージ、GPArotationパッケージにも、いくつかのHarmanの例があります。  Harman 24 mental tests問題はHarman74.corの基本データセットパッケージにある。他のHarmanデータセットとしては、John LoehlinがEFAの例として使った12の国勢調査区の5つの社会経済変数Harman.5がある。  多くのHarman (1967)データセットのもう1つは、Harman.politicalです。  これは、147の選挙区で取られた8つの政治変数を含みます。  SMCを共分散性とする主因子法は、表8.18のものと一致します。  このデータは，DziubianとShirkeyによって，因子妥当性のKaiser-Meyer-Olkin検定の例として使用されている．
 Harman.Holzinger：能力検査の9×9相関行列、N=696。Harman.Burt：「感情」項目の8×8の相関行列。N = 172 Harman.5：5つの社会経済的データについて12の国勢調査区（Harman p 14） Harman.political：p166。Harman.8 8つの身体的尺度Harman.Holzinger.Harman (1967, p 244)の9つの心理学的変数は、K.J.Holzingerの696人の参加者との未発表の授業ノートから取られたものである。  これは、4つの因子を持つ12のテストのサブセットです。これは、二因子解のもう1つの良い例である。  Bentler (2007)は、信頼性分析について議論するためにこのデータセットを使用しています。このデータは明確な2因子構造を示し、ω関数に含まれる信頼性のさまざまな推定値のいい例です。bifactorのHolzingerまたはHolzinger.9データセットと混同しないでください。holzinger.swinefordの26変数で301人の被験者からなるHolzinger-Swinefordデータセットも参照してください。これらのデータはKeith Widamanによって提供された。「Holzinger and Swineford (1939)のデータは、多くの研究者によってモデルデータセットとして使用されてきました。例えば、Harman (1976)は、多因子分析に関する彼の権威あるテキストの中で、"24の心理学的変数 "の例を顕著に使用し、このルーブリックの下で提示されたデータは、Grant-White学派（N = 145）の変数の24から構成されていた。Meredith (1964a, 1964b)は，Holzinger and Swinefordの研究からいくつかの変数を，選択下での要因不変性の研究で使用した．Joreskog (1971) は、Holzinger and Swinefordのデータを用いた多群確認的因子分析の彼の仕事に基づいて、データを4つのグループにサブセットしました。Rの "lavaan "パッケージを開発したRosseelは、lavaanパッケージをダウンロードすると、Holzinger and Swineford (1939)からの顕在変数の9つを "resident "データ集合として含みます。この "常駐 "データ集合には、9つの心理テスト（データ集合ではx1～x9と命名されている）に加えて、いくつかの背景変数が含まれています。これらのデータを分析したところ、変数の分布（平均、SD）が元の論文の標本統計と一致しないことがわかりました。例えば、lavaanの "resident "データセットでは、すべての顕在変数のスコアは0から10の間、サンプル平均は3から6の間、サンプルSDは1.0から1.5の間であった。オリジナルのデータセットでは、スコアの範囲はテストによってかなり異なっており、スコアが0から20の範囲にある変数もあれば、スコアが50から300以上の範囲にある顕在変数もあった。8つの "情動 "変数は、Harman (1967, p 164)から引用され、Burt (1939)から引用された。  これらは、9歳から12歳の172人の健常児のものである。  Harmanが指摘したように、この相関行列は特異であり、2乗多重相関＞1である。  (たとえば、omegaは、fm="minres "またはfm="pa "に対して警告メッセージを出しますが、fm="ml "に対して失敗します。)8つの身体変数の問題は、Harman (1976)から取られたもので、305人の女の子の8つの身体変数の間の相関を表します。  2つの相関クラスターは、"lankiness "の4つの尺度と "stockiness "の4つの尺度を表します。  元のデータは、Mullen (1939)の未発表の学位論文で報告された17の変数から選ばれたものである。変数6（"Bitrochanteric diamter"）は、お尻の外側の点間の距離である。  主軸のfa解（fm="pa"）は、fm="minres "と同様に、報告されているminres解と一致する。様々な身体測定を使った教育例に興味のある方は、gclusパッケージの身体データセットを参照してください。  Burtデータセットは、おそらく元の相関行列にタイプミスがあります。  Sorrow-Tendernessの相関を.87から.81に変更すると、相関が正定値になります。Jan DeLeeuwが指摘したように、Burtデータ集合は、Burtが1915年に報告したオリジナルの11変数から8変数のサブセットです。  この行列にも同じ問題があります。burtを参照。因子分析の有用なデモンストレーションとなる他のデータセットの例は、Bechtoldtの7つの2因子の例と、Harman74.corの24の能力測定です。psychパッケージ（つまり、Harman.8）、datasetaパッケージ、GPArotationパッケージにも、いくつかのHarmanの例があります。  Harman 24 mental tests問題はHarman74.corの基本データセットパッケージにある。他のHarmanデータセットとしては、John LoehlinがEFAの例として使った12の国勢調査区の5つの社会経済変数Harman.5がある。  多くのHarman (1967)データセットのもう1つは、Harman.politicalです。  これは、147の選挙区で取られた8つの政治変数を含みます。  SMCを共分散性とする主因子法は、表8.18のものと一致します。  このデータは，DziubianとShirkeyによって，因子妥当性のKaiser-Meyer-Olkin検定の例として使用されている．
 Harman.Holzinger：能力検査の9×9相関行列、N=696。Harman.Burt：「感情」項目の8×8の相関行列。N = 172 Harman.5：5つの社会経済的データについて12の国勢調査区（Harman p 14） Harman.political：p166。Harman.8 8つの身体的尺度Harman.Holzinger.Harman (1967, p 244)の9つの心理学的変数は、K.J.Holzingerの696人の参加者との未発表の授業ノートから取られたものである。  これは、4つの因子を持つ12のテストのサブセットです。これは、二因子解のもう1つの良い例である。  Bentler (2007)は、信頼性分析について議論するためにこのデータセットを使用しています。このデータは明確な2因子構造を示し、ω関数に含まれる信頼性のさまざまな推定値の良い例です。bifactorのHolzingerまたはHolzinger.9データセットと混同しないでください。holzinger.swinefordの26変数で301人の被験者からなるHolzinger-Swinefordデータセットも参照してください。これらのデータはKeith Widamanによって提供された。「Holzinger and Swineford (1939)のデータは、多くの研究者によってモデルデータセットとして使用されてきました。例えば、Harman (1976)は、多因子分析に関する彼の権威あるテキストの中で、"24の心理学的変数 "の例を顕著に使用し、このルーブリックの下で提示されたデータは、Grant-White学派（N = 145）の変数の24で構成されていた。Meredith (1964a, 1964b)は，Holzinger and Swinefordの研究からいくつかの変数を，選択下での要因不変性の研究で使用した．Joreskog (1971) は、Holzinger and Swinefordのデータを用いた多群確認的因子分析の彼の仕事に基づいて、データを4つのグループにサブセットしました。Rの "lavaan "パッケージを開発したRosseelは、lavaanパッケージをダウンロードすると、Holzinger and Swineford (1939)からの顕在変数の9つを "resident "データ集合として含みます。この "常駐 "データ集合には、9つの心理テスト（データ集合ではx1～x9と命名されている）に加えて、いくつかの背景変数が含まれています。これらのデータを分析したところ、変数の分布（平均、SD）が元の論文の標本統計と一致しないことがわかりました。例えば、lavaanの "resident "データセットでは、すべての顕在変数のスコアは0から10の間、サンプル平均は3から6の間、サンプルSDは1.0から1.5の間であった。オリジナルのデータセットでは、スコアの範囲はテストによってかなり異なっており、スコアが0から20の範囲にある変数もあれば、スコアが50から300以上の範囲にある顕在変数もあった。8つの "情動 "変数は、Harman (1967, p 164)から引用され、Burt (1939)から引用された。  これらは、9歳から12歳の172人の健常児のものである。  Harmanが指摘したように、この相関行列は特異であり、2乗多重相関＞1である。  (たとえば、omegaは、fm="minres "またはfm="pa "に対して警告メッセージを出しますが、fm="ml "に対して失敗します。)8つの身体変数の問題は、Harman (1976)から取られたもので、305人の女の子の8つの身体変数の間の相関を表します。  2つの相関クラスターは、"lankiness "の4つの尺度と "stockiness "の4つの尺度を表します。  元のデータは、Mullen (1939)の未発表の学位論文で報告された17の変数から選ばれたものである。変数6（"Bitrochanteric diamter"）は、お尻の外側の点間の距離である。  主軸のfa解（fm="pa"）は、fm="minres "と同様に、報告されているminres解と一致する。様々な身体測定を使った教育例に興味のある方は、gclusパッケージの身体データセットを参照してください。  Burtデータセットは、おそらく元の相関行列にタイプミスがあります。  Sorrow-Tendernessの相関を.87から.81に変更すると、相関が正定値になります。Jan DeLeeuwが指摘したように、Burtデータ集合は、Burtが1915年に報告したオリジナルの11変数から8変数のサブセットです。  この行列にも同じ問題があります。burtを参照。因子分析の有用なデモンストレーションとなる他のデータセットの例は、Bechtoldtの7つの2因子の例と、Harman74.corの24の能力測定です。psychパッケージ（つまり、Harman.8）、datasetaパッケージ、GPArotationパッケージにも、いくつかのHarmanの例があります。  Harman 24 mental tests問題はHarman74.corの基本データセットパッケージにある。他のHarmanデータセットとしては、John LoehlinがEFAの例として使った12の国勢調査区の5つの社会経済変数Harman.5がある。  多くのHarman (1967)データセットのもう1つは、Harman.politicalです。  これは、147の選挙区で取られた8つの政治変数を含みます。  SMCを共分散性とする主因子法は、表8.18のものと一致します。  このデータは，DziubianとShirkeyによって，因子妥当性のKaiser-Meyer-Olkin検定の例として使用されている．
 Harman.Holzinger：能力検査の9×9相関行列、N=696。Harman.Burt：「感情」項目の8×8の相関行列。N = 172 Harman.5：5つの社会経済的データについて12の国勢調査区（Harman p 14） Harman.political：p166。Harman.8 8つの身体的尺度Harman.Holzinger.Harman (1967, p 244)の9つの心理学的変数は、K.J.Holzingerの696人の参加者との未発表の授業ノートから取られたものである。  これは、4つの因子を持つ12のテストのサブセットです。これは、二因子解のもう1つの良い例である。  Bentler (2007)は、信頼性分析について議論するためにこのデータセットを使用しています。このデータは明確な2因子構造を示し、ω関数に含まれる信頼性のさまざまな推定値のいい例です。bifactorのHolzingerまたはHolzinger.9データセットと混同しないでください。holzinger.swinefordの26変数で301人の被験者からなるHolzinger-Swinefordデータセットも参照してください。これらのデータはKeith Widamanによって提供された。「Holzinger and Swineford (1939)のデータは、多くの研究者によってモデルデータセットとして使用されてきました。例えば、Harman (1976)は、多因子分析に関する彼の権威あるテキストの中で、"24の心理学的変数 "の例を顕著に使用し、このルーブリックの下で提示されたデータは、Grant-White学派（N = 145）の変数の24で構成されていた。Meredith (1964a, 1964b)は，Holzinger and Swinefordの研究からいくつかの変数を，選択下での要因不変性の研究で使用した．Joreskog (1971) は、Holzinger and Swinefordのデータを用いた多群確認的因子分析の彼の仕事に基づいて、データを4つのグループにサブセットしました。Rの "lavaan "パッケージを開発したRosseelは、lavaanパッケージをダウンロードすると、Holzinger and Swineford (1939)からの顕在変数の9つを "resident "データ集合として含みます。この "常駐 "データ集合には、9つの心理テスト（データ集合ではx1～x9と命名されている）に加えて、いくつかの背景変数が含まれています。これらのデータを分析したところ、変数の分布（平均、SD）が元の論文の標本統計と一致しないことがわかりました。例えば、lavaanの "resident "データセットでは、すべての顕在変数のスコアは0から10の間、サンプル平均は3から6の間、サンプルSDは1.0から1.5の間であった。オリジナルのデータセットでは、スコアの範囲はテストによってかなり異なっており、スコアが0から20の範囲にある変数もあれば、スコアが50から300以上の範囲にある顕在変数もあった。8つの "情動 "変数は、Harman (1967, p 164)から引用され、Burt (1939)から引用された。  これらは、9歳から12歳の172人の健常児のものである。  Harmanが指摘したように、この相関行列は特異であり、2乗多重相関＞1である。  (たとえば、omegaは、fm="minres "またはfm="pa "に対して警告メッセージを出しますが、fm="ml "に対して失敗します。)8つの身体変数の問題は、Harman (1976)から取られたもので、305人の女の子の8つの身体変数の間の相関を表します。  2つの相関クラスターは、"lankiness "の4つの尺度と "stockiness "の4つの尺度を表します。  元のデータは、Mullen (1939)の未発表の学位論文で報告された17の変数から選ばれたものである。変数6（"Bitrochanteric diamter"）は、お尻の外側の点間の距離である。  主軸のfa解（fm="pa"）は、fm="minres "と同様に、報告されているminres解と一致する。様々な身体測定を使った教育例に興味のある方は、gclusパッケージの身体データセットを参照してください。  Burtデータセットは、おそらく元の相関行列にタイプミスがあります。  Sorrow-Tendernessの相関を.87から.81に変更すると、相関が正定値になります。Jan DeLeeuwが指摘したように、Burtデータ集合は、Burtが1915年に報告したオリジナルの11変数から8変数のサブセットです。  この行列にも同じ問題があります。burtを参照。因子分析の有用なデモンストレーションとなる他のデータセットの例は、Bechtoldtの7つの2因子の例と、Harman74.corの24の能力測定です。psychパッケージ（つまり、Harman.8）、datasetaパッケージ、GPArotationパッケージにも、いくつかのHarmanの例があります。  Harman 24 mental tests問題はHarman74.corの基本データセットパッケージにある。他のHarmanデータセットとしては、John LoehlinがEFAの例として使った12の国勢調査区の5つの社会経済変数Harman.5がある。  多くのHarman (1967)データセットのもう1つは、Harman.politicalです。  これは、147の選挙区で取られた8つの政治変数を含みます。  SMCを共分散性とする主因子法は、表8.18のものと一致します。  このデータは、DziubianとShirkeyによって、Kaiser-Meyer-Olkinの因子妥当性検定の例として使用されています。
関数について教えるための例として含まれている。また、statsByで調和平均による重み付けに使用されている。0を含むことも（この場合、調和平均=0）、ゼロオプションに従ってNAに変換することもできる。  ゼロオプションを追加、2017年3月。
NA
NA
パス図の表現は、確証的因子分析では標準的になっているが、探索的因子分析ではまだ一般的ではない。  因子構造をグラフィカルに表現することで，構造を理解しやすくなる人もいる．デフォルトでは，矢印は潜在変数から観察変数に来る．  これはもちろん因子モデルです。  しかし、描画されるオブジェクトのクラスが'principal'であれば、矢印の方向を逆にすると、'潜在'変数はもはや潜在ではなく、ボックスとして表示されます。クラスター・モデルの場合、デフォルトでは因子として扱われますが、ic =TRUEの場合、成分モデルとして扱われます。fa.diagramはRgraphvizを使用せず、好ましい関数です。fa.graphは、Rgraphvizを使用せず、この機能を優先します。fa.graphは、外部グラフィックス・プログラムで使用するドット・コードを生成します。gパラメータにTRUEを指定することで、階層（2因子）モデルを描画することができます。  これにより、二因子構造を持つさまざまな因子変換をグラフィカルに表示することができます（例えば、bifactorやbiquartimin。  これらの構造を見つける別の方法についてはomegaを参照してください。  het.diagram関数は、複数レベルのヘテラル構造のケースを表示します。  また、尺度セット（EPI、NEO、BFIなど）間の相関のパターンを示すのにも使用できます。  この事例は、Thurstoneデータ集合からの3セットの4変数の間の関係を示すものです。extension.diagramは、fa.diagramやより汎用的なdiagram関数を使用するよりも若干コントロールしやすいfa.extendの結果を描画します。fa.rgraphでは、直交因子の場合はきれいなグラフが描画されますが、斜交因子の描画は許容範囲ですが、Rの外部でクリーンアップするか、fa.diagramを使用した方がよいでしょう。通常の入力はfaまたはICLUSTの出力から取られる。この後者のケースは、ICLUSTの結果をクラスタ構造ではなく、クラスタ負荷量の観点から表示する。  因子負荷行列を入力として与えることも可能である。  この場合、因子相関の Phi 行列を与えることも可能である。fa.graph は schmid$sl オブジェクトに適用し、ラベルは schmid$sl の rownames で指定する。  結果はドット言語でのプロットと完全に互換性を持たせるために編集が必要です。  結果の構造方程式確証分析のモデルを指定するには、代わりにstructure.diagramを使う。
これにより、複数の分布を素早く要約することができます。  特に、信頼性関数から得られる複数に分割された半分の結果を調べるときに便利です。  デフォルトでは、行と列の数が等しい正方形のプロットを作成しようとします。  しかし、特定のプロットでは、列と行の数を指定することができる。
Holzinger and Swineford (1937) は、精神能力について2因子モデル（1つの一般因子といくつかのグループ因子）を導入した。  これは、オメガ関数またはsemを用いて分析できる階層的因子構造のすばらしい実証データセットである。このようなデータを分析するには、いくつかの方法があります。  1つは、オメガ関数を使用して、Schmid-Leiman変換を使用して階層因数分解を行うことである。これは、探索的モデルとして、そしてomegaSemを用いた確認的モデルとして行うことができます。もう1つの方法は、通常の因子分析を行い、bifactor 回転またはbiquartimin 回転を使用することです。後者の2つの関数は、Jennrich and Bentler (2011) のバイファクター変換とバイクォーティミン変換を実装しています。  バイファクター回転は，局所極小の問題（Mansolf and Reise, 2016）に悩まされるので，探索的分析と確認的分析の混合が好ましいかもしれない．14の変数は、3つの空間テスト、3つのメンタル・スピード・テスト、4つの運動スピード・テスト、4つの言語テストを反映するように順序付けされている。  Holzingerからのもう1つのデータ集合（Holzinger.9）は、9つの認知能力（Holzinger, 1939）を表し、Karl Joreskog (2003)がMINRESアルゴリズムによる因子分析の例として使用し、LISRELのマニュアルにも例NPV.KMとして掲載されています。  このデータ集合は、Grant White中学校の9つのテストの得点です：  「t01_visperc""t02_cubes""t04_lozenges""t06_paracomp""t07_sentcomp""t09_wordmean""t10_addition""t12_countdot "および "t13_sccaps "で、lavaanパッケージの変数x1 ... x9（Grant-Whiteの学校）として使用されます。もう1つの古典的なデータ集合は、R. P. McDonald (1985, 1999)によって詳細に議論され、SASのPROC CALIS マニュアルと同様に sem パッケージで例として使用されている9変数のThurstone問題です。  これらの9つのテストは、ThurstoneとThurstone, 1941によって（他のデータに基づいて）3つの要因にグループ化された：Verbal Comprehension、Word Fluency、Reasoningである。元のデータはThurstone and Thurstone (1941)によるものだが、Bechthold (1961)によって再分析され、彼はデータセットを2つに分けた。サンプルサイズは213である。Thurstone (1933)に起因する9つの認知変数のもう1つのセットは、プリンストンのBrigham教授が大学入試委員会に報告した4,175人の学生のデータセットである。  Tucker (1958) は、Turstone and Thburstone (1941)の9変数を、彼のバッテリー間因子分析の例に使用しています。  二因子モデルの最近の応用は、心理的状態の測定である。Reiseのデータセットは、Consumer Assessment of Health Care Provider and Systems調査票に対する35,000以上の観察に基づく相関行列である。Reise, Morizot, and Hays (2007)は、1,000症例に基づく二因子解を記述している。    Reiseらによる5つの因子は、「迅速に治療を受ける」（1-3）、「医師がよくコミュニケーションをとる」（4-7）、「丁寧で親切なスタッフ」（8,9）、「必要な治療を受ける」（10-13）、「医療プランの顧客サービス」（14-16）である。2つのBechtoldtデータセットは、Thurstone and Thurstone (1941)からの2つのサンプルである。  これらは、17の変数を含み、そのうちの9つは、Thurstoneデータ集合を形成するためにMcDonaldによって使用されました。サンプルサイズはそれぞれ212と213である。提案された6つの因子は、記憶、言語、単語、空間、数、推論を反映し、暗記記憶因子を期待するすべての3つのマーカーがある。このセットから9変数がThurstoneデータセットに現れる。同様の構造を持つデータセットがHarmanデータセットにさらに2つある。  これには、Harman link{Harman.Holzinger}が使用したHolzingerの別の9変数（被験者696人）と、link{burt}の8感情変数が含まれます。二因子構造の検定のために調べる価値があるもう1つのデータ集合は、Keith Widamanによって提供されたHolzinger and Swineford (1939)のオリジナル・データを含むholzinger.swinefordデータ集合です。  これはpsychToolsパッケージに入っている。Bechtoldt.1: 能力検査の17 x 17相関行列、N = 212.Bechtoldt.2：能力検査の17 x 17相関行列，N = 213．Holzinger：能力検査の14 x 14相関行列，N = 355 Holzinger.9：能力検査の9 x 9相関行列，N = 145 Reise：健康満足度項目の16 x 16相関行列．  N = 35,000 Thurstone: 能力検査の9 x 9相関行列、N = 213 Thurstone.33: 能力検査の別の9 x 9相関行列、N = 4175 Thurstone:9：さらに別の9×9の能力テストの相関行列、N=710
HolzingerとSwineford（1937）は、精神能力について二因子モデル（1つの一般因子といくつかのグループ因子）を導入した。  これは、オメガ関数やsemを用いて分析できる階層的な因子構造のすばらしい実証データセットである。このようなデータを分析するには、いくつかの方法があります。  1つは、オメガ関数を使用して、Schmid-Leiman変換を使用して階層因数分解を行うことである。これは、探索的モデルとして、そしてomegaSemを用いた確認的モデルとして行うことができます。もう1つの方法は、通常の因子分析を行い、bifactor 回転またはbiquartimin 回転を使用することです。後者の2つの関数は、Jennrich and Bentler (2011) のバイファクター変換とバイクォーティミン変換を実装しています。  バイファクター回転は，局所最小の問題（Mansolf and Reise, 2016）に悩まされるので，探索的分析と確認的分析の混合が望ましいかもしれない．14の変数は、3つの空間テスト、3つのメンタル・スピード・テスト、4つの運動スピード・テスト、4つの言語テストを反映するように順序付けされている。  Holzingerからのもう1つのデータ集合（Holzinger.9）は、9つの認知能力（Holzinger, 1939）を表し、Karl Joreskog (2003)がMINRESアルゴリズムによる因子分析の例として使用し、LISRELのマニュアルにも例NPV.KMとして掲載されています。  このデータ集合は、Grant White中学校の9つのテストの得点です：  「t01_visperc""t02_cubes""t04_lozenges""t06_paracomp""t07_sentcomp""t09_wordmean""t10_addition""t12_countdot "および "t13_sccaps "で、lavaanパッケージの変数x1 ... x9（Grant-Whiteの学校）として使用されます。もう1つの古典的なデータ集合は、R. P. McDonald (1985, 1999)によって詳細に議論され、SASのPROC CALIS マニュアルと同様に sem パッケージで例として使用されている9変数のThurstone問題です。  これらの9つのテストは、ThurstoneとThurstone, 1941によって（他のデータに基づいて）3つの要因にグループ化された：Verbal Comprehension、Word Fluency、Reasoningである。元のデータはThurstone and Thurstone (1941)によるものだが、Bechthold (1961)によって再分析され、彼はデータセットを2つに分けた。サンプルサイズは213である。Thurstone (1933)に起因する9つの認知変数のもう1つのセットは、プリンストンのBrigham教授が大学入試委員会に報告した4,175人の学生のデータセットである。  Tucker (1958) は、Turstone and Thburstone (1941)の9変数を、彼のバッテリー間因子分析の例に使用しています。  二因子モデルの最近の応用は、心理的状態の測定である。Reiseのデータセットは、Consumer Assessment of Health Care Provider and Systems調査票に対する35,000を超える観察に基づく相関行列である。Reise, Morizot, and Hays (2007)は、1,000症例に基づく二因子解を記述している。    Reiseらによる5つの因子は、「迅速に治療を受ける」（1-3）、「医師がよくコミュニケーションをとる」（4-7）、「丁寧で親切なスタッフ」（8,9）、「必要な治療を受ける」（10-13）、「医療プランの顧客サービス」（14-16）である。2つのBechtoldtデータセットは、Thurstone and Thurstone (1941)からの2つのサンプルである。  これらは、17の変数を含み、そのうちの9つは、Thurstoneデータ集合を形成するためにMcDonaldによって使用されました。サンプルサイズはそれぞれ212と213である。提案された6つの因子は、記憶、言語、単語、空間、数、推論を反映し、暗記記憶因子を期待するすべての3つのマーカーがある。このセットから9変数がThurstoneデータセットに現れる。同様の構造を持つデータセットがHarmanデータセットにさらに2つある。  これには、Harman link{Harman.Holzinger}が使用したHolzingerの別の9変数（被験者696人）と、link{burt}の8感情変数が含まれます。二因子構造の検定のために調べる価値があるもう1つのデータ集合は、Keith Widamanによって提供されたHolzinger and Swineford (1939)のオリジナル・データを含むholzinger.swinefordデータ集合です。  これはpsychToolsパッケージに入っている。Bechtoldt.1: 能力検査の17 x 17相関行列、N = 212.Bechtoldt.2：能力検査の17 x 17相関行列，N = 213．Holzinger：能力検査の14 x 14相関行列，N = 355 Holzinger.9：能力検査の9 x 9相関行列，N = 145 Reise：健康満足度項目の16 x 16相関行列．  N = 35,000 Thurstone: 能力検査の9 x 9相関行列、N = 213 Thurstone.33: 能力検査の別の9 x 9相関行列、N = 4175 Thurstone:9：さらに別の9×9の能力テストの相関行列、N=710
Shrout and Fleiss (1979) は、k人の評価者がn個の対象について行った評価の信頼性について、6つのケースを検討している。McGraw and Wong (1996) は，10個のケースを検討しており，そのうちの6個はShrout and Fleissと同じで，4個は概念的に異なるが，Shrout and Fleissの6個と同じ式を使用している．クラス内相関は，評価者がすべて同じ "クラス "である場合に使用される．  つまり，それらを区別する論理的な方法がない．  たとえば，双子のペア間の相関や評価者間の相関などである．  変数が論理的に区別できる場合（たとえば、テスト上の異なる項目）、より典型的な係数は、クラス間相関（たとえば、Pearson r）に基づき、アルファまたはオメガのような統計量が使用されるかもしれない。アルファとICC3kは同一である。データが行（被験者）と列（評価者またはテスト）でレイアウトされている場合、さまざまなICCは、分散成分のさまざまな推定値の比によって見つけられる。  すべてのケースで、被験者はランダムに変化し、残差分散もランダムである。  モデル2と3の違いは、判定者（項目/テスト）をランダムと見るか、固定と見るかです。  さらに、判定者の絶対的一致を重視するか、単に一貫性を重視するかの違いもある。  Liljequist et al. (2019)で議論されているように、McGrawとWongは、ICC式の3つの形式だけを使用する5つのモデルを示している。モデル1は、ICC1による1ウェイモデルである：各ターゲットは、異なるジャッジによって評価され、ジャッジはランダムに選択される。  ICC(1,1) = ρ_{1,1} = \frac{σ^2_r}{σ^2_r + σ^2_w} (これは1元配置ANOVA固定効果モデルで， (MSB- MSW)/(MSB+ (nr-1)*MSW) によって求められる)   ICC2：k 個の審査員の無作為標本が各ターゲットを評価する．  この尺度は、評価の絶対的一致の1つである。  ICC(2,1) = ρ_{2,1} = \frac{σ^2_r}{σ^2_r + σ^2_c +σ^2_{rc｝  +σ^2_e}(MSB-MSE)/(MSB+(nr-1)*MSE+nr*(MSJ-MSE)/nc)として求められる ICC3：固定されたk人の判定員セットが各ターゲットを評価する。より多くの判定者集団への一般化はない。ICC(3,1) = ρ_{3,1} = ￢frac{σ^2_r}{σ^2_r + σ^2_c + σ^2_e}(MSB-MSE)/(MSB+(nr-1)*MSE)それでは、これらの場合それぞれについて、信頼性は1つの評価について推定するのか、k個の評価の平均について推定するのか。  (ICC1は評価者間の平均値の違いに敏感で、絶対的一致度の尺度である。ICC2とICC3は審査員間の平均値の違いは除去するが、審査員による評価者の相互作用に敏感である。  ICC2とICC3の違いは、評価者を固定効果と見るかランダム効果と見るかである。ICC1k, ICC2k, ICC3Kは、k人の評価者の平均を反映する。   lmer オプションを使用する場合は，欠損データが許される．  さらに、lme オブジェクトは分散分解を返す。  (これは、2つの機会からの項目で動作するtestRetestに似ています。) check.keysオプションは、デフォルトでは、総得点と負の相関を持つ項目を反転させます。  メッセージが発行される。
 アルゴリズムの広範な文書と正当性は、オリジナルの MBR 1979 https://personality-project.org/revelle/publications/iclust.pdf の論文にある。  アルゴリズムのさらなる議論とサンプル出力は、personality-project.orgのウェブページで入手可能である: https://personality-project.org/r/r.ICLUST.html 結果はICLUST.graphを使うのが最もよく可視化される。その結果はGraphvizプログラム用のドットファイルとして保存できる。https://www.graphviz.org/.iclust.diagramはクラスター図を作成するために自動的に呼び出される。  出来上がった図は、ドット・コードでできるほどきれいではないが、外部のグラフィック・プログラムを使いたくない場合には十分である。Rgraphvizをインストールすることで、ICLUSTはクラスター・グラフも提供できる。社会科学における一般的な問題は、理論的な関心や実際的な重要性を持つ構成要素を測定するための尺度や複合項目を構築することである。このプロセスでは、多くの場合、項目群を実施し、その中から特定の基準を満たすものを選択する。これらの基準は、合理的、経験的、あるいは要因的なものである。同様の問題は、すでに形成された尺度の妥当性を分析し、想定される構成要素が適切に測定されているかどうかを判断することである。この2つの問題は、数多くのテキストや無数の論文で議論されてきた。さまざまな手法の支持者は、表層妥当性、判別妥当性、構成概念妥当性、要因同質性、理論的重要性の重要性を主張してきた。Revelle (1979) は、階層クラスター分析を用いて、テストの一般的な因子飽和度を推定する新しい係数（ベータ）を推定することを提案した。  より最近では、Zinbarg, Revelle, Yovel and Li (2005) が、McDonaldのオメガとChronbachのアルファおよびRevelleのベータを比較した。彼らは，オメガ階層が最良の推定であると結論付けている．  オメガを推定するアルゴリズムは、このパッケージの一部として利用可能である。Revelle and Zinbarg (2009) は、アルファ、ベータ、オメガ、および他の信頼性の推定について論じている。オリジナルの ICLUST プログラムは CDC と IBM メインフレームで動作するように FORTRAN で書かれ、その後 PC-DOS で動作するように修正された。  iclustのRバージョンは、psychパッケージ用に書かれた全く新しいバージョンである。  このバージョンのiclustのヘルプが必要な場合、あるいはより多くの機能を希望する場合は、私に電子メールを送ってください。リクエストされた機能（まだ利用できません）は、特定の項目をクラスターを形成するものとして指定することです。  つまり、確認的クラスター分析を行うことです。  このプログラムには現在3つの主要な機能がある：クラスター、負荷量、グラフィックス。  2009年6月に、重み付きベータと重みなしベータというオプションが導入されました。  重みなしベータは、2つのクラスター間の相関に基づいてベータを計算し、Spearman-Brown予言公式を用いてテスト長を補正します。一方、重み付きベータは、2つのクラスター内の項目間の平均項目間相関を求め、それからベータを求めます。  つまり、平均相関 rbを持つサイズ N と M の2つのクラスター A と B の場合、重み付きベータは (N+M)^2 rb/(Va +Vb + 2Cab) となる。  未加重ベータは2rab/(1+rab)で、rab = Cab/sqrt(VaVb)である。   加重ベータはより適切な推定値であり、現在ではデフォルトとなっている。  また、2009年6月に、グラフィック出力のクラスタ-サブクラスタ相関を計算する際に、項目の重なりを補正する方法が修正されました。  これは最終的なクラスター解には影響しませんが、若干異なるパス値を生成します。  さらに、クラスタ-サブクラスタ相関を解く2つの方法があります。2つのクラスタ間の共分散Cab（平均rab = Cab/(N*M)）、クラスタ分散VaとVb（Va = N + N*(N-1)*ra） を考えると、クラスタAと結合クラスタABの相関は、以下のようになります。 orb) ((N^2)ra + Cab)/sqrt(Vab*Va) （オプション cor.gen=TRUE)または orb) (Va - N + Nra + Cab)/sqrt(Vab*Va) (option cor.gen=FALSE)デフォルトでは cor.gen=TRUE が使用されます。iclust は抽出するクラスタの数に関して最良と思われる解を出しますが、ユーザが同意しない場合もあります。  デフォルトの解よりも多くのクラスタを得るには、nclustersパラメータを希望する数に設定するだけです。  しかし、 alpha と beta の基準を満たすより少ないクラスタを得るには、 alpha=0 と beta=0 を設定し、それから nclusters を希望の数に設定することが必要なことがある。   精神能力の24のテストのクラスタリングHarmanによる24変数問題を用いたサンプル出力は、グラフィカルに表現することも、クラスタ順序で表現することもできる。デフォルトでは、ダイアグラム関数を用いてグラフィックを作成します。  もう1つの方法は、Rgraphvizパッケージ（BioConductorから）を使用することです。  このパッケージはインストールが困難な場合があるため、代替オプション（ICLUST.graph）があり、後続の処理のためにドット言語命令を記述する。  これはドット言語を使用する任意の表示プログラムに適したグラフィック命令を作成する。  ICLUST.rgraphは、Graphviz用のドットコードを作成します。 Rgraphvizを必要とするICLUST.rgraph関数では、より少ないオプションで、より低解像度のグラフが利用できます。 ドットコードは、Graphvizで直接見ることもできますし、市販のソフトウェアパッケージ（OmniGraffleなど）を使って微調整することもできます。(これは、VSS(Very Simple Structure)の出力とも一致しており、複雑度1のデータに対して明確な1因子解を示しています)。  もう1つの解決策は、もう少し厳しい基準のセットを要求し、3変数以上のすべてのクラスターについてベータ値の大きさの増加を要求することです。  これは4クラスター解を生成します。また、オリジナルのパラメータ設定を使用しながら、4クラスター解を求めることも可能です。少なくともHarman 24の精神能力測定については、クラスター・パターン行列と因子分析からの斜め回転解を比較することは興味深いです。  4因子斜交パターン解と4クラスター解の因子一致度は、4クラスターのうち3クラスターで.99以上、4クラスターで.97以上である。  クラスター・パターン行列（出力では見えないオブジェクトとして返されます）2012年9月、適合統計量（パターン適合とクラスター適合）は、（デフォルトで）対角を考慮しない（diagonal=FALSE）ように少し修正されました。  それまでは、対角はクラスター適合統計量に含まれていました。  パターン適合は因子分析に類似しており、モデル = P x 構造に基づいています。  そして、R* = R - modelで、適合度は、対角から外れた要素のsum(r*^2)/sum(r^2)の比です。  
 アルゴリズムの広範なドキュメントと正当性は、オリジナルのMBR 1979 https://personality-project.org/revelle/publications/iclust.pdf の論文で利用可能である。  アルゴリズムのさらなる議論とサンプル出力は、personality-project.orgのウェブページで入手できる： https://personality-project.org/r/r.ICLUST.html 結果はICLUST.graphを使うのが最もよく可視化される。その結果はGraphvizプログラム用のドットファイルとして保存できる。https://www.graphviz.org/.iclust.diagramはクラスター図を作成するために自動的に呼び出される。  出来上がった図は、ドット・コードでできるほどきれいではないが、外部のグラフィック・プログラムを使いたくない場合には十分である。Rgraphvizをインストールすることで、ICLUSTはクラスター・グラフも提供できる。社会科学における一般的な問題は、理論的な関心や実際的な重要性を持つ構成要素を測定するための尺度や複合項目を構築することである。このプロセスでは、多くの場合、項目群を実施し、その中から特定の基準を満たすものを選択する。これらの基準は、合理的、経験的、あるいは要因的なものである。同様の問題は、すでに形成された尺度の妥当性を分析し、想定される構成要素が適切に測定されているかどうかを判断することである。この2つの問題は、数多くのテキストや無数の論文で議論されてきた。さまざまな手法の支持者は、表層妥当性、判別妥当性、構成概念妥当性、要因同質性、理論的重要性の重要性を主張してきた。Revelle (1979) は、階層クラスター分析を用いて、テストの一般的な因子飽和度を推定する新しい係数（ベータ）を推定することを提案した。  より最近では、Zinbarg, Revelle, Yovel and Li (2005) が、McDonaldのオメガとChronbachのアルファおよびRevelleのベータを比較した。彼らは，オメガ階層が最良の推定であると結論づけている．  オメガを推定するアルゴリズムは、このパッケージの一部として利用可能である。Revelle and Zinbarg (2009) は、アルファ、ベータ、オメガ、および他の信頼性の推定について論じている。オリジナルの ICLUST プログラムは CDC と IBM メインフレームで動作するように FORTRAN で書かれ、その後 PC-DOS で動作するように修正された。  iclustのRバージョンは、psychパッケージ用に書かれた全く新しいバージョンである。  このバージョンのiclustのヘルプが必要な場合、あるいはより多くの機能を希望する場合は、私に電子メールを送ってください。リクエストされた機能（まだ利用できません）は、特定の項目をクラスターを形成するものとして指定することです。  つまり、確認的クラスター分析を行うことです。  このプログラムには現在3つの主な機能がある：クラスター、負荷量、グラフィックス。  2009年6月に、重み付きベータと重みなしベータというオプションが導入されました。  重みなしベータは、2つのクラスター間の相関に基づいてベータを計算し、Spearman-Brown予言公式を用いてテスト長を補正します。一方、重み付きベータは、2つのクラスター内の項目間の平均項目間相関を求め、それからベータを求めます。  つまり、平均相関 rbを持つサイズ N と M の2つのクラスター A と B の場合、重み付きベータは (N+M)^2 rb/(Va +Vb + 2Cab) となる。  未加重ベータは2rab/(1+rab)で、rab = Cab/sqrt(VaVb)である。   加重ベータはより適切な推定値であり、現在ではデフォルトとなっている。  また、2009年6月に、グラフィック出力のクラスタ-サブクラスタ相関を計算する際に、項目の重なりを補正する方法が修正されました。  これは最終的なクラスター解には影響しませんが、若干異なるパス値を生成します。  さらに、クラスタ-サブクラスタ相関を解く2つの方法があります。2つのクラスタ間の共分散Cab（平均rab = Cab/(N*M)）、クラスタ分散VaとVb（Va = N + N*(N-1)*ra） を考えると、クラスタAと結合クラスタABの相関は、以下のようになります。 orb) ((N^2)ra + Cab)/sqrt(Vab*Va) (option cor.gen=TRUE) orb) (Va - N + Nra + Cab)/sqrt(Vab*Va) (option cor.gen=FALSE)デフォルトでは cor.gen=TRUE が使用されます。iclust は抽出するクラスタ数に関して最良と思われる解を与えますが、ユーザが同意しない場合もあります。  デフォルトの解よりも多くのクラスタを得るには、nclustersパラメータを希望する数に設定するだけです。  しかし、 alpha と beta の基準を満たすより少ないクラスタを得るには、 alpha=0 と beta=0 を設定し、それから nclusters を希望の数に設定することが必要なことがある。   精神能力の24のテストのクラスタリングHarmanによる24変数問題を用いたサンプル出力は、グラフィカルに表現することも、クラスタ順序で表現することもできる。デフォルトでは、ダイアグラム関数を用いてグラフィックを作成します。  もう1つの方法は、Rgraphvizパッケージ（BioConductorから）を使用することです。  このパッケージはインストールが困難な場合があるため、代替オプション（ICLUST.graph）があり、後続の処理のためにドット言語命令を記述する。  これはドット言語を使用する任意の表示プログラムに適したグラフィック命令を作成する。  ICLUST.rgraphは、Graphviz用のドットコードを作成します。 Rgraphvizを必要とするICLUST.rgraph関数では、より少ないオプションで、より低解像度のグラフが利用できます。 ドットコードは、Graphvizで直接見ることもできますし、市販のソフトウェアパッケージ（OmniGraffleなど）を使って微調整することもできます。(これは、VSS(Very Simple Structure)の出力とも一致しており、複雑度1のデータに対して明確な1因子解を示しています)。  もう1つの解決策は、もう少し厳しい基準のセットを要求し、3変数以上のすべてのクラスターについてベータ値の大きさの増加を要求することです。  これは4クラスター解を生み出します。また、オリジナルのパラメータ設定を使用しながら、4クラスター解を求めることも可能です。少なくともHarman 24の精神能力測定については、クラスター・パターン行列と因子分析からの斜め回転解を比較することは興味深いです。  4因子斜交パターン解と4クラスター解の因子一致度は、4クラスターのうち3クラスターで.99以上、4クラスターで.97以上である。  クラスター・パターン行列（出力では見えないオブジェクトとして返されます）2012年9月、適合統計量（パターン適合とクラスター適合）は、（デフォルトで）対角を考慮しない（diagonal=FALSE）ように少し修正されました。  それまでは、対角はクラスター適合統計量に含まれていました。  パターン適合は因子分析に類似しており、モデル = P x 構造に基づいています。  そして、R* = R - modelで、適合度は対角から外れた要素のsum(r*^2)/sum(r^2)の比です。  
ICLUSTを参照
iclust.diagramは、Rgraphvizをインストールすることなく、ICLUST.rgraphのほとんどの機能を提供します。 ICLUSTから自動的に呼び出されます。Michael Kubovyの要望により、cluster.namesを指定することで、通常のC1 ...Cn名。  ドット言語グラフィックプログラムへのアクセスが可能であれば、オフライン編集用のドット出力を得るためにiclust.graph関数を使用する方が良いだろう。
iclust.graph関数は、出力ファイルを作成(または上書き)し、クラスター構造を示すドットコードを出力します。このドットファイルは、ドットビューア(例えば、https://www.graphviz.org/)に直接インポートすることができる。  dot "言語は強力なグラフィック記述言語であり、特にクラスター出力の表示に適している。  市販のグラフィックプログラム（OmniGraffleなど）もドットファイルを読み込む（そしてクリーンアップする）ことができる。  ICLUST.graphはICLUSTの結果からの出力を受け取り、それを処理して結果のきれいな画像を提供する。  元の変数は長方形で表示され、グラフの左側（順位方向が RL の場合）に並べられる。  クラスターは楕円で描かれ、クラスターのアルファ、ベータ、サイズを含む。  すべてのクラスタ情報を表示しないように出力をトリミングすることも可能です。min.size未満のクラスターは、アルファ、ベータ、サイズの情報を含まない小さな楕円として表示されます。ドットコードを直接Rで処理できれば良いのですが、Rgraphvizパッケージはすべてのプラットフォームで使用することが難しいため、ドットコードを直接記述しています。
出力ファイルを作成（または上書き）し、クラスター構造を示すドットコードを出力します。このドットファイルは、ドットビューア（例えば、https://www.graphviz.org/）に直接インポートすることができる。  dot "言語は強力なグラフィック記述言語であり、特にクラスター出力の表示に適している。  市販のグラフィックプログラム（OmniGraffleなど）もドットファイルを読み込む（クリーンアップする）ことができる。  ICLUST.rgraphは、ICLUSTの結果からの出力を受け取り、それを処理して結果のきれいな画像を提供する。  元の変数は長方形で表示され、グラフの左側（順位方向が RL の場合）に並べられる。  クラスターは楕円で描かれ、クラスターのアルファ、ベータ、サイズを含む。  すべてのクラスタ情報を表示しないように出力をトリミングすることも可能です。min.size 未満のクラスターは、アルファ、ベータ、サイズの情報を含まない小さな楕円として表示されます。
クラスターまたは因子分析の出力を解釈するとき、どの項目が各因子/クラスターに最大の負荷を持っているかという観点で項目をグループ化し、次に絶対因子負荷のサイズによって項目を並べ替えると便利です。安定したクラスター解は、このように定義されたクラスターからクラスターが形成されるときに、これらのクラスター定義の出力が変化しないものです。keys=TRUEオプションを使用すると、結果のクラスター・キーは、因子からクラスターを形成するために、元のデータまたは相関行列をスコアリングするために使用できます。
クラスターまたは因子分析の出力を解釈するとき、どの項目が各因子/クラスターに最も負荷が大きいかという観点から項目をグループ化し、次に絶対的な因子負荷の大きさによって項目を並べ替えると便利です。安定したクラスター解は、このように定義されたクラスターからクラスターが形成されたときに、これらのクラスター定義の出力が変化しないものです。
faに実装されている因子分析は、変数の集合の共分散（相関）構造を、潜在変数または「因子」の小さな集合で要約しようとします。  この解は、元の解を変更することなく、より多くの変数を持つより大きな空間に「拡張」することができる（fa.extension.Factorを参照）。  同様に，2番目の変数集合（Y 集合）の因子は，元の（X ）集合に拡張することができる．  そして，潜在変数のこれらの2つの集合は，探索的構造方程式モデルのために相関させることができる．  (これは、sem、lavaan、Mxのような伝統的な構造方程式モデリング・パッケージを用いた確証的因子モデル（CFA）ではなく、探索的因子分析（EFA）に基づくので、探索的です）出力は、faを用いた通常のEFAと非常に似ているように見えますが、実際には、2つの独立した因子分析（XとYのセット）が相互に拡張されます。  つまり、XとYのセットからの負荷量と構造行列が単に組み合わされ、2つの因子セット間の相関が発見される。バッテリ間因子分析は、2つのテストのバッテリに共通する因子を比較する方法として、Tucker (1958) によって開発された。   (現在開発中で、まだ完成していない）。いくつかの簡単な線形代数を用いて，変数の2つのセットの間の相互相関の因子を見つけるのは簡単である．  これは共分散性の推定を必要とせず、正準相関の手順と高度に関連している。  esemと系列間アプローチの違いは，まずX集合を因子分析し，次にそれらの因子をY集合の因子に関係づけることである．  一方，系列間因子分析は，両方の集合を結びつける因子の1つの集合を見つけようとするが，両方の集合を一緒に因数分解することとは異なる．  
回答の合計数をN、中央値をM、中央値での回答数をNm >1、中央値未満の回答数をNbとすると、回答がカテゴリ内で一様に分布しているという仮定で、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  入力が行列またはデータフレームの場合，各変数について分位数が報告される．
回答の合計数がNで、中央値がM、中央値での回答数がNm >1、Nb= 中央値より小さい回答数である場合、回答がカテゴリ内で一様に分布していると仮定すると、補間された中央値は M - .5w + w*(N/2 - Nb)/Nm となります。第1、第2、第3四分位数および一般四分位数への一般化は簡単です。少し異なる一般化により、補間された点と補間されていない点の差をグラフィカルに表示することができます。  これは、interp.values関数を使用します。入力が行列またはデータフレームの場合、各変数について分位数が報告されます。
古典的テスト理論では、項目の難易度を無視し、能力を期待得点として定義します： abilityi = theta(i) = x(i.)ゼロ・パラメータ・モデルでは、これらの平均得点を0から1まで、-4から4までの準ロジスティック・スケールに再スケーリングしますこれは、ロジスティック・マッピングを反映するために、生データを非線形変換しているにすぎません。基本的な1パラメータ（Rasch）モデルは、項目の困難さ（デルタj）を考慮する：p（i番目の被験者に対する項目jの正誤｜θ i, deltaj）= 1/(1+exp(deltaj - thetai)）項目の困難さ（デルタ）の推定値があれば、最適化によってθ iを見つけることができる 2パラメータモデルは、項目の感度（ベータj）を追加する：p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj-θ i)))delta, beta, theta を推定して、モデルのデータへの適合を最大化する。ここで使われる手順は、まずtheta = 0と仮定して項目の難易度を求め、次にそれらのデルタからthetaを求め、次にデルタとthetaからベータを求める。irt.faとscore.irtを参照してください。
古典的テスト理論では、項目の難易度を無視し、能力を期待得点として定義します： abilityi = theta(i) = x(i.)ゼロパラメータモデルでは、0から1までのこれらの平均得点を、-4から4までの準ロジスティック尺度に再スケールしますこれは、ロジスティック写像を反映するために、生データを非線形変換しているにすぎません。基本的な1パラメータ（Rasch）モデルは、項目の困難さ（デルタj）を考慮する：p（i番目の被験者に対する項目jの正誤｜θ i, deltaj）= 1/(1+exp(deltaj - thetai)）項目の困難さ（デルタ）の推定値があれば、最適化によってθ iを見つけることができる 2パラメータモデルは、項目の感度（ベータj）を追加する：p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj-θ i)))delta, beta, theta を推定して、モデルのデータへの適合を最大化する。ここで使われる手順は、まずtheta = 0と仮定して項目の難易度を求め、次にそれらのデルタからthetaを求め、次にデルタとthetaからベータを求める。irt.faとscore.irtを参照してください。
古典的テスト理論では、項目の難易度を無視し、能力を期待得点として定義します： abilityi = theta(i) = x(i.)ゼロパラメータモデルでは、0から1までのこれらの平均得点を、-4から4までの準ロジスティック尺度に再スケールしますこれは、ロジスティック写像を反映するために、生データを非線形変換しているにすぎません。基本的な1パラメータ（Rasch）モデルは、項目の困難さ（デルタj）を考慮する：p（i番目の被験者に対する項目jの正誤｜θ i, deltaj）= 1/(1+exp(deltaj - thetai)）項目の困難さ（デルタ）の推定値があれば、最適化によってθ iを見つけることができる 2パラメータモデルは、項目の感度（ベータj）を追加する：p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj-θ i)))delta, beta, theta を推定して、モデルのデータへの適合を最大化する。ここで使用される手順は、最初にtheta = 0と仮定して項目の難易度を求め、次にそれらのデルタからthetaを求め、次にデルタとthetaからベータを求める。irt.faとscore.irtを参照してください。
項目反応理論(別名「新しい心理測定法」)は、項目に対する個人の反応をロジスティック関数と個人(θ)および項目の困難さ(diff)パラメータでモデル化します。irt.item.diff.raschは、すべての被験者に対してθ=0であり、すべての項目が等しく識別可能であるという仮定で、項目の困難さを求めます。  psych-packageのIRT関数は、生産目的ではなく、教育目的である。  これらは正確であると信じられていますが、保証されているわけではありません。SAPAデータセットに関連する欠損データ構造に対しては、ldmパッケージよりも若干頑健なようです。irt.fa関数も代替手段である。irt.fa関数は4相関または多相関を求め、因子分析(fa)を用いてIRTパラメータに変換します。
irt.faは項目反応分析のプロセスを簡単にするためにいくつかの関数を1つにまとめたものです。  相関はテトラコロリックまたはポリコロリックのいずれかを使用して求められます。  その後、faを使用して、すべての正規オプションによる探索的因子分析が行われます。  そして、その結果は、IRTパラメータ（困難度と弁別）の観点で報告されるように整理され、より従来の因子分析のアウトプットと同様になります。さらに、相関ステップはやや時間がかかるので、最初のステップで発見された相関行列を用いて再分析を行うことができる。  この場合，fm="minchi" 因子分解法を使用したい場合，オブザベーションの数は， pairwiseCount から得られる行列として指定される必要がある．二項項目の四項相関行列は，（たとえば）最小残差因子分析関数fa を使用して因子分解され，結果の負荷量λ_i は，a = λ / (sqrt(1-λ^2))によって判別に変換される．難易度パラメータδは、テトラコロリックまたはポリコロリック関数のτパラメータから求められます。δ = τ / (sqrt(1-λ^2)同様の分析は、ポリコロリック相関と各項目応答に対する項目難易度（位置）の個別の推定値を用いて、離散項目応答で行うことができます。結果は、2値項目の場合はlink{plot.irt}を、多値項目の場合はlink{plot.poly}を用いてグラフィカルに表示することができます。  これらは、irt.fa出力をプロットすることによって呼び出されます。）   type="IIC "は項目情報関数をプロットし、type="test "はテスト情報関数をプロットします。  type="IIC "は項目特性応答関数をプロットし、type="test "はテスト情報関数をプロットします。プロット関数からの不可視出力は、特性のいくつかのレベルの関数としての項目情報の表、およびそれらの各レベルでの測定の標準誤差と信頼性を返します。  しかし、相関行列がすでにテトラコリック、ポリコリック、またはirt.faを使用した以前の分析で求められている場合は、その結果を直接処理することができます。  irt.faは分析からrho行列とtau行列を保存するので、入力が最初の実行で返されたオブジェクトであれば、同じデータセットのその後の分析はずっと速くなります。  同様の機能はomegaにもあります。出力はグラフィック表示で見るのが最適です。  irt.faの出力をプロットして、アイテムとテスト情報の関数を見る。  print関数は、項目の位置と識別を表示します。  irt.select関数は、さらなる分析のために先行分析のサブセットを選択できるようにするヘルパー関数です。まずirt.faを実行し、その後のirt.fa分析で分析する変数のサブセットを選択します。  より良い方法は、プロットして選択した項目の情報を見つけることでしょう。  irt.faオブジェクトのプロット関数は、ICC（項目特性曲線）、IIC（項目情報曲線）、またはテスト情報曲線をプロットします。また、"keys "オプションを使用することで、選択した項目に対してこれら3種類のプロットを行うことができます。これは、長い形式の因子分析に基づいて、短い形式のテストの情報特性を見ようとするときに、特に便利です。プロット機能は、各項目の情報のピークの位置だけでなく、特性の複数のレベルでの情報量、平均情報量（曲線の下の面積）も（見えないように）返します。  これらは印刷されるか、印刷のソート・オプションを使ってソートされた順序で印刷されます。
項目反応理論(別名「新しい心理測定」)は、項目に対する個人の反応をロジスティック関数と個人(θ)および項目困難度(diff)パラメータでモデル化します。irt.item.diff.raschは、すべての被験者についてθ=0であり、すべての項目が等しく識別できるという仮定で項目困難度を求めます。irt.discrimは、項目識別(β)パラメータを求めるために、irt.person.raschからこれらの困難度とθの推定値を取ります。  psych-packageのIRT関数は、生産目的ではなく、教育目的である。  これらは正確であると信じられていますが、保証されているわけではありません。SAPAデータセットに関連する欠損データ構造に対しては、ldmパッケージよりも若干頑健なようです。irt.fa関数も代替手段である。この関数はテトラコリック相関またはポリコリック相関を求め、因子分析(fa)を用いてIRTパラメータに変換します。
古典的テスト理論では、項目の難易度を無視し、能力を期待得点として定義します： abilityi = theta(i) = x(i.)ゼロパラメータ・モデルでは、これらの平均得点を0から1まで、-4から4までの準ロジスティック尺度に再スケーリングしますこれは、ロジスティック・マッピングを反映するために、生データを非線形変換しているにすぎません。基本的な1パラメータ（Rasch）モデルは、項目の難しさ（デルタj）を考慮する：p（i番目の被験者に対する項目jの正誤｜θ i, deltaj）= 1/(1+exp(deltaj - thetai)）項目の難しさ（デルタ）の推定値があれば、最適化によってθ iを見つけることができる。 2パラメータ・モデルは、項目の感度（ベータj）を追加する：p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj-θ i)))delta, beta, theta を推定して、モデルのデータへの適合を最大化する。ここで使用される手順は、最初にtheta = 0と仮定して項目の難易度を求め、次にそれらのデルタからthetaを求め、次にデルタとthetaからベータを求める。irt.faとscore.irtを参照してください。
この関数は多肢選択式の能力テストにおいて、項目の選択肢の質を分析する便利な方法です。  典型的な使用方法は、まずテストを採点し（例えば、score.multiple.choiceを使用して）、いくつかの採点キーにしたがって、score.irtに基づいた得点を求めることです。  そして、各選択肢の回答頻度を総得点に対してプロットします。  理想的な項目とは、ただ1つの選択肢（正しい選択肢）の回答確率が単調増加するような項目です。IRTベースでも単純合計ベースでも似たような結果になるため、この関数はscore.irtまたはscore.multiple.choiceで計算された得点に対して実行することができます。後者の場合、区切りの数は可能な得点の選択肢の数を超えてはなりません。
項目の位置（難易度）と弁別のセットがあれば、よりエレガントな方法で被験者の得点を求めることができますが、得点ベクトルX、位置δ、弁別βに対して、P(x|θ)=1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を求めるだけで、総得点よりも多くの情報を得ることができます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目が欠落する確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受験した項目がすべて正解した場合、その人のスコアは、すべての項目が正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報と採点キー行列を組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定します。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。irt.seは特定の値を持つ得点の標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
irt.faは項目反応分析のプロセスを簡単にするために、いくつかの機能を1つにまとめたものです。  相関はテトラコリックまたはポリコリックのいずれかを使用して求められます。  すべての正規オプションを用いた探索的因子分析は、faを使用して行われます。  そして、その結果は、IRTパラメータ（困難度と識別度）の観点で報告されるように整理され、より従来の因子分析の出力と同様になります。さらに、相関ステップはやや時間がかかるので、最初のステップで発見された相関行列を用いて再分析を行うことができる。  この場合，fm="minchi" 因子分解法を使用したい場合，オブザベーションの数は， pairwiseCount から得られる行列として指定される必要がある．二項項目の四項相関行列は，（たとえば）最小残差因子分析関数fa を使用して因数分解され，結果の負荷量λ_i は，a = λ / (sqrt(1-λ^2))によって判別に変換される．難易度パラメータδは、テトラコロリックまたはポリコロリック関数のτパラメータから求められます。δ = τ / (sqrt(1-λ^2)同様の分析は、ポリコロリック相関と各項目応答に対する項目難易度（位置）の個別の推定値を用いて、離散項目応答で行うことができます。結果は、2値項目の場合はlink{plot.irt}を、多値項目の場合はlink{plot.poly}を用いてグラフィカルに表示することができます。  これらは、irt.fa出力をプロットすることによって呼び出されます。）   type="IIC "は項目情報関数をプロットし、type="test "はテスト情報関数をプロットします。  type="IIC "は項目特性応答関数をプロットし、type="test "はテスト情報関数をプロットします。プロット関数からの不可視出力は、特性のいくつかのレベルの関数としての項目情報の表、およびそれらの各レベルでの測定の標準誤差と信頼性を返します。  しかし、相関行列がすでにテトラコリック、ポリコリック、またはirt.faを使用した以前の分析で求められている場合は、その結果を直接処理することができます。  irt.faは分析からrho行列とtau行列を保存するので、入力が最初の実行で返されたオブジェクトであれば、同じデータセットのその後の分析ははるかに速くなります。  同様の機能はomegaにもあります。出力はグラフィック表示で見るのが最適です。  irt.faの出力をプロットして、アイテムとテスト情報の関数を見る。  print関数は、項目の位置と識別を表示します。  irt.select関数は、さらなる分析のために先行分析のサブセットを選択できるようにするヘルパー関数です。まずirt.faを実行し、その後のirt.fa分析で分析する変数のサブセットを選択します。  より良い方法は、プロットして選択した項目の情報を見つけることでしょう。  irt.faオブジェクトのプロット関数は、ICC（項目特性曲線）、IIC（項目情報曲線）、またはテスト情報曲線をプロットします。また、"keys "オプションを使用することで、選択した項目に対してこれら3種類のプロットを行うことができます。これは、長い形式の因子分析に基づいて、短い形式のテストの情報特性を見ようとするときに、特に便利です。プロット機能は、各項目の情報のピークの位置だけでなく、特性の複数のレベルでの情報量、平均情報量（曲線の下の面積）も（見えないように）返します。  これらを印刷したり、印刷のソート・オプションを使ってソート順に印刷することができる。
項目の位置（難易度）と識別のセットがあれば、よりエレガントな方法で被験者の得点を求めることができますが、得点ベクトルX、位置δ、識別βに対して、P(x|θ)=1/(1+exp(β(δ-θ) ) の式に最も適合するθの値を見つけるだけで、合計得点以上の情報が得られます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報を採点キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。これらはmodパラメータを使って選択されます。irt.seは特定の値を持つスコアの標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
項目の位置（難易度）と弁別のセットがあれば、被験者の得点を求めるよりエレガントな方法がありますが、得点ベクトルX、位置δ、弁別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、合計得点よりも多くの情報が得られます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の得点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  もし、完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を得点化したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報を得点化キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。これらはmodパラメータを使って選択されます。irt.seは特定の値を持つスコアの標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
lowerCorは対角行列の下側を丸めたものを出力し、列名は桁数+3文字に省略されます。  デフォルトでは、変数の対削除を使用する。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
lowerCor は，列名を桁数 + 3 文字に省略し，丸められた対角行列の下側の行列を出力します．  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるために、psychに追加されました。
このシミュレーションはもともと、影響の測定におけるスキューの効果を比較するために開発されました（Rafaeli and Revelle, 2005を参照）。  このシミュレーションは、単純構造または円周構造を持つ感情またはパーソナリティ項目の一般的なシミュレーションを可能にするために拡張されました。  項目は、連続的な正規分布にすることも、n個のカテゴリー（例：-2、-1、0、1、2）に分けることもできる。  項目の平均が（例えば1）であっても、これらの範囲に制限することで項目が歪む可能性があります。  item.dichotを追加することで、異なる難易度（支持）の二項対立項目を持つ構造をテストすることができます。  2つの項目のセットに対して、単純構造または円周構造を持つ2つの因子データが生成され、1つは、low (easy)値より大きいすべての項目に対してスコア1を与え、もう1つは、high (hard)値より大きいすべての項目に対してスコア1を与える。つまり、すべての項目の支持率は50％と仮定される。  項目の難易度の効果を調べるために、lowを-1、highを1とすることもできる。これにより、項目の支持率は、簡単なものが0.84、難しいものが0.16となる。  各難易度セットの中で、最初の1/4は第1因子、2番目は第2因子、3番目は第1因子（ただし負荷量はマイナス）、4番目は第2因子（ただし負荷量はマイナス）に割り当てられる。sim.itemとsim.hierarchicalの結果を比較するのは便利です。sim.itemは、2つの直交因子と同様に、すべての項目を通る一般因子を生成します。  これは、標準的な回転技法では表現しにくいデータセットを生成します。  回転なしで3つの因子を抽出し、2番目と3番目の因子を回転させると、正しい解が得られます。  しかし、3因子の単純な斜め回転やオメガ分析では、根本的な構造を捉えることはできません。  最後の例をご覧ください。さらに、魅力的かもしれないもう1つの構造は、3次元の完全に複雑なデータです。  sim.sphericalはこのようなデータを生成する。
fa.lookupとlookupは、相関行列や因子負荷行列を要約するための簡単なヘルパー関数です。 bestItemsは、xの指定された列（基準）を列の（絶対）値に基づいてソートします。  デフォルトとして返されるのは、それらの絶対値 > cut を持つ変数の名前だけである。   項目内容と項目名の辞書がある場合、項目名に対応する rowname を持つ 2 列 (またはそれ以上) の行列として内容を含める。(辞書の例bfi.dictionaryを参照).lookupは、bestItemsによって使用され、xの値と一致するyのc1の値を見つける。  そして、lookup(rownames(x),y)によってxに対応する辞書の項目を見つけることができます。列が指定されていない場合は、rownames(y)によってマッチします。fa.lookupは、因子分析の出力を調べ、対応する変数名と内容が欲しいときに使用されます。返されたオブジェクトは、charオプションをTRUEに設定してdf2latex関数を使用することによりLaTexで印刷することができます。fa.lookupは、fa、pcaまたはomegaからの出力で動作します。  同様に、x変数の相関行列rが与えられたとき、別の項目または尺度と最も相関する項目を見つけ、辞書からその項目の内容を表示したい場合、bestItems(r,c1=xの列番号または名前, contents = y)item.lookupは、因子分析faからの出力と単純な記述統計（平均値のデータフレーム）を辞書と組み合わせます。  項目は、因子負荷量 > カットでグループ化され、項目平均でソートされる。  これにより、項目の支持の意味という観点から、尺度がどのように機能するかをよりよく理解することができます。 lookupItems は、ある内容を持つすべての項目を辞書から検索します。  返されたオブジェクトのrownameは項目番号であり、この番号を他の関数で使用して、それらの項目を持つ尺度の統計量（ωなど）を求めることができます。   項目ごとの尺度の相関行列が与えられると、その尺度と項目の相関も表示されます。
このシミュレーションは、もともと影響の測定におけるスキューの効果を比較するために開発されました（Rafaeli and Revelle, 2005を参照）。  このシミュレーションは、単純構造または円周構造を持つ感情またはパーソナリティ項目の一般的なシミュレーションを可能にするために拡張されました。  項目は、連続的な正規分布にすることも、n個のカテゴリー（例：-2、-1、0、1、2）に分けることもできる。  項目の平均が（例えば1）であっても、これらの範囲に制限することで項目が歪む可能性があります。  item.dichotを追加することで、異なる難易度（支持）の二項対立項目を持つ構造をテストすることができます。  2つの項目のセットに対して、単純構造または円周構造を持つ2つの因子データが生成され、1つは、low (easy)値より大きいすべての項目に対してスコア1を与え、もう1つは、high (hard)値より大きいすべての項目に対してスコア1を与える。つまり、すべての項目の支持率は50％と仮定される。  項目の難易度の効果を調べるために、lowを-1、highを1とすることもできる。これにより、項目の支持率は、簡単なものが0.84、難しいものが0.16となる。  各難易度セットの中で、最初の1/4は第1因子、2番目は第2因子、3番目は第1因子（ただし負荷量はマイナス）、4番目は第2因子（ただし負荷量はマイナス）に割り当てられる。sim.itemとsim.hierarchicalの結果を比較するのは便利です。sim.itemは、2つの直交因子と同様に、すべての項目を通る一般因子を生成します。  これは、標準的な回転技法では表現しにくいデータセットを生成します。  回転なしで3つの因子を抽出し、2番目と3番目の因子を回転させると、正しい解が得られます。  しかし、3因子の単純な斜め回転やオメガ分析では、根本的な構造を捉えることはできません。  最後の例をご覧ください。さらに、魅力的かもしれないもう1つの構造は、3次元の完全に複雑なデータです。  sim.sphericalはこのようなデータを作成します。
尺度に形成された項目の集合から基準を予測する場合、尺度の妥当性（つまり、各基準と尺度の相関）は、平均項目の妥当性（r_y）、尺度の項目の平均相互相関（r_x）、尺度の項目数（n）の関数である。  妥当性の限界はr_y/sqrt(r_x)である。  尺度の集合からの予測可能性は、基準によって異なる。これらの漸近値は、どの尺度をさらに開発するかの決定に役立つ。  
回転していない解から呼び出すと最高の結果が得られます。  回転させた解を用いて繰り返し呼び出すと、因子間の相関の推定が正しくなくなる。
fa.lookupとlookupは、相関行列または因子負荷行列を要約する簡単なヘルパー関数です。bestItemsは、xの指定された列（基準）を列の（絶対）値に基づいてソートします。  デフォルトとして返されるのは、それらの絶対値 > cut を持つ変数の名前だけである。   項目内容と項目名の辞書がある場合、項目名に対応する rowname を持つ 2 列 (またはそれ以上) の行列として内容を含める。(辞書の例bfi.dictionaryを参照).lookupは、bestItemsによって使用され、xの値と一致するyのc1の値を見つける。  そして、lookup(rownames(x),y)によってxに対応する辞書の項目を見つけることができます。列が指定されていない場合は、rownames(y)によってマッチします。fa.lookupは、因子分析の出力を調べ、対応する変数名と内容が欲しいときに使用されます。返されたオブジェクトは、charオプションをTRUEに設定してdf2latex関数を使用することによりLaTexで印刷することができます。fa.lookupは、fa、pcaまたはomegaからの出力で動作します。  同様に、x変数の相関行列rが与えられたとき、別の項目または尺度と最も相関する項目を見つけ、辞書からその項目の内容を表示したい場合、bestItems(r,c1=xの列番号または名前, contents = y)item.lookupは、因子分析faからの出力と単純な記述統計（平均値のデータフレーム）を辞書と組み合わせます。  項目は、因子負荷量 > カットでグループ化され、項目平均でソートされる。  これにより、項目の支持の意味という観点から、尺度がどのように機能するかをよりよく理解することができます。 lookupItems は、ある内容を持つすべての項目を辞書から検索します。  返されたオブジェクトのrownameは項目番号であり、この番号を他の関数で使用して、それらの項目を持つ尺度の統計量（ωなど）を求めることができます。   スケールと項目の相関行列が指定された場合、そのスケールと項目の相関も表示されます。
scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2pl のキーを用意する最も簡単な方法は、keys.list を指定することです。  以前のバージョン（1.6.9以前）では、キーはmake.keysを使用してすべての項目に対して-1、0、1のマトリックスとして形成されていました。  scoreItems、scoreOverlap、scoreIrt.1pl、scoreIrt.2pl関数のキーを作成するには3つの方法があります。make.keysでは、項目名で指定することも、位置で指定することも、その両方を混在させることもできます。keys2listでは、make.keysの処理を逆にし、キーとなる各項目の項目名を含むスコアリングキーのリストを返します。  sign=FALSEの場合、これは単なる採点項目のリストとなる。(scoreIrt.2plselectFromKeysに便利です。)keys.listから符号を取り除き、それらのキーに関連する項目名のベクトル(重複を削除)を作成します。  これはkeys.listを使ってスケールを定義し、keys.listのサブセットに含まれる項目だけを選択する場合に便利です。  これは、スピードアップのため、採点関数の中で行われるようになりました。これらのスコアリング関数scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2plは現在（>バージョン1.6.9）keys.listを入力として受け取ることができるので、make.keysはそれほど重要ではありませんが、ドキュメンテーションのために残してあります。項目名を指定するには、item.labels値を使用するか、データファイルの名前またはスコアリングされるデータファイルのcolnamesを最初の（nvars）位置に置く必要があります。番号（位置）で指定する場合、nvarsは、使用される項目の数だけでなく、スコアリングされるオブジェクトの項目の総数になります。  makePositiveKeysは、(bestScalesなどから)キーのサブセットを取得し、正と負にキー設定された項目に対して別々のキーを作成するのに便利です。
能力やパーソナリティの他の側面を測定するために使用される項目は、一般的にあまり信頼できません。  1つの提案として、項目を均質な項目複合（HIC）、因子的に均質な項目次元（FHID）、またはミニ尺度（パーセル）に形成することがあります。  link{parcels}は、score.itemsで使用するのに適したキー行列を形成することで、パーセルの発見を容易にします。  これらのキーは、n/2の最も類似したペア、またはn/3の最も類似したトリプレットを表します。アルゴリズムは簡単です：  サイズ＝2の場合、相関行列は最も相関の高いものを探す。  これらの2つの項目は最初の区画を形成し、行列から取り除かれる。  size=3 の場合，互いに最大の分散と共分散の合計を持つ3つの項目が見つけられる．  このトリプレットが最初の区画である。  3つの項目はすべて取り除かれ、次に最も類似したトリプレットが同定される。  この手順は，n/3個の区画が識別されるまで繰り返される．  
S^2 = diag(R^{-1})^{-1}、Q = SR^{-1}S とする。  そしてQは反画像相互相関行列という。  RとQのすべての対角外要素をsumr2=∑{R^2}、sumq2=∑{Q^2}とすると、SMA=sumr2/(sumr2 + sumq2)となる。  元々MSAは1 - sumq2/sumr2 (Kaiser, 1970)であったが、Kaiser and Rice, (1974)でSMA=sumr2/(sumr2 + sumq2)に修正された。  これは、Dziuban and Shirkey (1974)やSPSSで使用されている式である。Kaiser (1975)は、彼の楽しく派手なスタイルで、KMO > .9は驚異的で、.80sではmertitourious、.70sでは中程度、.60sではmedicore、.行列が因数分解可能であるかどうかの別の尺度として、バートレット検定 cortest.bartlettがあり、これは行列が恒等行列からどの程度逸脱しているかを検定する。
行列またはdata.frame xが与えられた場合、各列の歪度または尖度を求めます（歪度と尖度の場合）。  これらは、e1071パッケージの歪度（skewness）と尖度（kurtosis）で利用可能な選択肢と一致しています（それぞれの利点についてはJoanes and Gill (1998)を参照してください）。m_r = [sum(X- mx)^r]/n と定義すると、タイプ1は g_1 = m_3/(m_2)^{3/2} と g_2 = m_4/(m_2)^2 -3 で歪度と尖度を求めます。 タイプ2は G1 = g1 * √{n *(n-1)}/(n-2) と G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)) です。  タイプ3は、b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2}、b2 = [(n-1)/n]^{3/2}m_4/m_2^2）である。e1071との整合性、JoanesとGillとの整合性のため、現在は上記のように定義されています。しかし、リビジョン1.0.93から1.2.3では、kurtosiはデフォルトで尖度の不偏推定値を与えます(DeCarlo, 1997)。以前のバージョンでは、偏った推定を行う別の式を使用していました。  (これら2つの式の違いについては、e1071パッケージのkurtosis関数を参照してください。  デフォルトのタイプ1はe1071でタイプ2と呼ばれるものを与えた。  もう1つはタイプ3です)。  以前のリリースとの比較のために、type = 2を指定すると、古い推定値が得られます。  これらのタイプ番号は現在変更されている。  
推奨される関数はstructure.diagramで、これはRgraphvizを使用しませんが、ドットコードも生成しません。  3つのstructure関数はすべて、semまたはlavaanパッケージで使用するのに適したコマンドの行列を返します。  (sem.diagramとsem.graphは、semパッケージで実行された単純なCFAからの出力を変換し、structure.diagramまたはstructure.graph.lavaan.diagramを使って描画します。diagramは、lavaanパッケージで行われた単純なCFAからの出力（フィット）を変換し、structure.diagramを使って描画します。X変数間の相関（Rxが指定された場合） X変数とそれらの潜在因子（fxが指定された場合） 潜在Xと潜在Y（Phiが指定された場合） 潜在Yと観測されたY（fyが指定された場合） Y変数間の相関（Ryが指定された場合） 確定因子モデルではfxとPhiだけが指定され、構造モデルではfx、Phi、fyが含まれます。  lavaan.diagramは、適合のクラスに応じてfa.diagram、omega.diagram、iclust.diagramを呼び出すdiagram関数から呼び出すことができます。  これらの関数はすべて、dia.rect、dia.ellipse、dia.arrow、dia.curve、dia.curved.arrow、dia.shape などのさまざまな dia 関数を使用します。
lowerCorは、対角行列の下側を桁数に丸め、列名を桁数+3文字に省略したものを出力します。  デフォルトでは、変数のペアワイズ削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
これら3つの関数は、項目応答理論のデモンストレーションのための単純なヘルパー関数として提供されています。1パラメータ・ロジスティック（1PL）モデルは、Raschモデルとしても知られています。  1PL、2PL、3PL、4PL曲線は、適切なd (デルタ、項目の難易度)、a (判別、傾き)、c (ガンマ、推測)、z (ゼータ、上方漸近線)を選択することで描画できます。logitはlogisticの逆関数です。logistic.grmは、カットポイントがsであるr番目のカテゴリの段階的応答モデルの応答を作成します。
これらの3つの関数は、項目応答理論のデモンストレーションのための簡単なヘルパー関数として提供されています。1パラメータ・ロジスティック（1PL）モデルは、ラッシュ・モデルとしても知られています。  1PL、2PL、3PL、4PL曲線は、適切なd (デルタまたは項目の難易度)、a (判別または傾き)、c (ガンマまたは推測)、およびz (ゼータまたは上方漸近線)を選択することによって描画できます。logitは、logistic.grmの単なる逆関数で、カットポイントがsにあるr番目のカテゴリの段階的応答モデルの応答を作成します。
これらの3つの関数は、項目応答理論のデモンストレーションのための簡単なヘルパー関数として提供されています。1パラメータ・ロジスティック（1PL）モデルは、ラッシュ・モデルとしても知られています。  1PL、2PL、3PL、4PL曲線は、適切なd (デルタまたは項目の難易度)、a (判別または傾き)、c (ガンマまたは推測)、z (ゼータまたは上方漸近線)を選択することにより描画できます。logitはlogisticの逆関数です。logistic.grmは、カットポイントがsにあるr番目のカテゴリの段階的応答モデルの応答を作成します。
fa.lookupとlookupは、相関行列または因子負荷行列を要約する簡単なヘルパー関数です。bestItemsは、xの指定された列（基準）を列の（絶対）値に基づいてソートします。  デフォルトとして返されるのは、それらの絶対値 > cut を持つ変数の名前だけである。   項目内容と項目名の辞書がある場合、項目名に対応する rowname を持つ 2 列 (またはそれ以上) の行列として内容を含める。(辞書の例bfi.dictionaryを参照).lookupは、bestItemsによって使用され、xの値と一致するyのc1の値を見つける。  そして、lookup(rownames(x),y)によってxに対応する辞書の項目を見つけることができます。列が指定されていない場合は、rownames(y)によってマッチします。fa.lookupは、因子分析の出力を調べ、対応する変数名と内容が欲しいときに使用されます。返されたオブジェクトは、charオプションをTRUEに設定してdf2latex関数を使用することによりLaTexで印刷することができます。fa.lookupは、fa、pcaまたはomegaからの出力で動作します。  同様に、x変数の相関行列rが与えられたとき、別の項目または尺度と最も相関する項目を見つけ、辞書からその項目の内容を表示したい場合、bestItems(r,c1=xの列番号または名前, contents = y)item.lookupは、因子分析faからの出力と単純な記述統計（平均値のデータフレーム）を辞書と組み合わせます。  項目は、因子負荷量 > カットでグループ化され、項目平均でソートされる。  これにより、項目の支持の意味という観点から、尺度がどのように機能するかをよりよく理解することができます。 lookupItems は、ある内容を持つすべての項目を辞書から検索します。  返されたオブジェクトのrownameは項目番号であり、この番号を他の関数で使用して、それらの項目を持つ尺度の統計量（ωなど）を求めることができます。   項目と尺度の相関行列が指定された場合、その尺度と項目の相関も表示されます。
fa.lookupとlookupは、相関行列や因子負荷行列を要約するためのシンプルなヘルパー関数です。 bestItemsは、xの指定された列（基準）を列の（絶対）値に基づいてソートします。  デフォルトとして返されるのは、それらの絶対値 > cut を持つ変数の名前だけである。   項目内容と項目名の辞書がある場合、項目名に対応する rowname を持つ 2 列 (またはそれ以上) の行列として内容を含める。(辞書の例bfi.dictionaryを参照).lookupは、bestItemsによって使用され、xの値と一致するyのc1の値を見つける。  そして、lookup(rownames(x),y)によってxに対応する辞書の項目を見つけることができます。列が指定されていない場合は、rownames(y)によってマッチします。fa.lookupは、因子分析の出力を調べ、対応する変数名と内容が欲しいときに使用されます。返されたオブジェクトは、charオプションをTRUEに設定してdf2latex関数を使用することによりLaTexで印刷することができます。fa.lookupは、fa、pcaまたはomegaからの出力で動作します。  同様に、x変数の相関行列rが与えられたとき、別の項目または尺度と最も相関する項目を見つけ、辞書からその項目の内容を表示したい場合、bestItems(r,c1=xの列番号または名前, contents = y)item.lookupは、因子分析faからの出力と単純な記述統計（平均値のデータフレーム）を辞書と組み合わせます。  項目は、因子負荷量 > カットでグループ化され、項目平均でソートされる。  これにより、項目の支持の意味という観点から、尺度がどのように機能するかをよりよく理解することができます。 lookupItems は、ある内容を持つすべての項目を辞書から検索します。  返されたオブジェクトのrownameは項目番号であり、この番号を他の関数で使用して、それらの項目を持つ尺度の統計量（ωなど）を求めることができます。   項目と尺度の相関行列が指定された場合、その尺度と項目の相関も表示されます。
fa.lookupとlookupは、相関行列や因子負荷行列を要約するためのシンプルなヘルパー関数です。 bestItemsは、xの指定された列（基準）を列の（絶対）値に基づいてソートします。  デフォルトとして返されるのは、それらの絶対値 > cut を持つ変数の名前だけである。   項目内容と項目名の辞書がある場合、項目名に対応する rowname を持つ 2 列 (またはそれ以上) の行列として内容を含める。(辞書の例bfi.dictionaryを参照).lookupは、bestItemsによって使用され、xの値と一致するyのc1の値を見つける。  そして、lookup(rownames(x),y)によって、xに対応する辞書の項目を見つけることができます。列が指定されていない場合は、rownames(y)によってマッチします。fa.lookupは、因子分析の出力を調べ、対応する変数名と内容が欲しいときに使用されます。返されたオブジェクトは、charオプションをTRUEに設定してdf2latex関数を使用することによりLaTexで印刷することができます。fa.lookupは、fa、pcaまたはomegaからの出力で動作します。  同様に、x変数の相関行列rが与えられたとき、別の項目または尺度と最も相関する項目を見つけ、辞書からその項目の内容を表示したい場合、bestItems(r,c1=xの列番号または名前, contents = y)item.lookupは、因子分析faからの出力と単純な記述統計（平均値のデータフレーム）を辞書と組み合わせます。  項目は、因子負荷量 > カットでグループ化され、項目平均でソートされる。  これにより、項目の支持の意味という観点から、尺度がどのように機能するかをよりよく理解することができます。 lookupItems は、ある内容を持つすべての項目を辞書から検索します。  返されたオブジェクトのrownameは項目番号であり、この番号を他の関数で使用して、それらの項目を持つ尺度の統計量（ωなど）を求めることができます。   スケールと項目の相関行列が与えられた場合、そのスケールと項目の相関も表示されます。
lowerCor は，対角行列の下側を丸めたものを出力し，列名は桁数 + 3 文字に省略されます．  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
lowerCor は，列名を桁数 + 3 文字に省略し，丸められた対角行列の下側の行列を出力します．  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
行列が1つだけ提供された場合（つまり， upper がない場合），その行列は2つの正方行列に分解され，1つは対角の下側のエントリと等しく，もう1つは対角の上側のエントリと等しくなります．通常の場合、2つの対称行列が提供され、1つの非対称行列に結合される。diffがtrueの場合、upper off diagonal行列は2つの行列の差を反映する。
lsat6データセットは、McDonald (1999)と同様にltmパッケージで分析されている。どちらのデータセットもBock and Lieberman (1970)によって説明されています。どちらのデータセットも、IRT手順をテストし、irt.fa関数を用いたテトラコリック相関と項目因子分析の使用を示す有用な例です。
lsat6データセットは、McDonald (1999)と同様にltmパッケージで分析されています。どちらのデータセットもBock and Lieberman (1970)によって説明されています。どちらのセットも、IRT手順をテストし、irt.fa関数を用いたテトラコリック相関と項目因子分析の使用を示す有用な例です。
2つのグループがどのように異なるかを報告する方法はたくさんあります。  Cohenのd統計量は、プールされたグループ内標準偏差で表現された平均値の差です。  rは効果量の普遍的な尺度で、dの単純な関数であるが、-1～1の境界を持つ。t統計量は単にd * sqrt(n)/2 であり、したがって標本サイズを反映する。   (M2- M1)/Sp ここで Sp はプールされた標準偏差。  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}.  }  Cohens dはプールされた平方和の除数としてNを使う。  Hedges gはN-2を用いる。  Cohensのdの信頼区間は、dをtに変換し、tの信頼区間を求め、それをdsに変換することで求めることができる。  cohen.dの結果はerror.dots関数を使って表示することができます。  これには、辞書で提供されたラベルが含まれる。  0（1標本の場合）との比較のためにcohen.d.ciを使用して）信頼区間を求める場合、n1を指定します。  これはd = t/sqrt(n1)をもたらしますが、2つの標本間の差の場合はd = 2*t/sqrt(n) (標本サイズが等しい場合 n = n1+ n2)、または標本サイズが等しくない場合はd = t/sqrt(1/n1 + 1/n2) となります。7/14/21まで、私はtとしたがってp値を推定するために合計nを使っていました。  問い合わせ（ニュース参照）に応えて、実際の標本サイズns（n1とn2）を使用し、ヘッジg値に基づいてtを求めることに切り替えました。  これは、t.testでvar.equal = TRUEオプションで報告されたt値を生成します。報告された様々な信頼区間は正規理論に基づいており、慎重に解釈されるべきであるというコメントは、おそらく有益でしょう。cohen.d.byは、group2で定義されたデータの各サブセットについて、グループのCohenのdを求めます。  出力の要約は、各グループの各変数のd値の簡略化されたリストを生成する。  d.robustはAlgina et al. 2005)に従い、トリム平均(trim =.2)とWinsorize分散(trim =.2)を求めます。  m2tは、2つのグループの平均、標準偏差、標本サイズが与えられたときのStudentのt.t.検定を報告します。  これは、推定値は提供されているが生データが入手できない統計量をチェックするときに便利である。  デフォルトでは、プールされた推定分散を与えますが、pooledがFALSEの場合、Welchの補正が適用されます。マハラノビス距離は、個々のdsを結合し、それらのユニークな寄与度で重み付けします：  D=√{d'R^{-1}d}。デフォルトでは、cohen.dは2つのグループ間のマハラノビス距離を求めます（複数のDVがある場合）。これはすべてのDVの相関を求める必要があり、いくつかのペアが存在しないため、その行列が可逆でない場合は失敗することがあります。  したがって，MD=FALSEに設定すると，マハラノビスの計算ができなくなります．Marco del Giudice (2019)は，さまざまな重複係数の観点からdとMdを解釈する方法について議論した非常に有用な論文を持っています．これらは、d2OVL（1つの分布の重なり割合）、d2OVL2（共同分布の重なり割合）、d2CL（共通言語の効果量）、d2U3（上位群の割合が下位群の中央値を超える）を使用することで見つけることができます。OVL = 2 φ(-d/2) は重なり割合です（dが大きくなるほど小さくなります）。ここで Phi は正規分布の累積密度関数である.OVL_2 = OVL/(2-OVL)U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )最後の2つは(abs (d))で大きくなる。  CohenのdとMahalanobis Dのグラフ表示については、scatterHistの例、またはpsychTools::GERASデータセットからの例を参照してください。
2つのグループがどのように異なるかを報告する方法はたくさんあります。  Cohenのd統計量は、プールされたグループ内標準偏差で表現された平均値の差です。  rは効果量の普遍的な尺度で、dの単純な関数であるが、-1～1の境界である。t統計量は単にd * sqrt(n)/2 であり、したがって標本サイズを反映する。   (M2- M1)/Sp ここで Sp はプールされた標準偏差。  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}.  }  Cohens dはプールされた平方和の除数としてNを使う。  Hedges gはN-2を用いる。  Cohensのdの信頼区間は、dをtに変換し、tの信頼区間を求め、それをdsに変換することで求めることができる。  cohen.dの結果はerror.dots関数を使って表示することができます。  これには、辞書で提供されたラベルが含まれる。  0（1標本の場合）との比較のためにcohen.d.ciを使用して）信頼区間を求める場合、n1を指定します。  これはd = t/sqrt(n1)をもたらしますが、2つの標本間の差の場合はd = 2*t/sqrt(n) (標本サイズが等しい場合 n = n1+ n2)、または標本サイズが等しくない場合はd = t/sqrt(1/n1 + 1/n2) となります。7/14/21まで、私はtとしたがってp値を推定するために合計nを使っていました。  問い合わせ（ニュース参照）に応えて、実際の標本サイズns（n1とn2）を使用し、ヘッジg値に基づいてtを求めることに切り替えました。  これは、t.testでvar.equal = TRUEオプションで報告されたt値を生成します。報告された様々な信頼区間は正規理論に基づいており、慎重に解釈されるべきであるというコメントは、おそらく有益でしょう。cohen.d.byは、group2で定義されたデータの各サブセットについて、グループのCohenのdを求めます。  出力の要約は、各グループの各変数のd値の簡略化されたリストを生成する。  d.robustはAlgina et al. 2005)に従い、トリム平均(trim =.2)とWinsorize分散(trim =.2)を求めます。  m2tは、2つのグループの平均、標準偏差、標本サイズが与えられたときのStudentのt.t.検定を報告します。  これは、推定値は提供されているが生データが入手できない統計量をチェックするときに便利である。  デフォルトでは、プールされた推定分散を与えますが、pooledがFALSEの場合、Welchの補正が適用されます。マハラノビス距離は、個々のdsを結合し、それらのユニークな寄与度で重み付けします：  D=√{d'R^{-1}d}。デフォルトでは、cohen.dは2つのグループ間のマハラノビス距離を求めます（複数のDVがある場合）。これはすべてのDVの相関を求める必要があり、いくつかのペアが存在しないため、その行列が可逆でない場合は失敗することがあります。  したがって，MD=FALSEに設定すると，マハラノビスの計算ができなくなります．Marco del Giudice (2019)は，さまざまな重複係数の観点からdとMdを解釈する方法について議論した非常に有用な論文を持っています．これらは、d2OVL（1つの分布の重なり率）、d2OVL2（共同分布の重なり率）、d2CL（共通言語の効果量）、およびd2U3（上位群の割合が下位群の中央値を超える）の使用によって見つけることができます。OVL = 2 φ(-d/2) は重なり率です（dが大きくなるほど小さくなります）。ここで Phi は正規分布の累積密度関数である.OVL_2 = OVL/(2-OVL)U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )最後の2つは(abs (d))で大きくなる。  CohenのdとMahalanobis Dのグラフ表示については、scatterHistの例、またはpsychTools::GERASデータセットの例を参照してください。
信頼性分析用の例を構築する場合、共属データ構造をシミュレートするのが便利です。  これは最も単純な項目構造で、ただ1つの因子を持つだけです。主に信頼性理論の議論や因子得点の推定に用いられます。暗黙の共分散行列は，単にパターン %*% t(pattern)である．
多くの性格テストや認知テストは、階層的な因子構造を持っています。  実証のためには，母集団値または標本値のいずれかで，そのような行列を作成できると便利である．項目の因子負荷量（fload）と一般因子へのこれらの因子の負荷量（gload）の行列が与えられている場合、一般因子の法則（R = F' theta F ここで theta = g'g）を使用して母集団相関行列を作成します。  デフォルトでは，母集団相関行列が返される．gloadとfloadのデフォルト値は、JensenとWeng, 1994によって議論されたデータ行列を作成します。階層構造を作成するように書かれていますが、gload行列がすべて0の場合、非階層構造が生成されます。さらに別のモデルとして、独立結合がg因子モデルと同じ因子構造を生成することを示唆したGodfrey H. Thomson (1916)のモデルがあります。これはsim.bondsでシミュレートされている。  sim.hierarchicalとsim.bondsモデルのω解を比較します。どちらも妥当なωの値を生成しますが、一方は一般因子なしで生成されています。
項目の位置（難易度）と弁別のセットを与えて被験者の得点を求めるよりエレガントな方法がありますが、得点ベクトルX、位置δ、弁別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、総得点よりも多くの情報が得られます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報と採点キー行列を組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。これらはmodパラメータを使って選択されます。irt.seは特定の値を持つスコアの標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の科目の特定のスコアに基づいているわけではありません。
scoreItems、scoreOverlap、scoreIrt.1pl、scoreIrt.2plのキーを用意する最も簡単な方法はkeys.listを指定することです。  以前のバージョン（1.6.9以前）では、make.keysを使用してすべての項目に対して-1、0、1のマトリックスとしてキーを作成していました。  scoreItems、scoreOverlap、scoreIrt.1pl、scoreIrt.2pl関数のキーを作成するには3つの方法があります。make.keysでは、項目名で指定することも、位置で指定することも、その両方を混在させることもできます。keys2listでは、make.keysの処理を逆にし、キーとなる各項目の項目名を含むスコアリングキーのリストを返します。  sign=FALSEの場合、これは単なる採点項目のリストとなる。(scoreIrt.2plselectFromKeysに便利です。)keys.listから符号を取り除き、それらのキーに関連する項目名のベクトル(重複を削除)を作成します。  これはkeys.listを使ってスケールを定義し、keys.listのサブセットに含まれる項目だけを選択する場合に便利です。  これは、スピードアップのため、採点関数の中で行われるようになりました。これらのスコアリング関数scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2plは現在（>バージョン1.6.9）keys.listを入力として受け取ることができるので、make.keysはそれほど重要ではありませんが、ドキュメンテーションのために残してあります。項目名を指定するには、item.labels値を使用するか、データファイルの名前またはスコアリングされるデータファイルのcolnamesを最初の（nvars）位置に置く必要があります。番号（位置）で指定する場合、nvarsは、使用される項目の数だけでなく、スコアリングされるオブジェクトの項目の総数になります。  makePositiveKeysは、(例えばbestScalesから)キーのサブセットを取得し、正と負にキー設定された項目に対して別々のキーを作成するのに便利である。
scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2pl のキーを作成する最も簡単な方法は、keys.list を指定することです。  以前のバージョン（1.6.9以前）では、make.keysを使用してすべての項目に対して-1、0、1のマトリックスとしてキーを作成していました。  scoreItems、scoreOverlap、scoreIrt.1pl、scoreIrt.2pl関数のキーを作成するには3つの方法があります。make.keysでは、項目名で指定することも、位置で指定することも、その両方を混在させることもできます。keys2listでは、make.keysの処理を逆にし、キーとなる各項目の項目名を含むスコアリングキーのリストを返します。  sign=FALSEの場合、これは単なる採点項目のリストとなる。(scoreIrt.2plselectFromKeysに便利です。) 2plselectFromKeysはkeys.listから符号を取り除き、それらのキーに関連する項目名のベクトルを(重複を削除して)作成します。  これはkeys.listを使ってスケールを定義し、keys.listのサブセットに含まれる項目だけを選択する場合に便利です。  これは、スピードアップのため、採点関数の中で行われるようになりました。これらのスコアリング関数scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2plは現在（>バージョン1.6.9）keys.listを入力として受け取ることができるので、make.keysはそれほど重要ではありませんが、ドキュメンテーションのために残してあります。項目名を指定するには、item.labels値を使用するか、データファイルの名前またはスコアリングされるデータファイルのcolnamesを最初の（nvars）位置に置く必要があります。番号（位置）で指定する場合、nvarsは、使用される項目の数だけでなく、スコアリングされるオブジェクトの項目の総数になります。  makePositiveKeysは、(bestScalesなどから)キーの部分集合を取得し、正と負で別々のキーを作成するのに便利です。
少数の基準で多数の項目の相関を探索する場合、最も相関のある項目からスケールを形成するのが便利です（bestScales を参照）。  さまざまな尺度に渡る項目の分布の感覚を得るために、スケール・キーの集合によってグループ化されたそれらの相関（または確率の対数）を表示できます。入力として相関が与えられた場合（raw=FALSE）、基準（列）を持つ相関（行）を表示し、並べるために使用することもできます。
行列またはdata.frame xが与えられた場合、各列のskewまたはkurtosis（skewおよびkurtosisの場合）、またはmardiaの場合の多変量skewおよびkurtosisを求めます。バージョン1.2.3では、skewおよびkurtosisを求める際に、3つの異なるオプションが利用可能です。  これらは、e1071パッケージの歪度（skewness）と尖度（kurtosis）で利用可能な選択肢と一致しています（それぞれの利点についてはJoanes and Gill (1998)を参照してください）。m_r = [sum(X- mx)^r]/n と定義すると、タイプ1は g_1 = m_3/(m_2)^{3/2} と g_2 = m_4/(m_2)^2 -3 で歪度と尖度を求めます。 タイプ2は G1 = g1 * √{n *(n-1)}/(n-2) と G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)) です。  タイプ3は、b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2}、b2 = [(n-1)/n]^{3/2}m_4/m_2^2）である。e1071との整合性、JoanesとGillとの整合性のため、現在は上記のように定義されています。しかし、リビジョン1.0.93から1.2.3では、kurtosiはデフォルトで尖度の不偏推定値を与えます(DeCarlo, 1997)。以前のバージョンでは、偏った推定を行う別の式を使用していました。  (これら2つの式の違いについては、e1071パッケージのkurtosis関数を参照してください。  デフォルトのタイプ1はe1071でタイプ2と呼ばれるものを与えた。  もう1つはタイプ3です)。  以前のリリースとの比較のために、type = 2を指定すると、古い推定値が得られます。  これらのタイプ番号は現在変更されている。  
生データから重回帰や正準相関を計算する方が一般的ですが、相関や共分散の行列から計算することももちろん可能です。  この場合，関数への入力は，x（予測変数），y（基準変数），必要であれば z（共変量）の列番号（または名前）と同様に，正方共分散または相関行列である．入力は， y 変数の集合と x 変数の集合のどちらかで，これは lm の標準的な数式スタイルで記述できる（最後の例を参照）．  この場合、一対またはそれ以上の相互作用（積項）を指定することもできる。  デフォルトでは，積項を見つけるとき，予測変数はゼロ・センタリングされるが（Cohen, Cohen, West and Aiken, 2003），lm の結果やHayes (2013)で議論されている結果と一致させるために，このオプションをオフにできる（zero=FALSE）．  除去される共変量は、数式入力で負の符号を付けるか、z 変数を使用して指定します。  共変量を指定するとき，回帰は，偏微分された変数で行われたように行われることに注意すること．  これは、自由度とR2が、パーティション化された変数の回帰を反映することを意味する。(最後の例を参照) 共変量を使用する場合、従属変数だけでなく、独立変数（part=FALSE、デフォルト、これは偏相関を意味する）または独立変数（part=TRUE または part 別名semi-partial correlations）からも取り除く必要があります。  部分相関と偏相関の違いは、共変量の分散がDVとIVの両方から取り除かれるか（偏相関）、IVだけから取り除かれるか（部分相関）です。  回帰の傾きは変わりませんが、部分相関を使うとDVの分散量（したがって標準誤差）が大きくなります。  共変量に偏相関を使用することは，他の変数の効果を解釈するときに，回帰で共変量を使用することと同じである．追加出力は，Cohenの集合相関 (Cohen, 1982)を用いて発見されたR2である．  Cohen (1982) は、2つの変数集合の間の全体的な関係を測定する重相関の多変量一般化である集合相関を導入しました。これは正準相関 (Hotelling, 1936) と 1 - ∏(1-ρ_i^2) の応用で、ρ_i^2 は2乗正準相関です。  集合相関は、2つの変数の集合の間の共有分散（R2）の量である。  3番目の共変量集合を追加すると、集合相関は、多変量R2、部分R2、準部分R2を見つけることができます（準部分と2部分オプションはまだ実装されていません）。2つの集合間の R2 は， R2 = 1- |R| /(|Ry| * |Rx|) で，ここで R は x と y 変数の完全な相関行列で，Rx と Ry は関係する2つの集合である．  代替のT2 は，相加分散の割合で，正準の2乗の平均である．  (Cohen et al., 2003), Cramer and Nicewander (1979) も参照。  この平均は、非常に小さな正準相関を含むので、小さすぎる傾向がある。  Cohenらの警告は適切である：「しかし、最終的な分析においては、分析者は、関連性の尺度の選択において、手元の問題に対する実質的かつ方法論的な概念に導かれなければならない。( p613).さらに、2つの集合間の関連性のもう1つの尺度は、2つの集合間の単純な重みなしの相関である。つまり、Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1'))であり、Rxyは2つの集合間の相関の行列である。  これは各行列の相関の単純な(重み付けされていない)和である。この手法は、線形モデルのロバストな美しさを例証し、xとyの両方が1次元の場合に特に適切で、ベータが符号で異なる項目の場合は、大幅に過小評価されます。αで行われるように、重みなし相関を求めるとき、項目はすべて正符号になるように反転されます。  SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa,ICLUST,principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、setCorを使用して複数のR手続きで使用することができます。行列全体は相関が欠損していてもかまいませんが、予測に使用される行列のサブセットの相関は存在しなければなりません。オブザベーションの数が入力されると、従来の信頼区間、統計的有意性、および縮小推定値が報告されます。入力が長方形（正方形ではない）の場合、相関または共分散がデータから求められます。print関数はベータ重みについてt値とp値を報告し、summary関数はベータ重みだけを報告します。  つまり、VIF > 10は、共線性を定義する魔法のカットオフではない。  crossValidationは、setCorまたはbestScalesの結果を取り出し、その重みを別のデータセットに適用するために使用できます。matPlotは、crossValidationの値をプロットするために使用できます（x軸にラベルを指定してmatplotを呼び出すだけです）。matPlotは、凡例を描画し、線種を指定できるように改良されました。setCorLookupは、ベータ重みをソートし、辞書が与えられた場合、項目の内容とともに報告します。matRegは、主にmediateのヘルパー関数ですが、共分散行列と指定されたx、y、z変数を与える一般的な重回帰関数です。その出力は、ベータ、se、t、p、R2です。  matRegは、データ行列では動作せず、数式入力も受け付けません。  matRegは、データ行列では動作せず、数式入力も受け付けません。
因子分析の出力は、各変数の最大の因子負荷のサイズによってソートされ、そして行列の項目はそれらの負荷によって整理されます。  デフォルトでは、第1因子への負荷量によってソートされます。  代替では，任意のベクトルまたは行列に基づいて並べ替えることができる．
生データから重回帰や正準相関を計算することがより一般的ですが、相関や共分散の行列から計算することももちろん可能です。  この場合，関数への入力は，x（予測変数），y（基準変数），必要であれば z（共変量）の列番号（または名前）と同様に，正方共分散または相関行列である．入力は， y 変数の集合と x 変数の集合のどちらかで，これは lm の標準的な数式スタイルで記述できる（最後の例を参照）．  この場合、一対またはそれ以上の相互作用（積項）を指定することもできる。  デフォルトでは，積項を見つけるとき，予測変数はゼロ・センタリングされるが（Cohen, Cohen, West and Aiken, 2003），lm の結果やHayes (2013)で議論されている結果と一致させるために，このオプションをオフにできる（zero=FALSE）．  除去される共変量は、数式入力で負の符号を付けるか、z 変数を使用して指定します。  共変量を指定するとき，回帰は，偏微分された変数で行われたように行われることに注意すること．  これは、自由度とR2が、パーティション化された変数の回帰を反映することを意味する。(最後の例を参照) 共変量を使用する場合、従属変数だけでなく、独立変数（part=FALSE、デフォルト、これは偏相関を意味する）または独立変数（part=TRUE または part 別名semi-partial correlations）からも取り除く必要があります。  部分相関と偏相関の違いは、共変量の分散がDVとIVの両方から取り除かれるか（偏相関）、IVだけから取り除かれるか（部分相関）です。  回帰の傾きは変わりませんが、部分相関を使うとDVの分散量（したがって標準誤差）が大きくなります。  共変量に偏相関を使用することは，他の変数の効果を解釈するときに，回帰で共変量を使用することと同じである．追加出力は，Cohenの集合相関 (Cohen, 1982)を用いて発見されたR2である．  Cohen (1982) は、2つの変数集合の間の全体的な関係を測定する重相関の多変量一般化である集合相関を導入しました。これは正準相関 (Hotelling, 1936) と 1 - ∏(1-ρ_i^2) の応用で、ρ_i^2 は2乗正準相関です。  集合相関は、2つの変数の集合の間の共有分散（R2）の量である。  3番目の共変量集合を追加すると、集合相関は、多変量R2、部分R2、準部分R2を求めます（準部分と2部分オプションはまだ実装されていません）。 集合相関の詳細は、Cohen (1982)、Cohen (1988)、Cohen, Cohen, Aiken and West (2003)にあります。2つの集合間の R2 は， R2 = 1- |R| /(|Ry| * |Rx|) で，ここで R は x と y 変数の完全な相関行列で，Rx と Ry は関係する2つの集合である．  代替のT2 は，相加分散の割合で，正準の2乗の平均である．  (Cohen et al., 2003), Cramer and Nicewander (1979) も参照。  この平均は、非常に小さな正準相関を含むので、小さすぎる傾向がある。  Cohenらの警告は適切である：「しかし、最終的な分析においては、分析者は、関連性の尺度の選択において、手元の問題に対する実質的かつ方法論的な概念に導かれなければならない。( p613).さらに、2つの集合間の関連性のもう1つの尺度は、2つの集合間の単純な重みなしの相関である。つまり、Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1'))であり、Rxyは2つの集合間の相関の行列である。  これは各行列の相関の単純な(重み付けされていない)和である。この手法は、線形モデルのロバストな美しさを例証し、xとyの両方が1次元の場合に特に適切で、ベータが符号で異なる項目の場合は、大幅に過小評価されます。αで行われるように、重みなし相関を求めるとき、項目はすべて正符号になるように反転されます。  SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa,ICLUST,principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、setCorを使用して複数のR手続きで使用することができます。行列全体は相関が欠損していてもかまいませんが、予測に使用される行列のサブセットの相関は存在しなければなりません。オブザベーションの数が入力されると、従来の信頼区間、統計的有意性、および縮小推定値が報告されます。入力が長方形（正方形ではない）の場合、相関または共分散がデータから求められます。print関数はベータ重みについてt値とp値を報告し、summary関数はベータ重みだけを報告します。  つまり、VIF > 10は、共線性を定義する魔法のカットオフではない。  crossValidationは、setCorまたはbestScalesの結果を取り出し、その重みを別のデータセットに適用するために使用できます。matPlotは、crossValidationの値をプロットするために使用できます（x軸にラベルを指定してmatplotを呼び出すだけです）。matPlotは、凡例を描画し、線種を指定できるように改良されました。setCorLookupは、ベータ重みをソートし、辞書が与えられた場合、項目の内容とともに報告します。matRegは、主にmediateのヘルパー関数ですが、共分散行列と指定されたx、y、z変数を与える一般的な重回帰関数です。その出力は、ベータ、se、t、p、R2です。  matRegは、データ行列では動作せず、数式入力も受け付けません。  matRegは、データ行列では動作せず、計算式の入力も受け付けません。
生データから重回帰や正準相関を計算する方が一般的ですが、相関や共分散の行列から計算することももちろん可能です。  この場合，関数への入力は，x（予測変数），y（基準変数），そして必要であれば z（共変量）の列番号（または名前）と同様に，正方共分散または相関行列である．入力は， y 変数の集合と x 変数の集合のどちらかで，これは lm の標準的な数式スタイルで記述できる（最後の例を参照）．  この場合、一対またはそれ以上の相互作用（積項）を指定することもできる。  デフォルトでは，積項を見つけるとき，予測変数はゼロ・センタリングされるが（Cohen, Cohen, West and Aiken, 2003），lm の結果または Hayes (2013)で議論されている結果と一致させるために，このオプションをオフ（zero=FALSE）にできる．  除去される共変量は、数式入力で負の符号を付けるか、z 変数を使用して指定します。  共変量を指定するとき，回帰は，偏微分された変数で行われたように行われることに注意すること．  これは、自由度とR2が、パーティション化された変数の回帰を反映することを意味する。(最後の例を参照) 共変量を使用する場合、従属変数だけでなく、独立変数（part=FALSE、デフォルト、これは偏相関を意味する）または独立変数（part=TRUE または part 別名semi-partial correlations）からも取り除く必要があります。  部分相関と偏相関の違いは、共変量の分散がDVとIVの両方から取り除かれるか（偏相関）、IVだけから取り除かれるか（部分相関）です。  回帰の傾きは変わりませんが、部分相関を使うとDVの分散量（したがって標準誤差）が大きくなります。  共変量に偏相関を使用することは，他の変数の効果を解釈するときに，回帰で共変量を使用することと同じである．追加出力は，Cohenの集合相関 (Cohen, 1982)を用いて発見されたR2である．  Cohen (1982) は、2つの変数集合の間の全体的な関係を測定する重相関の多変量一般化である集合相関を導入しました。これは正準相関 (Hotelling, 1936) と 1 - ∏(1-ρ_i^2) の応用で、ρ_i^2 は2乗正準相関です。  集合相関は、2つの変数の集合の間の共有分散（R2）の量である。  3番目の共変量集合を追加すると、集合相関は、多変量R2、部分R2、準部分R2を求めます（準部分と2部分オプションはまだ実装されていません）。 集合相関の詳細は、Cohen (1982)、Cohen (1988)、Cohen, Cohen, Aiken and West (2003)にあります。2つの集合間の R2 は， R2 = 1- |R| /(|Ry| * |Rx|) で，ここで R は x と y 変数の完全な相関行列で，Rx と Ry は関係する2つの集合である．  代替のT2 は，相加分散の割合で，正準の2乗の平均である．  (Cohen et al., 2003), Cramer and Nicewander (1979) も参照。  この平均は、非常に小さな正準相関を含むので、小さすぎる傾向がある。  Cohenらの警告は適切である：「しかし、最終的な分析においては、分析者は、関連性の尺度の選択において、手元の問題に対する実質的かつ方法論的な概念に導かれなければならない。( p613).さらに、2つの集合間の関連性のもう1つの尺度は、2つの集合間の単純な重みなしの相関である。つまり、Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1'))であり、Rxyは2つの集合間の相関の行列である。  これは各行列の相関の単純な(重み付けされていない)和である。この手法は、線形モデルのロバストな美しさを例証し、xとyの両方が1次元の場合に特に適切で、ベータが符号で異なる項目の場合は、大幅に過小評価されます。αで行われるように、重みなし相関を求めるとき、項目はすべて正符号になるように反転されます。  SAPAプロジェクトでの典型的な使用法は、クラスタリングまたは因数分解（fa,ICLUST,principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、setCorを使用して複数のR手続きで使用することができます。行列全体は相関が欠損していてもかまいませんが、予測に使用される行列のサブセットの相関は存在しなければなりません。オブザベーションの数が入力されると、従来の信頼区間、統計的有意性、および縮小推定値が報告されます。入力が長方形（正方形ではない）の場合、相関または共分散がデータから求められます。print関数はベータ重みについてt値とp値を報告し、summary関数はベータ重みだけを報告します。  つまり、VIF > 10は、共線性を定義する魔法のカットオフではない。  crossValidationは、setCorまたはbestScalesの結果を取り出し、その重みを別のデータセットに適用するために使用できます。matPlotは、crossValidationの値をプロットするために使用できます（x軸にラベルを指定してmatplotを呼び出すだけです）。matPlotは、凡例を描画し、線種を指定できるように改良されました。setCorLookupは、ベータ重みをソートし、辞書が与えられた場合、項目の内容とともに報告します。matRegは、主にmediateのヘルパー関数ですが、共分散行列と指定されたx、y、z変数を与える一般的な重回帰関数です。その出力は、ベータ、se、t、p、R2です。  matRegは、データ行列では動作せず、数式入力も受け付けません。  matRegは、データ行列では動作せず、数式入力も受け付けません。
因子分析の出力は、各変数の最大の因子負荷のサイズによってソートされ、そして行列の項目はそれらの負荷によって整理されます。  デフォルトでは、第1因子への負荷量によってソートされます。  代替では、任意のベクトルまたは行列に基づいて並べ替えることができます。
線形モデリングを行うとき，メディエータの間接効果をコントロールする予測変数の直接効果を推定することは，しばしば便利である．  媒介に関する徹底的な議論は，Preacher and Hayes (2004)を参照．  mediate関数は、ブートストラップされた媒介/媒介効果の信頼区間を持つ、いくつかの基本的な媒介および媒介モデルを実行します。2つの予測変数 X と M、および基準変数 Yの場合、パスcでラベルされたYでのXの直接効果は、Mでのxの効果（パスa）とYでのMの効果（パスb）によって媒介されると言われます。  この部分効果（a b）は，X -c-> Yの直接効果を媒介すると言われる： X -a -> M -b-> Y with X -c'-> Y where c' = c - ab.媒介効果の有意性の検定は，データの多数の無作為再標本（置換あり）をブートストラップすることによって行われる．  媒介効果については，X -> Yの関係でのZの媒介効果は，XとZの（中心化された）積を取り，このXZ項を回帰に加えることによって見つけられる．デフォルトでは、モデレーション（積項）を行う前に、データはゼロ・センタリングされます。  これは，Cohen, Cohen, West and Aiken (2003)のアドバイスに従うものである．  しかし、Hayes (2013)で報告された分析に同意するために、データをゼロ・センタリングしないようにzero=FALSEオプションを設定することができます。   変数をパーシャル・アウトするには、それらを z 項で定義するか、またはmode:y1 ~ x1 + x2 + (m1)+ (m2) -z という式で負の項目として表現すると、z がパーシャル・アウトされた後、m1 と m2 を介したyへのx1 と x2 の効果を探索します。  y1 ~ x1 + x2*x3 + (m1)+ (m2) -z は、x1、x2、x3、およびx2とx3の積のyへの効果を探索し、zがパーティション除去された後、m1とm2を通して媒介される。  相関行列だけが提供された場合、ブートストラップされた値は、正規誤差を加えた元の共分散/相関行列に一致するデータからのブートストラップに基づいています。  これにより、生データが与えられない場合でも、媒介/媒介効果を検定することができます。  この関数は、Hayes (2013)のいくつかの基本的なケースと例、および関連するデータセットに対してテストされています。因果経路を直接区別できるような時間的要素がない限り（時間は方向を逆転させない）、媒介モデルの解釈には問題があります。因果経路（矢印）が逆になっている媒介モデル間の違いを比較することが有用だと考える人もいる。  これは間違いであり、行うべきではない（Thoemmes, 2015）。グラフィック出力のサイズを微調整するために、mediate.diagram関数でxlimとymlimを指定することができる。それ以外の場合、mediateとmoderateが生成するグラフィックは、デフォルトのxlimとymlimの値を使用します。相互作用項（moderation）またはmediated moderationは、積項として指定することができます。
線形モデリングを行うとき，メディエータの間接効果をコントロールする予測変数の直接効果を推定することは，しばしば便利である．  媒介に関する徹底的な議論は，Preacher and Hayes (2004)を参照．  mediate関数は、ブートストラップされた媒介/媒介効果の信頼区間を持つ、いくつかの基本的な媒介および媒介モデルを実行します。2つの予測変数 X と M、および基準変数 Yの場合、パスcでラベルされたYでのXの直接効果は、Mでのxの効果（パスa）とYでのMの効果（パスb）によって媒介されると言われます。  この部分効果（a b）は，X -c-> Yの直接効果を媒介すると言われる： X -a -> M -b-> Y with X -c'-> Y where c' = c - ab.媒介効果の有意性の検定は，データの多数の無作為再標本（置換あり）をブートストラップすることによって行われる．  媒介効果については，X -> Yの関係でのZの媒介効果は，XとZの（中心化された）積を取り，このXZ項を回帰に加えることによって見つけられる．デフォルトでは、モデレーション（積項）を行う前に、データはゼロ・センタリングされます。  これは，Cohen, Cohen, West and Aiken (2003)のアドバイスに従うものである．  しかし，Hayes (2013)で報告された分析に同意するために，データをゼロ・センター化しないようにzero=FALSEオプションを設定できる．   変数をパーシャル・アウトするには、それらを z 項で定義するか、またはmode:y1 ~ x1 + x2 + (m1)+ (m2) -z という式で負の項目として表現すると、z がパーシャル・アウトされた後、m1 と m2 を介したyへのx1 と x2 の効果を探索します。  y1 ~ x1 + x2*x3 + (m1)+ (m2) -z は、x1、x2、x3、およびx2とx3の積のyへの効果を探索し、zがパーティション除去された後、m1とm2を通して媒介される。  相関行列だけが提供された場合、ブートストラップされた値は、正規誤差を加えた元の共分散/相関行列に一致するデータからのブートストラップに基づいています。  これにより、生データが与えられない場合でも、媒介/媒介効果を検定することができます。  この関数は、Hayes (2013)のいくつかの基本的なケースと例、および関連するデータセットに対してテストされています。因果経路を直接区別できるような時間的要素がない限り（時間は方向を逆転させない）、媒介モデルの解釈には問題があります。因果経路（矢印）が逆になっている媒介モデル間の違いを比較することが有用だと考える人もいる。  これは間違いであり、行うべきではない（Thoemmes, 2015）。グラフィック出力のサイズを微調整するために、mediate.diagram関数でxlimとymlimを指定することができる。それ以外の場合、mediateとmoderateによって生成されるグラフィックは、デフォルトのxlimとymlimの値を使用する。相互作用項（moderation）またはmediated moderationは、積項として指定することができる。
楕円の寸法は、x変数とy変数の相関から計算され、sqrt(1+r)とsqrt(1-r)としてスケーリングされます。そして、size[1]およびsize[2]標準偏差単位としてスケーリングされる。   95%および99%の信頼度でスケーリングするには、c(1.64,2.32) を使用します。
この関数は、Synthetic Apeture Personality Assessment (SAPA) (https://www.sapa-project.org/)のデータセットの一部として特に有用で、連続変数（年齢、SAT V、SAT Qなど）と、International Personality Item Pool (IPIP)から取得された多値パーソナリティ項目、およびSAPAの一部として開発された二値実験IQ項目が混合されている（例えば、Revelle, Wilt and Rosenthal, 2010またはRevelle, Dworak and Condon, 2020を参照）。  これは非常に計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなりのスピードアップが可能である。(タイミング比較については注を参照。)これは、ポリコレやテトラコレを行う際に使用するコア数を調整するものである。最も速度が向上するのは、1コアから2コアにした場合です。  4コアにすると66％、8コアにすると75％の節約になるようだ。  irt.faを使った項目反応分析は、内部的に一貫性のある尺度を開発するために、多項目と二項項目で別々に行うことができる。この関数はJohn Foxのpolychorパッケージのhetcor関数ほど柔軟ではない。この関数は、John Foxのpolychorパッケージのhetcor関数ほど柔軟ではありません。変数をデータの種類（連続、多値、二値）ごとに整理することができることに注意してください。これは単純にc, p, dを指定することで行えます。これは、サブセットのためにカテゴリ数が限られている連続変数の場合に有利です。これは、まず変数のタイプを識別し、それらをタイプ（連続、多値、2値）ごとに整理し、適切な相関関数を呼び出し、そして結果の行列を結合します。   
この関数は、Synthetic Apeture Personality Assessment (SAPA) (https://www.sapa-project.org/)のデータセットの一部として特に有用で、連続変数（年齢、SAT V、SAT Qなど）と、International Personality Item Pool (IPIP)から取得した多値パーソナリティ項目、およびSAPAの一部として開発された二値実験IQ項目が混在している（例えば、Revelle, Wilt and Rosenthal, 2010またはRevelle, Dworak and Condon, 2020を参照）。  これは非常に計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなりのスピードアップが可能である。(タイミング比較については注を参照。)これは、ポリコレやテトラコレを行う際に使用するコア数を調整するものである。最も速度が向上するのは、1コアから2コアにした場合です。  4コアにすると66％、8コアにすると75％の節約になるようだ。  irt.faを使った項目反応分析は、内部的に一貫性のある尺度を開発するために、多項目と二項項目で別々に行うことができる。この関数はJohn Foxのpolychorパッケージのhetcor関数ほど柔軟ではない。この関数は、John Foxのpolychorパッケージのhetcor関数ほど柔軟ではありません。変数をデータの種類（連続、多値、二値）ごとに整理することができることに注意してください。これは単純にc, p, dを指定することで行えます。これは、サブセットのためにカテゴリ数が限られている連続変数の場合に有利です。これは、まず変数のタイプを識別し、それらをタイプ（連続、多値、2値）ごとに整理し、適切な相関関数を呼び出し、そして結果の行列を結合します。   
古典的な信頼性理論は，被験者によって異なる真のスコアによるオブザベーションの集合の分散の量を推定する．  一般化可能性理論は、このモデルを他の分散源、特に時間を含むように拡張します。  このアプローチを用いた古典的な研究は、複数の項目で複数の時点にわたって測定された人々である。  そこで問題となるのは、様々な個人差がどの程度安定しているかということである。クラス内相関(ICC)は、各被験者について項目ごとに、また各被験者について時間ごとに求められます。アルファ信頼度は、各被験者について、時系列にわたる項目で求められる。   さらに重要なことは、人、項目、時間、およびそれらの相互作用の分散成分が、古典的な分散分析（aov）またはマルチレベル混合効果モデリング（lme）によって発見されることです。  そして、これらは、一般化可能性のいくつかの異なる推定値を形成するために使用される。   これらの手順に関する非常に思慮深い議論は、ShroutとLaneの章にある。  分散成分は、個人間分散σ^2_P、項目間分散σ^2_I、経時的分散σ^2_T、およびそれらの交互作用です。そして、RKFは全項目、全時間にわたる全評価の平均の信頼性である(固定時間効果)。(Shrout and Lane, 式6）：Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))全項目にわたる1時点の汎化度(ランダム時間効果)は、R1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane式7にSean Laneによる補正を加えたもの)全項目にわたる平均時点の一般化可能性(ランダム効果)。(Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n. I n.T))Generalizability (ランダム効果).I n.T))変化得点の一般化可能性(Shrout and Lane, 方程式9)RC = (σ^2_PT)/(σ^2_PT + σ^2_e/(n. I)).もし計画が完全に交差していると考えられるなら、aov または lmerのどちらかが分散の成分を推定するために使用できる。  欠損データがなく、バランスのとれた計画では、これらは同じ答えを与えます。しかし、aovは欠損データで破綻し、大規模な問題では非常に遅く、メモリを非常に消費するようです（2.8 GHZ Intel Core I7のMac Powerbookで、88時点、3項目の209ケースで5,919秒）。lmerはこの設計を処理しますが、aovアプローチほど遅くありません（88時点、3項目の209ケースで242秒）。   計画が交差ではなく入れ子になっていると考えられる場合、分散成分はnlmeのlme関数を用いて求められます。これは非常に高速です（88時点、3項目の114ケースで3.5秒）。入れ子デザインは、Kランダム効果Nested (Shrout and Lane, equation 10):RKkRN=(σ^2_P)/(σ^2_P+σ^2_{T(P)}/n.p+σ^2_e/(n.I n.T))の一般化可能性につながり、そして最後に、項目にわたって平均された個人間差の信頼性につながります。  (ShroutとLane、式11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))残念ながら、入れ子分析を行うとき、lmeは収束に失敗するという不愉快なエラーを出すことがある。  これを解決するには、lmeをオフにしてlmerだけを使うと解決するようです（つまり、lme=FALSEとlmer=TRUEを設定します）。  (lmeはコアRの一部であり、その名前空間は心理学のロード時に自動的にアタッチされます）。多くの問題では、lmerは必要ないのでロードされません。  しかし、役に立つこともあります。  lmerを使うにはlme4パッケージがインストールされている必要がある。  lmerがインストールされ、要求されれば自動的にロードされる。薄い」パッケージを作るという観点から、lmerは必須ではなく推奨されている。  ワイド形式の場合は、グループ化変数、'time'変数、列番号または項目名を指定する。(最初の例を参照）。  ロング・フォーマットの場合、従属変数の列（名前または番号）を指定します。  (2番目の例を参照)mlArrangeはワイドなdata.frameを受け取り、それを格子xyplotに適した'長い'data.frameに整理します。  これはスタックに代わる便利な方法で、特にアンバランスなデザインに適しています。  ワイドなデータフレームは、grp（通常、被験者ID）、Time（通常、時間変化変数だが、何でもよい）ごとに編成された長いデータフレームに再編成され、各人、各時間内の項目をスタックする。  余分な変数は持ち越され、適切なgroupとTimeにマッチングされる。したがって、もしk個の項目に対するt個の時間ポイントでのN人の被験者がいて、N * t行のワイドフォーマットで、各行がk個の項目とe個の追加情報を持つ場合、N x t * k行×4 + e列のデータフレームが得られます。  長い出力の最初の4列は、id、時間、値、項目名で、残りの列は余分な値です。  mlArrangeは、各被験者のt個の時間次元にk個の項目をプロットします。
古典的な信頼性理論は，被験者によって異なる真のスコアによるオブザベーションの集合の分散の量を推定する．  一般化可能性理論は、このモデルを拡張して、他の分散源、特に時間を含める。  このアプローチを用いた古典的な研究は、複数の項目で複数の時点にわたって測定された人々である。  そこで問題となるのは、様々な個人差がどの程度安定しているかということである。クラス内相関(ICC)は、各被験者について項目ごとに、また各被験者について時間ごとに求められます。アルファ信頼度は、各被験者について、時系列にわたる項目で求められる。   さらに重要なことは、人、項目、時間、およびそれらの相互作用の分散成分が、古典的な分散分析（aov）またはマルチレベル混合効果モデリング（lme）によって発見されることです。  そして、これらは、一般化可能性のいくつかの異なる推定値を形成するために使用される。   これらの手順に関する非常に思慮深い議論は、ShroutとLaneの章にある。  分散成分は、個人間分散σ^2_P、項目間分散σ^2_I、経時的分散σ^2_T、およびそれらの交互作用です。そして、RKFは全項目、全時間にわたる全評価の平均の信頼性である(固定時間効果)。(Shrout and Lane, 式6）：Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))全項目にわたる1時点の汎化度(ランダム時間効果)は、R1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane式7にSean Laneによる補正を加えたもの)全項目にわたる平均時点の一般化可能性(ランダム効果)。(Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n. I n.T))Generalizability (ランダム効果).I n.T))変化得点の一般化可能性(Shrout and Lane, 方程式9)RC = (σ^2_PT)/(σ^2_PT + σ^2_e/(n. I)).もし計画が完全に交差していると考えられるなら、aov または lmerのどちらかが分散の成分を推定するために使用できる。  欠損データがなく、バランスのとれた計画では、これらは同じ答えを与えます。しかし、aovは欠損データで破綻し、大規模な問題では非常に遅く、メモリを非常に消費するようです（2.8 GHZ Intel Core I7搭載のMac Powerbookで、88時点、3項目の209ケースで5,919秒）。lmerはこの設計を処理しますが、aovアプローチほど遅くありません（88時点、3項目の209ケースで242秒）。   計画が交差ではなく入れ子になっていると考えられる場合、分散成分はnlmeのlme関数を用いて求められます。これは非常に高速です（88時点、3項目の114ケースで3.5秒）。入れ子デザインは、Kランダム効果Nested (Shrout and Lane, equation 10):RKkRN=(σ^2_P)/(σ^2_P+σ^2_{T(P)}/n.p+σ^2_e/(n.I n.T))の一般化可能性につながり、そして最後に、項目にわたって平均された個人間差の信頼性につながります。  (ShroutとLane、式11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))残念ながら、入れ子分析を行うとき、lmeは収束に失敗するという不愉快なエラーを出すことがある。  これを解決するには、lmeをオフにしてlmerだけを使うと解決するようです（つまり、lme=FALSEとlmer=TRUEを設定します）。  (lmeはコアRの一部であり、その名前空間は心理学のロード時に自動的にアタッチされます）。多くの問題では、lmerは必要ないのでロードされません。  しかし、役に立つこともあります。  lmerを使うにはlme4パッケージがインストールされている必要がある。  lmerがインストールされ、要求されれば自動的にロードされる。薄い」パッケージを作るという観点から、lmerは必須ではなく推奨されている。  ワイド形式の場合は、グループ化変数、'time'変数、列番号または項目名を指定する。(最初の例を参照）。  ロング・フォーマットの場合、従属変数の列（名前または番号）を指定します。  (2番目の例を参照)mlArrangeはワイドなdata.frameを受け取り、それを格子xyplotに適した'長い'data.frameに整理します。  これはスタックに代わる便利な方法で、特にアンバランスなデザインに適しています。  ワイドなデータフレームは、grp（通常、被験者ID）、Time（通常、時間変化変数だが、何でもよい）ごとに編成された長いデータフレームに再編成され、各人、各時間内の項目をスタックする。  余分な変数は持ち越され、適切なgroupとTimeにマッチングされる。したがって、もしk個の項目に対するt個の時間ポイントにN人の被験者がいて、N * t行のワイドフォーマットで、各行がk個の項目とe個の追加情報を持つ場合、N x t * k行×4 + e列のデータフレームが得られます。  長い出力の最初の4列は、id、時間、値、項目名で、残りの列は余分な値です。  mlArrangeは、各被験者のt個の時間次元にk個の項目をプロットします。
古典的な信頼性理論は，被験者によって異なる真のスコアによるオブザベーションの集合の分散の量を推定する．  一般化可能性理論は、このモデルを拡張して、他の分散源、特に時間を含める。  このアプローチを用いた古典的な研究は、複数の項目で複数の時点にわたって測定された人々である。  そこで問題となるのは、様々な個人差がどの程度安定しているかということである。クラス内相関(ICC)は、各被験者について項目ごとに、また各被験者について時間ごとに求められます。アルファ信頼度は、各被験者について、時系列にわたる項目で求められる。   さらに重要なことは、人、項目、時間、およびそれらの相互作用の分散成分が、古典的な分散分析（aov）またはマルチレベル混合効果モデリング（lme）によって発見されることです。  そして、これらは、一般化可能性のいくつかの異なる推定値を形成するために使用される。   これらの手順に関する非常に思慮深い議論は、ShroutとLaneの章にある。  分散成分は、個人間分散σ^2_P、項目間分散σ^2_I、経時的分散σ^2_T、およびそれらの交互作用です。そして、RKFは全項目、全時間にわたる全評価の平均の信頼性である(固定時間効果)。(Shrout and Lane, 式6）：Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))全項目にわたる1時点の汎化可能性(ランダム時間効果)は、R1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane式7にSean Laneによる補正を加えたもの)全項目にわたる平均時点の一般化可能性(ランダム効果)。(Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n. I n.T))Generalizability (ランダム効果).I n.T))変化得点の一般化可能性(Shrout and Lane, 方程式9)RC = (σ^2_PT)/(σ^2_PT + σ^2_e/(n. I)).もし計画が完全に交差していると考えられるなら、aov または lmerのどちらかが分散の成分を推定するために使用できる。  欠損データがなく、バランスのとれた計画では、これらは同じ答えを与えます。しかし、aovは欠損データで破綻し、大規模な問題では非常に遅く、メモリを非常に消費するようです（2.8 GHZ Intel Core I7搭載のMac Powerbookで、88時点、3項目の209ケースで5,919秒）。lmerはこの設計を処理しますが、aovアプローチほど遅くありません（88時点、3項目の209ケースで242秒）。   計画が交差ではなく入れ子になっていると考えられる場合、分散成分はnlmeのlme関数を用いて求められます。これは非常に高速です（88時点、3項目の114ケースで3.5秒）。入れ子デザインは、Kランダム効果Nested (Shrout and Lane, equation 10):RKkRN=(σ^2_P)/(σ^2_P+σ^2_{T(P)}/n.p+σ^2_e/(n.I n.T))の一般化可能性につながり、そして最後に、項目にわたって平均された個人間差の信頼性につながります。  (ShroutとLane、式11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))残念ながら、入れ子分析を行うとき、lmeは収束に失敗するという不愉快なエラーを出すことがある。  これを解決するには、lmeをオフにしてlmerだけを使うと解決するようです（つまり、lme=FALSEとlmer=TRUEを設定します）。  (lmeはコアRの一部であり、その名前空間は心理学のロード時に自動的にアタッチされます）。多くの問題では、lmerは必要ないのでロードされません。  しかし、役に立つこともあります。  lmerを使うにはlme4パッケージがインストールされている必要がある。  lmerがインストールされ、要求されれば自動的にロードされる。薄い」パッケージを作るという観点から、lmerは必須ではなく推奨されている。  ワイド形式の場合は、グループ化変数、'time'変数、列番号または項目名を指定する。(最初の例を参照）。  ロング・フォーマットの場合、従属変数の列（名前または番号）を指定します。  (2番目の例を参照)mlArrangeはワイドなdata.frameを受け取り、それを格子xyplotに適した'長い'data.frameに整理します。  これはスタックに代わる便利な方法で、特にアンバランスなデザインに適しています。  ワイドなデータフレームは、grp（通常、被験者ID）、Time（通常、時間変化変数だが、何でもよい）ごとに編成された長いデータフレームに再編成され、各人、各時間内の項目をスタックする。  余分な変数は持ち越され、適切なgroupとTimeにマッチングされる。したがって、もしk個の項目に対するt個の時間ポイントにN人の被験者がいて、N * t行のワイドフォーマットで、各行がk個の項目とe個の追加情報を持つ場合、N x t * k行×4 + e列のデータフレームが得られます。  長い出力の最初の4列は、id、時間、値、項目名で、残りの列は余分な値です。  mlArrangeは、各被験者のt個の時間次元にk個の項目をプロットします。
線形モデリングを行うとき、メディエータの間接効果をコントロールする予測変数の直接効果を推定することは、しばしば便利です。  媒介に関する徹底的な議論は，Preacher and Hayes (2004)を参照．  mediate関数は、ブートストラップされた媒介/媒介効果の信頼区間を持つ、いくつかの基本的な媒介および媒介モデルを実行します。2つの予測変数 X と M、および基準変数 Yの場合、パスcでラベルされたYでのXの直接効果は、Mでのxの効果（パスa）とYでのMの効果（パスb）によって媒介されると言われます。  この部分効果（a b）は，X -c-> Yの直接効果を媒介すると言われる： X -a -> M -b-> Y with X -c'-> Y where c' = c - ab.媒介効果の有意性の検定は，データの多数の無作為再標本（置換あり）をブートストラップすることによって行われる．  媒介効果については，X -> Yの関係でのZの媒介効果は，XとZの（中心化された）積を取り，このXZ項を回帰に加えることによって見つけられる．デフォルトでは、モデレーション（積項）を行う前に、データはゼロ・センタリングされます。  これは，Cohen, Cohen, West and Aiken (2003)のアドバイスに従うものである．  しかし，Hayes (2013)で報告された分析に同意するために，データをゼロ・センター化しないようにzero=FALSEオプションを設定できる．   変数をパーシャル・アウトするには、それらを z 項で定義するか、またはmode:y1 ~ x1 + x2 + (m1)+ (m2) -z という式で負の項目として表現すると、z がパーシャル・アウトされた後、m1 と m2 を介したyへのx1 と x2 の効果を探索します。  y1 ~ x1 + x2*x3 + (m1)+ (m2) -z は、x1、x2、x3、およびx2とx3の積のyへの効果を探索し、zがパーティション除去された後、m1とm2を通して媒介される。  相関行列だけが提供された場合、ブートストラップされた値は、正規誤差を加えた元の共分散/相関行列に一致するデータからのブートストラップに基づいています。  これにより、生データが与えられない場合でも、媒介/媒介効果を検定することができます。  この関数は、Hayes (2013)のいくつかの基本的なケースと例、および関連するデータセットに対してテストされています。因果経路を直接区別できるような時間的要素がない限り（時間は方向を逆転させない）、媒介モデルの解釈には問題があります。因果経路（矢印）が逆になっている媒介モデル間の違いを比較することが有用だと考える人もいる。  これは間違いであり、行うべきではない（Thoemmes, 2015）。グラフィック出力のサイズを微調整するために、mediate.diagram関数でxlimとymlimを指定することができる。それ以外の場合、mediateとmoderateによって生成されるグラフィックスは、デフォルトのxlimとymlimの値を使用します。相互作用項（moderation）またはmediated moderationは、積項として指定することができます。
被験者内の複数の尺度を調査するとき、すべての試行間の変動に加えて、試行ごとのオブザベーションの変動を考慮することが有用な場合があります。  連続した差の2乗平均 (mssd) および連続した差の2乗平均 (rmssd) は、 σ^2 = Σ(x_i - x_{i+1})^2 /(n-lag) ここで n-lag が使用されているのは、n-lag のケースしかないからです。被験者（グループ）ごとに複数のオブザベーションを持つ複数の被験者（グループ）の場合、グルーピング変数を指定すると、各グループの出力が得られます。   同様の関数は、matrixStats パッケージでも利用可能です。ただし、そのパッケージの varDiff 関数は、MeanSquare ではなく差の分散です。これは単にdiff関数からの結果に分散と標準偏差を適用したものです。おそらく気分を研究するときに便利なautoR関数は、指定されたラグについて各項目の自己相関を求めます。  また、rmssd(root means square successive difference)も返します。これはラグデータの相関を求めることによって行われる。
diagram関数は、適合入力のクラスに応じて、fa.diagram、omega.diagram、ICLUST.diagram、lavaan.diagram、またはbassAckward.diagramを呼び出します。  残りの関数は、fa.diagram、structure.diagram、omega.diagram、ICLUST.diagram、het.diagramで使用されるグラフィック・プリミティブです。これらの関数は、テキストを囲む矩形、楕円、三角形を作成し、それらを直線または曲線の矢印に接続します。描画を高速化するために、dia.rectとdia.arrowは実際の描画を抑制し、描画する位置と値を返すことができます。  これらの値は、textやrectから行列入力で直接呼び出すことができる。これは、多くの変数を処理する際のスピードアップにつながります。multi.rect、multi.self、multi.arrow、multi.curved.arrow関数は、適切なプリミティブから保存された出力を受け取り、それらを一度に描画します。各図形（楕円、矩形、三角形）には、矢印をつなぐのに使える左右、上下、中心の座標があります。曲線は両頭の矢印です。   デフォルトでは、ある位置から別の位置へと進み、左右（上下に進む場合）または上下（左から右に進む場合）にカーブします。  ヘルパー関数は、Rgraphvizとgraphvizをインストールする際に発生する不具合を回避するために開発されました。 これらの関数は、fa.diagram,het.diagramの中核を成しています。これらの関数が改良されるにつれて、より良いドキュメントが追加される予定です。  dia.coneは、因子不定性の問題を示すために、（オプションで）矢印を辺と中心とする円錐を描画します。
diagram関数は、フィット入力のクラスに応じて、fa.diagram、omega.diagram、ICLUST.diagram、lavaan.diagram、またはbassAckward.diagramを呼び出します。  残りの関数は、fa.diagram、structure.diagram、omega.diagram、ICLUST.diagram、het.diagramで使用されるグラフィック・プリミティブです。これらの関数は、テキストを囲む矩形、楕円、三角形を作成し、それらを直線または曲線の矢印に接続します。描画を高速化するために、dia.rectとdia.arrowは実際の描画を抑制し、描画する位置と値を返すことができます。  これらの値は、textやrectから行列入力で直接呼び出すことができる。これは、多くの変数を処理する際のスピードアップにつながります。multi.rect、multi.self、multi.arrow、multi.curved.arrow関数は、適切なプリミティブから保存された出力を受け取り、それらを一度に描画します。各図形（楕円、矩形、三角形）には、矢印をつなぐのに使える左右、上下、中心の座標があります。曲線は両頭の矢印です。   デフォルトでは、ある位置から別の位置へと進み、左右（上下に進む場合）または上下（左から右に進む場合）にカーブします。  ヘルパー関数は、Rgraphvizとgraphvizをインストールする際に発生する不具合を回避するために開発されました。 これらの関数は、fa.diagram,het.diagramの中核を成しています。これらの関数が改良されるにつれて、より良いドキュメントが追加される予定です。  dia.coneは、因子不定性の問題を示すために、（オプションで）矢印を辺と中心とする円錐を描画します。
これにより、複数の分布を素早く要約することができます。  特に、信頼性関数から得られる複数に分割された半分の結果を調べるときに便利です。  デフォルトでは，行と列の数が等しい正方形のプロットを作成しようとする．  しかし、列と行の数は、特定のプロットに対して指定することができる。
diagram関数は、適合入力のクラスに応じて、fa.diagram、omega.diagram、ICLUST.diagram、lavaan.diagram、またはbassAckward.diagramを呼び出します。  残りの関数は、fa.diagram、structure.diagram、omega.diagram、ICLUST.diagram、het.diagramで使用されるグラフィック・プリミティブです。これらの関数は、テキストを囲む矩形、楕円、三角形を作成し、それらを直線または曲線の矢印に接続します。描画を高速化するために、dia.rectとdia.arrowは実際の描画を抑制し、描画する位置と値を返すことができます。  これらの値は、textやrectから行列入力で直接呼び出すことができる。これは、多くの変数を処理する際のスピードアップにつながります。multi.rect、multi.self、multi.arrow、multi.curved.arrow関数は、適切なプリミティブから保存された出力を受け取り、それらを一度に描画します。各図形（楕円、矩形、三角形）には、矢印をつなぐのに使える左右、上下、中心の座標があります。曲線は両頭の矢印です。   デフォルトでは、ある位置から別の位置へと進み、左右（上下に進む場合）または上下（左から右に進む場合）にカーブします。  ヘルパー関数は、Rgraphvizとgraphvizをインストールする際に発生する不具合を回避するために開発されました。 これらの関数は、fa.diagram,het.diagramの中核を成しています。これらの関数が改良されるにつれて、より良いドキュメントが追加される予定です。  dia.coneは、因子不定性の問題を示すために、（オプションで）矢印を辺と中心とする円錐を描画します。
diagram関数は、フィット入力のクラスに応じて、fa.diagram、omega.diagram、ICLUST.diagram、lavaan.diagram、またはbassAckward.diagramを呼び出します。  残りの関数は、fa.diagram、structure.diagram、omega.diagram、ICLUST.diagram、het.diagramで使用されるグラフィック・プリミティブです。これらの関数は、テキストを囲む矩形、楕円、三角形を作成し、それらを直線または曲線の矢印に接続します。描画を高速化するために、dia.rectとdia.arrowは実際の描画を抑制し、描画する位置と値を返すことができます。  これらの値は、textやrectから行列入力で直接呼び出すことができる。これは、多くの変数を処理する際のスピードアップにつながります。multi.rect、multi.self、multi.arrow、multi.curved.arrow関数は、適切なプリミティブから保存された出力を受け取り、それらを一度に描画します。各図形（楕円、矩形、三角形）には、矢印をつなぐのに使える左右、上下、中心の座標があります。曲線は両頭の矢印です。   デフォルトでは、ある位置から別の位置へと進み、左右（上下に進む場合）または上下（左から右に進む場合）にカーブします。  ヘルパー関数は、Rgraphvizとgraphvizをインストールする際に発生する不具合を回避するために開発されました。 これらの関数は、fa.diagram,het.diagramの中核を成しています。これらの関数が改良されるにつれて、より良いドキュメントが追加される予定です。  dia.coneは、因子不確定性の問題を示すために、（オプションで）矢印を辺と中心とする円錐を描画します。
古典的な信頼性理論は、被験者によって異なる真のスコアに起因するオブザベーションの集合における分散の量を推定します。  一般化可能性理論は、このモデルを他の分散源、特に時間を含むように拡張します。  このアプローチを用いた古典的な研究は、複数の項目で複数の時点にわたって測定された人々である。  そこで問題となるのは、様々な個人差がどの程度安定しているかということである。クラス内相関(ICC)は、各被験者について項目ごとに、また各被験者について時間ごとに求められます。アルファ信頼度は、各被験者について、時系列にわたる項目で求められる。   さらに重要なことは、人、項目、時間、およびそれらの相互作用の分散成分が、古典的な分散分析（aov）またはマルチレベル混合効果モデリング（lme）によって発見されることです。  そして、これらは、一般化可能性のいくつかの異なる推定値を形成するために使用される。   これらの手順に関する非常に思慮深い議論は、ShroutとLaneの章にある。  分散成分は、個人間分散σ^2_P、項目間分散σ^2_I、経時的分散σ^2_T、およびそれらの交互作用です。そして、RKFは全項目、全時間にわたる全評価の平均の信頼性である(固定時間効果)。(Shrout and Lane, 式6）：Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))全項目にわたる1時点の汎化可能性(ランダム時間効果)は、R1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane式7にSean Laneによる補正を加えたもの)全項目にわたる平均時点の一般化可能性(ランダム効果)。(Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n. I n.T))Generalizability (ランダム効果).I n.T))変化得点の一般化可能性(Shrout and Lane, 方程式9)RC = (σ^2_PT)/(σ^2_PT + σ^2_e/(n. I)).もし計画が完全に交差していると考えられるなら、aov または lmerのどちらかが分散の成分を推定するために使用できる。  欠損データがなく、バランスのとれた計画では、これらは同じ答えを与えます。しかし、aovは欠損データで破綻し、大規模な問題では非常に遅く、メモリを非常に消費するようです（2.8 GHZ Intel Core I7搭載のMac Powerbookで、88時点、3項目の209ケースで5,919秒）。lmerはこの設計を処理しますが、aovアプローチほど遅くありません（88時点、3項目の209ケースで242秒）。   計画が交差ではなく入れ子になっていると考えられる場合、分散成分はnlmeのlme関数を用いて求められます。これは非常に高速です（88時点、3項目の114ケースで3.5秒）。入れ子デザインは、Kランダム効果Nested (Shrout and Lane, equation 10):RKkRN=(σ^2_P)/(σ^2_P+σ^2_{T(P)}/n.p+σ^2_e/(n.I n.T))の一般化可能性につながり、そして最後に、項目にわたって平均された個人間差の信頼性につながります。  (ShroutとLane、式11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))残念ながら、入れ子分析を行うとき、lmeは収束に失敗するという不愉快なエラーを出すことがある。  これを解決するには、lmeをオフにしてlmerだけを使うと解決するようです（つまり、lme=FALSEとlmer=TRUEを設定します）。  (lmeはコアRの一部であり、その名前空間は心理学のロード時に自動的にアタッチされます）。多くの問題では、lmerは必要ないのでロードされません。  しかし、役に立つこともあります。  lmerを使うにはlme4パッケージがインストールされている必要がある。  lmerがインストールされ、要求されれば自動的にロードされる。薄い」パッケージを作るという観点から、lmerは必須ではなく推奨されている。  ワイド形式の場合は、グループ化変数、'time'変数、列番号または項目名を指定する。(最初の例を参照）。  ロング・フォーマットの場合、従属変数の列（名前または番号）を指定します。  (2番目の例を参照)mlArrangeはワイドなdata.frameを受け取り、それを格子xyplotに適した'長い'data.frameに整理します。  これはスタックに代わる便利な方法で、特にアンバランスなデザインに適しています。  ワイドなデータフレームは、grp（通常、被験者ID）、Time（通常、時間変化変数であるが、何でもよい）ごとに編成された長いデータフレームに再編成され、各人と時間内の項目を積み重ねます。  余分な変数は持ち越され、適切なgroupとTimeにマッチングされる。したがって、もしk個の項目に対するt個の時間ポイントにN人の被験者がいて、N * t行のワイドフォーマットで、各行がk個の項目とe個の追加情報を持つとすると、N x t * k行×4 + e列のデータフレームが得られます。  長い出力の最初の4列は、id、時間、値、項目名で、残りの列は余分な値です。  mlArrangeは、各被験者のt個の時間次元にk個の項目をプロットします。
lowerCor は、対角行列の下側を丸め、列名を桁数+3文字に省略したものを出力しますが、丸めない完全な行列も返します。  デフォルトでは，変数のペアワイズ削除が使用される．  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
因子分析から最も解釈しやすい因子の数を決定することは、おそらく因子分析における最大の課題の1つです。  この問題には多くの解決策がありますが、どれも一様に最良とは言えません。  "因子数の問題を解くのは簡単だ、毎日朝食前にやっている"。  しかし、正しい解を知ることは難しい。(Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  3) 実データの固有値が、同じサイズのランダムなデータ集合の対応する固有値よりも小さくなるまで因子を抽出する（並列分析） fa.parallel.4連続する固有値の大きさをプロットし、scree検定（山の距骨の斜面をよじ登り、岩肌に近づいたときに見られる斜面の変化に類似した固有値の急激な低下）を適用する5) 固有値＜1になるまで主成分を抽出する6) 間隔がある限り因子を抽出する7) Very Simple Structure Criterion (VSS)を使用する8) Wayne VelicerのMinimum Average Partial (MAP)基準を使用する。それぞれの手続きには長所と短所がある。  カイ2乗検定または2乗変化検定は，もちろん被験者数に敏感で，多くの因子を見つけたい場合は，単に被験者を増やせばよいという無意味な状態になる．並列分析は，大きな標本ではランダム因子の固有値が非常に小さくなるという点で，標本サイズに部分的に敏感である．  scree検定は非常に魅力的ですが、screeがいつ "壊れる "かについての解釈の相違につながる可能性があります。解釈可能な因子を抽出することは、因子の数がデータよりも調査者の創造性を反映することを意味します。  VSSは、理解するのは非常に簡単ですが、データが因子的に非常に複雑な場合は、あまりうまく機能しません。(ほとんどの因子分析のユーザーは、すべての変数について最大の負荷量に注意を集中し、より小さな負荷量を無視することによって、因子出力を解釈する傾向があります。  非常に単純な構造は，オリジナルの相関行列を，オリジナルの因子行列(F)の単純化バージョン(S)によって再現されるものと比較することによって，この傾向を運用する．  R = SS' + U2.   S は，各変数の（絶対値で）最大 c 個の負荷量だけで構成される．  C （または複雑度）は，モデルのパラメータで，1から因子の数まで変化する．  VSS 基準は，元の相関に対する単純化モデルの適合を比較する．VSS = 1 -sumsquares(r*)/sumsquares(r) ここで R* は残差行列 R* = R - SS'であり、r* と r はそれぞれ R* と R の要素である。与えられた複雑度に対するVSSは、最適な（最も解釈しやすい）因子の数でピークに達する傾向がある(Revelle and Rocklin, 1979)。VSSはもともとメインフレームコンピュータ用のFortranで書かれたが、Pascalを使ってマイクロコンピュータ（例えばMacintosh OS 6-9）に適応されている。今回、VSSを計算するためのRコードを公開します。相関行列(例えばmy.matrix)を使って因子分析を行う場合、パラメータn.obsは因子分析のために指定する必要があることに注意してください：例えば、呼び出しはVSS(my.matrix,n.obs=500)です。  そうでない場合、デフォルトは1000になります。Wayne VelicerのMAP基準は、抽出する最適な成分数の追加テストとして追加されました。  nfactors関数は、VSSを行い、MAPを求め、他の多くの基準（例えば、BIC、複雑度、カイ2乗、...）を報告します。さまざまな回転オプションが利用できます。これらには、varimax、promax、obliminが含まれる。他にも追加可能です。  提案を歓迎します。
"多くの尺度は、その開発者や使用者によって、主に1つの潜在変数の測定であると仮定されています。尺度が測定の効果指標モデルに適合することも仮定されている場合（心理アセスメントではほとんど常にそうです）、その尺度の内部構造に関する証拠でそのような解釈をサポートすることが重要です。特に、そのような尺度の内部構造に関連する2つの特性を調べることが重要です。最初の特性は、尺度を形成するすべての指標が共通に潜在変数を測定しているかどうかに関係します。2番目の内部構造特性は、（指標の合計または平均から得られる）尺度スコアにおける分散のうち、すべての指標に共通するこの潜在変数が占める割合に関係します（Cronbach, 1951; McDonald, 1999; Revelle, 1979）。すなわち，効果指標尺度が，尺度を形成するすべての指標に共通する1つの潜在変数の測定が主であるなら，その潜在変数が尺度得点の分散の大部分を占めるはずである．別の言い方をすると，この分散比率は，指標のサンプリングから生じるすべての指標に共通する潜在変数での個人の立ち位置を推定するとき（すなわち，Lord, 1956の用語を使用すると，タイプ2またはタイプ12サンプリングのいずれかを扱うとき），サンプリングの変動に関する重要な情報を提供する．つまり、この分散比率は、尺度スコアと、尺度指標がサブセットである指標の無限の宇宙のすべての指標に共通する潜在変数との間の相関の2乗として解釈できます。さらに別の言い方をすると、この分散比率は信頼性係数としても有効性係数としても重要です。これは信頼性の問題で、この分散比が大きければ大きいほど、個人の観察された尺度スコアに基づいて、すべての尺度の指標に共通する潜在変数における個人の相対的な地位をより正確に予測できるからです。同時に、構成概念の妥当性は尺度の内部構造を包含しているため、この分散比は尺度の構成概念の妥当性にも影響します。(Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonaldは、テストの一般的な因子飽和の推定値として係数ω_h (ω_h)を提唱しています。  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf は，McDonaldの ω_hをCronbachの αとRevelleの βと比較して， ω_hが最良の推定であると結論している．(Zinbarg et al., 2006とRevelle and Zinbarg (2009)も参照）。   ω_hを求める1つの方法は、元のデータ集合の因子分析を行い、因子を斜めに回転させ、その相関行列を因子化し、Schmid-Leiman (schmid)変換を行って一般的な因子負荷量を求め、それからω_hを求めることです。  omega_hは、因子の推定方法によって異なります。  4つのオプションが利用可能で、3つはfa関数を使用しますが、ファクタリングの方法が異なります：デフォルトはminres因子解を行い、fm="pa "は主軸因子分析を行い、fm="mle "は最尤解を行い、fm="pc "は(principal)を使用して主成分分析を行います。  能力項目については、すべての項目が一般因子に正の負荷量を持つことが一般的です。  しかし、非認知的項目の場合、ある項目は正に得点され、ある項目は負に得点されることがよくあります。  おそらく、キー・ベクトルを指定することによって、項目がどの方向に得点されるかを指定する方がよいですが、 flip =TRUE (デフォルト)の場合、項目は一般因子に正の負荷量を持つように反転されます。  キーは scoreItems 関数を使用してスコアを見つけられるように報告されます。  このように項目を恣意的に反転させると、一般因子を過大評価する可能性があります。(シミュレートされたcircumplexの例を参照) ω_hの代替であるベータは，最悪のスプリット・ハーフ信頼性として定義される(Revelle, 1979)．  これはICLUST（元々はメインフレーム用に開発され、Fortranで書かれた階層クラスタリング・アルゴリズムで、現在ではpsychパッケージの一部となっている。  (ICLUSTアルゴリズムが尺度構築に有用である理由については、Cooksey and Soutar, 2005を参照）。ω関数は、ω_h係数を推定するために探索的因子分析を使用します。  ω_hを推定するために選ばれた手法に関係なく、耳を傾けるべき推奨は、ω_hを推定する前に、推定された一般因子負荷量のパターンを常に調査することである」ということを覚えておくことが重要です。このような検査は、EFAの文脈でも実施できる、尺度のすべての指標に共通する潜在変数があるという仮定の非公式なテストを構成します。もし負荷量が指標の比較的小さなサブセットでのみ顕著であったなら、これは共分散行列の根底に真の一般因子がないことを示唆します。このような非公式な仮定テストがあるだけで、ここで報告されたシミュレーションで時折生成される誤解を招くω_h推定値を誤って解釈する可能性から大いに保護することができたであろう。(Zinbargら, 2006, p 137).2つのグループ因子のうちの1つだけを反映したω推定値の問題の簡単な実証は、最後の例で見つけることができます。  ω解の質を反映する診断統計量は、他の固有値に対するg因子固有値の相対的な大きさの比較、一般因子分散（p2）である各項目の共通分散のパーセンテージ、p2の平均、およびp2の標準偏差を含みます。  omega_hは、3つ以上のサブファクターが抽出された場合のみ一意に定義されますが、2因子解を持つことが望まれることもあります。  デフォルトでは、2つのサブファクターが等しい負荷量を持つものとしてschmid抽出を強制することで、これを実現します。  この条件には3つのオプションがあります：2つの低次因子間の一般因子の負荷量を "等しい "と設定し、これはsqrt(因子間の斜め相関)になるか、または "第1 "か "第2 "に設定します。モデルが本当によく定義されていないことを示唆するメッセージが発行されます。この解決策は、Zinbarg et al., 2007で議論されている。  omegaでこれを行うには、callにoption="first" または option="second "を追加します。明らかに1因子解では意味がありませんが、もちろん、第1（そして唯一の）因子への負荷量の合計を求め、それらを2乗し、行列全体の分散と比較することは可能です。  ω_hに加えて、McDonaldの係数のもう1つはω_tである。  これはテストの全信頼性の推定値です。マクドナルドのω_tは、Guttmanの λ_6, guttmanに似ていますが、e_j^2を求めるために因子分析からの一意性の推定値（u^2）を用います。これは、テスト得点の分散V_xを4つの部分に分解することに基づきます：一般因子による分散、(vec{g})、グループ因子の集合による分散、(vec{f})（いくつかの項目には共通するが、すべての項目には共通しない因子）、各項目に固有な固有因子、(vec{s})、そしてランダム誤差の(vec{e})です。  (特異的分散は、テストが少なくとも2回行われない限り、無作為誤差と区別できないので、これらの両方を誤差にまとめるものもある)。x = cg + Af + Ds + et とすると、一般因子とグループ因子に基づく item_j の共同性 h_j^2 = c_j^2 + sum(f_ij^2)と、項目の固有分散 u_j^2 = σ_j^2 (1-h_j^2)は、テストの信頼性を推定するために使用できます。すなわち、h_j^2が項目_jの共同性であり、一般因子とグループ因子に基づくとすると、標準化項目については、e_j^2 = 1 - h_j^2 および ω_t = (1 cc' 1 + 1 AA' 1')/(V_x)h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.ここでMcDonald, 1978の2つのω係数とMcDonald, 1999の式6.20aのω_tとω_hを区別することが重要である。  ω_h = (1 cc' 1')/VxAnother estimate is the omega for an infinite length test with a similar structure with the observed test (ωH漸近法)．  これは ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1') によって求められる。Steve Reiseの提案に従い，説明される共通分散（ECV）も報告される．  これは、すべての固有値の合計に対する一般因子の固有値の比率である。  omegaへの入力は，相関行列または生データ行列，または因子間相関（Phi）行列を持つ因子パターン行列である．  omegaは、Schmid-Leiman変換を使用する探索的因子分析関数です。omegaSemは、最初にomegaを呼び出し、Schmid-Leiman解を受け取り、これを確認的semモデルに変換し、確認的モデルを実施するためにsemパッケージを呼び出します。行儀のよい問題では、EFA解とCFA解は実質的に同じになりますが、CFA解は必ずしもEFA解と一致しません。特に、推定R^2が1を超えることがあります（この例は、Harman 24の認知能力問題です）。さらに、すべてのEFA解が実行可能なCFA解を生成するわけではありません。  モデル仕様の誤りは、非常に奇妙なCFA推定値につながります。オメガに因子パターン行列と関連する因子間相関を与えることも可能です。  この場合、分析はこれらの行列に対して行われます。  これは、探索的EFAの解や回転オプションに満足できず、どうにか代替案を考え出した場合に特に便利です。(たとえば、指定されたm値でカイザー正規化されたPromax解を用いて、fm='pa'を用いたEFAを行いたい場合があります。) omegaFromSem は、 sem モデルからの出力を取り、それを用いて ω_h を求めます。  変数と因子の多重R^2によって求められる因子不確定性の推定値は、EFAモデルによって求められるものとは一致しません。  特に，推定 R^2 は，1 を超えることがある（この例は，Harman 24 の認知能力の問題である）．Ωの概念は，全体的なテストだけでなく，個々の因子にも適用できる．オメガの典型的な使用法は、インベントリ全体の下位尺度を識別することである。  その変動の一部は、インベントリの一般的な因子によるものであり、一部は各下位尺度の特定の分散によるものである。  したがって，いくつかの異なるオメガの推定を見つけることができる：各サブファクターで識別された項目の分散の何パーセントが，実際に一般因子によるものなのか．  どの分散が共通であるが、サブファクターに固有であるか、そして、各サブファクターの合計信頼できる分散は何か。   これらの結果は，omega.groupオブジェクトと通常出力の最後の数行で報告される．最後に，まだテスト中であるが，Waller (2017)から適応されたomegaDirectである．  これは階層因数分解(directSl)を行わず、Schmid-Leimanのような解への直接回転です。  この回転はオメガの観点から解釈される。  これは、代替手順omegaやomegaSemとの比較を可能にするために、ここに含まれています。  予備的な分析によると、一般因子がない場合には不適切な解を生成することが示唆されています。Moral: omega_hを見つけるのは厄介で、違いを理解するためには、omega、omegaSem、omegaDirect、そしてiclustの解を比較する必要があるでしょう。omegaから返される様々なオブジェクトは以下の通り：
omega.graphはRgraphvizパッケージを必要としますが、omega.diagramは必要としません。codeomegaはGPArotationパッケージを必要とします。
omega.graphはRgraphvizパッケージを必要とするが、omega.diagramは必要としない。codeomegaはGPArotationパッケージを必要とする。
"多くの尺度は、開発者やユーザによって、主に1つの潜在変数の尺度であると仮定されています。また、尺度が測定の効果指標モデルに準拠していると仮定される場合（心理アセスメントではほとんど常にそうです）、その尺度の内部構造に関する証拠でそのような解釈をサポートすることが重要です。特に、そのような尺度の内部構造に関連する2つの特性を調べることが重要です。最初の特性は、尺度を形成するすべての指標が共通に潜在変数を測定しているかどうかに関係します。2番目の内部構造特性は、（指標の合計または平均から得られる）尺度スコアにおける分散のうち、すべての指標に共通するこの潜在変数が占める割合に関係します（Cronbach, 1951; McDonald, 1999; Revelle, 1979）。すなわち，効果指標尺度が，尺度を形成するすべての指標に共通する1つの潜在変数の測定が主であるなら，その潜在変数が尺度得点の分散の大部分を占めるはずである．別の言い方をすると、この分散比率は、指標のサンプリングから生じるすべての指標に共通する潜在変数での個人の立ち位置を推定するとき（すなわち、Lord, 1956の用語を使用すると、タイプ2またはタイプ12サンプリングのいずれかを扱うとき）、サンプリングの変動に関する重要な情報を提供します。つまり、この分散比率は、尺度スコアと、尺度指標がサブセットである指標の無限の宇宙のすべての指標に共通する潜在変数との間の相関の2乗として解釈できます。さらに別の言い方をすると、この分散比率は信頼性係数としても有効性係数としても重要です。これは信頼性の問題で、この分散比が大きければ大きいほど、個人の観察された尺度スコアに基づいて、すべての尺度の指標に共通する潜在変数における個人の相対的な地位をより正確に予測できるからです。同時に、構成概念の妥当性は尺度の内部構造を包含しているため、この分散比は尺度の構成概念の妥当性にも影響します。(Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonaldは、テストの一般的な因子飽和の推定値として係数ω_h (ω_h)を提唱しています。  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf は，McDonaldの ω_hをCronbachの αとRevelleの βと比較して， ω_hが最良の推定であると結論している．(Zinbarg et al., 2006とRevelle and Zinbarg (2009)も参照）。   ω_hを求める1つの方法は、元のデータ集合の因子分析を行い、因子を斜めに回転させ、その相関行列を因子化し、Schmid-Leiman (schmid)変換を行って一般的な因子負荷量を求め、それからω_hを求めることです。  omega_hは、因子の推定方法によって異なります。  4つのオプションが利用可能で、3つはfa関数を使用しますが、ファクタリングの方法が異なります：デフォルトはminres因子解を行い、fm="pa "は主軸因子分析を行い、fm="mle "は最尤解を行い、fm="pc "は(principal)を使用して主成分分析を行います。  能力項目については、すべての項目が一般因子に正の負荷量を持つことが一般的です。  しかし、非認知的項目の場合、ある項目は正に得点され、ある項目は負に得点されることがよくあります。  おそらく、キー・ベクトルを指定することによって、項目がどの方向に得点されるかを指定する方がよいですが、 flip =TRUE (デフォルト)の場合、項目は一般因子に正の負荷量を持つように反転されます。  キーは scoreItems 関数を使用してスコアを見つけられるように報告されます。  このように項目を恣意的に反転させると、一般因子を過大評価する可能性があります。(シミュレートされたcircumplexの例を参照) ω_hの代替であるベータは，最悪のスプリット・ハーフ信頼性として定義される(Revelle, 1979)．  これはICLUST（元々はメインフレーム用に開発され、Fortranで書かれた階層クラスタリング・アルゴリズムで、現在はpsychパッケージの一部となっている。  (ICLUSTアルゴリズムが尺度構築に有用である理由については、Cooksey and Soutar, 2005を参照）。ω関数は、ω_h係数を推定するために探索的因子分析を使用します。  ω_hを推定するために選ばれた手法に関係なく、耳を傾けるべき推奨は、ω_hを推定する前に、推定された一般因子負荷量のパターンを常に検査することである」ということを覚えておくことが重要です。このような検査は、EFAの文脈でも実施できる、尺度のすべての指標に共通する潜在変数があるという仮定の非公式なテストを構成します。もし負荷量が指標の比較的小さなサブセットでのみ顕著であったなら、これは共分散行列の根底に真の一般因子がないことを示唆します。このような非公式な仮定テストがあるだけで、ここで報告されたシミュレーションで時折生成される誤解を招くω_h推定値を誤って解釈する可能性から大いに保護することができたであろう。(Zinbargら, 2006, p 137).2つのグループ因子のうちの1つだけを反映したω推定値の問題の簡単な実証は、最後の例で見つけることができます。  オメガ解の質を反映する診断統計量は、他の固有値に対するg因子固有値の相対サイズの比較、一般因子分散（p2）である各項目の共通分散のパーセンテージ、p2の平均、およびp2の標準偏差を含みます。  omega_hは、3つ以上のサブファクターが抽出された場合にのみ一意に定義されますが、2因子解を持つことが望まれることもあります。  デフォルトでは、2つのサブファクターが等しい負荷量を持つものとしてschmid抽出を強制することで、これを実現します。  この条件には3つのオプションがあります：2つの低次因子間の一般因子の負荷量を "等しい "と設定し、これはsqrt(因子間の斜め相関)となるか、"第1 "または "第2 "と設定し、この場合一般因子は第1または第2のグループ因子と等しくなります。モデルが本当によく定義されていないことを示唆するメッセージが発行されます。この解決法は、Zinbarg et al., 2007で議論されている。  omegaでこれを行うには、callにoption="first" または option="second "を追加します。明らかに1因子解では意味がありませんが、もちろん、第1（そして唯一の）因子への負荷量の合計を求め、それらを2乗し、全体の行列分散と比較することは可能です。  ω_hに加えて、McDonaldの係数のもう1つはω_tである。  これはテストの全信頼性の推定値です。マクドナルドのω_tは、Guttmanの λ_6, guttmanに似ていますが、e_j^2を求めるために因子分析からの一意性の推定値（u^2）を用います。これは、テスト得点の分散V_xを4つの部分に分解することに基づきます：一般因子による分散、(vec{g})、グループ因子の集合による分散、(vec{f})（いくつかの項目には共通するが、すべての項目には共通しない因子）、各項目に固有な固有因子、(vec{s})、そしてランダム誤差の(vec{e})です。  (特異的分散は、テストが少なくとも2回行われない限り、無作為誤差と区別できないので、これらの両方を誤差にまとめるものもある)。x = cg + Af + Ds + et とすると、一般因子とグループ因子に基づく item_j の共同性 h_j^2 = c_j^2 + sum(f_ij^2)と、項目の固有分散 u_j^2 = σ_j^2 (1-h_j^2)は、テストの信頼性を推定するために使用できます。すなわち、h_j^2が項目_jの共同性であり、一般因子とグループ因子に基づくとすると、標準化項目については、e_j^2 = 1 - h_j^2 および ω_t = (1 cc' 1 + 1 AA' 1')/(V_x)h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.ここでMcDonald, 1978の2つのω係数とMcDonald, 1999の式6.20aのω_tとω_hを区別することが重要である。  ω_h = (1 cc' 1')/VxAnother estimate is the omega for an infinite length test with a similar structure with the observed test (ωH漸近)．  これは ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1') によって求められる。Steve Reiseの提案に従い，説明される共通分散（ECV）も報告される．  これは，すべての固有値の合計に対する一般因子の固有値の比率である．  omegaへの入力は，相関行列または生データ行列，または因子間相関（Phi）行列を持つ因子パターン行列である．  omegaは、Schmid-Leiman変換を使用する探索的因子分析関数です。omegaSemは、最初にomegaを呼び出し、Schmid-Leiman解を受け取り、これを確認的semモデルに変換し、確認的モデルを実行するためにsemパッケージを呼び出します。行儀のよい問題では、EFA解とCFA解は実質的に同じになりますが、CFA解は必ずしもEFA解と一致しません。特に、推定R^2が1を超えることがあります（この例は、Harman 24の認知能力問題です）。さらに、すべてのEFA解が実行可能なCFA解を生成するわけではありません。  モデル仕様の誤りは、非常に奇妙なCFA推定値につながります。オメガに因子パターン行列と関連する因子間相関を与えることも可能です。  この場合、分析はこれらの行列に対して行われます。  これは、探索的EFAの解や回転オプションに満足できず、どうにか代替案を考え出した場合に特に便利です。(たとえば、指定されたm値でカイザー正規化されたPromax解を用いて、fm='pa'を用いたEFAを行いたい場合があります。) omegaFromSem は、 sem モデルからの出力を取り、それを用いて ω_h を求めます。  変数と因子の多重R^2によって求められる因子不確定性の推定値は、EFAモデルによって求められるものとは一致しません。  特に，推定 R^2 は，1 を超えることがある（この例は，Harman 24 の認知能力の問題である）．Ωの概念は，全体的なテストだけでなく，個々の因子にも適用できる．オメガの典型的な使用法は、インベントリ全体の下位尺度を識別することである。  その変動の一部は、インベントリの一般的な因子によるものであり、一部は各下位尺度の特定の分散によるものである。  したがって，いくつかの異なるオメガの推定を見つけることができる：各サブファクターで識別された項目の分散の何パーセントが，実際に一般因子によるものなのか．  どの分散が共通であるが、サブファクターに固有であるか、そして、各サブファクターの合計信頼できる分散は何か。   これらの結果は，omega.groupオブジェクトと通常出力の最後の数行で報告される．最後に，まだテスト中であるが，Waller (2017)から適応されたomegaDirectである．  これは階層因数分解(directSl)を行わず、Schmid-Leimanのような解への直接回転です。  この回転はオメガの観点から解釈される。  これは、代替手順omegaやomegaSemとの比較を可能にするために、ここに含まれています。  予備的な分析によると、一般因子がない場合には不適切な解を生成することが示唆されている。omegaから返される様々なオブジェクトは以下の通りです：
"多くの尺度は、開発者やユーザによって、主に1つの潜在変数の尺度であると仮定されています。尺度が測定の効果指標モデルに準拠することも仮定されている場合（心理アセスメントではほとんど常にそうです）、その尺度の内部構造に関する証拠でそのような解釈をサポートすることが重要です。特に、そのような尺度の内部構造に関連する2つの特性を調べることが重要です。最初の特性は、尺度を形成するすべての指標が共通に潜在変数を測定しているかどうかに関係します。2番目の内部構造特性は、（指標の合計または平均から得られる）尺度スコアにおける分散のうち、すべての指標に共通するこの潜在変数が占める割合に関係します（Cronbach, 1951; McDonald, 1999; Revelle, 1979）。すなわち，効果指標尺度が，尺度を形成するすべての指標に共通する1つの潜在変数の測定が主であるなら，その潜在変数が尺度得点の分散の大部分を占めるはずである．別の言い方をすると，この分散比率は，指標のサンプリングから生じるすべての指標に共通する潜在変数での個人の立ち位置を推定するとき（すなわち，Lord, 1956の用語を使用すると，タイプ2またはタイプ12サンプリングのいずれかを扱うとき），サンプリングの変動に関する重要な情報を提供する．つまり、この分散比率は、尺度スコアと、尺度指標がサブセットである指標の無限の宇宙のすべての指標に共通する潜在変数との間の相関の2乗として解釈できます。さらに別の言い方をすると、この分散比率は信頼性係数としても有効性係数としても重要です。これは信頼性の問題で、この分散比が大きければ大きいほど、個人の観察された尺度スコアに基づいて、すべての尺度の指標に共通する潜在変数における個人の相対的な地位をより正確に予測できるからです。同時に、構成概念の妥当性は尺度の内部構造を包含しているため、この分散比は尺度の構成概念の妥当性にも影響します。(Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonaldは、テストの一般的な因子飽和の推定値として係数ω_h (ω_h)を提唱しています。  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf は，McDonaldの ω_hをCronbachの αとRevelleの βと比較して， ω_hが最良の推定であると結論している．(Zinbarg et al., 2006とRevelle and Zinbarg (2009)も参照）。   ω_hを求める1つの方法は、元のデータ集合の因子分析を行い、因子を斜めに回転させ、その相関行列を因子化し、Schmid-Leiman (schmid)変換を行って一般的な因子負荷量を求め、それからω_hを求めることです。  omega_hは、因子の推定方法によって異なります。  4つのオプションが利用可能で、3つはfa関数を使用しますが、ファクタリングの方法が異なります：デフォルトはminres因子解を行い、fm="pa "は主軸因子分析を行い、fm="mle "は最尤解を行い、fm="pc "は(principal)を使用して主成分分析を行います。  能力項目については、すべての項目が一般因子に正の負荷量を持つことが一般的です。  しかし、非認知的項目の場合、ある項目は正に得点され、ある項目は負に得点されることがよくあります。  おそらく、キー・ベクトルを指定することによって、項目がどの方向に得点されるかを指定する方がよいですが、 flip =TRUE (デフォルト)の場合、項目は一般因子に正の負荷量を持つように反転されます。  キーは scoreItems 関数を使用してスコアを見つけられるように報告されます。  このように項目を恣意的に反転させると、一般因子を過大評価する可能性があります。(シミュレートされたcircumplexの例を参照) ω_hの代替であるベータは，最悪のスプリット・ハーフ信頼性として定義される(Revelle, 1979)．  これはICLUST（元々はメインフレーム用に開発され、Fortranで書かれた階層クラスタリング・アルゴリズムで、現在はpsychパッケージの一部となっている。  (ICLUSTアルゴリズムが尺度構築に有用である理由については、Cooksey and Soutar, 2005を参照）。ω関数は、ω_h係数を推定するために探索的因子分析を使用します。  ω_hを推定するために選ばれた手法に関係なく、耳を傾けるべき推奨は、ω_hを推定する前に、推定された一般因子負荷量のパターンを常に調査することである」ということを覚えておくことが重要です。このような検査は、EFAの文脈でも実施できる、尺度のすべての指標に共通する潜在変数があるという仮定の非公式なテストを構成します。もし負荷量が指標の比較的小さなサブセットでのみ顕著であったなら、これは共分散行列の根底に真の一般因子がないことを示唆します。このような非公式な仮定テストがあるだけで、ここで報告されたシミュレーションで時折生成される誤解を招くω_h推定値を誤って解釈する可能性から大いに保護することができたであろう。(Zinbargら, 2006, p 137).2つのグループ因子のうちの1つだけを反映したω推定値の問題の簡単な実証は、最後の例で見つけることができます。  ω解の質を反映する診断統計量は、他の固有値に対するg因子固有値の相対的な大きさの比較、一般因子分散（p2）である各項目の共通分散のパーセンテージ、p2の平均、およびp2の標準偏差を含みます。  omega_hは、3つ以上のサブファクターが抽出された場合のみ一意に定義されますが、2因子解を持つことが望まれることもあります。  デフォルトでは、2つのサブファクターが等しい負荷量を持つものとしてschmid抽出を強制することで、これを実現します。  この条件には3つのオプションがあります：2つの低次因子間の一般因子の負荷量を "等しい "と設定し、これはsqrt(因子間の斜め相関)となるか、"第1 "または "第2 "と設定し、この場合一般因子は第1または第2のグループ因子と等しくなります。モデルが本当によく定義されていないことを示唆するメッセージが発行されます。この解決法は、Zinbarg et al., 2007で議論されている。  omegaでこれを行うには、callにoption="first" または option="second "を追加します。明らかに1因子解では意味がありませんが、もちろん、第1（そして唯一の）因子への負荷量の合計を求め、それらを2乗し、全体の行列分散と比較することは可能です。  ω_hに加えて、McDonaldの係数のもう1つはω_tである。  これはテストの全信頼性の推定値です。マクドナルドのω_tは、Guttmanの λ_6, guttmanに似ていますが、e_j^2を求めるために因子分析からの一意性の推定値（u^2）を用います。これは、テスト得点の分散V_xを4つの部分に分解することに基づきます：一般因子による分散、(vec{g})、グループ因子の集合による分散、(vec{f})（いくつかの項目には共通するが、すべての項目には共通しない因子）、各項目に固有な固有因子、(vec{s})、そしてランダム誤差の(vec{e})です。  (特異的分散は、テストが少なくとも2回行われない限り、無作為誤差と区別できないので、これらの両方を誤差にまとめるものもある)。x = cg + Af + Ds + et とすると、一般因子とグループ因子に基づく item_j の共同性 h_j^2 = c_j^2 + sum(f_ij^2)と、項目の固有分散 u_j^2 = σ_j^2 (1-h_j^2)は、テストの信頼性を推定するために使用できます。すなわち、h_j^2が項目_jの共同性であり、一般因子とグループ因子に基づくとすると、標準化項目については、e_j^2 = 1 - h_j^2 および ω_t = (1 cc' 1 + 1 AA' 1')/(V_x)h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.ここでMcDonald, 1978の2つのω係数とMcDonald, 1999の式6.20aのω_tとω_hを区別することが重要である。  ω_h = (1 cc' 1')/VxAnother estimate is the omega for an infinite length test with a similar structure with the observed test (ωH漸近法)．  これは ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1') によって求められる。Steve Reiseの提案に従い，説明される共通分散（ECV）も報告される．  これは，すべての固有値の合計に対する一般因子の固有値の比率である．  omegaへの入力は，相関行列または生データ行列，または因子間相関（Phi）行列を持つ因子パターン行列である．  omegaは、Schmid-Leiman変換を使用する探索的因子分析関数です。omegaSemは、最初にomegaを呼び出し、Schmid-Leiman解を受け取り、これを確認的semモデルに変換し、確認的モデルを実施するためにsemパッケージを呼び出します。行儀のよい問題では、EFA解とCFA解は実質的に同じになりますが、CFA解は必ずしもEFA解と一致しません。特に、推定R^2が1を超えることがあります（この例は、Harman 24の認知能力問題です）。さらに、すべてのEFA解が実行可能なCFA解を生成するわけではありません。  モデル仕様の誤りは、非常に奇妙なCFA推定値につながります。オメガに因子パターン行列と関連する因子間相関を与えることも可能です。  この場合、分析はこれらの行列に対して行われます。  これは、探索的EFAの解や回転オプションに満足できず、どうにか代替案を考え出した場合に特に便利です。(たとえば、指定されたm値でカイザー正規化されたPromax解を用いて、fm='pa'を用いたEFAを行いたい場合があります。) omegaFromSem は、 sem モデルからの出力を取り、それを用いて ω_h を求めます。  変数と因子の多重R^2によって求められる因子不確定性の推定値は、EFAモデルによって求められるものとは一致しません。  特に，推定 R^2 は，1 を超えることがある（この例は，Harman 24 の認知能力の問題である）．Ωの概念は，全体的なテストだけでなく，個々の因子にも適用できる．オメガの典型的な使用法は、インベントリ全体の下位尺度を識別することである。  その変動の一部は、インベントリの一般的な因子によるものであり、一部は各下位尺度の特定の分散によるものである。  したがって，いくつかの異なるオメガの推定を見つけることができる：各サブファクターで識別された項目の分散の何パーセントが，実際に一般因子によるものなのか．  どの分散が共通であるが、サブファクターに固有であるか、そして、各サブファクターの合計信頼できる分散は何か。   これらの結果は，omega.groupオブジェクトと通常出力の最後の数行で報告される．最後に，まだテスト中であるが，Waller (2017)から適応されたomegaDirectである．  これは階層因数分解(directSl)を行わず、Schmid-Leimanのような解への直接回転です。  この回転はオメガの観点から解釈される。  これは、代替手順omegaやomegaSemとの比較を可能にするために、ここに含まれています。  予備的な分析によると、一般因子がない場合には不適切な解を生成することが示唆されています。Moral: omega_hを見つけるのは厄介で、違いを理解するためには、omega、omegaSem、omegaDirect、そしてiclustの解を比較する必要があるでしょう。omegaから返される様々なオブジェクトは以下の通りです：
"多くの尺度は、開発者やユーザによって、主に1つの潜在変数の尺度であると仮定されています。尺度が測定の効果指標モデルに準拠することも仮定されている場合（心理アセスメントではほとんど常にそうです）、その尺度の内部構造に関する証拠でそのような解釈をサポートすることが重要です。特に、そのような尺度の内部構造に関連する2つの特性を調べることが重要です。最初の特性は、尺度を形成するすべての指標が共通に潜在変数を測定しているかどうかに関係します。2番目の内部構造特性は、（指標の合計または平均から得られる）尺度スコアにおける分散のうち、すべての指標に共通するこの潜在変数が占める割合に関係します（Cronbach, 1951; McDonald, 1999; Revelle, 1979）。すなわち，効果指標尺度が，尺度を形成するすべての指標に共通する1つの潜在変数の測定が主であるなら，その潜在変数が尺度得点の分散の大部分を占めるはずである．別の言い方をすると，この分散比率は，指標のサンプリングから生じるすべての指標に共通する潜在変数での個人の立ち位置を推定するとき（すなわち，Lord, 1956の用語を使用すると，タイプ2またはタイプ12サンプリングのいずれかを扱うとき），サンプリングの変動に関する重要な情報を提供する．つまり、この分散比率は、尺度スコアと、尺度指標がサブセットである指標の無限の宇宙のすべての指標に共通する潜在変数との間の相関の2乗として解釈できます。さらに別の言い方をすると、この分散比率は信頼性係数としても有効性係数としても重要です。これは信頼性の問題で、この分散比が大きければ大きいほど、個人の観察された尺度スコアに基づいて、すべての尺度の指標に共通する潜在変数における個人の相対的な地位をより正確に予測できるからです。同時に、構成概念の妥当性は尺度の内部構造を包含しているため、この分散比は尺度の構成概念の妥当性にも影響します。(Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonaldは、テストの一般的な因子飽和の推定値として係数ω_h (ω_h)を提唱しています。  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf は，McDonaldの ω_hをCronbachの αとRevelleの βと比較して， ω_hが最良の推定であると結論している．(Zinbarg et al., 2006とRevelle and Zinbarg (2009)も参照）。   ω_hを求める1つの方法は、元のデータ集合の因子分析を行い、因子を斜めに回転させ、その相関行列を因子化し、Schmid-Leiman (schmid)変換を行って一般的な因子負荷量を求め、それからω_hを求めることです。  omega_hは、因子の推定方法によって異なります。  4つのオプションが利用可能で、3つはfa関数を使用しますが、ファクタリングの方法が異なります：デフォルトはminres因子解を行い、fm="pa "は主軸因子分析を行い、fm="mle "は最尤解を行い、fm="pc "は(principal)を使用して主成分分析を行います。  能力項目については、すべての項目が一般因子に正の負荷量を持つことが一般的です。  しかし、非認知的項目の場合、ある項目は正に得点され、ある項目は負に得点されることがよくあります。  おそらく、キー・ベクトルを指定することによって、項目がどの方向に得点されるかを指定する方がよいですが、 flip =TRUE (デフォルト)の場合、項目は一般因子に正の負荷量を持つように反転されます。  キーは scoreItems 関数を使用してスコアを見つけられるように報告されます。  このように項目を恣意的に反転させると、一般因子を過大評価する可能性があります。(シミュレートされたcircumplexの例を参照) ω_hの代替であるベータは，最悪のスプリット・ハーフ信頼性として定義される(Revelle, 1979)．  これはICLUST（元々はメインフレーム用に開発され、Fortranで書かれた階層クラスタリング・アルゴリズムで、現在はpsychパッケージの一部となっている。  (ICLUSTアルゴリズムが尺度構築に有用である理由については、Cooksey and Soutar, 2005を参照）。ω関数は、ω_h係数を推定するために探索的因子分析を使用します。  ω_hを推定するために選ばれた手法に関係なく、耳を傾けるべき推奨は、ω_hを推定する前に、推定された一般因子負荷量のパターンを常に検査することである」ということを覚えておくことが重要です。このような検査は、EFAの文脈でも実施できる、尺度のすべての指標に共通する潜在変数があるという仮定の非公式なテストを構成します。もし負荷量が指標の比較的小さなサブセットでのみ顕著であったなら、これは共分散行列の根底に真の一般因子がないことを示唆します。このような非公式な仮定テストがあるだけで、ここで報告されたシミュレーションで時折生成される誤解を招くω_h推定値を誤って解釈する可能性から大いに保護することができたであろう。(Zinbargら, 2006, p 137).2つのグループ因子のうちの1つだけを反映したω推定値の問題の簡単な実証は、最後の例で見つけることができます。  ω解の質を反映する診断統計量は、他の固有値に対するg因子固有値の相対的な大きさの比較、一般因子分散（p2）である各項目の共通分散のパーセンテージ、p2の平均、およびp2の標準偏差を含みます。  omega_hは、3つ以上のサブファクターが抽出された場合にのみ一意に定義されますが、2因子解を持つことが望まれることもあります。  デフォルトでは、2つのサブファクターが等しい負荷量を持つものとしてschmid抽出を強制することで、これを実現します。  この条件には3つのオプションがあります：2つの低次因子間の一般因子の負荷量を "等しい "と設定し、これはsqrt(因子間の斜め相関)になるか、または "第1 "か "第2 "に設定します。モデルが本当によく定義されていないことを示唆するメッセージが発行されます。この解決法は、Zinbarg et al., 2007で議論されている。  omegaでこれを行うには、callにoption="first" または option="second "を追加します。明らかに1因子解では意味がありませんが、もちろん、第1（そして唯一の）因子への負荷量の合計を求め、それらを2乗し、行列全体の分散と比較することは可能です。  ω_hに加えて、McDonaldの係数のもう1つはω_tである。  これはテストの全信頼性の推定値です。マクドナルドのω_tは、Guttmanのλ_6、guttmanに似ていますが、e_j^2を求めるために因子分析からの一意性の推定値（u^2）を用います。これは、テスト得点の分散V_xを4つの部分に分解することに基づきます：一般因子による分散、(vec{g})、グループ因子の集合による分散、(vec{f})（いくつかの項目には共通するが、すべての項目には共通しない因子）、各項目に固有な固有因子、(vec{s})、そしてランダム誤差の(vec{e})です。  (特異的分散は、テストが少なくとも2回行われない限り、無作為誤差と区別できないので、これらの両方を誤差にまとめるものもある)。x = cg + Af + Ds + et とすると、一般因子とグループ因子に基づく item_j の共同性 h_j^2 = c_j^2 + sum(f_ij^2)と、項目の固有分散 u_j^2 = σ_j^2 (1-h_j^2)は、テストの信頼性を推定するために使用できます。すなわち、h_j^2が項目_jの共同性であり、一般因子とグループ因子に基づくとすると、標準化項目については、e_j^2 = 1 - h_j^2 および ω_t = (1 cc' 1 + 1 AA' 1')/(V_x)h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.ここでMcDonald, 1978の2つのω係数とMcDonald, 1999の式6.20aのω_tとω_hを区別することが重要である。  ω_h = (1 cc' 1')/VxAnother estimate is the omega for an infinite length test with a similar structure with the observed test (ωH漸近)．  これは ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1') によって求められる。Steve Reiseの提案に従い，説明される共通分散（ECV）も報告される．  これは，すべての固有値の合計に対する一般因子の固有値の比率である．  omegaへの入力は，相関行列または生データ行列，または因子間相関（Phi）行列を持つ因子パターン行列である．  omegaは、Schmid-Leiman変換を使用する探索的因子分析関数です。omegaSemは、最初にomegaを呼び出し、Schmid-Leiman解を受け取り、これを確認的semモデルに変換し、確認的モデルを実施するためにsemパッケージを呼び出します。行儀のよい問題では、EFA解とCFA解は実質的に同じになりますが、CFA解は必ずしもEFA解と一致しません。特に、推定R^2が1を超えることがあります（この例は、Harman 24の認知能力問題です）。さらに、すべてのEFA解が実行可能なCFA解を生成するわけではありません。  モデル仕様の誤りは、非常に奇妙なCFA推定値につながります。オメガに因子パターン行列と関連する因子間相関を与えることも可能です。  この場合、分析はこれらの行列に対して行われます。  これは、探索的EFAの解や回転オプションに満足できず、どうにか代替案を考え出した場合に特に便利です。(たとえば、指定されたm値でカイザー正規化されたPromax解を用いて、fm='pa'を用いたEFAを行いたい場合があります。) omegaFromSem は、 sem モデルからの出力を取り、それを用いて ω_h を求めます。  変数と因子の多重R^2によって求められる因子不確定性の推定値は、EFAモデルによって求められるものとは一致しません。  特に，推定 R^2 は，1 を超えることがある（この例は，Harman 24 の認知能力の問題である）．Ωの概念は，全体的なテストだけでなく，個々の因子にも適用できる．オメガの典型的な使用法は、インベントリ全体の下位尺度を識別することである。  その変動の一部は、インベントリの一般的な因子によるものであり、一部は各下位尺度の特定の分散によるものである。  したがって，いくつかの異なるオメガの推定を見つけることができる：各サブファクターで識別された項目の分散の何パーセントが，実際に一般因子によるものなのか．  どの分散が共通であるが、サブファクターに固有であるか、そして、各サブファクターの合計信頼できる分散は何か。   これらの結果は，omega.groupオブジェクトと通常出力の最後の数行で報告される．最後に，まだテスト中であるが，Waller (2017)から適応されたomegaDirectである．  これは階層因数分解(directSl)を行わず、Schmid-Leimanのような解への直接回転です。  この回転はオメガの観点から解釈される。  これは、代替手順omegaやomegaSemとの比較を可能にするために、ここに含まれています。  予備的な分析によると、一般因子がない場合には不適切な解を生成することが示唆されている。omegaから返される様々なオブジェクトは以下の通りです：
"多くの尺度は、開発者やユーザによって、主に1つの潜在変数の尺度であると仮定されています。尺度が測定の効果指標モデルに準拠することも仮定されている場合（心理アセスメントではほとんど常にそうです）、その尺度の内部構造に関する証拠でそのような解釈をサポートすることが重要です。特に、そのような尺度の内部構造に関連する2つの特性を調べることが重要です。最初の特性は、尺度を形成するすべての指標が共通に潜在変数を測定しているかどうかに関係します。2番目の内部構造特性は、（指標の合計または平均から得られる）尺度スコアにおける分散のうち、すべての指標に共通するこの潜在変数が占める割合に関係します（Cronbach, 1951; McDonald, 1999; Revelle, 1979）。すなわち，効果指標尺度が，尺度を形成するすべての指標に共通する1つの潜在変数の測定が主であるなら，その潜在変数が尺度得点の分散の大部分を占めるはずである．別の言い方をすると，この分散比率は，指標のサンプリングから生じるすべての指標に共通する潜在変数での個人の立ち位置を推定するとき（すなわち，Lord, 1956の用語を使用すると，タイプ2またはタイプ12サンプリングのいずれかを扱うとき），サンプリングの変動に関する重要な情報を提供する．つまり、この分散比率は、尺度スコアと、尺度指標がサブセットである指標の無限の宇宙のすべての指標に共通する潜在変数との間の相関の2乗として解釈できます。さらに別の言い方をすると、この分散比率は信頼性係数としても有効性係数としても重要です。これは信頼性の問題で、この分散比が大きければ大きいほど、個人の観察された尺度スコアに基づいて、すべての尺度の指標に共通する潜在変数における個人の相対的な地位をより正確に予測できるからです。同時に、構成概念の妥当性は尺度の内部構造を包含しているため、この分散比は尺度の構成概念の妥当性にも影響します。(Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonaldは、テストの一般的な因子飽和の推定値として係数ω_h (ω_h)を提唱しています。  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf は，McDonaldの ω_hをCronbachの αとRevelleの βと比較して， ω_hが最良の推定であると結論している．(Zinbarg et al., 2006とRevelle and Zinbarg (2009)も参照）。   ω_hを求める1つの方法は、元のデータ集合の因子分析を行い、因子を斜めに回転させ、その相関行列を因子化し、Schmid-Leiman (schmid)変換を行って一般的な因子負荷量を求め、それからω_hを求めることです。  omega_hは、因子の推定方法によって異なります。  4つのオプションが利用可能で、3つはfa関数を使用しますが、ファクタリングの方法が異なります：デフォルトはminres因子解を行い、fm="pa "は主軸因子分析を行い、fm="mle "は最尤解を行い、fm="pc "は(principal)を使用して主成分分析を行います。  能力項目については、すべての項目が一般因子に正の負荷量を持つことが一般的です。  しかし、非認知的項目の場合、ある項目は正に得点され、ある項目は負に得点されることがよくあります。  おそらく、キー・ベクトルを指定することによって、項目がどの方向に得点されるかを指定する方がよいですが、 flip =TRUE (デフォルト)の場合、項目は一般因子に正の負荷量を持つように反転されます。  キーは scoreItems 関数を使用してスコアを見つけられるように報告されます。  このように項目を恣意的に反転させると、一般因子を過大評価する可能性があります。(シミュレートされたcircumplexの例を参照) ω_hの代替であるベータは，最悪のスプリット・ハーフ信頼性として定義される(Revelle, 1979)．  これはICLUST（元々はメインフレーム用に開発され、Fortranで書かれた階層クラスタリング・アルゴリズムで、現在ではpsychパッケージの一部となっている。  (ICLUSTアルゴリズムが尺度構築に有用である理由については、Cooksey and Soutar, 2005を参照）。ω関数は、ω_h係数を推定するために探索的因子分析を使用します。  ω_hを推定するために選ばれた手法に関係なく、耳を傾けるべき推奨は、ω_hを推定する前に、推定された一般因子負荷量のパターンを常に検査することである」ということを覚えておくことが重要です。このような検査は、EFAの文脈でも実施できる、尺度のすべての指標に共通する潜在変数があるという仮定の非公式なテストを構成します。もし負荷量が指標の比較的小さなサブセットでのみ顕著であったなら、これは共分散行列の根底に真の一般因子がないことを示唆します。このような非公式な仮定テストがあるだけで、ここで報告されたシミュレーションで時折生成される誤解を招くω_h推定値を誤って解釈する可能性から大いに保護することができたであろう。(Zinbargら, 2006, p 137).2つのグループ因子のうちの1つだけを反映したω推定値の問題の簡単な実証は、最後の例で見つけることができます。  オメガ解の品質を反映する診断統計量は、他の固有値に対するg因子固有値の相対サイズの比較、一般因子分散（p2）である各項目の共通分散のパーセンテージ、p2の平均、およびp2の標準偏差を含みます。  omega_hは、3つ以上のサブファクターが抽出された場合のみ一意に定義されますが、2因子解を持つことが望まれることもあります。  デフォルトでは、2つのサブファクターが等しい負荷量を持つものとしてschmid抽出を強制することで、これを実現します。  この条件には3つのオプションがあります：2つの低次因子間の一般因子の負荷量を "等しい "と設定し、これはsqrt(因子間の斜め相関)となるか、"第1 "または "第2 "と設定し、この場合一般因子は第1または第2のグループ因子と等しくなります。モデルが本当によく定義されていないことを示唆するメッセージが発行されます。この解決法は、Zinbarg et al., 2007で議論されている。  omegaでこれを行うには、callにoption="first" または option="second "を追加します。明らかに1因子解では意味がありませんが、もちろん、第1（そして唯一の）因子への負荷量の合計を求め、それらを2乗し、全体の行列分散と比較することは可能です。  ω_hに加えて、McDonaldの係数のもう1つはω_tである。  これはテストの全信頼性の推定値です。マクドナルドのω_tは、Guttmanのλ_6、guttmanに似ていますが、e_j^2を求めるために因子分析からの一意性の推定値（u^2）を用います。これは、テスト得点の分散V_xを4つの部分に分解することに基づきます：一般因子による分散、(vec{g})、グループ因子の集合による分散、(vec{f})（いくつかの項目には共通するが、すべての項目には共通しない因子）、各項目に固有な固有因子、(vec{s})、そしてランダム誤差の(vec{e})です。  (特異的分散は、テストが少なくとも2回行われない限り、無作為誤差と区別できないので、これらの両方を誤差にまとめるものもある)。x = cg + Af + Ds + et とすると、一般因子とグループ因子に基づく item_j の共同性 h_j^2 = c_j^2 + sum(f_ij^2)と、項目の固有分散 u_j^2 = σ_j^2 (1-h_j^2)は、テストの信頼性を推定するために使用できます。すなわち、h_j^2が項目_jの共同性であり、一般因子とグループ因子に基づくとすると、標準化項目については、e_j^2 = 1 - h_j^2 および ω_t = (1 cc' 1 + 1 AA' 1')/(V_x)h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.ここで、McDonald, 1978の2つのω係数とMcDonald, 1999の式6.20aのω_tとω_hを区別することが重要である。  ω_h = (1 cc' 1')/VxAnother estimate is the omega for an infinite length test with a similar structure with the observed test (ωH漸近法)．  これは ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1') によって求められる。Steve Reiseの提案に従い，説明される共通分散（ECV）も報告される．  これは，すべての固有値の合計に対する一般因子の固有値の比率である．  omegaへの入力は，相関行列または生データ行列，または因子間相関（Phi）行列を持つ因子パターン行列である．  omegaは、Schmid-Leiman変換を使用する探索的因子分析関数です。omegaSemは、最初にomegaを呼び出し、Schmid-Leiman解を受け取り、これを確認的semモデルに変換し、確認的モデルを実施するためにsemパッケージを呼び出します。行儀のよい問題では、EFA解とCFA解は実質的に同じになりますが、CFA解は必ずしもEFA解と一致しません。特に、推定R^2が1を超えることがあります（この例は、Harman 24の認知能力問題です）。さらに、すべてのEFA解が実行可能なCFA解を生成するわけではありません。  モデル仕様の誤りは、非常に奇妙なCFA推定値につながります。オメガに因子パターン行列と関連する因子間相関を与えることも可能です。  この場合、分析はこれらの行列に対して行われます。  これは、探索的EFAの解や回転オプションに満足できず、どうにか代替案を考え出した場合に特に便利です。(たとえば、指定されたm値でカイザー正規化されたPromax解を用いて、fm='pa'を用いたEFAを行いたい場合があります。) omegaFromSem は、 sem モデルからの出力を取り、それを用いて ω_h を求めます。  変数と因子の多重R^2によって求められる因子不確定性の推定値は、EFAモデルによって求められるものとは一致しません。  特に，推定 R^2 は，1 を超えることがある（この例は，Harman 24 の認知能力の問題である）．Ωの概念は，全体的なテストだけでなく，個々の因子にも適用できる．オメガの典型的な使用法は、インベントリ全体の下位尺度を識別することである。  その変動の一部は、インベントリの一般的な因子によるものであり、一部は各下位尺度の特定の分散によるものである。  したがって，いくつかの異なるオメガの推定を見つけることができる：各サブファクターで識別された項目の分散の何パーセントが，実際に一般因子によるものなのか．  どの分散が共通であるが、サブファクターに固有であるか、そして、各サブファクターの合計信頼できる分散は何か。   これらの結果は，omega.groupオブジェクトと通常出力の最後の数行で報告される．最後に，まだテスト中であるが，Waller (2017)から適応されたomegaDirectである．  これは階層因数分解(directSl)を行わず、Schmid-Leimanのような解への直接回転です。  この回転はオメガの観点から解釈される。  これは、代替手順omegaやomegaSemとの比較を可能にするために、ここに含まれています。  予備的な分析によると、一般因子がない場合には不適切な解を生成することが示唆されている。omegaから返される様々なオブジェクトは以下の通り：
statsのmahalanobis関数とヘルプページからの引用です。
従来の帰無仮説有意性検定（Null Hypothesis Significance Test: NHST）は、効果がないという帰無仮説が与えられた場合に、そのデータを観測する可能性です。  しかし、これは帰無仮説の確率については何も教えてくれません。  Peter Killeen (2005)は、より有用な尺度として再現確率（probability of replication）を紹介した。  p.repは、観察された統計量の1尾確率値に基づいています。  p.rep.tでは、セルサイズが不均等な場合、効果量の推定値は、調和平均セルサイズに対する平均セルサイズの比率で調整される（Rownow et al., 2000を参照）。   
従来の帰無仮説有意性検定（Null Hypothesis Significance Test: NHST）は、効果がないという帰無仮説が与えられた場合に、データが観察される可能性である。  しかし、これは帰無仮説の確率については何も教えてくれません。  Peter Killeen (2005)は、より有用な尺度として再現確率（probability of replication）を紹介した。  p.repは、観察された統計量の1尾確率値に基づいています。  p.rep.tでは、セルサイズが不均等な場合、効果量の推定値は、調和平均セルサイズに対する平均セルサイズの比率で調整される（Rownow et al., 2000を参照）。   
従来の帰無仮説有意性検定（Null Hypothesis Significance Test: NHST）は、効果がないという帰無仮説が与えられた場合に、データが観察される可能性である。  しかし、これは帰無仮説の確率については何も教えてくれません。  Peter Killeen (2005)は、より有用な尺度として再現確率（probability of replication）を紹介した。  p.repは、観察された統計量の1尾確率値に基づいています。  p.rep.tでは、セルサイズが不均等な場合、効果量の推定値は、調和平均セルサイズに対する平均セルサイズの比率で調整される（Rownow et al., 2000を参照）。   
従来の帰無仮説有意性検定（Null Hypothesis Significance Test: NHST）は、効果がないという帰無仮説が与えられた場合に、データが観察される可能性である。  しかし、これは帰無仮説の確率については何も教えてくれません。  Peter Killeen (2005)は、より有用な尺度として再現確率（probability of replication）を紹介した。  p.repは、観察された統計量の1尾確率値に基づいています。  p.rep.tでは、セルサイズが不均等な場合、効果量の推定値は、調和平均セルサイズに対する平均セルサイズの比率で調整される（Rownow et al., 2000を参照）。   
2つの独立した相関の差のzを求めるには、まずFisher r-z変換を用いてzスコアに変換し、2つの相関の差のzを求める。  デフォルトの仮定は、グループのサイズが同じであるということですが、n2 を指定することにより、異なるサイズのグループでも検定を行うことができます。相関が独立でない場合（すなわち、それらが同じ標本から得られたものである場合）、3番目の変数r(yz)との相関を指定しなければなりません。2つの従属相関の差のt統計量を求めよ。
恥ずかしげもなくpairsのヘルプページから引用しました。  panel.cor、panel.cor.scale、panel.histはすべてpairsのヘルプページから引用しています。pairs.panelは、プロットする変数の数が6-10個程度以下の場合に最も便利です。異なるグループを異なる色で表示するには、21と25の間のプロット文字(pch)を使用し、グループによって背景色が異なるように設定します（2番目の例を参照）。(2番目の例を参照).約10個以上の変数をプロットするときは、gapパラメータを1より小さい値(例えば0)に設定すると便利です。  また、cor.plotの使用も検討してください。さらに、約100-200ケース以上のプロットを行う場合、プロット文字をポイントに設定すると便利です。(pch=".")点なしで相関楕円とベスト・フィットのloessを描くと便利なことがあります。(points.false=TRUE)。
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対になるオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析faや信頼性分析omegaやscoreOverlapが不可能になります。pairwiseCountBigは、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントの項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを使って欠落した相関をインプットします。このテクニックは、SAPA項目のスケールベースの構造を利用します。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
SAPAプロジェクトで使用されているMMCAR（Massively Missing Completely at Random）計画を使用する場合、対のオブザベーションの数（pairwiseCount）をカウントすることが重要です。  もしオブザベーションが1個以下のペアがあると、相関がNA値になり、その後の因子分析 fa や信頼性分析 omega や scoreOverlap が不可能になります。pairwiseCountBig は、大規模データ集合のセルをカウントするために使用できます。  これは、bigCorに類似しており、相関の各ペアのセルサイズを返します。ある値より少ないカウントを持つ項目ペアを識別するために、pairwiseReportは、'cut'オブザベーションより少ないそれらのペアの名前を報告します。  デフォルトでは、問題のある項目の数だけを報告します。short=FALSEでは、n.obs < cutの項目が表示されます。paiwise countsの特定の表で値 <= n minを持つ特定のペアは、pairwiseZeroで与えられるかもしれません。  相関の欠落の問題を解決するために、pairwiseImputeを用いて欠落した相関をインプットします。この手法は、SAPA項目のスケールベースの構造を利用しています。  ある尺度内の項目（例えば、能力項目と同様の文字数系列）は、別の尺度（例えば、行列推理）の項目と、これら2つの尺度間の項目間平均相関の平均で相関するようにインプットされます。  スケール内およびスケール間の平均相関は pairwiseImpute によって報告され、fix paremeter が指定されている場合は、インプットされた相関行列が返されます。セル・サイズをカウントする時間は、被験者数と項目数の2乗によって線形に変化する。  pairwiseSampleは、このような大きなデータセットのsize=sizeのサンプルを取り、平均、中央値、0.05、0.25、0.5、0.75、0.95分位数などの基本的な記述統計量を返します。   
能力または性格の他の側面の測定に使用される項目は、一般的にあまり信頼できない。  1つの提案は、項目を均質な項目複合（HIC）、因子的に均質な項目次元（FHID）またはミニ尺度（パーセル）に形成することである。  link{parcels}は、score.itemsで使用するのに適したキー行列を形成することで、パーセルの発見を容易にします。  これらのキーは、n/2の最も類似したペア、またはn/3の最も類似したトリプレットを表します。アルゴリズムは簡単です：  サイズ=2の場合、相関行列は最も相関の高いものを探す。  これらの2つの項目は最初の区画を形成し、行列から取り除かれる。  size=3 の場合，互いに最大の分散と共分散の合計を持つ3つの項目が見つけられる．  このトリプレットが最初の区画である。  3つの項目はすべて取り除かれ、次に最も類似したトリプレットが同定される。  この手順は，n/3個の区画が識別されるまで繰り返される．  
1つは、完全な偏相関行列（つまり、各変数から他のすべての変数を偏相関させる）を求める方法です。  これは単に生データまたは相関行列を指定することで行うことができる。  (生データの場合、相関は用途と方法によって求められる)。  この場合はデータ行列を指定すればよい。あるいは、データをX行列とY行列として考えると、相関Rを持つ(D = X + Y)となり、X予測変数のY変数との偏相関は、R^(-1)の最後の列になります。2番目の使用法は，別の集合(x)から変数の集合(y)を部分化することである．いくつかの変数（たとえば、性別、年齢、学歴）の効果を、別の変数の集合の相関から部分化することが便利なことがある。  これは，さまざまな多重相関の残差を求め，これらの残差を相関させることによって，手間のかかることができる．  行列代数の代替案は、それを直接行うことである。相関の信頼区間と "有意 "を見つけるには、 n = n - s（sは共変量の数）でcorr.p関数を使用します。数式入力を使用する場合は、X と Y を指定し、減算によってパーティション化された変数を指定します。    Fransisco Wilheimからの心のこもったリクエストに応えて、私は呼び出しで指定された変数の相関を求めることにしました（以前は相関行列全体を求めていましたが、これは時間の無駄であり、変数が数値でない場合は壊れてしまいます）非正定値行列の場合は、行列のPinv（擬似逆行列）を求めます。
主成分分析（PCA）や因子分析（EFA）を含む多くのデータ削減手法がある。  nRn = nFk kFn' + U2 ここで k は n よりはるかに小さい．主成分の場合，項目の一意性はゼロと仮定され，相関行列または共分散行列のすべての要素が適合される．すなわち， nRn = nFk kFn' 成分モデルと因子モデルの間の経験的な主な違いは，各項目の分散の取り扱いである．  哲学的には，成分は観測された変数の重み付き合成であるが，因子モデルでは，変数は因子の重み付き合成である．項目数が増えるにつれて，2つのモデルの差は小さくなる．  因子負荷量は，項目数が大きくなるにつれて漸近的な成分負荷量である．n x n の相関行列では，n 個の主成分が相関行列を完全に再現する．  回転主成分は、主成分（固有値分解に伴う軸）ではなく、単なる成分であることを認識することが重要です。  このことを指摘するために、回転されていない主成分はPCiとラベル付けされ、回転されたPCはRCi（回転された成分）とラベル付けされ、斜めに変換された成分はTCi（変換された成分）とラベル付けされるようになった。(この提案をしてくれたUlrike Grompingに感謝する）回転と変換は、psychの一部（Promaxとcluster）、基本Rの一部（varimax）、またはGPArotationの一部（simplimax、quartimax、obliminなど）のいずれかである。  さまざまな回転/変換オプションのうち、varimax、Varimax、quartimax、bentlerT、geominT、bifactor は直交回転を行います。oblimin、quartimin、simplimax、bentlerQ、geominQ、biquartiminは斜め変換です。これらのほとんどは、GPArotationパッケージを呼び出すだけである。cluster "オプションは、varimax解のクラスター表現によって定義された構造に対して、的を絞った回転を行います。  オプションの "keys "パラメータを指定すると、"target "オプションは、キー行列として与えられたターゲットに回転する。(target.rotを参照)回転行列(rot.mat)は、これらすべてのオプションから返されます。これは GPArotation パッケージが返す Th (theta?) オブジェクトの逆行列です。  因子の相関は，Phi = Th' Thによって求めることができる報告された統計量のいくつかは，主成分分析よりもむしろ（最尤）因子分析に適しており，これらの他のモデルとの比較を可能にするために報告されている．項目については，顕著な項目をスコアリングして（たとえば，scoreItemsを使用して）成分スコアを見つけることが典型的であるが，成分スコアは，回帰の重みがR^(-1) lambda である回帰によって見つけられ，ここで lambda は成分負荷量の行列である．   回帰アプローチは，因子分析関数 fa と並行して行われる．  回帰の重みは，相関行列の逆行列に成分負荷量を掛けて求められる．   この結果、成分得点は、標準化入力の標準得点（平均=0, sd = 1）となる。  princomp は、デフォルトでは、データ行列を標準化せず、成分自体も標準化しない。  回帰の重みは，パターン行列ではなく構造行列から求められる．得点が covar オプション = TRUE で求められる場合、得点は標準化されず、単に平均中心化されます。  Jolliffe (2002) は，回転成分の解釈が複雑である理由を議論している．   Rencher (1992) は、回転成分の使用を推奨していない。ここで使用されているアプローチは、因子分析の伝統と一致している。  項目と成分得点の相関は、（構造行列で報告された）成分負荷量と密接に一致する（はずである）。print.psych関数の出力は、（パターン行列からの）成分負荷量、h2（共同性）、u2（一意性）、com（その変数の成分負荷量の複雑さ（下記参照））を表示します。  直交解の場合，h2 は，単に成分負荷量の2乗の行和である．しかし、斜め解の場合は、（2乗した）直交成分負荷量の行和になります（回転や変換は共同性を変えないことを覚えておいてください）。  この情報は、print関数からオブジェクトのVaccountとして（見えないように）返される。
多くの予測状況では，2値予測子（accept/reject）が2値基準（success/failure）に対して検証される．  多項相関は，予測変数と規準が連続的な2変量正規変数であるかのように，また x と y の両方が2分された正規分布であると仮定すると4項相関になるように，基礎となるピアソン相関を推定するが，ファイ係数は，0と1の行列に適用されるピアソンである．ファイ係数はYule (1912)によって最初に報告されたが，Yule Q係数と混同してはならない。2×2の表が与えられたときのさまざまな関連性の尺度に関する非常に有用な議論と、おそらくYule Q係数を選ぶべき理由については、Warren (2008)を参照。カウントの2 x 2表が与えられると、すべてのカウントを合計の分数に変換し、Phi = [a- (a+b)*(a+c)]/sqrt((a+b)(c+d)(a+c)(b+d) ) = (a - R1 * C1)/sqrt(R1 * R2 * C1 * C2)これは、ユール係数Qとは対照的である、ここで Q = (ad - bc)/(ad+bc) これは[a- (a+b)*(a+c)]/(ad+bc) と同じである。ファイ係数は2値データに適用されるピアソン相関に過ぎないので、データセットからファイの行列を見つけるには、cor または lowerCor または corr.をテストします。  
ファイ相関における異なるベースレートの問題、そしてそれらがポリコレ相関を使うことによって部分的に解決される方法のデモンストレーション。私のデモの中ではあまり面白いものではない。  https://personality-project.org/r/simulating-personality.html、https://personality-project.org/r/r.datageneration.html。
これはほとんど自明のことです。  例を見てください。
以前はmvtnormパッケージが必要でしたが、現在はmnormtに置き換えられています。
これらの関数は、行列の各セルに対して Yule2poly、Yule2phi または phi2poly を呼び出します。詳細はこれらの関数を参照してください。  例については phi.demo を参照してください。
以前は mvtnorm パッケージが必要でしたが, 現在は mnormt に置き換えられています。
 行列 X の特異値分解は UdV であり, フルランク行列の場合, d は固有値のベクトル, U と V は固有ベクトルの行列です.逆行列は U/d だけである。  したがって、フルランク未満の行列方程式（例えば、シュミッド・シュミッド・ライマン解）を解くには、一般化逆行列を求める必要があります。
プロットするために適切な値を渡す。  type="IIC"（デフォルト）は項目特性応答関数をプロットします。type="IIC "は項目情報関数をプロットし、type="test "はテスト情報関数をプロットします。irt.faの結果をプロットする場合、元のirt.faの呼び出しで使用されたデータの種類に応じて、plot.irtまたはplot.polyを呼び出すことに注意してください。  これらは、"psych "型のオブジェクトに対してインターセプトされる汎用plot関数への呼び出しです。  plotは、plot.reliabilityと同様に、fa、irt.fa、ICLUST、omega、principalから返されるpsychオブジェクトに使用できます。fa.plot関数では、"jiggle "パラメータが使用できます(タイプが因子またはクラスタの場合、plot.psychから呼び出されます)。  jiggle=TRUEとすると、プロットする前に点がわずかにジッターされます（量によって制御されます）。  このオプションは、同一の因子負荷量を持つ項目をプロットする場合（例えば、仮想モデルを比較する場合）に便利です。irt.faからのオブジェクトは、「タイプ」（項目情報、項目特性、またはテスト情報）に従ってプロットされます。  さらに、キーマトリクスを使用すれば、選択した項目のプロットも可能です。  irt情報のプロットは、3つの不可視のオブジェクトを返します。それは、各項目の特徴レベルの情報の要約、各項目の平均曲線下面積（平均情報量）、および項目が最も情報量が多い場所です。plot.polyで複数の因子解をプロットする場合、mainには、各因子に1つずつ、名前のベクトルを指定できます。  デフォルトは、main + 因子番号です。また、得点キーと項目の難易度だけ、または因子分析と項目の難易度から、IRTのようなプロットを作成することも可能です。  これらは真のIRTタイプの分析ではなく、パラメータはデータから推定されたものではなく、任意の項目の集合に対する項目の位置と識別を示すものです。  print.residualsは、fa、irt.fa、omega、principalによって形成されたモデルの残差をグラフィカルに調べ、いくつかの方法で表示することができます。  "qq "は標準化または非標準化残差の分位数、"chi "は期待されるカイ2乗値に対してプロットされた標準化または非標準化残差の2乗の分位数、"hist "は未処理または標準化残差のヒストグラム、"cor "は残差相関のcorPlotを表示します。
プロットするために適切な値を渡す。  type="IIC"（デフォルト）は項目特性応答関数をプロットします。type="IIC "は項目情報関数をプロットし、type="test "はテスト情報関数をプロットします。irt.faの結果をプロットする場合、元のirt.faの呼び出しで使用されたデータの種類に応じて、plot.irtまたはplot.polyを呼び出すことに注意してください。  これらは、"psych "型のオブジェクトに対してインターセプトされる汎用plot関数への呼び出しです。  plotは、plot.reliabilityと同様に、fa、irt.fa、ICLUST、omega、principalから返されるpsychオブジェクトに使用できます。fa.plot関数では、"jiggle "パラメータが使用できます(タイプが因子またはクラスタの場合、plot.psychから呼び出されます)。  jiggle=TRUEとすると、プロットする前に点がわずかにジッターされます（量によって制御されます）。  このオプションは、同一の因子負荷量を持つ項目をプロットする場合（例えば、仮想モデルを比較する場合）に便利です。irt.faからのオブジェクトは、「タイプ」（項目情報、項目特性、またはテスト情報）に従ってプロットされます。  さらに、キーマトリクスを使用すれば、選択した項目のプロットも可能です。  irt情報のプロットは、3つの不可視のオブジェクトを返します。それは、各項目の特徴レベルの情報の要約、各項目の平均曲線下面積（平均情報量）、および項目が最も情報量が多い場所です。plot.polyで複数の因子解をプロットする場合、mainには、各因子に1つずつ、名前のベクトルを指定できます。  デフォルトは、main + 因子番号です。また、得点キーと項目の難易度だけ、または因子分析と項目の難易度から、IRTのようなプロットを作成することも可能です。  これらは真のIRTタイプの分析ではなく、パラメータはデータから推定されたものではなく、任意の項目の集合に対する項目の位置と識別を示すものです。  print.residualsは、fa、irt.fa、omega、principalによって形成されたモデルの残差をグラフィカルに調べ、いくつかの方法で表示することができます。  「qq" は標準化または非標準化残差の分位を表示し，"chi" は期待されるカイ2乗値に対してプロットされた標準化または非標準化残差の2乗の分位を表示し，"hist" は生残差または標準化残差のヒストグラムを描き，"cor" は残差相関のcorPlotを表示する．
Cattellの "scree" 検定は，因子数の問題に対するもっとも簡単な検定の1つである．  Horn (1965) の "並列 "分析は，同様に説得力のある手順である．  最も最適な因子数を決定するための他の手順には，VSS（Very Simple Structure）基準（VSS ）とVelicerのMAP手順（VSSに含まれる）を見つけることが含まれる．fa.parallelは，主成分と因子解（デフォルトではminres）の固有値をプロットし，元のデータ行列と同じサイズのランダム行列に対しても同じことを行います．  fa.parallelにcor=polyオプションを指定すると、fa.parallel.polyが明示的に行うこと、つまり多項式因子と四項式因子の並列分析が行われます。もしデータが2値であれば、fa.parallel.polyは実データとシミュレートデータの4値相関を求め、そうでなければカテゴリ数が10以下であれば多値相関を求めます。  fa.parallel.polyはテトラコロリック/ポリコロリック相関の計算が複雑なため、fa.parallelより遅いことに注意してください。  fa.parallel.polyの機能はfa.parallelにcor=polyオプション(etc)オプションで含まれていますが、古いfa.parallel.polyは直接呼び出す人のために残されています。つまり、fa.parallelは現在、corオプションが "tet "または "poly "に設定されている場合、テトラコリックまたはポリコリックを直接行います。  fa.parallel.polyと同様、これは時間がかかる。  (ntrials)ランダム解の平均を示す。  エラーバーは通常非常に小さく、デフォルトでは抑制されているが、要求があれば表示することができる。  simオプションをTRUE（デフォルト）に設定すると、ランダムな正規データだけでなく、再標本化されたデータに対しても並列解析が行われます。sim=FALSEの場合は、並列解析は再標本化データに対してのみ行われます。    どちらの手続きも同意する傾向があります。  バージョン1.5.4では、シミュレーション/再標本化データの分位数を指定し、標準偏差または標準誤差をプロットする機能を追加しました。  デフォルトでは、これは95パーセンタイルに設定されています。  因子の数の問題を推定する別の方法は、Very Simple Structure (Revelle and Rocklin, 1979)のドキュメント(VSS)で議論されており、Wayne VelicerのMAPアルゴリズム(Veicer, 1976)が含まれています。  因子の並列分析は，実際，見かけよりも難しい．  共分散度が自乗重相関（SMC） smcによって推定される場合、元データの固有値は、主要因子だけでなくマイナー因子も反映します（このようなデータをシミュレートするにはsim.minorを参照してください）。  ランダムデータはもちろん構造を持たないので、マイナー因子の存在によって因子の数が上方に偏る傾向があります。  デフォルトでは、fa.parallelは1因子のminres解に基づいて共分散度を推定します。  これは交互性を過小評価することになりますが、シミュレートされたデータセットや実際のデータセット（例えば、bfiやHarman74）において、より良い解を導くようです。  他のアルゴリズム（例えば、paranパッケージのparan関数）との比較のために、smc=TRUEに設定すると、共通性の推定値としてsmcsが使用されます。さらにもう1つのオプション（Florian Scharfによって提案された）は、特定の因子モデル（例えば、nfactors > 1を指定）に基づいて固有値を推定することです。   結果を印刷すると、シミュレーション値よりも大きいオリジナルデータの固有値が表示されます。パラレル分析についての悲しい観察は、サンプルサイズに敏感であるということです。  つまり、大きなデータセットでは、ランダムデータの固有値は1に非常に近くなります。これは、サンプルサイズの関数として因子数の異なる推定につながります。  bfiデータセット（最初の25項目は5因子モデルを表すことを意味する）の因子構造を考えてみよう。  200以下のサンプルでは、並列分析は5因子を示唆するが、1000以上では6因子と成分が示される。  これは、実データの固有値が不安定なためではなく、nが大きくなるにつれてランダムデータが1に近づくためです。nfactors=1では6因子が示唆されますが、nfactors=5を指定すると、bfiの並列分析では12因子が抽出されるべきことが示唆されます！fa.parallel.polyで2値データをシミュレートする場合、シミュレートされたデータは元データと同じ難しさを持ちます。  これは機能的には、シミュレートされた結果と再サンプリングされた結果が非常によく似ていることを意味します。  なお、fa.parallel.polyは機能的にはfa.parallelにcor="poly "オプションを付けたものに置き換えられています。多くの心理関数と同様に、fa.parallelはマルチコア処理ができるように変更されています。  多くのpsych関数と同様に、fa.parallelもマルチコア処理ができるように変更されています。多数の反復計算を行う場合は、コア数を可能な限り増やした方が明らかに高速です（options("mc.cores=n "コマンドを使用。nはdetectCores()から決定されます）。
プロットに適切な値を渡す。  irt.faの結果をプロットするには、3つのオプションがあります：type="IIC"(デフォルト)は、項目特性応答関数をプロットします。type="IIC "は、項目情報関数をプロットします。type="test "は、テスト情報関数をプロットします。irtの結果をプロットするには、元のirt.fa呼び出しで使用されたデータのタイプに応じて、plot.irtまたはplot.polyを呼び出すことに注意してください。  これらは、"psych "型のオブジェクトに対してインターセプトされる汎用plot関数への呼び出しです。  plotは、plot.reliabilityと同様に、fa、irt.fa、ICLUST、omega、principalから返されるpsychオブジェクトに使用できます。fa.plot関数では、"jiggle "パラメータが使用できます(タイプが因子またはクラスタの場合、plot.psychから呼び出されます)。  jiggle=TRUEとすると、プロットする前に点がわずかにジッターされます（量によって制御されます）。  このオプションは、同一の因子負荷量を持つ項目をプロットする場合（例えば、仮想モデルを比較する場合）に便利です。irt.faからのオブジェクトは、「タイプ」（項目情報、項目特性、またはテスト情報）に従ってプロットされます。  さらに、キーマトリクスを使用すれば、選択した項目のプロットも可能です。  irt情報のプロットは、3つの不可視のオブジェクトを返します。それは、各項目の特徴レベルの情報の要約、各項目の平均曲線下面積（平均情報量）、および項目が最も情報量が多い場所です。plot.polyで複数の因子解をプロットする場合、mainには、各因子に1つずつ、名前のベクトルを指定できます。  デフォルトは、main + 因子番号です。また、得点キーと項目の難易度だけ、または因子分析と項目の難易度から、IRTのようなプロットを作成することも可能です。  これらは真のIRTタイプの分析ではなく、パラメータはデータから推定されたものではなく、任意の項目の集合に対する項目の位置と識別を示すものです。  print.residualsは、fa、irt.fa、omega、principalによって形成されたモデルの残差をグラフィカルに調べ、いくつかの方法で表示することができます。  "qq "は標準化または非標準化残差の分位数、"chi "は期待されるカイ2乗値に対してプロットされた標準化または非標準化残差の2乗の分位数、"hist "は未処理または標準化残差のヒストグラム、"cor "は残差相関のcorPlotを表示します。
reliabilityは基本的にomegah、unidim、splitHalfのラッパーに過ぎない。Revelle and Condon (2019) は、任意の尺度について少なくとも3つの信頼性統計量を報告することを推奨しています。hist オプションをtrueに設定すると、各テストのスプリット・ハーフ値のヒストグラムと密度プロットも表示されます。reliability の出力を error.dots に渡すと、複数の尺度の信頼性統計量をグラフィカルに表示できます。  しかし、基本的な情報を表示するには、plot.reliability関数を呼び出すだけの方が便利です。1つの尺度の詳細な分析を行うには、完全なオメガ分析を行うことをお勧めします。信頼性関数は、ユーザが複数の尺度（おそらくscoreItemsを使用して採点されたもの）を持っていて、すべての尺度についてより完全な信頼性情報を得たい場合のためのものです。  提案に従って、キーを気にせず、単にオメガとスプリットハーフを行い、結果を描画する機能が追加されました。  keys=NULLを指定するか、使用する項目を指定するだけです。(最初の例を参照してください。)plot.reliabilityは、オメガ、アルファ、一次元性の推定値と同様に、スプリットハーフ値の分布のドットチャート要約を提供します。  plotコマンドを発行するだけでも呼び出すことができます。
plot に適切な値を渡す。  type="IIC"（デフォルト）は項目特性応答関数をプロットします。type="IIC "は項目情報関数をプロットし、type="test "はテスト情報関数をプロットします。irt.faの結果をプロットする場合、元のirt.fa呼び出しで使用されたデータの種類に応じて、plot.irtまたはplot.polyを呼び出すことに注意してください。  これらは、"psych "型のオブジェクトに対してインターセプトされる汎用plot関数への呼び出しです。  plotは、plot.reliabilityと同様に、fa、irt.fa、ICLUST、omega、principalから返されるpsychオブジェクトに使用できます。fa.plot関数では、"jiggle "パラメータが使用できます(タイプが因子またはクラスタの場合、plot.psychから呼び出されます)。  jiggle=TRUEとすると、プロットする前に点がわずかにジッターされます（量によって制御されます）。  このオプションは、同一の因子負荷量を持つ項目をプロットする場合（例えば、仮想モデルを比較する場合）に便利です。irt.faからのオブジェクトは、「タイプ」（項目情報、項目特性、またはテスト情報）に従ってプロットされます。  さらに、キーマトリクスを使用すれば、選択した項目のプロットも可能です。  irt情報のプロットは、3つの不可視のオブジェクトを返します。それは、各項目の特徴レベルの情報の要約、各項目の平均曲線下面積（平均情報量）、および項目が最も情報量が多い場所です。plot.polyで複数の因子解をプロットする場合、mainには、各因子に1つずつ、名前のベクトルを指定できます。  デフォルトは、main + 因子番号です。また、得点キーと項目の難易度だけ、または因子分析と項目の難易度から、IRTのようなプロットを作成することも可能です。  これらは真のIRTタイプの分析ではなく、パラメータはデータから推定されたものではなく、任意の項目の集合に対する項目の位置と識別を示すものです。  print.residualsは、fa、irt.fa、omega、principalによって形成されたモデルの残差をグラフィカルに調べ、いくつかの方法で表示することができます。  "qq "は標準化残差または非標準化残差の分位を表示し，"chi "は期待されるカイ2乗値に対してプロットされた標準化残差または非標準化残差の2乗の分位を表示し，"hist "は生残差または標準化残差のヒストグラムを描き，"cor "は残差相関のcorPlotを表示します．
因子分析/クラスター分析の多くの使用法は、項目が1つの大きな負荷量を持つ単純な構造を仮定していますが、パーソナリティや影響項目のようないくつかの領域は、より複雑な構造を持ち、2つの因子に高い負荷量を持つ項目もあります。  (これらの項目は複雑度2であると言われています。）  因子負荷量を極座標で表現することで、この構造がより容易に知覚されます。各因子のペアについて、項目負荷量は、第1因子との角度、および2つの因子と共有される項目の分散量に対応するベクトルの長さに変換されます。  2次元構造の場合，これは角度の列とベクトルの長さの列につながる．  n 因子の場合，これは角度の n* (n-1)/2 列とベクトルの長さの等しい数につながる．
テトラコロリック相関は，2変量正規性を仮定して，度数の2 x 2 表から潜在的なピアソン相関を推論する．  推定手順は2段階のML．  各組の項目のセル度数が求められる．テトラコリックスの場合、カウントがゼロのセルは、連続性の補正として0.5に置き換えられる（correct=TRUE）。データは通常、真/偽（4値法）または限定された回答数（多値法）のいずれかで採点されたアンケートへの回答の生データ行列になります。  どちらの場合も、限界度数は正規理論のしきい値に変換され、各項目ペアの結果の表は、観察されたセル度数と観察された限界度数を生成する（推測された）潜在ピアソン相関に変換されます。  (例としてdraw.tetraとdraw.corを参照)これは計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなり高速化できる。  polychoricまたはtetrachoricを行う際に使用するコアの数はoptionsコマンドで指定することができる。最も速度が向上するのは1コアから2コアにすることで、約50％の節約になる。  4コアにすると66％、8コアにすると75％の節約になるようだ。  options("mc.cores"=4)は、コア数を4に設定します。tetrachoricとpolychoricは、x変数（列）とy変数（行）のセットの非対称相関行列を求めることができます。  これは、解を基本集合から別の集合に拡張する場合に便利である。  四分位相関は様々な文脈で使用され，1つはテスト得点の項目反応理論（IRT）分析で，もう1つは共存統計量の相関係数への変換で重要である．  この2番目の文脈で、セル度数に対する係数の感度の例が明らかになります。Kirk (1973)のテストデータセットを考えてみましょう。彼は、テトラコリック相関（例参照）に対するMLアルゴリズムの有効性を報告しています。  以前のバージョンはJohn Foxのpolychor関数を使用していましたが、現在はpolyc関数に置き換えられています。poly.matはpolycor関数の代替ラッパーでした。双列相関と多列相関は、観測された点-双列相関と点-多列相関（それ自体は単なるピアソン相関です）に相当する推定潜在相関です。polyserial 関数は、行列またはデータフレーム入力で動作し、応答頻度の全体的な（すべての観察されたケースの）確率で補正されたペアワイズ Pearson r を見つけることによって欠損データを扱います。  これは、大量の欠損データがあり、完全な症例がないSAPA手順（https://www.sapa-project.org/）（Revelle et al. 2010, 2016, 2020）に特に有用です。同様のデータについては、International Cognitive Ability Resource (https://www.icar-project.org/)も参照してください。能力検査や性格検査の行列は、通常、通常のピアソン相関を使用する場合よりも、テトラコロリック相関やポリコロリック相関を使用する場合の方が、よりきれいな構造を持ちます。バイセリアル相関（単なるピアソン相関であるポイント・バイセリアル相関と混同しないように）は，xとyの間の潜在相関で，yは連続，xは2値であるが，（観察されない）連続正規変数を表すと仮定される．p = xのレベル1の確率，q = 1 - pとする． zp = pに関連するz スコアの正規縦軸とする．MacCallumら, 2002によってうまく議論されているように，平均で人為的に2分すると，ポイント2等分線は2等分線の0.8になる（実際には0.798）．  同様に、ファイ係数（2分値データでのピアソン）は、実数値（rho）の2[arcsin(rho)/pi)になります。「アドホック」多列相関、rps は、単に r = r * sqrt(n-1)/n) σ y /∑(zpi) であり、ここで zpi は、項目回答間のカットポイント境界の正規等価での正規曲線の縦線です。(Olsson, 1982) これらはすべて、相関の正確なML推定に使用されるべきJohn Foxのpolychorパッケージからインスパイアされ（そして、そこから適応され）ました。  特に、polychorパッケージのhetcor関数を参照してください。  polychoricの結果は、correct=FALSE、global=FALSEを使用した場合、polychorの答えと少なくとも小数点以下5桁で一致します。特に欠損データを含むデータセットからのテトラコロリック相関の場合、行列が正定値にならないことがあります。  さまざまな平滑化の選択肢がありますが、ここで行われるのは、相関行列の固有値分解を行い、すべての負の固有値を10 * .Machine$double.epsに設定し、正の固有値を変数の数の合計になるように正規化し、相関行列を再構成することです。   非常に小さなデータセットの場合、特にglobal=FALSEオプションを使用している場合、または一度に1つの相関だけを行う場合、多項式相関の連続性の補正が困難になる可能性があります。補正値を小さく設定する（つまり、correct =.1）ことで解決するようです。  John Uebersax (2015)は、ポリコーリック相関とテトラコーリック相関の両方が、過去の発見方法であるテトラコーリックやポリコーリックではなく、発見方法から潜在相関または潜在連続相関と呼ばれるべきであるという興味深い指摘をしています。つまり，人為的に2つ（四分位）またはそれ以上（多分位）の値に分割すると，観察された度数のn x n 表が得られる2つの潜在変数の間の相関は何かということである．  連続変数、カテゴリー変数、2値変数の組み合わせについては、mixed.corを参照してください。応答選択肢の数が可変のデータを使用する場合は、polychoricのglobal=FALSEオプションを使用する必要があります。  (この条件が検出された場合、自動的に設定されます).二分値データで比較的小さなサンプルの場合、いくつかのセルが空であったり、再サンプリングされた行列が正半定値でない場合、警告が出されます。これは、multi.cores(Macを使用している場合のデフォルト)を使用している場合、深刻な問題につながります。解決策は、multi.coresを使用しないことであるようです（例えば、options(mc.cores =1) mixedCorは、N = 4,000、145項目のspiデータセットに対してpolychoricを呼び出し、2.4GHz 8コアのIntel i9を搭載したMac book Pro上で、1コアで130秒、2コアで68秒、4コアで37秒、8コアで22秒、16コアで22.8秒かかりました。  ポリコレ相関（spi[11:145]）を見つけるだけで、4コアでは34秒、8コアでは22秒かかった。(2022年更新：M1 maxチップを使用)すべての145変数のmixedCor：1コア=54、2コア=28、4コア=15.8秒、8コア=8.9秒。  ポリコレだけの場合は、4コア＝13.7秒、8コア＝7.4秒。     カテゴリの数が増えるにつれて、計算時間も増加する。  約8カテゴリー以上では、通常のピアソン相関とポリコレの差はあまり大きくない*。  しかし，必要であれば，カテゴリー数（num.cat）を大きな値に設定してもよい．  (バージョン2.0.6で追加).*FoldnesとGronnebergの最近の論文によると、カテゴリ数が大きくても、カテゴリデータにポリコレを使用することが適切であることが示唆されている。  これは特に、カテゴリー度数の分布が非常に非正規分布の場合である。
テトラコリック相関は，2変量正規性の仮定で度数の2 x 2 表から潜在的なピアソン相関を推論する．  推定手順は2段階のML．  各組の項目のセル度数が求められる．テトラコリックスの場合、カウントがゼロのセルは、連続性の補正として0.5に置き換えられる（correct=TRUE）。データは通常、真/偽（4値法）または限定された回答数（多値法）のいずれかで採点されたアンケートへの回答の生データ行列になります。  どちらの場合も、限界度数は正規理論のしきい値に変換され、各項目ペアの結果の表は、観察されたセル度数と観察された限界度数を生成する（推測された）潜在ピアソン相関に変換されます。  (例としてdraw.tetraとdraw.corを参照)これは計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなり高速化できる。  polychoricまたはtetrachoricを行う際に使用するコアの数はoptionsコマンドで指定することができる。最も速度が向上するのは1コアから2コアにすることで、約50％の節約になる。  4コアにすると66％、8コアにすると75％の節約になるようだ。  options("mc.cores"=4)は、コア数を4に設定します。tetrachoricとpolychoricは、x変数（列）とy変数（行）のセットの非対称相関行列を求めることができます。  これは、解を基本集合から別の集合に拡張する場合に便利である。  四分位相関は様々な文脈で使用され，1つはテスト得点の項目反応理論（IRT）分析で，もう1つは共存統計量の相関係数への変換で重要である．  この2番目の文脈で、セル度数に対する係数の感度の例が明らかになります。Kirk (1973)のテストデータセットを考えてみましょう。彼は、テトラコリック相関（例参照）に対するMLアルゴリズムの有効性を報告しています。  以前のバージョンはJohn Foxのpolychor関数を使用していましたが、現在はpolyc関数に置き換えられています。poly.matはpolycor関数の代替ラッパーでした。双列相関と多列相関は、観測された点-双列相関と点-多列相関（それ自体は単なるピアソン相関です）に相当する推定潜在相関です。polyserial 関数は、行列またはデータフレーム入力で動作し、応答頻度の全体的な（すべての観察されたケースの）確率で補正されたペアワイズ Pearson r を見つけることによって欠損データを扱います。  これは、大量の欠損データがあり、完全な症例がないSAPA手順（https://www.sapa-project.org/）（Revelle et al. 2010, 2016, 2020）に特に有用です。同様のデータについては、International Cognitive Ability Resource (https://www.icar-project.org/)も参照してください。能力検査や性格検査の行列は、通常、通常のピアソン相関を使用する場合よりも、テトラコロリック相関やポリコロリック相関を使用する場合の方が、よりきれいな構造を持ちます。バイセリアル相関（単なるピアソン相関であるポイント・バイセリアル相関と混同しないように）は，xとyの間の潜在相関で，yは連続，xは2値であるが，（観察されない）連続正規変数を表すと仮定される．p = xのレベル1の確率，q = 1 - pとする． zp = pに関連するz スコアの正規縦軸とする．MacCallumら, 2002によってうまく議論されているように，平均で人為的に2分すると，ポイント2等分線は2等分線の0.8になる（実際には0.798）．  同様に、ファイ係数（2分値データでのピアソン）は、実数値（rho）の2[arcsin(rho)/pi)になります。「アドホック」多列相関、rps は、単に r = r * sqrt(n-1)/n) σ y /∑(zpi) であり、ここで zpi は、項目回答間のカットポイント境界の正規等価での正規曲線の縦線です。(Olsson, 1982) これらはすべて、相関の正確なML推定に使用されるべきJohn Foxのpolychorパッケージからインスパイアされ（そして、そこから適応され）ました。  特に、polychorパッケージのhetcor関数を参照してください。  polychoricの結果は、correct=FALSE、global=FALSEを使用した場合、polychorの答えと少なくとも小数点以下5桁で一致します。特に欠損データを含むデータセットからのテトラコロリック相関の場合、行列が正定値にならないことがあります。  さまざまな平滑化の選択肢がありますが、ここで行われるのは、相関行列の固有値分解を行い、すべての負の固有値を10 * .Machine$double.epsに設定し、正の固有値を変数の数の合計になるように正規化し、相関行列を再構成することです。   非常に小さなデータセットの場合、特にglobal=FALSEオプションを使用している場合、または一度に1つの相関だけを行う場合、多項式相関の連続性の補正が困難になる可能性があります。補正値を小さく設定する（つまり、correct =.1）ことで解決するようです。  John Uebersax (2015)は、ポリコーリック相関とテトラコーリック相関の両方が、過去の発見方法であるテトラコーリックやポリコーリックではなく、発見方法から潜在相関または潜在連続相関と呼ばれるべきであるという興味深い指摘をしています。つまり，人為的に2つ（四分位）またはそれ以上（多分位）の値に分割すると，観察された度数のn x n 表が得られる2つの潜在変数の間の相関は何かということである．  連続変数、カテゴリー変数、2値変数の組み合わせについては、mixed.corを参照してください。応答選択肢の数が可変のデータを使用する場合は、polychoricのglobal=FALSEオプションを使用する必要があります。  (この条件が検出された場合、自動的に設定されます).二分値データで比較的小さなサンプルの場合、いくつかのセルが空であったり、再サンプリングされた行列が正半定値でない場合、警告が出されます。これは、multi.cores(Macを使用している場合のデフォルト)を使用している場合、深刻な問題につながります。解決策は、multi.coresを使用しないことであるようです（例えば、options(mc.cores =1) mixedCorは、2.4GHz 8コアのIntel i9を搭載したMac book Pro上で、spiデータセットのN = 4,000、145アイテムに対してpolychoricを呼び出し、1コアで130秒、2コアで68秒、4コアで37秒、8コアで22秒、16コアで22.8秒かかりました。  ポリコレ相関（spi[11:145]）を見つけるだけで、4コアでは34秒、8コアでは22秒かかった。(2022年更新：M1 maxチップを使用) 145変数すべてに対するmixedCor：1コア=54、2コア=28、4コア=15.8秒、8コア=8.9秒。  ポリコレだけの場合は、4コア＝13.7秒、8コア＝7.4秒。     カテゴリの数が増えるにつれて、計算時間も増加する。  約8カテゴリー以上では、通常のピアソン相関とポリコレの差はあまり大きくない*。  しかし，必要であれば，カテゴリー数（num.cat）を大きな値に設定してもよい．  (バージョン2.0.6で追加).*FoldnesとGronnebergの最近の論文によると、カテゴリ数が大きくても、カテゴリデータにポリコレを使用することが適切であることが示唆されている。  これは特に、カテゴリー度数の分布が非常に非正規分布の場合である。
テトラコリック相関は，2変量正規性の仮定で度数の2 x 2 表から潜在的なピアソン相関を推論する．  推定手順は2段階のML．  各組の項目のセル度数が求められる．テトラコリックスの場合、カウントがゼロのセルは、連続性の補正として0.5に置き換えられる（correct=TRUE）。データは通常、真/偽（4値法）または限定された回答数（多値法）のいずれかで採点されたアンケートへの回答の生データ行列になります。  どちらの場合も、限界度数は正規理論のしきい値に変換され、各項目ペアの結果の表は、観察されたセル度数と観察された限界度数を生成する（推測された）潜在ピアソン相関に変換されます。  (例としてdraw.tetraとdraw.corを参照)これは計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなり高速化できる。  polychoricまたはtetrachoricを行う際に使用するコアの数はoptionsコマンドで指定することができる。最も速度が向上するのは1コアから2コアにすることで、約50％の節約になる。  4コアにすると66％、8コアにすると75％の節約になるようだ。  options("mc.cores"=4)は、コア数を4に設定します。tetrachoricとpolychoricは、x変数（列）とy変数（行）のセットの非対称相関行列を求めることができます。  これは、解を基本集合から別の集合に拡張する場合に便利である。  四分位相関は様々な文脈で使用され，1つはテスト得点の項目反応理論（IRT）分析で，もう1つは共存統計量の相関係数への変換で重要である．  この2番目の文脈で、セル度数に対する係数の感度の例が明らかになります。Kirk (1973)のテストデータセットを考えてみましょう。彼は、テトラコリック相関（例参照）に対するMLアルゴリズムの有効性を報告しています。  以前のバージョンはJohn Foxのpolychor関数を使用していましたが、現在はpolyc関数に置き換えられています。poly.matはpolycor関数の代替ラッパーでした。双列相関と多列相関は、観測された点-双列相関と点-多列相関（それ自体は単なるピアソン相関です）に相当する推定潜在相関です。polyserial 関数は、行列またはデータフレーム入力で動作し、応答頻度の全体的な（すべての観察されたケースの）確率で補正されたペアワイズ Pearson r を見つけることによって欠損データを扱います。  これは、大量の欠損データがあり、完全な症例がないSAPA手順（https://www.sapa-project.org/）（Revelle et al. 2010, 2016, 2020）に特に有用です。同様のデータについては、International Cognitive Ability Resource (https://www.icar-project.org/)も参照してください。能力検査や性格検査の行列は、通常、通常のピアソン相関を使用する場合よりも、テトラコロリック相関やポリコロリック相関を使用する場合の方が、よりきれいな構造を持ちます。双列相関（単なるピアソン相関である点双列相関と混同しないように）は，xとyの間の潜在相関で，yは連続，xは2値であるが，（未観測の）連続正規変数を表すと仮定される．p = xのレベル1の確率，q = 1 - pとする． zp = pに関連するz スコアの正規縦軸とする．MacCallumら, 2002によってうまく議論されているように，平均で人為的に2分すると，ポイント2等分線は2等分線の0.8になる（実際には0.798）．  同様に、ファイ係数（2分値データでのピアソン）は、実数値（rho）の2[arcsin(rho)/pi)になります。「アドホック」多列相関、rps は、単に r = r * sqrt(n-1)/n) σ y /∑(zpi) であり、ここで zpi は、項目回答間のカットポイント境界の正規等価での正規曲線の縦線です。(Olsson, 1982) これらはすべて、相関の正確なML推定に使用されるべきJohn Foxのpolychorパッケージからインスパイアされ（そして、そこから適応され）ました。  特に、polychorパッケージのhetcor関数を参照してください。  polychoricの結果は、correct=FALSE、global=FALSEを使用した場合、polychorの答えと少なくとも小数点以下5桁で一致します。特に欠損データを含むデータセットからのテトラコロリック相関の場合、行列が正定値にならないことがあります。  さまざまな平滑化の選択肢がありますが、ここで行われるのは、相関行列の固有値分解を行い、すべての負の固有値を10 * .Machine$double.epsに設定し、正の固有値を変数の数の合計になるように正規化し、相関行列を再構成することです。   非常に小さなデータ・セットの場合、多項式相関の連続性の補正は、特にglobal=FALSEオプションを使用する場合、または一度に1つの相関だけを行う場合、困難につながる可能性があります。補正値を小さく設定する（つまり、correct =.1）ことで解決するようです。  John Uebersax (2015)は、ポリコーリック相関とテトラコーリック相関の両方が、過去の発見方法であるテトラコーリックやポリコーリックではなく、発見方法から潜在相関または潜在連続相関と呼ばれるべきであるという興味深い指摘をしています。つまり，人為的に2つ（四分位）またはそれ以上（多分位）の値に分割すると，観察された度数のn x n 表が得られる2つの潜在変数の間の相関は何かということである．  連続変数、カテゴリー変数、2値変数の組み合わせについては、mixed.corを参照してください。応答選択肢の数が可変のデータを使用する場合は、polychoricでglobal=FALSEオプションを使用する必要があります。  (この条件が検出された場合、自動的に設定されます).二分値データで比較的小さなサンプルの場合、いくつかのセルが空であったり、再サンプリングされた行列が正半定値でない場合、警告が出されます。これは、multi.cores(Macを使用している場合のデフォルト)を使用している場合、深刻な問題につながります。解決策は、multi.coresを使用しないことであるようです（例えば、options(mc.cores =1) mixedCorは、2.4GHz 8コアのIntel i9を搭載したMac book Pro上で、spiデータセットのN = 4,000、145アイテムに対してpolychoricを呼び出し、1コアで130秒、2コアで68秒、4コアで37秒、8コアで22秒、16コアで22.8秒かかりました。  ポリコレ相関（spi[11:145]）を見つけるだけで、4コアでは34秒、8コアでは22秒かかった。(2022年更新：M1 maxチップを使用) 145変数すべてに対するmixedCor：1コア=54、2コア=28、4コア=15.8秒、8コア=8.9秒。  ポリコレだけの場合は、4コア＝13.7秒、8コア＝7.4秒。     カテゴリの数が増えるにつれて、計算時間も増加する。  約8カテゴリー以上では、通常のピアソン相関とポリコレの差はあまり大きくない*。  しかし，必要であれば，カテゴリー数（num.cat）を大きな値に設定してもよい．  (バージョン2.0.6で追加).*FoldnesとGronnebergの最近の論文によると、カテゴリ数が大きくても、カテゴリデータにポリコレを使用することが適切であることが示唆されている。  これは特に、カテゴリー度数の分布が非常に非正規分布の場合である。
テトラコリック相関は，2変量正規性の仮定で度数の2 x 2 表から潜在的なピアソン相関を推論する．  推定手順は2段階のML．  各組の項目のセル度数が求められる．テトラコリックスの場合、カウントがゼロのセルは、連続性の補正として0.5に置き換えられる（correct=TRUE）。データは通常、真/偽（4値法）または限定された回答数（多値法）のいずれかで採点されたアンケートへの回答の生データ行列になります。  どちらの場合も、限界度数は正規理論のしきい値に変換され、各項目ペアの結果の表は、観察されたセル度数と観察された限界度数を生成する（推測された）潜在ピアソン相関に変換されます。  (例としてdraw.tetraとdraw.corを参照)これは計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなり高速化できる。  polychoricまたはtetrachoricを行う際に使用するコアの数はoptionsコマンドで指定することができる。最も速度が向上するのは1コアから2コアにすることで、約50％の節約になる。  4コアにすると66％、8コアにすると75％の節約になるようだ。  options("mc.cores"=4)は、コア数を4に設定します。tetrachoricとpolychoricは、x変数（列）とy変数（行）のセットの非対称相関行列を求めることができます。  これは、解を基本集合から別の集合に拡張する場合に便利である。  四分位相関は様々な文脈で使用され，1つはテスト得点の項目反応理論（IRT）分析で，もう1つは共存統計量の相関係数への変換で重要である．  この2番目の文脈で、セル度数に対する係数の感度の例が明らかになります。Kirk (1973)のテストデータセットを考えてみましょう。彼は、テトラコリック相関（例参照）に対するMLアルゴリズムの有効性を報告しています。  以前のバージョンはJohn Foxのpolychor関数を使用していましたが、現在はpolyc関数に置き換えられています。poly.matはpolycor関数の代替ラッパーでした。双列相関と多列相関は、観測された点-双列相関と点-多列相関（それ自体は単なるピアソン相関です）に相当する推定潜在相関です。polyserial 関数は、行列またはデータフレーム入力で動作し、応答頻度の全体的な（すべての観察されたケースの）確率で補正されたペアワイズ Pearson r を見つけることによって欠損データを扱います。  これは、大量の欠損データがあり、完全な症例がないSAPA手順（https://www.sapa-project.org/）（Revelle et al. 2010, 2016, 2020）に特に有用です。同様のデータについては、International Cognitive Ability Resource (https://www.icar-project.org/)も参照してください。能力検査や性格検査の行列は、通常、通常のピアソン相関を使用する場合よりも、テトラコロリック相関やポリコロリック相関を使用する場合の方が、よりきれいな構造を持ちます。バイセリアル相関（単なるピアソン相関であるポイント・バイセリアル相関と混同しないように）は，xとyの間の潜在相関で，yは連続，xは2値であるが，（観察されない）連続正規変数を表すと仮定される．p = xのレベル1の確率，q = 1 - pとする． zp = pに関連するz スコアの正規縦軸とする．MacCallumら, 2002によってうまく議論されているように，平均で人為的に2分すると，ポイント2等分線は2等分線の0.8になる（実際には0.798）．  同様に、ファイ係数（2分値データでのピアソン）は、実数値（rho）の2[arcsin(rho)/pi)になります。「アドホック」多列相関、rps は、単に r = r * sqrt(n-1)/n) σ y /∑(zpi) であり、ここで zpi は、項目回答間のカットポイント境界の正規等価での正規曲線の縦線です。(Olsson, 1982) これらはすべて、相関の正確なML推定に使用されるべきJohn Foxのpolychorパッケージからインスパイアされ（そして、そこから適応され）ました。  特に、polychorパッケージのhetcor関数を参照してください。  polychoricの結果は、correct=FALSE、global=FALSEを使用した場合、polychorの答えと少なくとも小数点以下5桁で一致します。特に欠損データを含むデータセットからのテトラコロリック相関の場合、行列が正定値にならないことがあります。  さまざまな平滑化の選択肢がありますが、ここで行われるのは、相関行列の固有値分解を行い、すべての負の固有値を10 * .Machine$double.epsに設定し、正の固有値を変数の数の合計になるように正規化し、相関行列を再構成することです。   非常に小さなデータセットの場合、特にglobal=FALSEオプションを使用している場合、または一度に1つの相関だけを行う場合、多項式相関の連続性の補正が困難になる可能性があります。補正値を小さく設定する（つまり、correct =.1）ことで解決するようです。  John Uebersax (2015)は、ポリコーリック相関とテトラコーリック相関の両方が、過去の発見方法であるテトラコーリックやポリコーリックではなく、発見方法から潜在相関または潜在連続相関と呼ばれるべきであるという興味深い指摘をしています。つまり，人為的に2つ（四分位）またはそれ以上（多分位）の値に分割すると，観察された度数のn x n 表が得られる2つの潜在変数の間の相関は何かということである．  連続変数、カテゴリー変数、2値変数の組み合わせについては、mixed.corを参照してください。応答選択肢の数が可変のデータを使用する場合は、polychoricでglobal=FALSEオプションを使用する必要があります。  (この条件が検出された場合、自動的に設定されます).二分値データで比較的小さなサンプルの場合、いくつかのセルが空であったり、再サンプリングされた行列が正半定値でない場合、警告が出されます。これは、multi.cores(Macを使用している場合のデフォルト)を使用している場合、深刻な問題につながります。解決策は、multi.coresを使用しないことであるようです（例えば、options(mc.cores =1) mixedCorは、N = 4,000、145項目のspiデータセットに対してpolychoricを呼び出し、2.4GHz 8コアのIntel i9を搭載したMac book Pro上で、1コアで130秒、2コアで68秒、4コアで37秒、8コアで22秒、16コアで22.8秒かかりました。  ポリコレ相関（spi[11:145]）を見つけるだけで、4コアでは34秒、8コアでは22秒かかった。(2022年更新：M1 maxチップを使用) 145変数すべてに対するmixedCor：1コア=54、2コア=28、4コア=15.8秒、8コア=8.9秒。  ポリコレだけの場合は、4コア＝13.7秒、8コア＝7.4秒。     カテゴリの数が増えるにつれて、計算時間も増加する。  約8カテゴリー以上では、通常のピアソン相関とポリコレの差はあまり大きくない*。  しかし，必要であれば，カテゴリー数（num.cat）を大きな値に設定してもよい．  (バージョン2.0.6で追加).*FoldnesとGronnebergの最近の論文によると、カテゴリ数が大きくても、カテゴリデータにポリコレを使用することが適切であることが示唆されている。  これは特に、カテゴリ度数の分布が非常に非正規分布の場合である。
NA
尺度に形成された項目の集合から基準を予測する場合，尺度の妥当性（すなわち，各基準との尺度の相関）は，平均項目の妥当性（r_y），尺度の項目の平均相互相関（r_x），尺度の項目数（n）の関数である．  妥当性の限界はr_y/sqrt(r_x)である。  尺度の集合からの予測可能性は、基準によって異なる。これらの漸近値は、どの尺度をさらに開発するかの決定に役立つ。  
主成分分析（PCA）や因子分析（EFA）を含む多くのデータ削減手法がある。  nRn = nFk kFn' + U2 ここで k は n よりはるかに小さい．主成分の場合，項目の一意性はゼロと仮定され，相関行列または共分散行列のすべての要素が適合される．すなわち， nRn = nFk kFn' 成分モデルと因子モデルの間の経験的な主な違いは，各項目の分散の取り扱いである．  哲学的には，成分は観測された変数の重み付き合成であるが，因子モデルでは，変数は因子の重み付き合成である．項目数が増えるにつれて，2つのモデルの差は小さくなる．  因子負荷量は，項目数が大きくなるにつれて漸近的な成分負荷量である．n x n の相関行列では，n 個の主成分が相関行列を完全に再現する．  回転主成分は、主成分（固有値分解に伴う軸）ではなく、単なる成分であることを認識することが重要です。  このことを指摘するために、回転されていない主成分はPCiとラベル付けされ、回転されたPCはRCi（回転された成分）とラベル付けされ、斜めに変換された成分はTCi（変換された成分）とラベル付けされるようになった。(この提案をしてくれたUlrike Grompingに感謝する）回転と変換は、psychの一部（Promaxとcluster）、基本Rの一部（varimax）、またはGPArotationの一部（simplimax、quartimax、obliminなど）のいずれかである。  さまざまな回転/変換オプションのうち、varimax、Varimax、quartimax、bentlerT、geominT、bifactor は直交回転を行います。oblimin、quartimin、simplimax、bentlerQ、geominQ、biquartiminは斜め変換です。これらのほとんどは、GPArotationパッケージを呼び出すだけである。cluster "オプションは、varimax解のクラスター表現によって定義された構造に対して、的を絞った回転を行います。  オプションの "keys "パラメータを指定すると、"target "オプションは、キー行列として与えられたターゲットに回転する。(target.rotを参照)回転行列(rot.mat)は、これらすべてのオプションから返されます。これは GPArotation パッケージが返す Th (theta?) オブジェクトの逆行列です。  因子の相関は，Phi = Th' ThSome of the statistics are more appropriate for (maximum likelihood) factor analysis rather than principal components analysis, and are reported to allow comparison with these other models.項目については，顕著な項目をスコアリングすることによって（たとえば，scoreItemsを使用して）成分スコアを見つけることが典型的であるが，成分スコアは，回帰の重みがR^(-1) lambdaである回帰によって見つけられる（ここで，λは成分負荷量の行列である）．   回帰アプローチは，因子分析関数 fa と並行して行われる．  回帰の重みは，相関行列の逆行列に成分負荷量を掛けて求められる．   この結果、成分得点は、標準化入力の標準得点（平均=0, sd = 1）となる。  princomp は、デフォルトでは、データ行列を標準化せず、成分自体も標準化しない。  回帰の重みは，パターン行列ではなく構造行列から求められる．得点が covar オプション = TRUE で求められる場合、得点は標準化されず、単に平均中心化されます。  Jolliffe (2002) は，回転成分の解釈が複雑である理由を議論している．   Rencher (1992) は、回転成分の使用を推奨していない。ここで使用されているアプローチは、因子分析の伝統と一致している。  項目と成分得点の相関は、（構造行列で報告された）成分負荷量と密接に一致する（はずである）。print.psych関数の出力は、（パターン行列からの）成分負荷量、h2（共同性）、u2（一意性）、com（その変数の成分負荷量の複雑さ（下記参照））を表示します。  直交解の場合，h2 は，単に成分負荷量の2乗の行和である．しかし、斜め解の場合は、（2乗した）直交成分負荷量の行和になります（回転や変換は共同性を変えないことを覚えておいてください）。  この情報は、print関数からオブジェクトのVaccountとして（目に見えない形で）返される。
print.psychとsummary.psychは、ハイライトだけを表示する汎用メソッドを使用します。  他に何が利用できるかを確認するには、特定のオブジェクトの構造を求めてください: (str(theobject) ).あるいは、完全な出力を得るには、unclass(theobject)し、それを表示します。追加機能として、promax関数が因子負荷行列に適用された場合、通常の出力は回転行列を提供するだけです。(これはJohn FoxとUli KellerによるR-helpリストへの提案に従ったものです）。  もう1つの方法は、factanalオブジェクトに直接Promax関数を使用することです。
これらの関数の中で最も有用なのは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminでしょう。もう1つはTargetQで、これはターゲットのNA値の欠損を許容する。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013) によって提案された varimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目は、その負荷が最も高いグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷とグループ因子がある純粋な2因子モデル）を扱うことはできません。
lowerCor は、対角行列の下側を丸めたものを出力し、列名は桁数 + 3 文字に省略されます。  デフォルトでは、変数のペアワイズ削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
これらの関数の中で最も有用なものは、Jennrich and Bentler (2011)によって導入された斜めバイファクター回転を実装するbiquartiminの2つであろう。2番目はTargetQで、ターゲットのNA値の欠損を許容します。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013) によって提案された varimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、その最も高い負荷量を持つグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷量とグループ因子がある純粋なバイファクターモデル）を扱いません。
3つのビネット（overview.pdfとpsych_for_sem.pdf）は、パッケージの有用な紹介です。これらは、Rのビネットとして見つけることもできるし、https://personality-project.org/r/psych/intro.pdf https://personality-project.org/r/psych/overview.pdf と https://personality-project.org/r/psych/psych_for_sem.pdf からダウンロードすることもできる。  さらに、https://personality-project.org/r/ に多くの "HowTo "があります。このパッケージでより重要な関数は、多変量データの分析用で、項目合成の尺度構築に役立つ関数に重点を置いています。しかし、read.file、read.clipboard、describe、pairs.panel、error.bars、error.dotsなど、基本的なデータ操作に便利な関数も多数あり、基本的なデータ入力や記述分析に役立ちます。1）データの基本的な特性は何か？ describeは、ベクトル、行列の列、またはdata.framesの基本的な要約統計量（平均、sd、中央値、mad、範囲、最小値、最大値、歪度、尖度、標準誤差）を報告します。statsByは、グループによって構成されたデータに対して、グループ内およびグループ間の相関行列、グループ差のICC、グループによって構成された基本的な記述統計量など、さらに詳細な情報を提供します。pairs.panelは、散布図行列（SPLOM）、ヒストグラム、尺度や項目のピアソン相関を表示します。error.barsは、変数の平均と関連する信頼区間をプロットします。) 最も適切な複合項目の数は?標準的なピアソン相関を求めた後、またはテトラコリック相関やポリコリック相関を求めた後、相関行列の次元性を調べることができます。因子/成分の数の問題は、因子分析、クラスター分析、主成分分析の標準的な問題です。残念ながら，合意された答えはない．最適な因子数の問題に対する答えとして，VSS (Very Simple Structure) 手法の集合が提案されている．  他の手続き（VSS.scree、VSS.parallel、fa.parallel、MAP）もこの問題に取り組んでいます。nfactorsは、これらのアプローチのいくつかを1つの便利な関数にまとめたものです。残念ながら、この問題に対する最良の答えはない。3) どのような合成を形成するのが最適か？  これは、主成分（principal）、主軸（factor.pa）、最小残差（factor.minres）因子分析（すべてfa関数の一部）、および結果をグラフィカルに表示する（fa.diagram）を使って答えられるかもしれないが、クラスター分析手法を使ってこの問題に取り組む方がより有用な場合もある。  ICLUSTの以前のバージョン（例えば、Revelle, 1979）は、最大に一貫した独立項目合成を形成することに特に成功していることが示されている。  ICLUST.graphからのグラフィカルな出力は、Graphvizドット言語を使用し、Graphvizに適したファイルを書くことができる。 Rgraphvizが利用できる場合、これらのグラフはRで作成できる。クラスターと因子分析の出力のグラフィカルな組織は、クラスター/因子負荷量によって項目をプロットし、最も高い負荷量を持つ次元に項目を割り当てるcluster.plotを使用して行うことができる。  4) 特定の項目複合は、1つの構成要素をどの程度反映しているか？  これは信頼性と一般的な因子飽和の問題である。  この問題に対する複数の解決策は，（Cronbach の）アルファ（alpha, scoreItems），（Revelle の）ベータ（ICLUST），および（McDonald の）オメガ（オメガ階層およびオメガ合計の両方）に帰結する．また、irt.fa 項目反応理論のテクニックを適用して、4元相関行列または多元相関行列の因子分析を行い、その結果を項目困難度と項目識別の標準的な2つのパラメータ化に変換することによっても調べることができます。  項目の情報関数は、項目が最も効果的である場所を示唆する。5) いくつかのアプリケーションでは、データ行列は、異なる人々のための異なる項目のサンプリングから合成的に結合される。  いわゆるSAPA（Synthetic Aperture Personality Assessement：合成開口性格検査）技法は、1人の人がすべての項目を取っていないにもかかわらず、大きな相関行列や共分散行列を形成することを可能にする。このようなデータセットを分析するには、元のデータセットではなく、項目の共分散行列に基づいて項目合成を形成するのが簡単である。  これらの行列は、多くの関数（cluster.cor、fa、ICLUST、principal、mat.regress、factor2cluster.6など）を使って分析することができます。alphaは、1つの尺度を形成する項目の項目間相関だけでなく、いくつかの信頼性の推定値を報告します。score.itemsは、尺度得点、項目間相関、尺度間相関、係数alpha、alpha-1、G6+を報告します。score.multiple.choiceは、多肢選択式の項目を得点化したり、多肢選択式の項目を二分法（0/1）形式に変換したりします。7) 合計または平均の古典的テスト理論（CTT）に基づく得点に加え、1および2パラメータIRTに基づく得点は、scoreIrt.1pl、scoreIrt.2pl、またはより一般的にはscoreIrtで見つけることができる。CTT推定値との相関は高いが、これらのスコアは異なる項目の難易度を利用しており、欠損データの問題に特に適している。8) データがマルチレベル構造を持つ場合（例えば、項目が被験者にネストされ、時間が被験者にネストされる）、multilevel.reliability aka mlr関数は、被験者のデータ、被験者の時間などに対する一般化係数を推定します。mlPlotは、各被験者の時間に対する項目のプロットを提供します。mlArrangeは、従来のワイド出力フォーマットを取り、いくつかのマルチレベル関数に必要なロングフォーマットに変換します。マルチレベルデータに有用な他の関数には、statsByとfaByがある。    sim.anovaは、3元配置分散分析(ANOVA)または線形モデル（反復測定の有無にかかわらず）をシミュレートしたデータを作成します。sim.itemは単純構造データを作成し、sim.circはcircplex構造データを作成し、sim.dichotは二項対立項目のcircplexまたは単純構造データを作成します。  sim.structuralは、一般的な構造モデルに適合する相関行列とデータ行列を生成します。(ビネットを参照).パーソナリティの項目を検討するとき、ある人々は、項目がcircumplex構造を持つ2次元空間に表現されていると議論したがります。  circumplex適合のテストが開発されている。  項目をcircumplexで表現する場合、極座標で見るのが便利である。2つの独立相関または従属相関の差を検定するr.test、2つの表からファイ係数またはユール係数を求める関数、相関係数の信頼区間を求める関数が追加された。    sat.actは、年齢と性別による自己申告のテスト得点のデータです。 galton Galtonの親子の身長のデータセットです。peasは、スイートピーの遺伝に関するGaltonのオリジナルデータセットを再現します。  heightsとcubitsはさらに多くのGaltonデータを提供し、vegetablesは野菜のGuilford選好行列を提供する。citiesは米国の11都市間の航空会社のマイル数を提供する（多次元尺度法のデモデータ）。便利なデータ入力と記述統計クラスター分析と因子分析によるデータ削減信頼性分析のための機能（上記にもいくつかある）。Synthetic Aperture Personality Assessment に特に有用な手続きシミュレートされたデータセットを生成するための関数 Graphical functions (require Rgraphviz) - deprecated Rgraphviz を必要としない Graphical functions Circular statistics (for circadian data analysis) Miscellaneous functionsFunctions that are under development and not recommended for casual use psych or psychTools package に含まれるデータセット psych のデモンストレーションとしても使えるデバッグ関数。
lowerCor は、列名を桁数 + 3 文字に省略し、丸められた対角行列の下側の行列を出力します。  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
NA
NA
入力に応じて、4つの異なる相関の検定が行われます1。ここで、t = r* sqrt(n-2)/sqrt(1-r^2) および se = sqrt((1-r^2)/(n-2)) である。）2) サンプル・サイズ n と n2 (指定されていない場合は n2 = n)の場合、z変換された相関の差のzを2つのzスコアの差の標準誤差で割ったものを求めます： t = (z_1 - z_2) * sqrt(1/((n_1)-3 + (n_2-3))).3) サンプル・サイズ n、相関 r12、r13、r23の場合、2つの従属相関（r12 vs r13）の差を検定します。  4) 標本サイズnについて，異なる変数を含む2つの従属相関の差を検定する。Steiger (1980), Table 1の相関を考えよう：これらはすべて同じ被験者から得られたものなので，どの検定も従属相関でなければならない。  従属相関では、少なくとも3つの相関を指定する必要があります（たとえば、r12, r13, r23）。  位置で指定され、従属相関の検定を行う場合、3つの相関が指定されると、それらはr12, r13, r23の順序であると仮定される：ケースA: 時刻1での男らしさ（M1）は言語能力0.5（r12）と相関し、時刻1での女らしさ（F1）は言語能力r13 =.4と相関し、M1はF1と相関する（r23= .1）。  そして，相関が与えられる： r12 = .4, r13 = .5, r23 = .1, t = -.89 for n =103, すなわち，r.test(n=103, r12=.4, r13=.5,r23=.1) ケース B: 2つの変数（たとえば．FとV)の相関が時間の経過とともに同じであるかどうかを検定する（例：F1V1 = F2V2） r.test(n = 103, r12 = 0.5, r34 = 0.6, r23 = 0.5, r13 = 0.7, r14 = 0.5, r24 = 0.8)
NA
NA
2つのグループがどのように異なるかを報告する方法はたくさんある。  コーエンのd統計量は、プールされたグループ内標準偏差で表現された平均値の差に過ぎない。  rは、効果量の普遍的な尺度で、dの単純な関数であるが、-1～1の境界である。t統計量は、単にd * sqrt(n)/2 であり、したがって標本サイズを反映する。   (M2- M1)/Sp ここで Sp はプールされた標準偏差。  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}.  }  Cohens dはプールされた平方和の除数としてNを使う。  Hedges gはN-2を用いる。  Cohensのdの信頼区間は、dをtに変換し、tの信頼区間を求め、それをdsに変換することで求めることができる。  cohen.dの結果はerror.dots関数を使って表示することができます。  これには、辞書で提供されたラベルが含まれる。  0（1標本の場合）との比較のためにcohen.d.ciを使用して）信頼区間を求める場合、n1を指定します。  これはd = t/sqrt(n1)をもたらしますが、2つの標本間の差の場合はd = 2*t/sqrt(n) (標本サイズが等しい場合 n = n1+ n2)、または標本サイズが等しくない場合はd = t/sqrt(1/n1+1/n2)となります。dを求め、これをd2tを用いてtに変換するので、問題は分散をどのようにプールするかです。7/14/21まで、私はtとしたがってp値を推定するために合計nを使っていました。  問い合わせ（ニュース参照）に応えて、実際の標本サイズns（n1とn2）を使用し、ヘッジg値に基づいてtを求めることに切り替えました。  これは、t.testでvar.equal = TRUEオプションで報告されたt値を生成します。報告された様々な信頼区間は正規理論に基づいており、慎重に解釈されるべきであるというコメントは、おそらく有益でしょう。cohen.d.byは、group2で定義されたデータの各サブセットについて、グループのCohenのdを求めます。  出力の要約は、各グループの各変数のd値の簡略化されたリストを生成する。  d.robustはAlgina et al. 2005)に従い、トリム平均(trim =.2)とWinsorize分散(trim =.2)を求めます。  m2tは、2つのグループの平均、標準偏差、標本サイズが与えられたときのStudentのt.t.検定を報告します。  これは、推定値は提供されているが生データが入手できない統計量をチェックするときに便利である。  デフォルトでは、プーリングされた分散の推定値を与えますが、pooled が FALSE の場合、Welch の補正が適用されます。マハラノビス距離は、個々の ds を結合し、それらのユニークな寄与度で重み付けします：  D=√{d'R^{-1}d}。デフォルトでは、cohen.dは2つのグループ間のマハラノビス距離を求めます（複数のDVがある場合）。これはすべてのDVの相関を求める必要があり、いくつかのペアが存在しないため、その行列が可逆でない場合は失敗することがあります。  したがって，MD=FALSEに設定すると，マハラノビスの計算ができなくなります．Marco del Giudice (2019)は，さまざまな重複係数の観点からdとMdを解釈する方法について議論した非常に有用な論文を持っています．これらは、d2OVL（1つの分布の重なり率）、d2OVL2（共同分布の重なり率）、d2CL（共通言語の効果量）、およびd2U3（上位群の割合が下位群の中央値を超える）の使用によって見つけることができます。OVL = 2 φ(-d/2) は重なり率です（dが大きくなるほど小さくなります）。ここで Phi は正規分布の累積密度関数である.OVL_2 = OVL/(2-OVL)U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )最後の2つは、(abs (d))で大きくなる。  CohenのdとMahalanobis Dのグラフ表示については、scatterHistの例、またはpsychTools::GERASデータセットの例を参照してください。
NA
多変量プロファイルの表示は、一連の線（例えば、matplotを参照）、色（例えば、corPlotを参照）、またはレーダープロットやスパイダープロットで行うことができます。スパイダーは、円周構造を持つと考えられるデータを表示するのに特に適しています。1つの変数だけを他のいくつかの変数の関数として表示するには、レーダーを使用します。  複数のプロットを作成するには、スパイダーを使用する。  いくつかのy値だけを比較するときの追加オプションは、オーバーレイ・プロットである。  あるいは、プロット・オプションを設定して、1ページに複数のプロットを行うこともできます。
調査の参加者が1つの変数で選ばれると、その変数の分散が減少し、結果として相関が減少します。  Thorndike (1949) は、範囲制限の4つのケースを検討しました。  他の人たちは、この議論を続けていますが、ケース番号を変えています。  非制限標本と同様に、制限標本での相関を見つけるために使用できる。  範囲制限に対する信頼性の補正とは異なる。
lowerCor は，対角行列の下側を丸めたものを出力し，列名は桁数 + 3 文字に省略されます．  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
Holzinger and Swineford (1937) は、精神能力について二因子モデル（1つの一般因子といくつかのグループ因子）を導入した。  これは、omega関数またはsemを使用して分析できる階層的因子構造のすばらしい実証データセットです。このようなデータを分析するには、いくつかの方法があります。  1つは、オメガ関数を使用して、Schmid-Leiman変換を使用して階層因数分解を行うことである。これは、探索的モデルとして、そしてomegaSemを用いた確認的モデルとして行うことができます。もう1つの方法は、通常の因子分析を行い、bifactor 回転またはbiquartimin 回転を使用することです。後者の2つの関数は、Jennrich and Bentler (2011) のバイファクター変換とバイクォーティミン変換を実装しています。  バイファクター回転は，局所最小の問題（Mansolf and Reise, 2016）に悩まされるので，探索的分析と確認的分析の混合が望ましいかもしれない．14の変数は、3つの空間テスト、3つのメンタル・スピード・テスト、4つの運動スピード・テスト、4つの言語テストを反映するように順序付けされている。  Holzingerからのもう1つのデータ集合（Holzinger.9）は、9つの認知能力（Holzinger, 1939）を表し、Karl Joreskog (2003)がMINRESアルゴリズムによる因子分析の例として使用し、LISRELのマニュアルにも例NPV.KMとして掲載されています。  このデータ集合は、Grant White中学校の9つのテストの得点です：  「t01_visperc""t02_cubes""t04_lozenges""t06_paracomp""t07_sentcomp""t09_wordmean""t10_addition""t12_countdot "および "t13_sccaps "で、lavaanパッケージの変数x1 ... x9（Grant-White校の場合）として表示されます。もう1つの古典的なデータ集合は、R. P. McDonald (1985, 1999)によって詳細に議論され、SASのPROC CALIS マニュアルと同様に sem パッケージで例として使用されている9変数のThurstone 問題です。  これらの9つのテストは、ThurstoneとThurstone, 1941によって（他のデータに基づいて）3つの要因にグループ化された：Verbal Comprehension、Word Fluency、Reasoningである。元のデータはThurstone and Thurstone (1941)によるものだが、Bechthold (1961)によって再分析され、彼はデータセットを2つに分けた。サンプルサイズは213である。Thurstone (1933)に起因する9つの認知変数のもう1つのセットは、プリンストンのBrigham教授が大学入試委員会に報告した4,175人の学生のデータセットである。  Tucker (1958) は、Turstone and Thburstone (1941)の9変数を、彼のバッテリー間因子分析の例に使用しています。  二因子モデルの最近の応用は、心理的状態の測定である。Reiseのデータセットは、Consumer Assessment of Health Care Provider and Systems調査票に対する35,000以上の観察に基づく相関行列である。Reise, Morizot, and Hays (2007)は、1,000症例に基づく二因子解を記述している。    Reiseらによる5つの因子は、「迅速に治療を受ける」（1-3）、「医師がよくコミュニケーションをとる」（4-7）、「丁寧で親切なスタッフ」（8,9）、「必要な治療を受ける」（10-13）、「医療プランの顧客サービス」（14-16）である。2つのBechtoldtデータセットは、Thurstone and Thurstone (1941)からの2つのサンプルである。  これらは、17の変数を含み、そのうちの9つは、Thurstoneデータ集合を形成するためにMcDonaldによって使用されました。サンプルサイズはそれぞれ212と213である。提案された6つの因子は、記憶、言語、単語、空間、数、推論を反映し、暗記記憶因子を期待するすべての3つのマーカーがある。このセットから9変数がThurstoneデータセットに現れる。同様の構造を持つデータセットがHarmanデータセットにさらに2つある。  これには、Harman link{Harman.Holzinger}が使用したHolzingerの別の9変数（被験者696人）と、link{burt}の8感情変数が含まれます。二因子構造の検定のために調べる価値があるもう1つのデータ集合は、Keith Widamanによって提供されたHolzinger and Swineford (1939)のオリジナル・データを含むholzinger.swinefordデータ集合です。  これはpsychToolsパッケージに入っている。Bechtoldt.1: 能力検査の17 x 17相関行列、N = 212.Bechtoldt.2：能力検査の17 x 17相関行列，N = 213．Holzinger：能力検査の14 x 14相関行列，N = 355 Holzinger.9：能力検査の9 x 9相関行列，N = 145 Reise：健康満足度項目の16 x 16相関行列．  N = 35,000 Thurstone: 能力検査の9 x 9相関行列、N = 213 Thurstone.33: 能力検査の別の9 x 9相関行列、N = 4175 Thurstone:9：さらに別の9×9の能力テストの相関行列、N=710
信頼性は基本的に、omegah、unidim、splitHalfのラッパーにすぎない。Revelle and Condon (2019)は、任意の尺度について少なくとも3つの信頼性統計量を報告することを推奨しているが、ここではそれを簡単に行えるようにする。hist オプションをtrueに設定すると、各テストのスプリット・ハーフ値のヒストグラムと密度プロットも表示されます。reliability の出力を error.dots に渡すと、複数の尺度の信頼性統計量をグラフィカルに表示できます。  しかし、基本的な情報を表示するには、plot.reliability関数を呼び出すだけの方が便利です。1つの尺度の詳細な分析を行うには、完全なオメガ分析を行うことをお勧めします。信頼性関数は、ユーザが複数の尺度を持っていて（おそらくscoreItemsを使用して採点している）、すべての尺度についてより完全な信頼性情報を得たい場合のためのものです。  提案に従って、キーを気にせず、単にオメガとスプリットハーフを行い、結果を描画する機能が追加されました。  keys=NULLを指定するか、使用する項目を指定するだけです。(最初の例を参照してください。)plot.reliabilityは、オメガ、アルファ、一次元性の推定値と同様に、スプリットハーフ値の分布のドットチャート要約を提供します。  plotコマンドを発行するだけでも呼び出すことができる。
NA
現在、fa、principal、omega、irt.fa、fa.extensionに対して実装されている。
現在、fa、principal、omega、irt.fa、fa.extensionに実装されている。
より大きな項目の集合から尺度の集合の合計点または平均点を求めるプロセスは、応用心理測定学および心理測定研究における典型的な問題である。  項目の相互相関から尺度の構造を決定することはできますが、尺度の平均値や分散を求め、さらに分析を行うには、合計点または平均点に基づいて得点を求めるのが一般的です。  奇妙なことに、パーソナリティ尺度のスコアは通常合計で与えられますが、態度のスコアは平均で与えられます。  scoreItems のデフォルトは平均値です。項目の尺度でスケールスコアを報告する方が理にかなっているように思われるからです。  複数の尺度を採点する場合、各尺度の項目のリストと、項目を採点する方向があると便利です。  これはmake.keysを使用してkeys.matrixに変換することもできますし、keys.listとして直接入力することもできます。  尺度の信頼性の推定には、"クロンバックのアルファ"、ガットマンのラムダ6、平均項目間相関など様々なものがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関の場合、 alpha = k * av.r/(1+ (k-1)*av.r).  したがって、アルファ値はテストの長さとテストの同質性の増加関数である。  意外なことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するアプローチは複数存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。α (Cronbach, 1951)はGuttmanのλ3 (Guttman, 1945)と同じで、Lambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx) = (n)/(n-1)(Vx-tr(Vx)/Vx = α計算がとても簡単で、ほとんどの市販プログラムで利用できるためか、αは間違いなく最も頻繁に報告されている内部一貫性の信頼性の尺度です。アルファは、すべての可能なスピット・ハーフ信頼度（テストの長さで補正）の平均です。  単因子テストでは、これは第1因子飽和度の妥当な推定値ですが、テストに微細構造がある場合（すなわち、"塊状 "の場合）は、係数 beta (Revelle, 1979; ICLUSTを参照)とomega_hierchical (omegaを参照) (McDonald, 1999; Revelle and Zinbarg, 2009)が、一般的な因子飽和度のより適切な推定値です。  Guttman's Lambda 6 (G6) は，他のすべての項目の線形回帰（2乗重相関または smc），より正確には，誤差の分散 e_j^2，islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx を説明できる各項目の分散量を考慮する．2乗重相関は、項目の共同性の下界であり、項目数が増えるほど、よりよい推定になります。G6は、テストの塊状性にも敏感であり、単一要因構造の尺度として取られるべきではありません。  塊状のテストでは、アルファより大きくなる。  項目負荷量が等しいテストでは、 α > G6 であるが、負荷量が不均等であったり、一般的な因子がある場合は、 G6 > α である。1つの尺度を採点する場合、尺度内の項目だけからG6を計算するのが普通ですが、論理的には、利用可能なすべての項目から項目信頼性を推定するのが適切です。  アルファとG6*は、どちらもテスト内の項目数と、テスト内の項目の平均相互相関の正の関数です。  ここで行われているように、項目分散とテスト全体の分散から計算される場合、生のアルファは項目分散の違いに敏感です。標準化アルファは、共分散ではなく相関に基づいている。アルファは、KR20として知られるKuderとRichardsonによって開発された2分項目のテストに対する信頼性の初期の推定値の一般化であり、ショートカット近似KR21である。(有用な指標は、信頼できる分散と信頼できない分散の比であり、シグナル/ノイズ比として知られている（Revelle, in prep; Revelle and Condon, in press.を参照）。  これは、s/n = n r/(1-nr) (Cronbach and Gleser, 1964; Revelle and Condon (in press))である。非標準化アルファの標準誤差は、Duhachek and Iacobucci (2005)の式を用いて報告される。単一の尺度のより完全な信頼性分析は、階層的因子分析に基づいてomega_hierchicalとomega_totalを求めるomega関数を用いて行うことができる。  信頼性の最大下界（Greatest Lower Bound）の代替推定値は、guttman関数にあります。Alphaは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子飽和度の低い推定値であり（Revelle and Zinbarg, 2009; Zinbarg et al.とはいえ、これは報告される一般的な統計量である。一般に、アルファの使用は推奨されず、より適切な推定値（omega_hierchical および omega_total）の使用が推奨されるべきである。尺度間の相関は、信頼性の欠如によって減衰する。  信頼性のために相関を補正する（各尺度の信頼性の平方根で割る）ことで、構造を示すのに役立つことがある。  これは尺度間相関行列で行われ、対角線の下に生の相関、対角線の上に減衰していない相関が表示されます。欠損値を扱うには、いくつかの代替方法があります。  デフォルトでは、欠損値はその項目の対応する中央値で置き換えられます。  その代わりに平均値を使うこともできますし（imput="mean"）、欠損値のある被験者だけを取り除くこともできます（missing = FALSE）。  欠損が多いデータでは、利用可能な回答の平均を求めることもできます（impute="none"）。  これは、SAPAプロジェクト（https://www.sapa-project.org/ を参照）の尺度の平均を発見するのに便利です。ほとんどの尺度は、尺度の項目のランダムな部分標本から推定されます。この場合、アルファ信頼度は、各尺度の項目総数に基づいているため、著しく過大評価されます。  観察されたアルファ値」は、項目間相関と項目数の関数であるアルファ値の標準形式を使用して、各尺度の平均回答項目数に基づいています。imput="none "オプションを使用すると、合計を求める（totals="TRUE"）ことができますが、得点は実際の回答パターンよりも回答した項目数を反映するようになるため、警告が表示されます。  1つの可能性は、回答が欠落している尺度だけの得点を削除することです。  scoreItems のデフォルトは、欠損項目を中央値でインプットしますが、scoreFAst のデフォルトは、インプットせず、得点した項目の平均値または合計値に基づいて尺度の得点を返す必要があることに注意してください。これは信頼性の計算を変更し（項目数が減少します）、それらの項目が得点を求めるのに使用されないことを意味します。  delete=FALSEオプションを使用すると、これらの変数は削除されず、得点が求められます。  SMCが正しくないという様々な警告が出ます。  項目に分散がない場合、何を期待しますか？scoreItems は、信頼性統計量だけを見つけるために相関行列に適用することができます。  scoreFast はスコア（インピュテーションの有無に関わらず）を求めるだけで、他の統計量を報告しません。  scoreVeryFastは、さらに削ぎ落とされ、インピュテーションを行わず、観測データに基づいてスコアだけを求めます。統計はありません。
より大きな項目セットから、尺度セットの合計スコアまたは平均スコアを求めるプロセスは、応用心理測定学および心理測定研究における典型的な問題です。  項目の相互相関から尺度の構造を決定することはできますが、尺度の平均値や分散を求め、さらに分析を行うためには、合計点または平均点に基づいて得点を求めるのが一般的です。  奇妙なことに、パーソナリティ尺度のスコアは通常合計で与えられますが、態度のスコアは平均で与えられます。  scoreItems のデフォルトは平均値です。項目の尺度でスケールスコアを報告する方が理にかなっているように思われるからです。  複数の尺度を採点する場合、各尺度の項目のリストと、項目を採点する方向があると便利です。  これはmake.keysを使用してkeys.matrixに変換することもできますし、keys.listとして直接入力することもできます。  尺度の信頼性の推定には、"クロンバックのアルファ"、ガットマンのラムダ6、平均項目間相関など様々なものがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関の場合、 alpha = k * av.r/(1+ (k-1)*av.r).  したがって、アルファ値はテストの長さとテストの同質性の増加関数である。  意外なことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するアプローチは複数存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。α (Cronbach, 1951)はGuttmanのλ3 (Guttman, 1945)と同じで、Lambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx) = (n)/(n-1)(Vx-tr(Vx)/Vx = α計算がとても簡単で、ほとんどの市販プログラムで利用できるためか、αは間違いなく最も頻繁に報告されている内部一貫性の信頼性の尺度です。アルファは、すべての可能なスピット・ハーフ信頼度（テストの長さで補正）の平均です。  単因子テストでは、これは第1因子飽和度の妥当な推定値ですが、テストに微細構造がある場合（すなわち、"塊状 "の場合）は、係数 beta (Revelle, 1979; ICLUSTを参照)とomega_hierchical (omegaを参照) (McDonald, 1999; Revelle and Zinbarg, 2009)が、一般的な因子飽和度のより適切な推定値です。  Guttman's Lambda 6 (G6) は，他のすべての項目の線形回帰（2乗重相関または smc），より正確には，誤差の分散 e_j^2，islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx を説明できる各項目の分散量を考慮する．2乗重相関は、項目の共同性の下界であり、項目数が増えるほど、よりよい推定になります。G6は、テストの塊状性にも敏感であり、単一要因構造の尺度として取られるべきではありません。  塊状のテストでは、アルファより大きくなる。  項目負荷量が等しいテストでは、 α > G6 であるが、負荷量が不均等であったり、一般的な因子がある場合は、 G6 > α である。1つの尺度を採点する場合、尺度内の項目だけからG6を計算するのが普通ですが、論理的には、利用可能なすべての項目から項目信頼性を推定するのが適切です。  アルファとG6*は、どちらもテスト内の項目数と、テスト内の項目の平均相互相関の正の関数です。  ここで行われているように、項目分散とテスト全体の分散から計算される場合、生のアルファは項目分散の違いに敏感です。標準化アルファは、共分散ではなく相関に基づいている。アルファは、KR20として知られるKuderとRichardsonによって開発された2分項目のテストに対する信頼性の初期の推定値の一般化であり、ショートカット近似KR21である。(有用な指標は、信頼できる分散と信頼できない分散の比であり、シグナル/ノイズ比として知られている（Revelle, in prep; Revelle and Condon, in press.を参照）。  これは、s/n = n r/(1-nr) (Cronbach and Gleser, 1964; Revelle and Condon (in press))である。非標準化アルファの標準誤差は、Duhachek and Iacobucci (2005)の式を用いて報告される。単一の尺度のより完全な信頼性分析は、階層的因子分析に基づいてomega_hierchicalとomega_totalを求めるomega関数を用いて行うことができる。  信頼性の最大下界（Greatest Lower Bound）の代替推定値は、guttman関数にあります。Alphaは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子飽和度の低い推定値であり（Revelle and Zinbarg, 2009; Zinbarg et al.とはいえ、これは報告される一般的な統計量である。一般に、アルファの使用は推奨されず、より適切な推定値（omega_hierchical および omega_total）の使用が推奨されるべきである。尺度間の相関は、信頼性の欠如によって減衰する。  信頼性のために相関を補正する（各尺度の信頼性の平方根で割る）ことで、構造を示すのに役立つことがある。  これは尺度間相関行列で行われ、対角線の下に生の相関、対角線の上に減衰していない相関が表示されます。欠損値を扱うには、いくつかの代替方法があります。  デフォルトでは、欠損値はその項目の対応する中央値で置き換えられます。  その代わりに平均値を使うこともできますし（imput="mean"）、欠損値のある被験者だけを取り除くこともできます（missing = FALSE）。  欠損が多いデータでは、利用可能な回答の平均を求めることもできます（impute="none"）。  これは、SAPAプロジェクト（https://www.sapa-project.org/ を参照）の尺度の平均を発見するのに便利です。ほとんどの尺度は、尺度の項目のランダムな部分標本から推定されます。この場合、アルファ信頼度は、各尺度の項目総数に基づいているため、著しく過大評価されます。  観察されたアルファ値」は、項目間相関と項目数の関数であるアルファ値の標準形式を使用して、各尺度の平均回答項目数に基づいています。imput="none "オプションを使用すると、合計を求める（totals="TRUE"）ことができますが、得点は実際の回答パターンよりも回答した項目数を反映するようになるため、警告が表示されます。  1つの可能性は、回答が欠落している尺度だけの得点を削除することです。  scoreItems のデフォルトは、欠損項目を中央値でインプットしますが、scoreFAst のデフォルトは、インプットせず、得点した項目の平均値または合計値に基づいて尺度の得点を返す必要があることに注意してください。これは信頼性の計算を変更し（項目数が減少します）、それらの項目が得点を求めるのに使用されないことを意味します。  delete=FALSEオプションを使用すると、これらの変数は削除されず、得点が求められます。  SMCが正しくないという様々な警告が出ます。  項目に分散がない場合、何を期待しますか？scoreItems は、信頼性統計量だけを見つけるために相関行列に適用することができます。  scoreFast はスコア（インピュテーションの有無に関わらず）を求めるだけで、他の統計量を報告しません。  scoreVeryFastは、さらに削ぎ落とされ、インピュテーションを行わず、観測データに基づいてスコアだけを求めます。統計はありません。
あまり複雑な関数ではありませんが、ltmやeRMパッケージのIRT関数を使用する前に項目を反転させる必要がある場合に便利です。ほとんどの心理関数は、分析の前に反転する必要はなく、関数内で反転します。
被験者内の複数の尺度を調査するとき、すべての試行間の変動に加えて、試行ごとのオブザベーションの変動を考慮することが有用な場合があります。  連続した差の2乗平均 (mssd) および連続した差の2乗平均 (rmssd) は、 σ^2 = Σ(x_i - x_{i+1})^2 /(n-lag) ここで n-lag が使用されているのは、n-lag のケースしかないからです。被験者（グループ）ごとに複数のオブザベーションを持つ複数の被験者（グループ）の場合、グルーピング変数を指定すると、各グループの出力が得られます。   同様の関数は、matrixStats パッケージでも利用可能です。ただし、そのパッケージの varDiff 関数は、MeanSquare ではなく差の分散です。これは単にdiff関数からの結果に分散と標準偏差を適用したものです。おそらく気分を研究するときに便利なautoR関数は、指定されたラグについて各項目の自己相関を求めます。  また、rmssd(root means square successive difference)も返します。これはラグ・データの相関を求めることによって行われる。
これらの項目は、オンライン能力測定法を開発するSAPAプロジェクト(https://www.sapa-project.org/)の一環として収集されたものである(Revelle, Wilt and Rosenthal, 2009)。  スコアの平均値は全国的な標準値よりも高いことから、オンライン性格・能力テストを受ける人の自己選択と、スコアの自己申告バイアスの両方が示唆されている。iq.itemsデータセットも参照のこと。  
モデルがデータにどの程度フィットするかは、統計学の古典的な問題である。  スケーリングの適合統計量の1つは、元の推定値と比較した残差行列の大きさです。
pairs.panelからいくつかのトリックを取り入れた、レイアウトと棒グラフの単純なアプリケーションです。  様々なオプションにより、相関楕円（平均から1シグマと2シグマ）、lowessスムース、線形フィット、ヒストグラム上の密度曲線、相関の値を指定することができます。 ellipse = TRUEはsmooth = TRUEを意味します。  gridオプションは、散布図に背景グリッドを提供します。グループ化変数を使用する場合、各重心の周りに楕円（デフォルトは1 sd）を描画します。これはマハラノビス距離を示すときに便利です。数式入力でグループ化変数を指定することもできます。  )2つのグループのデータをプロットする場合、2つの重心間に矢印を描くことにより、グループ間のマハラノビス差を表示することができる。  この場合、pch=". "を使うと便利です。
pairs.panelからいくつかのトリックを取り入れた、レイアウトと棒グラフの単純な応用です。  様々なオプションにより、相関楕円（平均から1シグマと2シグマ）、lowessスムース、線形フィット、ヒストグラム上の密度曲線、相関の値を指定できます。 ellipse = TRUEはsmooth = TRUEを意味します。  gridオプションは、散布図に背景グリッドを提供します。グループ化変数を使用する場合、各重心の周りに楕円（デフォルトは1 sd）を描画します。これはマハラノビス距離を示すときに便利です。数式入力でグループ化変数を指定することもできます。  )2つのグループのデータをプロットする場合、2つの重心間に矢印を描くことにより、グループ間のマハラノビス差を表示することができる。  この場合、pch=". "を使うと便利です。
Schmid Leiman直交化は、能力領域では典型的ですが、非認知パーソナリティ領域ではあまり見られません。  S-Lは、オメガを推定するための一般因子への項目の負荷量を求める1つの方法です。典型的な例は、不安と抑うつの研究である。  代替モデルは、ICLUSTのような階層的クラスター分析技法を考慮することです。GPArotationパッケージが必要です。3因子は、解を一意に定義するために必要な最小数ですが、2因子解を許容することは時折有用です。  この条件には3つの可能なオプションがあります：2つの低次因子間の一般因子の負荷量を "等しい "に設定し、これはsqrt(因子間の斜交相関)になるか、"第1 "または "第2 "に設定し、この場合、一般因子は第1または第2のグループ因子と等しくなります。モデルが本当によく定義されていないことを示唆するメッセージが発行される．階層モデルの適切性をテストするための診断ツールは、一般因子の分散である各変数の共通分散のパーセンテージであるp2です。  一般に、p2 はあまり分散を持たないはずです。
 Schmid and Leiman (1957)からの2つの人工相関行列．Chen ら (2006)からの1つの実共分散行列と1つの人工共分散行列．  Schmid: Schmid-Leiman 変換を示すために作成された 12 x 12 の人工相関行列：対角上に共分散を持つ12 x 12の行列。  これを共分散行列として扱うと，6 x 6 因子解を示す Chen: Chen ら (2006)の健康関連QOL項目の18 x 18 共分散行列．オブザベーションの数 = 403.  最初の項目は、生活の質の尺度である。  残りの17項目は4つのサブファクターを形成する：項目は以下の通りである：「推論や問題解決が困難か？  "言われたことやされたことへの反応が遅い"、"混乱して一度にいくつもの行動を始める"。  "物や約束をどこに置いたか忘れるか？"; "集中するのが難しいか？" (b) 活力下位尺度：「疲れを感じるか？  「(R)「疲れていると感じますか？(右) "疲れを感じるか"、"活力に満ちているか"。(R).(c）メンタルヘルス下位尺度：c）精神的健康下位尺度：「穏やかで平和な気分ですか」（R）、「落ち込んでブルーな気分ですか」（R）、「とても幸せな気分ですか」（R）、「とても神経質な気分ですか」（R）、「何も元気が出ないほど落ち込んでいますか」（R）。 d）疾病の心配下位尺度：(d)病気の心配下位尺度：「健康のことで恐れていましたか」；「健康についてイライラしていましたか」；「健康は生活の心配事でしたか」。西Chen et al. (2006)による16 x 16の人工共分散行列。
 Schmid and Leiman (1957)からの2つの人工相関行列。Chen et al. (2006)からの実共分散行列と人工共分散行列。  Schmid: Schmid-Leiman 変換を示すために作成された 12 x 12 の人工相関行列：対角上に共分散を持つ12 x 12の行列。  これを共分散行列として扱うと，6 x 6 因子解を示す Chen: Chen ら (2006)の健康関連QOL項目の18 x 18 共分散行列．オブザベーションの数 = 403.  最初の項目は、生活の質の尺度である。  残りの17項目は4つのサブファクターを形成する：項目は以下の通りである：「推論や問題解決が困難か？  "言われたことやされたことへの反応が遅い"、"混乱して一度にいくつもの行動を始める"。  "物や約束をどこに置いたか忘れるか？"; "集中するのが難しいか？" (b) 活力下位尺度：「疲れを感じるか？  「(R)「疲れていると感じますか？(右) "疲れを感じるか"、"活力に満ちているか"。(R).(c）メンタルヘルス下位尺度：c）精神的健康下位尺度：「穏やかで平和な気分ですか」（R）、「落ち込んでブルーな気分ですか」（R）、「とても幸せな気分ですか」（R）、「とても神経質な気分ですか」（R）、「何も元気が出ないほど落ち込んでいますか」（R）。 d）疾病の心配下位尺度：(d)病気の心配下位尺度：「健康のことで恐れていましたか」；「健康のことでイライラしていましたか」；「健康は生活の心配事でしたか」。西Chen et al. (2006)の16 x 16人工共分散行列。
この関数は、scoreItems（複数尺度の場合）と単一尺度のalphaに置き換えられています。より大きな項目集合を与えて、尺度集合の合計スコアまたは平均スコアを求めるプロセスは、心理測定研究における典型的な問題です。  尺度の構造は項目間の相関から求めることができますが、尺度の平均値や分散を求め、さらに分析を行うには、合計スコアや平均スコアを求めるのが一般的です。  尺度の信頼性のさまざまな推定には、「クロンバックのアルファ」や平均項目間相関などがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関で、アルファ = k * av.r/(1+ (k-1)*av.r) となります。  したがって、アルファはテストの長さとテストの同質性の増加関数である。  アルファは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子の飽和度（Zinbarg et al., 2005を参照）の低い推定値であり、総信頼性を過小評価するため、テストの総信頼性の良い推定値ではあるが完全ではない。いずれにせよ，これは報告するのに有用な統計量である．
項目の位置（難易度）と識別のセットがあれば、よりエレガントな方法で被験者の得点を求めることができますが、得点ベクトルX、位置δ、識別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、合計得点よりも多くの情報を得ることができます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単にRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報と採点キー行列を組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと、複数の尺度それぞれについて採点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。これらはmodパラメータを使って選択されます。irt.seは特定の値を持つスコアの標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
項目の位置（難易度）と識別のセットがあれば、よりエレガントな方法で被験者の得点を求めることができますが、得点ベクトルX、位置δ、識別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、合計得点よりも多くの情報を得ることができます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と相関があるようで、その値は.99を超える。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plはirt.faを各尺度の項目に個別に適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  もし、完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を得点化したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報を得点化キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと、複数の尺度のそれぞれについて採点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。irt.seは特定の値を持つ得点の標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
項目の位置（難易度）と識別のセットがあれば、被験者の得点を求めるよりエレガントな方法がありますが、得点ベクトルX、位置δ、識別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、合計得点よりも多くの情報が得られます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と相関があるようで、その値は.99を超える。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合、irt.tauを使用して生データから項目の難易度を求めるか、この情報を採点キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定します。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。これらはmodパラメータを使って選択されます。irt.seは特定の値を持つスコアの標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
より大きな項目群から尺度群の合計点または平均点を求めるプロセスは、応用心理測定学や心理測定研究における典型的な問題です。  尺度の構造は項目の相互相関から求めることができますが、尺度の平均値や分散を求め、さらに分析を行うためには、項目の合計点または平均点に基づいて得点を求めるのが一般的です。  奇妙なことに、パーソナリティ尺度のスコアは通常合計で与えられますが、態度のスコアは平均で与えられます。  scoreItems のデフォルトは平均値です。項目の尺度でスケールスコアを報告する方が理にかなっているように思われるからです。  複数の尺度を採点する場合、各尺度の項目のリストと、項目を採点する方向があると便利です。  これはmake.keysを使用してkeys.matrixに変換することもできますし、keys.listとして直接入力することもできます。  尺度の信頼性の推定には、"クロンバックのアルファ"、ガットマンのラムダ6、平均項目間相関など様々なものがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関の場合、 α = k * av.r/(1+ (k-1)*av.r).  したがって、アルファ値はテストの長さとテストの同質性の増加関数である。  意外なことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するアプローチは複数存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。α (Cronbach, 1951)はGuttmanのλ3 (Guttman, 1945)と同じで、Lambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx) = (n)/(n-1)(Vx-tr(Vx)/Vx = α計算がとても簡単で、ほとんどの市販プログラムで利用できるためか、αは間違いなく最も頻繁に報告されている内部一貫性の信頼性の尺度です。アルファは、すべての可能なスピット・ハーフ信頼度（テストの長さで補正）の平均です。  単因子テストでは、これは第1因子飽和度の妥当な推定値ですが、テストに微細構造がある場合（すなわち、"塊状 "の場合）は、係数 beta (Revelle, 1979; ICLUSTを参照)とomega_hierchical (omegaを参照) (McDonald, 1999; Revelle and Zinbarg, 2009)が、一般的な因子飽和度のより適切な推定値です。  Guttman's Lambda 6 (G6) は，他のすべての項目の線形回帰（2乗重相関または smc），より正確には，誤差の分散 e_j^2，islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx を説明できる各項目の分散量を考慮する．2乗重相関は、項目の共同性の下界であり、項目数が増えるほど、よりよい推定になります。G6は、テストの塊状性にも敏感であり、単一要因構造の尺度として取られるべきではありません。  塊状のテストでは、アルファより大きくなる。  項目負荷量が等しいテストでは、 α > G6 であるが、負荷量が不均等であったり、一般的な因子がある場合は、 G6 > α である。1つの尺度を採点する場合、尺度内の項目だけからG6を計算するのが普通ですが、論理的には、利用可能なすべての項目から項目信頼度を推定するのが適切です。  アルファとG6*は、どちらもテスト内の項目数と、テスト内の項目の平均相互相関の正の関数です。  ここで行われているように、項目分散とテスト全体の分散から計算される場合、生のアルファは項目分散の違いに敏感です。標準化アルファは、共分散ではなく相関に基づいている。アルファは、KuderとRichardsonによって開発された、KR20として知られる2分項目のテストに対する信頼性の初期の推定値、およびKR21として知られるショートカット近似の一般化である。(有用な指標は、信頼できる分散と信頼できない分散の比であり、シグナル/ノイズ比として知られている（Revelle, in prep; Revelle and Condon, in press.を参照）。  これは、s/n = n r/(1-nr) (Cronbach and Gleser, 1964; Revelle and Condon (in press))である。非標準化アルファの標準誤差は、Duhachek and Iacobucci (2005)の式を用いて報告される。単一の尺度のより完全な信頼性分析は、階層的因子分析に基づいてomega_hierchicalとomega_totalを求めるomega関数を用いて行うことができる。  信頼性の最大下界（Greatest Lower Bound）の代替推定値は、guttman関数にあります。Alphaは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子飽和度の低い推定値であり（Revelle and Zinbarg, 2009; Zinbarg et al.とはいえ、これは報告される一般的な統計量である。一般に、アルファの使用は推奨されず、より適切な推定値（omega_hierchical および omega_total）の使用が推奨されるべきである。尺度間の相関は、信頼性の欠如によって減衰する。  信頼性のために相関を補正する（各尺度の信頼性の平方根で割る）ことで、構造を示すのに役立つことがある。  これは尺度間相関行列で行われ、対角線の下に生の相関、対角線の上に減衰していない相関が表示されます。欠損値を扱うには、いくつかの代替方法があります。  デフォルトでは、欠損値はその項目の対応する中央値で置き換えられます。  その代わりに平均値を使うこともできますし（imput="mean"）、欠損値のある被験者だけを取り除くこともできます（missing = FALSE）。  欠損が多いデータでは、利用可能な回答の平均を求めることもできます（impute="none"）。  これは、SAPAプロジェクト（https://www.sapa-project.org/ を参照）の尺度の平均を発見するのに便利です。ほとんどの尺度は、尺度の項目のランダムな部分標本から推定されます。この場合、アルファ信頼度は、各尺度の項目総数に基づいているため、著しく過大評価されます。  観察されたアルファ値」は、項目間相関と項目数の関数であるアルファ値の標準形式を使用して、各尺度の平均回答項目数に基づいています。imput="none "オプションを使用すると、合計を求める（totals="TRUE"）ことができますが、得点は実際の回答パターンよりも回答した項目数を反映するようになるため、警告が表示されます。  1つの可能性は、回答が欠落している尺度だけの得点を削除することです。  scoreItems のデフォルトは、欠損項目を中央値でインプットしますが、scoreFAst のデフォルトは、インプットせず、得点した項目の平均値または合計値に基づいて尺度の得点を返す必要があることに注意してください。これは信頼性の計算を変更し（項目数が減少します）、それらの項目が得点を求めるのに使用されないことを意味します。  delete=FALSEオプションを使用すると、これらの変数は削除されず、得点が求められます。  SMCが正しくないという様々な警告が出ます。  項目に分散がない場合、何を期待しますか？scoreItems は、信頼性統計量だけを見つけるために相関行列に適用することができます。  scoreFast はスコア（インピュテーションの有無に関わらず）を求めるだけで、他の統計量を報告しません。  scoreVeryFastは、さらに削ぎ落とされ、インピュテーションを行わず、観測データに基づいてスコアだけを求めます。統計量もありません。
基本的にscore.itemsと多肢選択式からright/wrongへの変換が組み合わされている。item-wholeの相関はitem overlapのために膨らんでいる。サンプルデータセットはSynthetic Aperture Personality Assessment personality and ability test https://www.sapa-project.org/ から取られている。
これらは、合成相関行列を形成するためにSAPA (https://www.sapa-project.org/)手続きで使用される関数の3つです。  項目の相関行列があれば、それらの項目から構成される尺度の相関行列を求めるのは簡単です。これは、元のデータ行列から行うこともできますし、 scoreItems を使って相関行列から行うこともできます。  キーが重なっている（項目が複数のスケールで採点されている）場合、scoreOverlap は、重なっている共分散（重なっている場合は分散）を、その項目の平均相関または smc 推定値のいずれかを使用して、項目の「真の」分散の対応する最良推定値に置き換えることによって、この重なりを調整します。  これはアルファ信頼性を求めるときの操作と似ています。  これは、Cureton (1966) や Bashaw and Anderson (1966)によって提案されたアイデアと似ていますが、smcまたは平均項目間相関（デフォルト）を使用します。SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa, ICLUST, principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、mat.regress.元の相関にキー行列の(転置)を事前および事後に乗算する多重相関手続きで使用することができます。元の行列からいくつかの相関が欠落している場合、それらの低レベルの相関に基づくスケール間相関の欠落値（NA）につながります。impute=TRUE (デフォルト)の場合、警告が出され、各尺度の欠落していない要素の平均相関に基づいて相関がインプットされます。信頼性のアルファ推定値は、共分散ではなく項目の相関に基づいているので、このアルファ推定値は "標準化アルファ" と呼ばれることもある。  生項目が利用できる場合、標準化アルファと scoreItems を用いて求めた生アルファを比較することが有用である。  scoreOverlapは、尺度や関連する下位尺度を開発するとき、または尺度の代替バージョンを比較するときに、重要な問題に答えます。  項目の重なりの影響を除去することで、観測された合計（平均）スコアによって推定される潜在変数間の関係をよりよく推定することができます。  これは、各被験者に複数のオケージョンがあるESM研究を行う場合に便利です。   また、被験者の部分集合の相関を求めることもできます。例をご覧ください。  欠測の多いESMデータでは、相関行列が正-半正定値にならない可能性が高いことに注意してください。scoreByは、被験者内の相関を調べたい（ESMデータなど）、または被験者のグループ内の相関を調べたい（サブグループによる相関構造の安定性を調べる）、マルチレベルモデルを調べるときに便利です。  どちらの場合も、まずデータをstatsByで処理しなければならない。  尺度の分散を求めるには、statsByのcor="cov "オプションを使う必要がある。
より大きな項目集合を与えて、尺度集合の合計スコアまたは平均スコアを求めるプロセスは、応用心理測定学および心理測定研究における典型的な問題です。  尺度の構造は項目間の相関から求めることができますが、尺度の平均、分散を求め、さらに分析を行うには、合計または平均項目スコアに基づいてスコアを求めるのが一般的です。  奇妙なことに、パーソナリティ尺度のスコアは通常合計値で与えられますが、態度のスコアは平均値で与えられます。  scoreItems のデフォルトは平均値です。項目の尺度でスケールスコアを報告する方が理にかなっているように思われるからです。  複数の尺度を採点する場合、各尺度の項目のリストと、項目を採点する方向があると便利です。  これはmake.keysを使用してkeys.matrixに変換することもできますし、keys.listとして直接入力することもできます。  尺度の信頼性の推定には、"クロンバックのアルファ"、ガットマンのラムダ6、平均項目間相関など様々なものがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関の場合、 α = k * av.r/(1+ (k-1)*av.r).  したがって、アルファ値はテストの長さとテストの同質性の増加関数である。  意外なことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するアプローチは複数存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。α (Cronbach, 1951)はGuttmanのλ3 (Guttman, 1945)と同じで、Lambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx) = (n)/(n-1)(Vx-tr(Vx)/Vx = α計算がとても簡単で、ほとんどの市販プログラムで利用できるためか、αは間違いなく最も頻繁に報告されている内部一貫性の信頼性の尺度です。アルファは、すべての可能なスピット・ハーフ信頼度（テストの長さで補正）の平均です。  単因子テストでは、これは第1因子飽和度の妥当な推定値ですが、テストに微細構造がある場合（すなわち、"塊状 "の場合）は、係数 beta (Revelle, 1979; ICLUSTを参照)とomega_hierchical (omegaを参照) (McDonald, 1999; Revelle and Zinbarg, 2009)が、一般的な因子飽和度のより適切な推定値です。  Guttman のラムダ 6 (G6) は，他のすべての項目の線形回帰（2乗重相関または smc），より正確には，誤差の分散 e_j^2 を説明できる各項目の分散量を考慮し，イスラマダ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.2乗重相関は、項目の共同性の下界であり、項目数が増えるほど、よりよい推定になります。G6は、テストの塊状性にも敏感であり、単一要因構造の尺度として取られるべきではありません。  塊状のテストでは、アルファより大きくなる。  項目負荷量が等しいテストでは、 α > G6 であるが、負荷量が不均等であったり、一般的な因子がある場合は、 G6 > α である。1つの尺度を採点する場合、尺度内の項目だけからG6を計算するのが普通ですが、論理的には、利用可能なすべての項目から項目信頼性を推定するのが適切です。  アルファとG6*は、どちらもテスト内の項目数と、テスト内の項目の平均相互相関の正の関数です。  ここで行われているように、項目分散とテスト全体の分散から計算される場合、生のアルファは項目分散の違いに敏感です。標準化アルファは、共分散ではなく相関に基づいている。アルファは、KR20として知られるKuderとRichardsonによって開発された2分項目のテストに対する信頼性の初期の推定値の一般化であり、ショートカット近似KR21である。(有用な指標は、信頼できる分散と信頼できない分散の比であり、シグナル/ノイズ比として知られている（Revelle, in prep; Revelle and Condon, in press.を参照）。  これは、s/n = n r/(1-nr) (Cronbach and Gleser, 1964; Revelle and Condon (in press))である。非標準化アルファの標準誤差は、Duhachek and Iacobucci (2005)の式を用いて報告される。単一の尺度のより完全な信頼性分析は、階層的因子分析に基づいてomega_hierchicalとomega_totalを求めるomega関数を用いて行うことができる。  信頼性の最大下界（Greatest Lower Bound）の代替推定値は、guttman関数にあります。Alphaは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子飽和度の低い推定値であり（Revelle and Zinbarg, 2009; Zinbarg et al.とはいえ、これは報告される一般的な統計量である。一般に、アルファの使用は推奨されず、より適切な推定値（omega_hierchical および omega_total）の使用が推奨されるべきである。尺度間の相関は信頼性の欠如によって減衰する。  信頼性のために相関を補正する（各尺度の信頼性の平方根で割る）ことで、構造を示すのに役立つことがある。  これは尺度間相関行列で行われ、対角線の下に生の相関、対角線の上に減衰していない相関が表示されます。欠損値を扱うには、いくつかの代替方法があります。  デフォルトでは、欠損値はその項目の対応する中央値で置き換えられます。  その代わりに平均値を使うこともできますし（imput="mean"）、欠損値のある被験者だけを取り除くこともできます（missing = FALSE）。  欠損が多いデータでは、利用可能な回答の平均を求めることもできます（impute="none"）。  これは、SAPAプロジェクト（https://www.sapa-project.org/ を参照）の尺度の平均を発見するのに便利です。ほとんどの尺度は、尺度の項目のランダムな部分標本から推定されます。この場合、アルファ信頼度は、各尺度の項目総数に基づいているため、著しく過大評価されます。  観察されたアルファ値」は、項目間相関と項目数の関数であるアルファ値の標準形式を使用して、各尺度の平均回答項目数に基づいています。imput="none "オプションを使用すると、合計を求める（totals="TRUE"）ことができますが、得点は実際の回答パターンよりも回答された項目数を反映するようになるため、警告が表示されます。  1つの可能性は、回答が欠落している尺度だけの得点を削除することです。  scoreItems のデフォルトは、欠損項目を中央値でインプットしますが、scoreFAst のデフォルトはインプットせず、得点した項目の平均値または合計値に基づいて尺度の得点を返す必要があることに注意してください。これは信頼性の計算を変更し（項目数が減少します）、それらの項目が得点を求めるのに使用されないことを意味します。  delete=FALSEオプションを使用すると、これらの変数は削除されず、得点が求められます。  SMCが正しくないという様々な警告が出ます。  項目に分散がない場合、何を期待しますか？scoreItems は、信頼性統計量だけを見つけるために相関行列に適用することができます。  scoreFast はスコア（インピュテーションの有無に関わらず）を求めるだけで、他の統計量を報告しません。  scoreVeryFastは、さらに削ぎ落とされ、インピュテーションを行わず、観測データに基づいてスコアだけを求めます。統計はありません。
項目の位置（難易度）と識別のセットがあれば、よりエレガントな方法で被験者の得点を求めることができますが、得点ベクトルX、位置δ、識別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最もフィットするθの値を見つけるだけで、合計得点よりも多くの情報を得ることができます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と相関があるようで、その値は.99を超える。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。最後の例を参照してください。2つのラッパー関数 scoreIrt.1pl と scoreIrt.2pl は非常に高速で、1因子モデル（scoreIrt.2pl）または単なる Rasch のようなスコアリングで一度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  もし、完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を得点化したい場合は、irt.tauを使用して生データから項目の難易度を求めるか、この情報を得点化キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定します。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと、複数の尺度のそれぞれについて採点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。irt.seは特定の値を持つ得点の標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
項目の位置（難易度）と弁別のセットがあれば、被験者の得点を求めるよりエレガントな方法がありますが、得点ベクトルX、位置δ、弁別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、合計得点よりも多くの情報が得られます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出した場合、またはすべての項目で最高点だけを出した場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目が欠落する確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受験した項目がすべて正解した場合、その人のスコアは、すべての項目が正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合、irt.tauを使用して生データから項目の難易度を求めるか、この情報を採点キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。これらはmodパラメータを使って選択されます。irt.seは特定の値を持つスコアの標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
項目の位置（難易度）と弁別のセットがあれば、被験者の得点を求めるよりエレガントな方法がありますが、得点ベクトルX、位置δ、弁別βに対して、P(x|θ) = 1/(1+exp(β(δ - θ) ) の式に最も適合するθの値を見つけるだけで、合計得点よりも多くの情報が得られます。  完全なデータでは、総得点とirt推定値はほぼ完全に相関する。  boundsパラメータは推定値の下限と上限を設定する。  boundsパラメータは推定値の下限と上限を設定します。これは、被験者がすべての項目で最低点だけを出したり、すべての項目で最高点だけを出したりする場合に関係します。以前は(1.6.12以前)、このような場合、すべての項目の欠落確率を求め、これを正規分布に基づく分位点数に変換し、その分位点数の1/2に相当するz値を割り当てることによって、これらの分位点数を推定していました。  同様に、ある人が受けた項目をすべて正解した場合、その人のスコアは、すべての項目を正解する確率に相当するzの分位数として定義され、分布の半分を上に移動する。  これらの推定値が上限または下限を超える場合は、それらの境界に調整されます。1.6.9では、手順が大きく変わっています。  すべての項目は、与えられたすべての項目よりも簡単な合格項目と、与えられたどの項目よりも難しい不合格項目とで境界があると仮定するようになった。  Rには、Full Information Maximum Likeliood IRTに基づく推定を提供する、よりエレガントなパッケージがいくつかある。特にMIRTパッケージが優れているようだ。  ldmパッケージは、2値データではMIRTと同等の推定値を与えますが、多値データでは不安定な推定値を出すので避けるべきです。   scoreIrtの推定値はFIMLベースではないが、MIRTの推定値と.99を超える相関があるようだ。  scoreIrtは基本的な構造をうまく復元しているようです。異なるデータセット（標準化サンプルなど）の項目パラメータを使用する場合は、統計量をデータフレームとして指定し、最初の列を項目識別、次の列を項目困難度としてください。scoreIrt.1plとscoreIrt.2plの2つのラッパー関数は非常に高速で、1因子モデル（scoreIrt.2pl）または単なるRaschのようなスコアリングで1度に1つまたは多数の尺度をスコアリングするためのものです。  scoreIrt.2plは、各尺度の項目に個別にirt.faを適用し、2plスコアを求めます。keys.listは各尺度の採点項目のリストです。  項目名の前に負の符号を付けると、その項目は逆に得点されます（scoreIrt.1plに関連します。  別の方法として、make.keys を用いてキー行列を作成することもできます。  キー行列は 1、0、-1 の行列で、ある項目が特定の要素に対して得点されるべきか否かを表します。  詳細は scoreItems または make.keys を参照してください。  デフォルトのケースは、絶対弁別度 > cut の全項目を採点することです。完全なIRT分析は行わないが、項目の位置の違いを利用して尺度を採点したい場合、irt.tauを使用して生データから項目の難易度を求めるか、この情報を採点キー行列と組み合わせ（scoreItemsとmake.keysを参照）、irt.stats.likeを使用して準IRT統計量を作成します。   これは準ラッシュモデルを行うことと同じで、すべての項目が等しく識別できると仮定されます。  この場合、タウ値は（irt.tauを使用して）最初に求めることもできますし、スコアリングを行う前に求めることもできます。  これはすべてscoreIrt.1plの中で行われます。このようなirtベースのスコアは、大量の欠損データ（例えばSAPAデータセット）に基づいてスケールを見つける場合に特に有用です。  完全なirt分析を行わなくても、異なる項目の難易度を考慮することができます。scoreIrt.2pl は生データファイルと複数の尺度それぞれについて得点する項目のリストを受け取ります。  そして、これらが因数分解され（現在は各スケールに対して1つの因子のみ）、負荷量と難易度がスコアリングに使用されます。  従来は、2つの異なる測定基準とモデルが使用されていた。  ロジスティック・メトリックとモデル、そしてノーマル・メトリックとモデルである。irt.seは特定の値を持つ得点の標準誤差を求めます。  これらはirt.faによって計算された情報曲線に基づいており、特定の被験者の特定のスコアに基づいているわけではありません。
より大きな項目群から尺度群の合計点または平均点を求めるプロセスは、応用心理測定学や心理測定研究における典型的な問題です。  尺度の構造は項目の相互相関から求めることができますが、尺度の平均値や分散を求め、さらに分析を行うためには、項目の合計点または平均点に基づいて得点を求めるのが一般的です。  奇妙なことに、パーソナリティ尺度のスコアは通常合計値で与えられますが、態度のスコアは平均値で与えられます。  scoreItems のデフォルトは平均値です。項目の尺度でスケールスコアを報告する方が理にかなっているように思われるからです。  複数の尺度を採点する場合、各尺度の項目のリストと、項目を採点する方向があると便利です。  これはmake.keysを使用してkeys.matrixに変換することもできますし、keys.listとして直接入力することもできます。  尺度の信頼性の推定には、"クロンバックのアルファ"、ガットマンのラムダ6、平均項目間相関など様々なものがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関の場合、 α = k * av.r/(1+ (k-1)*av.r).  したがって、アルファ値はテストの長さとテストの同質性の増加関数である。  意外なことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するアプローチは複数存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。α (Cronbach, 1951)はGuttmanのλ3 (Guttman, 1945)と同じで、Lambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx) = (n)/(n-1)(Vx-tr(Vx)/Vx = α計算がとても簡単で、ほとんどの市販プログラムで利用できるためか、αは間違いなく最も頻繁に報告されている内部一貫性の信頼性の尺度です。アルファは、すべての可能なスピット・ハーフ信頼度（テストの長さで補正）の平均です。  単因子テストでは、これは第1因子飽和度の妥当な推定値ですが、テストに微細構造がある場合（すなわち、"塊状 "の場合）は、係数 beta (Revelle, 1979; ICLUSTを参照)とomega_hierchical (omegaを参照) (McDonald, 1999; Revelle and Zinbarg, 2009)が、一般的な因子飽和度のより適切な推定値です。  Guttman's Lambda 6 (G6) は，他のすべての項目の線形回帰（2乗重相関または smc），より正確には，誤差の分散 e_j^2，islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx を説明できる各項目の分散量を考慮する．2乗重相関は、項目の共同性の下界であり、項目数が増えるほど、よりよい推定になります。G6は、テストの塊状性にも敏感であり、単一要因構造の尺度として取られるべきではありません。  塊状のテストでは、アルファより大きくなる。  項目負荷量が等しいテストでは、 α > G6 であるが、負荷量が不均等であったり、一般的な因子がある場合は、 G6 > α である。1つの尺度を採点する場合、尺度内の項目だけからG6を計算するのが普通ですが、論理的には、利用可能なすべての項目から項目信頼性を推定するのが適切です。  アルファとG6*は、どちらもテスト内の項目数と、テスト内の項目の平均相互相関の正の関数です。  ここで行われているように、項目分散とテスト全体の分散から計算される場合、生のアルファは項目分散の違いに敏感です。標準化アルファは、共分散ではなく相関に基づいている。アルファは、KR20として知られるKuderとRichardsonによって開発された2分項目のテストに対する信頼性の初期の推定値の一般化であり、ショートカット近似KR21である。(有用な指標は、信頼できる分散と信頼できない分散の比であり、シグナル/ノイズ比として知られている（Revelle, in prep; Revelle and Condon, in press.を参照）。  これは、s/n = n r/(1-nr) (Cronbach and Gleser, 1964; Revelle and Condon (in press))である。非標準化アルファの標準誤差は、Duhachek and Iacobucci (2005)の式を用いて報告される。単一の尺度のより完全な信頼性分析は、階層的因子分析に基づいてomega_hierchicalとomega_totalを求めるomega関数を用いて行うことができる。  信頼性の最大下界（Greatest Lower Bound）の代替推定値は、guttman関数にあります。Alphaは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子飽和度の低い推定値であり（Revelle and Zinbarg, 2009; Zinbarg et al.とはいえ、これは報告される一般的な統計量である。一般に、アルファの使用は推奨されず、より適切な推定値（omega_hierchical および omega_total）の使用が推奨されるべきである。尺度間の相関は、信頼性の欠如によって減衰する。  信頼性のために相関を補正する（各尺度の信頼性の平方根で割る）ことで、構造を示すのに役立つことがある。  これは尺度間相関行列で行われ、対角線の下に生の相関、対角線の上に減衰していない相関が表示されます。欠損値を扱うには、いくつかの代替方法があります。  デフォルトでは、欠損値はその項目の対応する中央値で置き換えられます。  その代わりに平均値を使うこともできますし（imput="mean"）、欠損値のある被験者だけを取り除くこともできます（missing = FALSE）。  欠損が多いデータでは、利用可能な回答の平均を求めることもできます（impute="none"）。  これは、SAPAプロジェクト（https://www.sapa-project.org/ を参照）の尺度の平均を発見するのに便利です。ほとんどの尺度は、尺度の項目のランダムな部分標本から推定されます。この場合、アルファ信頼度は、各尺度の項目総数に基づいているため、著しく過大評価されます。  観察されたアルファ値」は、項目間相関と項目数の関数であるアルファ値の標準形式を使用して、各尺度の平均回答項目数に基づいています。imput="none "オプションを使用すると、合計を求める（totals="TRUE"）ことができますが、得点は実際の回答パターンよりも回答した項目数を反映するようになるため、警告が表示されます。  1つの可能性は、回答が欠落している尺度だけの得点を削除することです。  scoreItems のデフォルトは、欠損項目を中央値でインプットしますが、scoreFAst のデフォルトは、インプットせず、得点した項目の平均値または合計値に基づいて尺度の得点を返す必要があることに注意してください。これは信頼性の計算を変更し（項目数が減少します）、それらの項目が得点を求めるのに使用されないことを意味します。  delete=FALSEオプションを使用すると、これらの変数は削除されず、得点が求められます。  SMCが正しくないという様々な警告が出ます。  項目に分散がない場合、何を期待しますか？scoreItems は、信頼性統計量だけを見つけるために相関行列に適用することができます。  scoreFast はスコア（インピュテーションの有無に関わらず）を求めるだけで、他の統計量を報告しません。  scoreVeryFastは、さらに削ぎ落とされ、インピュテーションを行わず、観測データに基づいてスコアだけを求めます。統計はありません。
これらは、SAPA (https://www.sapa-project.org/)手続きで合成相関行列を形成するために使用される3つの関数です。  項目の相関行列があれば、それらの項目で構成される尺度の相関行列を見つけるのは簡単です。これは、元のデータ行列から行うこともできますし、 scoreItems を使って相関行列から行うこともできます。  キーが重なっている（項目が複数のスケールで採点されている）場合、scoreOverlap は、重なっている共分散（重なっている場合は分散）を、その項目の平均相関または smc 推定値のいずれかを使用して、項目の「真の」分散の対応する最良推定値に置き換えることによって、この重なりを調整します。  これはアルファ信頼性を求めるときの操作と似ています。  これは、Cureton (1966) や Bashaw and Anderson (1966)によって提案されたアイデアと似ていますが、smcまたは平均項目間相関（デフォルト）を使用します。SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa, ICLUST, principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、mat.regress.元の相関にキー行列の(転置)を事前および事後に乗算する多重相関手続きで使用することができます。元の行列からいくつかの相関が欠落している場合、それらの低レベルの相関に基づくスケール間相関の欠落値（NA）につながります。impute=TRUE (デフォルト)の場合、警告が出され、各尺度の欠落していない要素の平均相関に基づいて相関がインプットされます。信頼性のアルファ推定値は、共分散ではなく項目の相関に基づいているので、このアルファ推定値は "標準化アルファ "と呼ばれることもある。  生項目が利用できる場合、標準化アルファと scoreItems を用いて求めた生アルファを比較することが有用である。  scoreOverlapは、尺度や関連する下位尺度を開発するとき、または尺度の代替バージョンを比較するときに、重要な問題に答えます。  項目の重なりの影響を除去することで、観測された合計（平均）スコアによって推定される潜在変数間の関係をよりよく推定することができます。  これは、各被験者に複数のオケージョンがあるESM研究を行う場合に便利です。   また、被験者の部分集合の相関を求めることもできます。例をご覧ください。  欠測の多いESMデータでは、相関行列が正-半正定値にならない可能性が高いことに注意してください。scoreByは、被験者内の相関を調べたい（例えば、ESMデータの場合）、または被験者のグループ内の相関を調べたい（サブグループによる相関構造の安定性を調べる場合）多階層モデルを調べるときに便利です。  どちらの場合も、まずデータをstatsByで処理しなければならない。  尺度の分散を求めるには、statsByのcor="cov "オプションを使う必要がある。
より大きな項目集合を与えて、尺度集合の合計スコアや平均スコアを求めるプロセスは、応用心理測定学や心理測定研究における典型的な問題です。  尺度の構造は項目間の相関から求めることができますが、尺度の平均、分散を求め、さらに分析を行うには、合計または平均項目スコアに基づいてスコアを求めるのが一般的です。  奇妙なことに、パーソナリティ尺度のスコアは通常合計で与えられますが、態度のスコアは平均で与えられます。  scoreItems のデフォルトは平均値です。項目の尺度でスケールスコアを報告する方が理にかなっているように思われるからです。  複数の尺度を採点する場合、各尺度の項目のリストと、項目を採点する方向があると便利です。  これはmake.keysを使用してkeys.matrixに変換することもできますし、keys.listとして直接入力することもできます。  尺度の信頼性の推定には、"クロンバックのアルファ"、ガットマンのラムダ6、平均項目間相関など様々なものがあります。  k = 尺度の項目数、av.r = 尺度の項目間の平均相関の場合、 α = k * av.r/(1+ (k-1)*av.r).  したがって、アルファ値はテストの長さとテストの同質性の増加関数である。  意外なことに、スピアマン（1904）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するアプローチは複数存在する。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。α (Cronbach, 1951)はGuttmanのλ3 (Guttman, 1945)と同じで、Lambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx) = (n)/(n-1)(Vx-tr(Vx)/Vx = α計算がとても簡単で、ほとんどの市販プログラムで利用できるためか、αは間違いなく最も頻繁に報告されている内部一貫性の信頼性の尺度です。アルファは、すべての可能なスピット・ハーフ信頼度（テストの長さで補正）の平均です。  単因子テストでは、これは第1因子飽和度の妥当な推定値ですが、テストに微細構造がある場合（すなわち、"塊状 "の場合）は、係数 beta (Revelle, 1979; ICLUSTを参照)とomega_hierchical (omegaを参照) (McDonald, 1999; Revelle and Zinbarg, 2009)が、一般的な因子飽和度のより適切な推定値です。  Guttman's Lambda 6 (G6) は，他のすべての項目の線形回帰（2乗重相関または smc），より正確には，誤差の分散 e_j^2，islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx を説明できる各項目の分散量を考慮する．2乗重相関は、項目の共同性の下界であり、項目数が増えるほど、よりよい推定になります。G6は、テストの塊状性にも敏感であり、単一要因構造の尺度として取られるべきではありません。  塊状のテストでは、アルファより大きくなる。  項目負荷量が等しいテストでは、 α > G6 であるが、負荷量が不均等であったり、一般的な因子がある場合は、 G6 > α である。1つの尺度を採点する場合、尺度内の項目だけからG6を計算するのが普通ですが、論理的には、利用可能なすべての項目から項目信頼度を推定するのが適切です。  アルファとG6*は、どちらもテスト内の項目数と、テスト内の項目の平均相互相関の正の関数です。  ここで行われているように、項目分散とテスト全体の分散から計算される場合、生のアルファは項目分散の違いに敏感です。標準化アルファは、共分散ではなく相関に基づいている。アルファは、KuderとRichardsonによって開発された、KR20として知られる2分項目のテストに対する信頼性の初期の推定値、およびKR21として知られるショートカット近似の一般化である。(有用な指標は、信頼できる分散と信頼できない分散の比であり、シグナル/ノイズ比として知られている（Revelle, in prep; Revelle and Condon, in press.を参照）。  これは、s/n = n r/(1-nr) (Cronbach and Gleser, 1964; Revelle and Condon (in press))である。非標準化アルファの標準誤差は、Duhachek and Iacobucci (2005)の式を用いて報告される。単一の尺度のより完全な信頼性分析は、階層的因子分析に基づいてomega_hierchicalとomega_totalを求めるomega関数を用いて行うことができる。  信頼性の最大下界（Greatest Lower Bound）の代替推定値は、guttman関数にあります。Alphaは、一般因子の大きさを著しく過大評価する可能性があるため、テストの一般因子飽和度の低い推定値であり（Revelle and Zinbarg, 2009; Zinbarg et al.とはいえ、これは報告される一般的な統計量である。一般に、アルファの使用は推奨されず、より適切な推定値（omega_hierchical および omega_total）の使用が推奨されるべきである。尺度間の相関は、信頼性の欠如によって減衰する。  信頼性のために相関を補正する（各尺度の信頼性の平方根で割る）ことで、構造を示すのに役立つことがある。  これは尺度間相関行列で行われ、対角線の下に生の相関、対角線の上に減衰していない相関が表示されます。欠損値を扱うには、いくつかの代替方法があります。  デフォルトでは、欠損値はその項目の対応する中央値で置き換えられます。  その代わりに平均値を使うこともできますし（imput="mean"）、欠損値のある被験者だけを取り除くこともできます（missing = FALSE）。  欠損が多いデータでは、利用可能な回答の平均を求めることもできます（impute="none"）。  これは、SAPAプロジェクト（https://www.sapa-project.org/ を参照）の尺度の平均を発見するのに便利です。ほとんどの尺度は、尺度の項目のランダムな部分標本から推定されます。この場合、アルファ信頼度は、各尺度の項目総数に基づいているため、著しく過大評価されます。  観察されたアルファ値」は、項目間相関と項目数の関数であるアルファ値の標準形式を使用して、各尺度の平均回答項目数に基づいています。imput="none "オプションを使用すると、合計を求める（totals="TRUE"）ことができますが、得点は実際の回答パターンよりも回答した項目数を反映するようになるため、警告が表示されます。  1つの可能性は、回答が欠落している尺度だけの得点を削除することです。  scoreItems のデフォルトは、欠損項目を中央値でインプットしますが、scoreFAst のデフォルトは、インプットせず、得点した項目の平均値または合計値に基づいて尺度の得点を返さなければならないことに注意してください。これは信頼性の計算を変更し（項目数が減少します）、それらの項目が得点を求めるのに使用されないことを意味します。  delete=FALSEオプションを使用すると、これらの変数は削除されず、得点が求められます。  SMCが正しくないという様々な警告が出ます。  項目に分散がない場合、何を期待しますか？scoreItems は、信頼性統計量だけを見つけるために相関行列に適用することができます。  scoreFast はスコア（インピュテーションの有無に関わらず）を求めるだけで、他の統計量を報告しません。  scoreVeryFastは、さらに削ぎ落とされ、インピュテーションを行わず、観測データに基づいてスコアだけを求めます。統計はありません。
bestScalesの重みを使って相関重み付けされたスコアを求めるためのものですが、lmの係数によって返されるような別の重み行列を使うこともできます。   
最適な因子数を選択する多くの方法の中に、scree検定があります。  screeを表示し、ランダムに並列化された解と比較するためのより良い関数が fa.parallel にある。
直接行うこともできるが、時に厄介な問題を解決する。  指定された値をNAに置き換えるか、範囲内の値に再コード化する。
ベクトル、行列、data.frameの標準偏差を求めます。  R < 2.7.0またはR >= 2.8.0で見つかった機能を返すようにstats:sd関数を適応させただけです。この問題は修正されたようなので、SDはいずれ削除されるでしょう。
scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2plのキーを用意する最も簡単な方法は、keys.listを指定することです。  以前のバージョン（1.6.9以前）では、キーはmake.keysを使用してすべての項目に対して-1、0、1のマトリックスとして形成されていました。  scoreItems、scoreOverlap、scoreIrt.1pl、scoreIrt.2pl関数のキーを作成するには3つの方法があります。make.keysでは、項目名で指定することも、位置で指定することも、その両方を混在させることもできます。keys2listでは、make.keysの処理を逆にし、キーとなる各項目の項目名を含むスコアリングキーのリストを返します。  sign=FALSEの場合、これは単なる採点項目のリストとなる。(scoreIrt.2plselectFromKeysに便利です。)keys.listから符号を取り除き、それらのキーに関連する項目名のベクトル(重複を削除)を作成します。  これはkeys.listを使ってスケールを定義し、keys.listのサブセットに含まれる項目だけを選択する場合に便利です。  これは、スピードアップのため、採点関数の中で行われるようになりました。これらのスコアリング関数scoreItems, scoreOverlap, scoreIrt.1pl, scoreIrt.2plは現在（>バージョン1.6.9）keys.listを入力として受け取ることができるので、make.keysはそれほど重要ではありませんが、ドキュメンテーションのために残してあります。項目名を指定するには、item.labels値を使用するか、データファイルの名前またはスコアリングされるデータファイルのcolnamesを最初の（nvars）位置に置く必要があります。番号（位置）で指定する場合、nvarsは、使用される項目の数だけでなく、スコアリングされるオブジェクトの項目の総数になります。  makePositiveKeysは、(bestScalesなどから)キーのサブセットを取得し、正と負にキー設定された項目に対して別々のキーを作成するのに便利です。
お勧めの関数はstructure.diagramで、Rgraphvizを使用しませんが、ドットコードも生成しません。  3つのstructure関数はすべて、semまたはlavaanパッケージで使用するのに適したコマンドの行列を返します。  (sem.diagramとsem.graphは、semパッケージで実行された単純なCFAからの出力を変換し、structure.diagramまたはstructure.graph.lavaan.diagramを使って描画します。diagramは、lavaanパッケージで行われた単純なCFAからの出力（フィット）を変換し、structure.diagramを使って描画します。X変数間の相関（Rxが指定された場合） X変数とそれらの潜在因子（fxが指定された場合） 潜在Xと潜在Y（Phiが指定された場合） 潜在Yと観測されたY（fyが指定された場合） Y変数間の相関（Ryが指定された場合） 確定因子モデルではfxとPhiだけが指定され、構造モデルではfx、Phi、fyが含まれます。  lavaan.diagramは、適合のクラスに応じてfa.diagram、omega.diagram、iclust.diagramを呼び出すdiagram関数から呼び出すことができます。  これらの関数はすべて、dia.rect、dia.ellipse、dia.arrow、dia.curve、dia.curved.arrow、dia.shape などのさまざまな dia 関数を使用します。
推奨関数はstructure.diagramで、これはRgraphvizを使用しないが、ドットコードも生成しない。  3つのstructure関数はすべて、semまたはlavaanパッケージで使用するのに適したコマンドの行列を返します。  (sem.diagramとsem.graphは、semパッケージで実行された単純なCFAからの出力を変換し、structure.diagramまたはstructure.graph.lavaan.diagramを使って描画します。diagramは、lavaanパッケージで行われた単純なCFAからの出力（フィット）を変換し、structure.diagramを使って描画します。X変数間の相関（Rxが指定された場合） X変数とそれらの潜在因子（fxが指定された場合） 潜在Xと潜在Y（Phiが指定された場合） 潜在Yと観測されたY（fyが指定された場合） Y変数間の相関（Ryが指定された場合） 確定因子モデルではfxとPhiだけが指定され、構造モデルではfx、Phi、fyが含まれます。  lavaan.diagramは、適合のクラスに応じてfa.diagram、omega.diagram、iclust.diagramを呼び出すdiagram関数から呼び出すことができます。  これらの関数はすべて、dia.rect、dia.ellipse、dia.arrow、dia.curve、dia.curved.arrow、dia.shape などのさまざまな dia 関数を使用します。
生データから重回帰や正準相関を計算する方が一般的ですが、相関や共分散の行列から計算することももちろん可能です。  この場合，関数への入力は，x（予測変数），y（基準変数），必要であれば z（共変量）の列番号（または名前）と同様に，正方共分散または相関行列である．入力は， y 変数の集合と x 変数の集合のどちらかで，これは lm の標準的な数式スタイルで記述できる（最後の例を参照）．  この場合、一対またはそれ以上の相互作用（積項）を指定することもできる。  デフォルトでは，積項を見つけるとき，予測変数はゼロ・センタリングされるが（Cohen, Cohen, West and Aiken, 2003），lm の結果または Hayes (2013)で議論されている結果と一致させるために，このオプションをオフ（zero=FALSE）にできる．  除去される共変量は、数式入力で負の符号を付けるか、z 変数を使用して指定します。  共変量を指定するとき，回帰は，偏微分された変数で行われたように行われることに注意すること．  これは、自由度とR2が、パーティション化された変数の回帰を反映することを意味する。(最後の例を参照) 共変量を使用する場合、従属変数だけでなく、独立変数（part=FALSE、デフォルト、これは偏相関を意味する）または独立変数（part=TRUE または part 別名semi-partial correlations）だけからそれらを取り除く必要があります。  部分相関と偏相関の違いは、共変量の分散がDVとIVの両方から取り除かれるか（偏相関）、IVだけから取り除かれるか（部分相関）です。  回帰の傾きは変わりませんが、部分相関を使うとDVの分散量（したがって標準誤差）が大きくなります。  共変量に偏相関を使用することは，他の変数の効果を解釈するときに，回帰で共変量を使用することと同じである．追加出力は，Cohenの集合相関 (Cohen, 1982)を用いて発見されたR2である．  Cohen (1982) は、2つの変数集合の間の全体的な関係を測定する重相関の多変量一般化である集合相関を導入しました。これは正準相関 (Hotelling, 1936) と 1 - ∏(1-ρ_i^2) の応用で、ρ_i^2 は2乗正準相関です。  集合相関は、2つの変数の集合の間の共有分散（R2）の量である。  3番目の共変量集合を追加すると、集合相関は、多変量R2、部分R2、準部分R2を求めます（準部分と2部分オプションはまだ実装されていません）。 集合相関の詳細は、Cohen (1982)、Cohen (1988)、Cohen, Cohen, Aiken and West (2003)にあります。2つの集合間の R2 は， R2 = 1- |R| /(|Ry| * |Rx|) で，ここで R は x と y 変数の完全な相関行列で，Rx と Ry は関係する2つの集合である．  代替のT2 は，相加分散の割合で，正準の2乗の平均である．  (Cohen et al., 2003), Cramer and Nicewander (1979) も参照。  この平均は、非常に小さな正準相関を含むので、小さすぎる傾向がある。  Cohenらの警告は適切である：「しかし、最終的な分析においては、分析者は、関連性の尺度の選択において、手元の問題に対する実質的かつ方法論的な概念に導かれなければならない。( p613).さらに、2つの集合間の関連性のもう1つの尺度は、2つの集合間の単純な重みなしの相関である。つまり、Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1'))であり、Rxyは2つの集合間の相関の行列である。  これは各行列の相関の単純な(重み付けされていない)和である。この手法は、線形モデルのロバストな美しさを例証し、xとyの両方が1次元の場合に特に適切で、ベータが符号で異なる項目の場合は、大幅に過小評価されます。αで行われるように、重みなし相関を求めるとき、項目はすべて正符号になるように反転されます。  SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa,ICLUST,principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、setCorを使用して複数のR手続きで使用することができます。行列全体は相関が欠損していてもかまいませんが、予測に使用される行列のサブセットの相関は存在しなければなりません。オブザベーションの数が入力されると、従来の信頼区間、統計的有意性、および縮小推定値が報告されます。入力が長方形（正方形ではない）の場合、相関または共分散がデータから求められます。print関数はベータ重みについてt値とp値を報告し、summary関数はベータ重みだけを報告します。  つまり、VIF > 10は、共線性を定義する魔法のカットオフではない。  crossValidationは、setCorまたはbestScalesの結果を取り出し、その重みを別のデータセットに適用するために使用できます。matPlotは、crossValidationの値をプロットするために使用できます（x軸にラベルを指定してmatplotを呼び出すだけです）。matPlotは、凡例を描画し、線種を指定できるように改良されました。setCorLookupは、ベータ重みをソートし、辞書が与えられた場合、項目の内容とともに報告します。matRegは、主にmediateのヘルパー関数ですが、共分散行列と指定されたx、y、z変数を与える一般的な重回帰関数です。その出力は、ベータ、se、t、p、R2です。  matRegは、データ行列では動作せず、数式入力も受け付けません。  matRegは、データ行列では動作せず、計算式の入力も受け付けません。
生データから重回帰や正準相関を計算する方が一般的ですが、相関や共分散の行列から計算することももちろん可能です。  この場合，関数への入力は，x（予測変数），y（基準変数），そして必要であれば z（共変量）の列番号（または名前）と同様に，正方共分散または相関行列である．入力は， y 変数の集合と x 変数の集合のどちらかで，これは lm の標準的な数式スタイルで記述できる（最後の例を参照）．  この場合、一対またはそれ以上の相互作用（積項）を指定することもできる。  デフォルトでは，積項を見つけるとき，予測変数はゼロ・センタリングされるが（Cohen, Cohen, West and Aiken, 2003），lm の結果やHayes (2013)で議論されている結果と一致させるために，このオプションをオフにできる（zero=FALSE）．  除去される共変量は、数式入力で負の符号を付けるか、z 変数を使用して指定します。  共変量を指定するとき，回帰は，偏微分された変数で行われたように行われることに注意すること．  これは、自由度とR2が、パーティション化された変数の回帰を反映することを意味する。(最後の例を参照) 共変量を使用する場合、従属変数だけでなく、独立変数（part=FALSE、デフォルト、これは偏相関を意味する）または独立変数（part=TRUE または part 別名semi-partial correlations）からも取り除く必要があります。  部分相関と偏相関の違いは、共変量の分散がDVとIVの両方から取り除かれるか（偏相関）、IVだけから取り除かれるか（部分相関）です。  回帰の傾きは変わりませんが、部分相関を使うとDVの分散量（したがって標準誤差）が大きくなります。  共変量に偏相関を使用することは，他の変数の効果を解釈するときに，回帰で共変量を使用することと同じである．追加出力は，Cohenの集合相関 (Cohen, 1982)を用いて発見されたR2である．  Cohen (1982) は、2つの変数集合の間の全体的な関係を測定する重相関の多変量一般化である集合相関を導入しました。これは正準相関 (Hotelling, 1936) と 1 - ∏(1-ρ_i^2) の応用で、ρ_i^2 は2乗正準相関です。  集合相関は、2つの変数の集合の間の共有分散（R2）の量である。  3番目の共変量集合を追加すると、集合相関は、多変量R2、部分R2、準部分R2を求めます（準部分と2部分オプションはまだ実装されていません）。 集合相関の詳細は、Cohen (1982)、Cohen (1988)、Cohen, Cohen, Aiken and West (2003)にあります。2つの集合間の R2 は， R2 = 1- |R| /(|Ry| * |Rx|) で，ここで R は x と y 変数の完全な相関行列で，Rx と Ry は関係する2つの集合である．  代替のT2 は，相加分散の割合で，正準の2乗の平均である．  (Cohen et al., 2003), Cramer and Nicewander (1979) も参照。  この平均は、非常に小さな正準相関を含むので、小さすぎる傾向がある。  Cohenらの警告は適切である：「しかし、最終的な分析においては、分析者は、関連性の尺度の選択において、手元の問題に対する実質的かつ方法論的な概念に導かれなければならない。( p613).さらに、2つの集合間の関連性のもう1つの尺度は、2つの集合間の単純な重みなしの相関である。つまり、Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1'))であり、Rxyは2つの集合間の相関の行列である。  これは各行列の相関の単純な(重み付けされていない)和である。この手法は、線形モデルのロバストな美しさを例証し、xとyの両方が1次元の場合に特に適切で、ベータが符号で異なる項目の場合は、大幅に過小評価されます。αで行われるように、重みなし相関を求めるとき、項目はすべて正符号になるように反転されます。  SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa,ICLUST,principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、setCorを使用して複数のR手続きで使用することができます。行列全体は相関が欠損していてもかまいませんが、予測に使用される行列のサブセットの相関は存在しなければなりません。オブザベーションの数が入力されると、従来の信頼区間、統計的有意性、および縮小推定値が報告されます。入力が長方形（正方形ではない）の場合、相関または共分散がデータから求められます。print関数はベータ重みについてt値とp値を報告し、summary関数はベータ重みだけを報告します。  つまり、VIF > 10は、共線性を定義する魔法のカットオフではない。  crossValidationは、setCorまたはbestScalesの結果を取り出し、その重みを別のデータセットに適用するために使用できます。matPlotは、crossValidationの値をプロットするために使用できます（x軸にラベルを指定してmatplotを呼び出すだけです）。matPlotは、凡例を描画し、線種を指定できるように改良されました。setCorLookupは、ベータ重みをソートし、辞書が与えられた場合、項目の内容とともに報告します。matRegは、主にmediateのヘルパー関数ですが、共分散行列と指定されたx、y、z変数を与える一般的な重回帰関数です。その出力は、ベータ、se、t、p、R2です。  matRegは、データ行列では動作せず、数式入力も受け付けません。  matRegは、データ行列では動作せず、計算式の入力も受け付けません。
生データから重回帰や正準相関を計算する方が一般的ですが、相関や共分散の行列から計算することももちろん可能です。  この場合，関数への入力は，x（予測変数），y（基準変数），および必要であれば z（共変量）の列番号（または名前）と同様に，正方共分散または相関行列である．入力は， y 変数の集合と x 変数の集合のどちらかで，これは lm の標準的な数式スタイルで記述できる（最後の例を参照）．  この場合、一対またはそれ以上の相互作用（積項）を指定することもできる。  デフォルトでは，積項を見つけるとき，予測変数はゼロ・センタリングされるが（Cohen, Cohen, West and Aiken, 2003），lm の結果やHayes (2013)で議論されている結果と一致させるために，このオプションをオフにできる（zero=FALSE）．  除去される共変量は、数式入力で負の符号を付けるか、z 変数を使用して指定します。  共変量を指定するとき，回帰は，あたかも偏微分された変数で行われたかのように行われることに注意すること．  これは、自由度とR2が、パーティション化された変数の回帰を反映することを意味する。(最後の例を参照) 共変量を使用する場合、従属変数だけでなく、独立変数（part=FALSE、デフォルト、これは偏相関を意味する）または独立変数（part=TRUE または part 別名semi-partial correlations）だけからそれらを取り除く必要があります。  部分相関と偏相関の違いは、共変量の分散がDVとIVの両方から取り除かれるか（偏相関）、IVだけから取り除かれるか（部分相関）です。  回帰の傾きは変わりませんが、部分相関を使うとDVの分散量（したがって標準誤差）が大きくなります。  共変量に偏相関を使用することは，他の変数の効果を解釈するときに，回帰で共変量を使用することと同じである．追加出力は，Cohenの集合相関 (Cohen, 1982)を用いて発見されたR2である．  Cohen (1982) は、2つの変数集合の間の全体的な関係を測定する重相関の多変量一般化である集合相関を導入しました。これは正準相関 (Hotelling, 1936) と 1 - ∏(1-ρ_i^2) の応用で、ρ_i^2 は2乗正準相関です。  集合相関は、2つの変数の集合の間の共有分散（R2）の量である。  3番目の共変量集合を追加すると、集合相関は、多変量R2、部分R2、準部分R2を求めます（準部分と2部分オプションはまだ実装されていません）。 集合相関の詳細は、Cohen (1982)、Cohen (1988)、Cohen, Cohen, Aiken and West (2003)にあります。2つの集合間の R2 は， R2 = 1- |R| /(|Ry| * |Rx|) で，ここで R は x と y 変数の完全な相関行列で，Rx と Ry は関係する2つの集合である．  代替のT2 は，相加分散の割合で，正準の2乗の平均である．  (Cohen et al., 2003), Cramer and Nicewander (1979) も参照。  この平均は、非常に小さな正準相関を含むので、小さすぎる傾向がある。  Cohenらの警告は適切である：「しかし、最終的な分析においては、分析者は、関連性の尺度の選択において、手元の問題に対する実質的かつ方法論的な概念に導かれなければならない。( p613).さらに、2つの集合間の関連性のもう1つの尺度は、2つの集合間の単純な重みなしの相関である。つまり、Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1'))であり、Rxyは2つの集合間の相関の行列である。  これは各行列の相関の単純な(重み付けされていない)和である。この手法は、線形モデルのロバストな美しさを例証し、xとyの両方が1次元の場合に特に適切で、ベータが符号で異なる項目の場合は、大幅に過小評価されます。αで行われるように、重みなし相関を求めるとき、項目はすべて正符号になるように反転されます。  SAPAプロジェクトでの典型的な使用法は、クラスタリングまたはファクタリング（fa,ICLUST,principalを参照）によって項目の複合を形成し、これらの結果からクラスタを抽出し（factor2cluster）、cluster.corを用いて複合相関行列を形成することです。  この縮小行列の変数は、setCorを使用して複数のR手続きで使用することができます。行列全体は相関が欠損していてもかまいませんが、予測に使用される行列のサブセットの相関は存在しなければなりません。オブザベーションの数が入力されると、従来の信頼区間、統計的有意性、および縮小推定値が報告されます。入力が長方形（正方形ではない）の場合、相関または共分散がデータから求められます。print関数はベータ重みについてt値とp値を報告し、summary関数はベータ重みだけを報告します。  つまり、VIF > 10は、共線性を定義する魔法のカットオフではない。  crossValidationは、setCorまたはbestScalesの結果を取り出し、その重みを別のデータセットに適用するために使用できます。matPlotは、crossValidationの値をプロットするために使用できます（x軸にラベルを指定してmatplotを呼び出すだけです）。matPlotは、凡例を描画し、線種を指定できるように改良されました。setCorLookupは、ベータ重みをソートし、辞書が与えられた場合、項目の内容とともに報告します。matRegは、主にmediateのヘルパー関数ですが、共分散行列と指定されたx、y、z変数を与える一般的な重回帰関数です。その出力は、ベータ、se、t、p、R2です。  matRegは、データ行列では動作せず、数式入力も受け付けません。  matRegは、データ行列では動作せず、計算式の入力も受け付けません。
fa.lookupとlookupは、相関行列または因子負荷行列を要約するための単純なヘルパー関数です。bestItemsは、xの指定された列（基準）を列の（絶対）値に基づいてソートします。  デフォルトとして返されるのは、それらの絶対値 > cut を持つ変数の名前だけである。   項目内容と項目名の辞書がある場合、項目名に対応する rowname を持つ 2 列 (またはそれ以上) の行列として内容を含める。(辞書の例bfi.dictionaryを参照).lookupは、bestItemsによって使用され、xの値と一致するyのc1の値を見つける。  そして、lookup(rownames(x),y)によって、xに対応する辞書の項目を見つけることができます。列が指定されていない場合は、rownames(y)によってマッチします。fa.lookupは、因子分析の出力を調べ、対応する変数名と内容が欲しいときに使用されます。返されたオブジェクトは、charオプションをTRUEに設定してdf2latex関数を使用することによりLaTexで印刷することができます。fa.lookupは、fa、pcaまたはomegaからの出力で動作します。  同様に、x変数の相関行列rが与えられたとき、別の項目または尺度と最も相関する項目を見つけ、辞書からその項目の内容を表示したい場合、bestItems(r,c1=xの列番号または名前, contents = y)item.lookupは、因子分析faからの出力と単純な記述統計（平均値のデータフレーム）を辞書と組み合わせます。  項目は、因子負荷量 > カットでグループ化され、項目平均でソートされる。  これにより、項目の支持の意味という観点から、尺度がどのように機能するかをよりよく理解することができます。 lookupItems は、ある内容を持つすべての項目を辞書から検索します。  返されたオブジェクトのrownameは項目番号であり、この番号を他の関数で使用して、それらの項目を持つ尺度の統計量（ωなど）を求めることができます。   スケールと項目の相関行列が与えられた場合、そのスケールと項目の相関も表示されます。
lowerCor は，対角行列の下側を丸めたものを出力し，列名は桁数 + 3 文字に省略されます．  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のビネットを参照してください。  ここで説明するシミュレーションは、中核となる機能です。  シミュレーションは、1つの因子構造（fx）または2つの因子構造（fxとfy）をシミュレートし、fxとfyは変数の因子負荷量を表します。  fyの使用は、特にセミモデルのシミュレーションに適しています。  これは、sim.structuralのヘルプに詳しく説明されています。おそらく、最も理解しやすいシミュレーションは、sim.structuralです。  因子モデル(fx)と、おそらく2つの因子セットPhiの間の相互相関を持つfy。  これは相関行列R = fx' phi fyを生成します。  因子は、muを指定することにより、その平均値を異ならせることができます。  sim.structureのデフォルト値は、因子間のシンプレックス構造を持つ4因子12変数のデータ集合を生成します。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルで変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では，因子の1つが "一般的 "とラベルされ，ω推定値が大きすぎる．  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは，グループ内の相関がグループ平均間の相関を意味せず，またそれによっても暗示されないことを実証することを可能にする．
ANOVA, 回帰および信頼性について教えるための簡単なシミュレーション．  デフォルトでは、カテゴリーIV（因子）が生成されます。  2水準以上のIVでは、線形モデルとanovaの違いを比較の観点から示します。これらは、withinの値として指定されたrと平均の相互相関（信頼度）を持ちます。中心化と非中心化の効果を実証するには、 factors = center=FALSE とします。デフォルトは、IVをセンタリングします。中心化しないことで、高次の交互作用項を考慮すると、低次の効果が不正確になります。
多くの性格テストや認知テストは、階層的な因子構造を持っています。  デモの目的には、母集団値または標本値のいずれかで、このような行列を作成できると便利です。項目の因子負荷量（fload）と一般因子へのこれらの因子の負荷量（gload）の行列が与えられている場合、一般因子の法則（R = F' theta F ここで theta = g'g）を使用して母集団相関行列を作成します。  デフォルトでは，母集団相関行列が返される．gloadとfloadのデフォルト値は、JensenとWeng, 1994によって議論されたデータ行列を作成します。階層構造を作成するように書かれていますが、gload行列がすべて0である場合、非階層構造が生成されます。さらに別のモデルとして、独立結合がg因子モデルと同じ因子構造を生成することを示唆したGodfrey H. Thomson (1916)のモデルがあります。これはsim.bondsでシミュレートされている。  sim.hierarchicalとsim.bondsモデルのω解を比較します。どちらも妥当なωの値を生成しますが、一方は一般因子なしで生成されました。
このシミュレーションはもともと、影響の測定におけるスキューの効果を比較するために開発されたものである（Rafaeli and Revelle, 2005参照）。  このシミュレーションは、単純構造または円周構造を持つ感情またはパーソナリティ項目の一般的なシミュレーションを可能にするために拡張されました。  項目は、連続的な正規分布にすることも、n個のカテゴリー（例：-2、-1、0、1、2）に分けることもできる。  項目の平均が（例えば1）であっても、これらの範囲に制限することで項目が歪む可能性があります。  item.dichotを追加することで、異なる難易度（支持）の二項対立項目を持つ構造をテストすることができます。  2つの項目セットに対して、単純構造または円周構造を持つ2つの因子データが生成され、1つは、low (easy)値より大きいすべての項目に対してスコア1を与え、もう1つは、high (hard)値より大きいすべての項目に対してスコア1を与える。つまり、すべての項目の支持率は50％と仮定される。  項目の難易度の効果を調べるために、lowを-1、highを1とすることもできる。これにより、項目の支持率は、簡単なものが0.84、難しいものが0.16となる。  各難易度セットの中で、最初の1/4は第1因子、2番目は第2因子、3番目は第1因子（ただし負荷量はマイナス）、4番目は第2因子（ただし負荷量はマイナス）に割り当てられる。sim.itemとsim.hierarchicalの結果を比較するのは便利です。sim.itemは、2つの直交因子と同様に、すべての項目を通る一般因子を生成します。  これは、標準的な回転技法では表現しにくいデータセットを生成します。  回転なしで3つの因子を抽出し、2番目と3番目の因子を回転させると、正しい解が得られます。  しかし、3因子の単純な斜め回転やオメガ分析では、根本的な構造を捉えることはできません。  最後の例をご覧ください。さらに、魅力的かもしれないもう1つの構造は、3次元の完全に複雑なデータです。  sim.sphericalはこのようなデータを作成します。
信頼性分析用の例を作成する場合、共属データ構造をシミュレートするのが便利です。  これは最も単純な項目構造で、因子が1つしかありません。主に信頼性理論の議論や因子得点の推定に用いられます。暗黙の共分散行列は，パターン %*% t(pattern)だけである．
測定モデル fx と構造モデル Phi を考えると，モデルは f %*% Phi %*% t(f).   信頼性は f %*% t(f). f φ f' で、各テストの信頼性は、項目の共同性、または単にモデルのダイアグです。相関行列を作成する場合，(uniq=NULL)，対角は1に設定され，そうでない場合，対角は diag(model) + uniq で，結果の構造は共分散行列である．構造モデルの特別なケースは，並列検定，タウ等価検定，および共属検定などの1因子モデルである．  これらは、構造行列 = 1 とし、因子負荷量のベクトルを定義することで作成できます。また、sim.congenericでも同じことができます。一般的にはsimCor、別名sim.correlationを使用します。これは特定のサンプルサイズに対して、指定された相関行列からサンプリングされたデータを作成します。必要であれば、サンプルの相関行列を返します。  data=TRUEを指定すると、サンプルデータも返されます。  これは、元の行列の固有値分解とランダムな正規偏差の行列（Brian RipleyのMASSパッケージのmvnorm関数から採用したコード）を使用します。  これらの結果のスコアは、いくつかの変換（skew オプションを参照）を使って変換したり、変数のすべてまたは選択されたセット（vars オプション）を2値変数（quant オプションを参照）にしたりすることができます。  
このシミュレーションは、もともと、影響の測定におけるskewの効果を比較するために開発されました（Rafaeli and Revelle, 2005を参照）。  このシミュレーションは、単純構造または円周構造を持つ情動またはパーソナリティ項目の一般的なシミュレーションを可能にするために拡張されました。  項目は、連続的な正規分布にすることも、n個のカテゴリー（例：-2、-1、0、1、2）に分けることもできる。  項目の平均が（例えば1）であっても、これらの範囲に制限することで項目が歪む可能性があります。  item.dichotを追加することで、異なる難易度（支持）の二項対立項目を持つ構造をテストすることができます。  2つの項目のセットに対して、単純構造または円周構造を持つ2つの因子データが生成され、1つは、low (easy)値より大きいすべての項目に対してスコア1を与え、もう1つは、high (hard)値より大きいすべての項目に対してスコア1を与える。つまり、すべての項目の支持率は50％と仮定される。  項目の難易度の効果を調べるために、lowを-1、highを1とすることもできる。これにより、項目の支持率は、簡単なものが0.84、難しいものが0.16となる。  各難易度セットの中で、最初の1/4は第1因子、2番目は第2因子、3番目は第1因子（ただし負荷量はマイナス）、4番目は第2因子（ただし負荷量はマイナス）に割り当てられる。sim.itemとsim.hierarchicalの結果を比較するのは便利です。sim.itemは、2つの直交因子と同様に、すべての項目を通る一般因子を生成します。  これは、標準的な回転技法では表現しにくいデータセットを生成します。  回転なしで3つの因子を抽出し、2番目と3番目の因子を回転させると、正しい解が得られます。  しかし、3因子の単純な斜め回転やオメガ分析では、根本的な構造を捉えることはできません。  最後の例をご覧ください。さらに、魅力的かもしれないもう1つの構造は、3次元の完全に複雑なデータです。  sim.sphericalはそのようなデータを作成します。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールです。  真理」を知ることで、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、付属のビネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成される構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って2値項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけを抽出した場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関が、グループ平均間の相関を意味するものでも、それによって暗示されるものでもないことを示すことができる。
多くの性格テストや認知テストは階層的な因子構造を持っている。  実証のためには、母集団値または標本値でそのような行列を作成できると便利です。項目の因子負荷量（fload）と一般因子へのこれらの因子の負荷量（gload）の行列が与えられている場合、一般因子の法則（R = F' theta F ここで theta = g'g）を使用して母集団相関行列を作成します。  デフォルトでは，母集団相関行列が返される．gloadとfloadのデフォルト値は、JensenとWeng, 1994によって議論されたデータ行列を作成します。階層構造を作成するように書かれていますが、gload行列がすべて0の場合、非階層構造が生成されます。さらに別のモデルとして、独立結合がg因子モデルと同じ因子構造を生成することを示唆したGodfrey H. Thomson (1916)のモデルがあります。これはsim.bondsでシミュレートされている。  sim.hierarchicalとsim.bondsモデルのω解を比較します。どちらも妥当なΩの値を生成しますが、一方は一般因子なしで生成されています。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールです。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用に関するもっと長い議論については、添付のビネットを参照されたい。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関が、グループ平均間の相関を意味するものでも、グループ平均間の相関によって暗示されるものでもないことを実証することができる。
このシミュレーションはもともと、影響の測定におけるゆがみの効果を比較するために開発されたものである（Rafaeli and Revelle, 2005参照）。  このシミュレーションは、単純構造または円周構造を持つ感情またはパーソナリティ項目の一般的なシミュレーションを可能にするために拡張されました。  項目は、連続的な正規分布にすることも、n個のカテゴリー（例：-2、-1、0、1、2）に分けることもできる。  項目の平均が（例えば1）であっても、これらの範囲に制限することで項目が歪む可能性があります。  item.dichotを追加することで、異なる難易度（支持）の二項対立項目を持つ構造をテストすることができます。  2つの項目のセットに対して、単純構造または円周構造を持つ2つの因子データが生成され、1つは、low (easy)値より大きいすべての項目に対してスコア1を与え、もう1つは、high (hard)値より大きいすべての項目に対してスコア1を与える。つまり、すべての項目の支持率は50％と仮定される。  項目の難易度の効果を調べるために、lowを-1、highを1とすることもできる。これにより、項目の支持率は、簡単なものが0.84、難しいものが0.16となる。  各難易度セットの中で、最初の1/4は第1因子、2番目は第2因子、3番目は第1因子（ただし負荷量はマイナス）、4番目は第2因子（ただし負荷量はマイナス）に割り当てられる。sim.itemとsim.hierarchicalの結果を比較するのは便利です。sim.itemは、2つの直交因子と同様に、すべての項目を通る一般因子を生成します。  これは、標準的な回転技法では表現しにくいデータセットを生成します。  回転なしで3つの因子を抽出し、2番目と3番目の因子を回転させると、正しい解が得られます。  しかし、3因子の単純な斜め回転やオメガ分析では、根本的な構造を捉えることはできません。  最後の例をご覧ください。さらに、魅力的かもしれないもう1つの構造は、3次元の完全に複雑なデータです。  sim.sphericalはそのようなデータを生成します。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールです。  真理」を知ることで、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のビネットを参照してください。  ここで説明するシミュレーションは、中核となる機能です。  シミュレーションは、1つの因子構造（fx）または2つの因子構造（fxとfy）をシミュレートし、fxとfyは変数の因子負荷量を表します。  fyの使用は、特にセミモデルのシミュレーションに適しています。  これは、sim.structuralのヘルプで詳しく説明されています。おそらく、最も理解しやすいシミュレーションは、sim.structuralです。  因子モデル(fx)と、おそらく2つの因子セットPhiの間の相互相関を持つfy。  これは相関行列R = fx' phi fyを生成します。  因子は、muを指定することにより、その平均値を異ならせることができます。  sim.structureのデフォルト値は、因子間のシンプレックス構造を持つ4因子12変数のデータ集合を生成します。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルで変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけを抽出した場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関が、グループ平均間の相関を意味するものでも、それによって暗示されるものでもないことを実証することができる。
sim.multiは、伝統的なマルチレベルデータの2レベル構造をモデル化するために、被験者内データを生成します。これは、ntrialにわたって測定された被験者内データが、被験者内と被験者間でどのように独立に変化するかを示すことを意図しています。  nvar変数への負荷量を持つn因子について、n.obs被験者についての因子スコアが作成されます。  単純構造モデルが仮定されるので，nvar/因子への負荷量は，各因子について負荷量に設定され，その他は0に設定される．  (これは被験者ごとに異なっていてもよい)。スコアはβ.iの傾きで経時的に変化し、時間の正弦と余弦の関数として日周的に変化することができる(24時間/日をラジアンに変換)。  誤差は、すべての試行に追加され、1ラグで試行をまたいで関連づけることができます。したがって、AR1=1とすると、時刻tでの誤差=誤差+t -1での誤差となります。 sim.multilevelは、単にプール相関（群間相関と群内相関の混合）をシミュレートし、マルチレベル・モデリングに固有の問題をよりよく理解できるようにします。  データ(wg)は特定の群内構造(rwg)で作成されます。  独立データ(bg)もグループ間構造(rbg)で作成される。このデータ行列には ncases 個の行があるが，ngroups 個の独立ケースしかないことに注意．  つまり、すべてのngroupsケースは繰り返しである。結果のデータ・フレーム(xy)は、wgとbgの重み付き和である。  これは、観測されたrxyからrwgとrbgを推定する逆手順で、statsBy関数によって行われます。* η_{y_{within}} * r_{xy_{within* r_{xy_{within}}+ η_{x_{between}} * η_{x_{y_{between* η_{y_{between}} * r_{xy_{between* r_{xy_{between}}
グループ内相関とグループ間相関の独立性の基本概念は、Pedhazur (1997)やBliese (2009)によって非常に明確に議論されています。 sim.multiは、伝統的なマルチレベル・データの2レベル構造をモデルするために、被験者内データを生成します。これは、ntrialにわたって測定された被験者内データが、被験者内と被験者間でどのように独立に変化するかを示すことを意図しています。  nvar変数への負荷量を持つn因子について、n.obs被験者についての因子スコアが作成されます。  単純構造モデルが仮定されるので，nvar/因子への負荷量は，各因子について負荷量に設定され，その他は0に設定される．  (これは被験者ごとに異なっていてもよい)。スコアはβ.iの傾きで経時的に変化し、時間の正弦と余弦の関数として日周的に変化することができる(24時間/日をラジアンに変換)。  誤差は、すべての試行に追加され、1ラグで試行をまたいで関連づけることができます。したがって、AR1=1とすると、時刻tでの誤差=誤差+t -1での誤差となります。 sim.multilevelは、単にプール相関（群間相関と群内相関の混合）をシミュレートし、マルチレベル・モデリングに固有の問題をよりよく理解できるようにします。  データ(wg)は特定の群内構造(rwg)で作成されます。  独立データ(bg)もグループ間構造(rbg)で作成される。このデータ行列には ncases 個の行があるが，ngroups 個の独立ケースしかないことに注意．  つまり、すべてのngroupsケースは繰り返しである。結果のデータ・フレーム(xy)は、wgとbgの重み付き和である。  これは、観測されたrxyからrwgとrbgを推定する逆手順で、statsBy関数によって行われます。* η_{y_{within}} * r_{xy_{within* r_{xy_{within}}+ η_{x_{between}} * η_{x_{y_{between* η_{y_{between}} * r_{xy_{between* r_{xy_{between}}
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のビネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って2値項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成される構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では，因子の1つが "一般的 "とラベルされ，ω推定値が大きすぎる．  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って2値項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成される構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って2値項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけを抽出した場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))で、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけを抽出した場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))で、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけを抽出した場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  sim.structureのデフォルト値は、4因子12変数のデータセットを生成し、因子間はシンプレックス構造になっています。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルに変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布すると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))です。ここで、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K個の因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関がグループ平均間の相関を意味するものでも、またそれによって暗示されるものでもないことを示すことができる。
データ構造のシミュレーションは、心理測定研究や教育において非常に有用なツールである。  真理」を知ることによって、様々なアルゴリズムがどの程度真理を捉えることができるかを見ることができる。  心理測定におけるシミュレーションの使用については、添付のヴィネットを参照してください。  ここで説明するシミュレーションは、中核となる機能です。  シミュレーションは、1つの因子構造（fx）または2つの因子構造（fxとfy）をシミュレートし、fxとfyは変数の因子負荷量を表します。  fyの使用は、特にセミモデルのシミュレーションに適しています。  これは、sim.structuralのヘルプに詳しく説明されています。おそらく、最も理解しやすいシミュレーションは、sim.structuralです。  因子モデル(fx)と、おそらく2つの因子セットPhiの間の相互相関を持つfy。  これは相関行列R = fx' phi fyを生成します。  因子は、muを指定することにより、その平均値を異ならせることができます。  sim.structureのデフォルト値は、因子間のシンプレックス構造を持つ4因子12変数のデータ集合を生成します。これと項目のシンプレックス（sim.simplex）は、自己回帰成分（alpha）と安定形質成分（λ）を持つSTARSモデルで変換することもできます。探索的因子分析の特別な課題である2つのデータ構造は、シンプレックス構造とマイナー因子の存在です。  sim.simplex構造は，通常，発達または学習の文脈で発生し，隣接する変数間ではrの相関構造を持ち，n 個離れた変数ではr^nの相関構造を持つ．  1つの潜在変数（r）だけを推定する必要がありますが、構造はnvar-1個の因子を持ちます。  シンプレックスの代替バージョンは、状態-特性-自己回帰構造（STARS）で、これは、自己回帰パスαを持つシンプレックスの状態構造とパスλを持つ特性構造の両方を持ちます。これは、sim.simplexで、ゼロでないλ値を指定してシミュレートします。因子構造の多くのシミュレーションは、主要因子を除いて、すべての残差が0付近に正規分布していると仮定します。  したがって、課題は主要因を識別することです。sim.minorはこのような構造を生成します。  生成された構造は、いくつかの小さな相関残差を持つ主要因子構造を持つと考えることができます。これらのシミュレーションを完全なものにするために、一般因子の可能性が考慮される。  簡単のために、sim.minorでは、g、fmajor、fminorからサンプリングされる負荷量の集合を指定することができます。  また、完全な因子行列を指定することもできます。もう1つの考慮すべき構造は、いくつかのグループ因子を持つ一般因子の直接モデリングです。  これは sim.general.coefficient ω を用いて行われます。係数 ω は、単因子検定（おそらくいくつかの下位因子を持つ検定）の一般的な因子の飽和の非常に有用な指標ですが、複数の独立因子の場合には問題があります。  この状況では、因子の1つが "一般的 "とラベルされ、ω推定値が大きすぎます。  この状況は、generalをNULLとしてsim.omega関数を使用して調べることができます。  一般因子が存在する場合、sim.omegaの結果は、EFAまたはSEMから推定されたomegaは、それを識別するためにかなり良い仕事をするが、Schmid-Leiman変換を使用したEFAアプローチは、SEMアプローチよりもいくらかロバストであることを示唆しています。sim.rasch、sim.irt、sim.npl、sim.npnの4つのirtシミュレーションは、項目反応モデルに従って二項対立項目をシミュレートします。sim.irtは、モデルの仕様によって、sim.npl（ロジスティック・モデルの場合）またはsim.npn（正規モデルの場合）を呼び出すだけです。ロジスティック・モデルは、P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ)))で、γは下側漸近または推測パラメータ、ζは上側漸近（通常は1）、αは項目識別、δは項目困難度です。  1 Paramater Logistic (Rasch) モデルでは， gamma=0, zeta=1, alpha=1 で，項目困難度が指定する唯一の自由パラメータである．3PLまたは3PNモデルでは、項目は推測パラメータc =γでも異なる。4PLモデルと4PNモデルでは、上方漸近線z= ζも指定される。  (正規モデル（irt.npn）は、irt.nplで使用されるロジスティック関数の代わりにpnormを使用して確率を計算しますが、パラメータの意味は同じです。  ロジスティック・モデルのa = αパラメータ = 1.702で、2つのモデルは実質的に同じです。2分法のIRTシミュレーションと並行して、多項目モデルをシミュレーションするpolyバージョンもあります。  これらは、いくつのカテゴリーをシミュレートするかという追加パラメータを持っています。  さらに、sim.poly.ideal関数は、回答確率が各被験者の理想点からの距離によって変化する理想点または展開モデルをシミュレートします。  これは、パーソナリティ・アンケートの回答のより適切なモデルであると主張する人もいる。  このモデルは、2因子モデルに適合するようなシンプレクスのような構造をもたらします。  デフォルトでは、θパラメータは平均mu=0、sd=1の正規分布として各関数で作成されます。  別のシミュレーションから等価なθを指定したい場合や、特定の実験条件で固定されたθを指定したい場合は、以前のシミュレーションの出力からθオブジェクトを取り出すか、必要なプロパティを用いてθオブジェクトを作成してください。これまでの関数は、すべて1つの潜在形質を仮定しています。  また、sim.poly.mat関数を用いて、特定の構造を持つ2値項目や多値項目をシミュレーションすることもできます。  これは，母集団の相関行列，母集団のマージン，標本サイズを入力として取ります．  sim.structure測定モデルと構造モデルを1つのデータ行列にまとめる関数。  構造方程式モデルを理解するのに便利。  sim.congeneric古典的なテスト理論を実証するために、同種の項目/テストを作成する関数。sim.hierarchical 階層（2因子）構造を持つデータを作成する関数。 sim.item 単純構造または円周構造を持つ項目を作成する関数。sim.circ 円周構造を持つデータを作成する関数。sim.dichot 単純構造または円周構造を持つ2値項目データを作成する関数。sim.minor n個のオブザベーションに対して、nfact個の主要因子とnvar/2個の「マイナー」因子で定義されるnvar個の変数の因子構造を作成する関数。  標準因子モデルは，K個の主要因子（K << nvar）が変数間の相関を説明することを仮定するが，R = FF' + U^2 ここで，R はランクPで，F は因子係数のP x K 行列で，U は一意性の対角行列である．  しかし，多くの場合，特に項目で作業する場合，同様に考慮される必要がある多くの小さな因子（相関残差と呼ばれることもある）がある．  これは，R = FF' + MM' + U^2 のようなデータ構造になり，ここで R は相関の P x P 行列，F は因子負荷の P x K 行列，M は小因子負荷の P x P/2 行列，U は一意性の対角行列（P x P）です．  このような相関行列は、K因子だけが抽出された場合、適合度という点ではχ^2値が悪くなります。sim.minorは、負荷量が0.6から0.8の大きな因子と、負荷量が-0.2から0.2の小さな因子を持つこのようなデータセットを生成します。  sim.parallelは、並列分析がどのように機能するかを示すために、sim.minorを使っていくつかのシミュレーションデータセットを作成します。  sim.anova反復測定の有無にかかわらず、3元配置バランスANOVAまたは線形モデルをシミュレートします。sim.multilevel マルチレベルモデリングの基本概念を理解するために、マルチレベル構造を作成することができます。  集計されたデータの相関は、「生態学的相関」と呼ばれることがあります。  グループレベルと個人レベルの相関が独立であることは、このような推論を問題にします。  このシミュレーションは、グループ内の相関が、グループ平均間の相関を意味するものでも、グループ平均間の相関によって暗示されるものでもないことを実証することができる。
このシミュレーションはもともと、影響の測定におけるゆがみの効果を比較するために開発されたものである（Rafaeli and Revelle, 2005参照）。  このシミュレーションは、単純構造または円周構造を持つ感情またはパーソナリティ項目の一般的なシミュレーションを可能にするために拡張されました。  項目は、連続的な正規分布にすることも、n個のカテゴリー（例：-2、-1、0、1、2）に分けることもできる。  項目の平均が（例えば1）であっても、これらの範囲に制限することで項目が歪む可能性があります。  item.dichotを追加することで、異なる難易度（支持）の二項対立項目を持つ構造をテストすることができます。  2つの項目のセットに対して、単純構造または円周構造を持つ2つの因子データが生成され、1つは、low (easy)値より大きいすべての項目に対してスコア1を与え、もう1つは、high (hard)値より大きいすべての項目に対してスコア1を与える。つまり、すべての項目の支持率は50％と仮定される。  項目の難易度の効果を調べるために、lowを-1、highを1とすることもできる。これにより、項目の支持率は、簡単なものが0.84、難しいものが0.16となる。  各難易度セットの中で、最初の1/4は第1因子、2番目は第2因子、3番目は第1因子（ただし負荷量はマイナス）、4番目は第2因子（ただし負荷量はマイナス）に割り当てられる。sim.itemとsim.hierarchicalの結果を比較するのは便利です。sim.itemは、2つの直交因子と同様に、すべての項目を通る一般因子を生成します。  これは、標準的な回転技法では表現しにくいデータセットを生成します。  回転なしで3つの因子を抽出し、2番目と3番目の因子を回転させると、正しい解が得られます。  しかし、3因子の単純な斜め回転やオメガ分析では、根本的な構造を捉えることはできません。  最後の例をご覧ください。さらに、魅力的かもしれないもう1つの構造は、3次元の完全に複雑なデータです。  sim.sphericalはこのようなデータを生成します。
測定モデルfxと構造モデルPhiを考えると、モデルは f %*% Phi %*% t(f)となります。   信頼性はf %*% t(f)です。 f φ f'で、各テストの信頼性は項目の共同性、またはモデルのダイアグだけです。相関行列を作成する場合，(uniq=NULL)，対角は1に設定され，そうでない場合，対角は diag(model) + uniq で，結果の構造は共分散行列である．構造モデルの特別なケースは，並列検定，タウ等価検定，および共属検定などの1因子モデルである．  これらは、構造行列 = 1 とし、因子負荷量のベクトルを定義することで作成できます。また、sim.congenericでも同じことができます。一般的にはsimCor、別名sim.correlationを使用します。これは特定のサンプルサイズに対して、指定された相関行列からサンプリングされたデータを作成します。必要であれば、サンプルの相関行列を返します。  data=TRUEを指定すると、サンプルデータも返されます。  これは、元の行列の固有値分解とランダムな正規偏差の行列（Brian RipleyのMASSパッケージのmvnorm関数から採用したコード）を使用します。  これらの結果のスコアは、いくつかの変換（skew オプションを参照）を使って変換したり、変数のすべてまたは選択されたセット（vars オプション）を2値変数（quant オプションを参照）にしたりすることができます。  
測定モデル fx と構造モデル Phi が与えられると、モデルは f %*% Phi %*% t(f) となる。   信頼性は f %*% t(f). f φ f' で、各テストの信頼性は、項目の共同性またはモデルのダイアグだけです。相関行列を作成する場合，(uniq=NULL)，対角は1に設定され，そうでない場合，対角は diag(model) + uniq で，結果の構造は共分散行列である．構造モデルの特別なケースは，並列検定，タウ等価検定，および共属検定などの1因子モデルである．  これらは、構造行列 = 1 とし、因子負荷量のベクトルを定義することで作成できます。また、sim.congenericでも同じことができます。一般的にはsimCor、別名sim.correlationを使用します。これは特定のサンプルサイズに対して、指定された相関行列からサンプリングされたデータを作成します。必要であれば、サンプルの相関行列を返します。  data=TRUEを指定すると、サンプルデータも返されます。  これは、元の行列の固有値分解とランダムな正規偏差の行列（Brian RipleyのMASSパッケージのmvnorm関数から採用したコード）を使用します。  これらの結果のスコアは、いくつかの変換（skew オプションを参照）を使って変換したり、変数のすべてまたは選択されたセット（vars オプション）を2値変数（quant オプションを参照）にしたりすることができます。  
NA
測定モデル fx と構造モデル Phi が与えられると、モデルは f %*% Phi %*% t(f) となる。   信頼性は f %*% t(f). f φ f' で、各テストの信頼性は、項目の共同性またはモデルのダイアグだけです。相関行列を作成する場合，(uniq=NULL)，対角は1に設定され，そうでない場合，対角は diag(model) + uniq で，結果の構造は共分散行列である．構造モデルの特別なケースは，並列検定，タウ等価検定，および共属検定などの1因子モデルである．  これらは、構造行列 = 1 とし、因子負荷量のベクトルを定義することで作成できます。あるいは、sim.congenericでも同じことができます。一般的にはsimCor、別名sim.correlationを使用します。これは特定のサンプルサイズに対して、指定された相関行列からサンプリングされたデータを作成します。必要であれば、サンプルの相関行列を返します。  data=TRUEを指定すると、サンプルデータも返されます。  これは、元の行列の固有値分解とランダムな正規偏差の行列（Brian RipleyのMASSパッケージのmvnorm関数から採用したコード）を使用します。  これらの結果のスコアは、いくつかの変換（skew オプションを参照）を使って変換したり、変数のすべてまたは選択されたセット（vars オプション）を2値変数（quant オプションを参照）にしたりすることができます。  
"心理データを表現するための一般的なモデルは、単純構造である（Thurstone, 1947）。1つの一般的な解釈によると、項目または尺度が1つだけの因子にゼロでない因子負荷量を持つとき、データは単純構造である(Revelle & Rocklin, 1979)。単純構造の一般的な適用にもかかわらず、いくつかの心理学モデルは単純構造の欠如によって定義される。サーカンプレックス（Guttman, 1954）は、単純構造が欠如しているモデルの一種である。第一に、円周構造は変数が相互に関連していることを最低限意味している。第二に、circumplex構造は、問題の領域が2つの次元によって最適に表現されることを意味する。第三に、circumplex 構造は、単純構造のように変数が2つの軸に沿ってグループ化したり、固まったりするのではなく、直交する軸のペアの間に常に間歇的な変数が存在することを意味する(Saucier, 1992)。理想的な場合、この性質は、円の円周に沿った変数の等間隔に反映される（Gurtman, 1994; Wiggins, Steiger, & Gaelick, 1981）。第4に、円周構造は、変数が円の中心から一定の半径を持つことを意味し、これはすべての変数が2つの円周次元で等しい共同性を持つことを意味する（Fisher, 1997; Gurtman, 1994）。第5に、円plex 構造は、すべての回転がドメインの等しく良い表現であることを意味する（Conte & Plutchik, 1981; Larsen & Diener, 1992）。(ActonとRevelle, 2004)ActonとRevelleは、10個の円周構造のテストの有効性を検討し、4個のテストが円周構造と単純構造、または円周構造と楕円構造を識別するのに特に優れていることを発見した。残念なことに、彼らの研究はPascalで行われたため、簡単に入手することができない。等間隔性のギャップ検定軸の等質性のフィッシャーの検定回転に対する無関心性の検定任意の回転にわたって因子負荷量の2乗の分散が等しいことの検定。circ.sim.plotは、変数の数と標本サイズの関数として、circplex、ellipsoid、simple structureのデータに対する4つの検定を比較します。  このプロットからわかることは、1つの検定がこれらの代替構造を識別するのに十分なわけではないが、4つの検定のセットが非常によくできているということである。  特定のデータ集合の構造を検定するとき、4つの検定の結果をシミュレートされたデータと比較することで、そのデータの構造的特性の良い兆候が得られます。
行列またはdata.frame xが与えられたら、各列の歪度または尖度を求めます（歪度と尖度の場合）。  これらは、e1071パッケージの歪度（skewness）と尖度（kurtosis）で利用可能な選択肢と一致しています（それぞれの利点についてはJoanes and Gill (1998)を参照してください）。m_r = [sum(X- mx)^r]/n と定義すると、タイプ1は g_1 = m_3/(m_2)^{3/2} と g_2 = m_4/(m_2)^2 -3 で歪度と尖度を求めます。 タイプ2は G1 = g1 * √{n *(n-1)}/(n-2) と G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)) です。  タイプ3は、b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2}、b2 = [(n-1)/n]^{3/2}m_4/m_2^2）である。e1071との整合性、JoanesとGillとの整合性のため、現在は上記のように定義されています。しかし、リビジョン1.0.93から1.2.3では、kurtosiはデフォルトで尖度の不偏推定値を与えます(DeCarlo, 1997)。以前のバージョンでは、偏った推定を行う別の式を使用していました。  (これら2つの式の違いについては、e1071パッケージのkurtosis関数を参照してください。  デフォルトのタイプ1はe1071でタイプ2と呼ばれるものを与えた。  もう1つはタイプ3です)。  以前のリリースとの比較のために、type = 2を指定すると、古い推定値が得られます。  これらのタイプ番号は現在変更されている。  
NA
多変量プロファイルの表示は、一連の線(例えば、matplotを参照)、色(例えば、corPlotを参照)、またはレーダープロットやスパイダープロットで行うことができます。スパイダーは、円周構造を持つと考えられるデータを表示するのに特に適しています。1つの変数だけを他のいくつかの変数の関数として表示するには、レーダーを使用します。  複数のプロットを作成するには、スパイダーを使用する。  いくつかのy値だけを比較するときの追加オプションは、オーバーレイ・プロットである。  あるいは、プロット・オプションを設定して、1ページに複数のプロットを行うこともできる。
驚いたことに、スピアマン（1904年）が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するための複数のアプローチが存在する。非常にポピュラーですが、クロンバックのα（1951）はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。16項目以下のテストにsplitHalfを使用すると、すべての可能な分割をかなり簡単に見つけることができます。  17項目以上のテストでは、n.sample分割がランダムに発見されます。したがって、16 個以下の項目の場合、上限値と下限値は正確です。  17項目以上の場合、それらは近いですが、おそらく最高信頼度をわずかに過小評価し、最低信頼度を過大評価するでしょう。  guttman関数は、Guttman (1945)が議論した6つの推定値、Berge and Zergers (1978)の10個のうちの4つ、およびsplitHalfを使ったRevelleのβ (1979)を含みます。コンパニオン関数の ω は、 ω hierarchical (ω_h) と ω total (ω_t) を計算します。Guttmanの最初の推定 λ_1 は，項目の分散がすべて誤差であると仮定します：λ 1= 1-tr(Vx)/Vxこれは明らかに過小評価です.2番目の境界 λ_2 は，対角を，対角から外れた要素の平方和の平方根の関数に置き換えます．  C_2 = ￢vec{1}( ￢vec{V}-diag(￢vec{V})^2 ￢vec{1}' とすると、λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x )事実上、これは対角線をn * 対角線外要素の平均二乗の平方根で置き換えています。  Guttmanの第3の下界、λ_3もλ_1を修正し、各項目の真の分散を項目間の平均共分散として推定し、もちろんCronbachのαと同じです。 λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx) = ((n)/(n-1))(Vx-tr(Vx)/Vx = αこれは対角要素を平均対角外要素に置き換えただけです。  λ_2 ≥ λ_3 で、共分散が同一でなければ λ_2 > λ_3 である。λ_3 と λ_2 はともに λ_1 の補正であり、この補正は連続的な改良の無限集合として一般化できる。(Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)ここで、p_h = sum(σ^2h, h = 0, 1, 2, ... r-1 andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978)。  μ_0＝λ_3＝α、μ_1＝λ_2であることは明らかである。μ_r≧μ_{r-1}≧・・・μ_1≧μ_0であるが、この系列は最初の2ステップ以降はあまり改善されない。Guttmanの第4の下界、λ_4はもともと任意のスピット・ハーフ信頼性として提案されたが、最大のスプリット・ハーフ信頼性と解釈されている。X}を2つの部分、相関r_{ab}で、 \vec{X}_a と \vec{X}_b に分割すると、λ 4 = 4rab/(Va + Vb + 2rabVaVb)となり、これは通常の分割半信頼性ですが、この場合は最も類似した分割の信頼性です。16以下の項目の場合、これはすべての可能な分割を試すことによって求められる。  Guttmanの第5の下界であるλ_5は、対角値を項目間共分散2乗和の最大値（項目全体）の平方根の2倍に置き換えますλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.).λ_1より優れているが、λ_5は対角の補正を過小評価する。  より良い推定は、λ_3で使用される補正に類似している：λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman の最終的な境界は、他のすべての項目の線形回帰で説明できる各項目の分散の量（2乗重相関またはsmc）を考慮します、より正確には，誤差の分散 e_j^2 で，λ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx である．smcは全ての項目から求められる。  score.items関数で報告されるGuttmanのλ_6, λ_6*の修正は、選択された尺度の項目だけでなく、与えられた項目のプール全体からsmcを求めることです。  Guttmanのλ_4は最大のスプリットハーフ信頼性です。  もともとは3つの異なるアプローチからの出力を組み合わせることで求めていましたが、現在ではsplitHalfを使用して（16項目以下の場合）総当たりで最大値を求めるか、相当数のランダムな分割を行うことで置き換えられています。以前から試みられていたアルゴリズムには次のようなものがあります： a) 逆相関行列のICLUSTを行う。  ICLUSTは通常、最も明確なクラスタを形成する。  相関を逆にすることで、最も関連性の高いクラスターを見つける傾向がある。  b) あるいは、相関のkmeansクラスタリング（対角線を0に置き換えて擬似距離を作る）により、2つの類似したクラスタを生成することができる。c) 第1主因子上の順序に基づいて項目を2つのクラスタに割り当てることにより、クラスタを識別。  (Highest to cluster 1, next 2 to cluster 2, etc.) これら3つの手順は，項目を2つの分割に割り当てるためのキーベクトルを生成する．  スプリット・ハーフの信頼性の最大値は、これら3つのアプローチの最大値をとることで求められる。  ブルートフォース法とサンプリング法は、より安定で大きな推定値を提供するようである。splitHalf に実装されているもう1つの手続きは、可能なすべてのスプリットハーフ(n items <= 16の場合)を実際に形成するか、テスト長で補正された10,000(またはそれ以上)のスプリットハーフをサンプリングします。  この関数は、最良分割と最悪分割を項目キーとして返します。  24項目までは妥当な時間で処理できますが、24項目を超えるとかなり遅くなります。24 個の項目のすべての可能な分割を行うには、 1,352,078 個の分割を考える。  2.4GHzの8コアを搭載したMacProで24項目の問題を実行した場合のタイミングは、デフォルトの10,000サンプルで0.24秒、30,000サンプルで0.678秒、すべての可能性で22.58秒である。  これらのサンプルサイズでの最大分割の値は、デフォルトのサンプルサイズ10000の3回の複製で.799,/.804と.800、30,000の2セットで.805と.806、網羅的探索で.812でした。  1つは、glb で、最大のスプリット・ハーフ信頼度 λ_4 を求めます。これはテストを項目の集合とみなし、項目をどのように分割するのが最適かを検討する。他の2つ、glb.faとglb.algebraicは、行列の対角の重みづけの代替方法です。glb.faは、因子の数が正の固有値を持つ数である因子モデルから変数の共通性を推定します。  そして、信頼性は、glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/Vxで求められます。この推定は、Rcsdpパッケージのcsdpの呼び出しを使用するAndreas Moeltnerによって書かれたglb.algebraicによって求められるものとは少し異なります。彼のアルゴリズムは、JacksonとWoodhouseによるglbの記述により近いものであるが、サンプルサイズが小さい場合、正のバイアス（すなわち、いくつかの項目の信頼性を過剰に推定する；それらは=1と言われる）を持つようである。  この2つのアルゴリズムについては、現在さらに調査中である。glb.algebraicと比較すると、glb.faはサンプルサイズが小さい（n < 500）場合は（正の）バイアスが少ないようですが、サンプルサイズが大きい（> 1000）場合は大きくなります。これは変数の数と相互作用するので、等しいバイアスのサンプルサイズは変数の数の関数として異なります。  しかし，その差は小さい．標本サイズが大きくなると、glb.algebraicは母集団の値に収束するようであるが、glb.faは正のバイアスを持つ。
マルチレベル・データは、心理学研究では一般的です。マルチレベル・データでは、上位のグループ化変数に入れ子になっている被験者についてオブザベーションが行われます。  データは実験的（参加者は実験条件に入れ子になっている）かもしれないし、観察的（学生は教室に、学生は大学の専攻に入れ子になっている）かもしれない。この種のデータを分析するには、ランダム効果モデルや混合効果モデル、より一般的にはマルチレベル・モデルを使用します。  少なくとも2つの非常に強力なパッケージ（nlmeとmultilevel）があり、階層（マルチレベル）データ構造の複雑な分析を可能にします。  data.frameまたは行列(data)のグループ変数(group)について、基本的な記述統計量(mean, sd, n)とグループ内相関(cors=TRUE)が各グループについて求められます。  全分散と比較したグループ化変数に関連する分散量は、第1種クラス内相関 (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))ここで npr は、各グループ内のケースの平均数です。群間差の信頼性は、平均が群内変動に関してどれだけ異なるかを反映するICC2によって求めることができる。  ICC2=(MSb-MSw)/MSb.平均2乗間は標本サイズに敏感なので，この推定も標本サイズを反映する．おそらく，statsByの最も有用な部分は，変数間の観察された相関を2つの部分に分解することである：グループ内相関とグループ間相関．これは、Pedazur (1997)やマルチレベル・パッケージのBlieseによって議論された、観測された相関をグループ内のプール相関(rwg)とグループ間の平均の重み付き相関に分解することに従います。* eta_{y_{wg}}* r_{xy_{wg}}  + eta_{x_{bg}} * eta_{x_{bgx_{bg}} * eta_{y_{bg}}* r_{xy_{bg}}ここで、r_{xy}は正規相関で、グループ内相関r_{xy_{wg}}とグループ間相関r_{xy_{bg}}に分解でき、etaはグループ内値、つまりグループ平均とのデータの相関である。  つまり、「生態学的相関」（グループ間相関）から低レベル（グループ内）相関を推論することは不適切である。  しかし、より高いレベルで推論を行うのであれば、これらのグループ間相関は依然として非常に意味のあるものである。  グループ間にプールされたグループ内相関を求めるには、実際には2つの方法がある。  すべてのグループ内の相関を求め、それらを標本サイズで重みづけして、このプール値（pooled）を報告することができます。  これはcorsオプションをTRUEに設定すると得られます。  これは論理的には、標本サイズで重み付けしたメタ分析相関を行うことと同じです。  もう1つの方法rwgは、各被験者のスコアがグループ平均からの偏差スコアとして与えられるとき、共分散、分散、したがって相関を考慮します。  四分位相関、多分位相関、または混合相関を発見した場合、これらの2つの推定値は異なります。weightsパラメータが指定された場合、プールされた相関は、標本サイズではなく、指定された重みでグループを重み付けすることによって求められます。r_{xy_{wg}}, pwgの信頼値と有意性は、グループ内のプールされたケース数を反映しますが、r_{xy_{bg}} , pbgは、グループ内のプールされたケース数を反映します。pbg} , グループの数。sim.multilevelは、マルチレベル構造のシミュレーションデータを生成します。statsBy.boot関数は、グループ化変数ntrialsを何回も無作為化して、statsBy出力を求めます。  これには長い時間がかかり、大量の出力が得られます。  この出力は、statsBy.boot.summary 関数で目的の変数を指定して、関連する変数について要約することができます。  この2つの関数は、単なるグループ分けが大きなグループ間相関をもたらすかどうかを調べるのに便利です。データを教育水準でグループ分けした場合のさまざまな能力テスト間の関係（statsBy(sat.act, "education")）や、影響度データを影響度操作内と操作間で分析した場合（statsBy(flat,group="Film")）を考えてみましょう。faByはstatsByの出力を使って、各グループ内の相関行列の因子分析を行います。freeパラメータがFALSEの場合、各解は（可能な限り）グループ解に向かって回転されます。  出力は、各因子解のリストと、すべてのグループの負荷量と因子間相関の要約行列です。
マルチレベル・データは、心理学研究では一般的です。マルチレベル・データでは、オブザベーションは、ある上位レベルのグループ化変数に入れ子になっている被験者について取られます。  データは実験的（参加者が実験条件に入れ子になっている）かもしれないし、観察的（学生が教室に、学生が大学の専攻に入れ子になっている）かもしれない。この種のデータを分析するには、ランダム効果モデルや混合効果モデル、より一般的にはマルチレベル・モデルを使用します。  少なくとも2つの非常に強力なパッケージ（nlmeとmultilevel）があり、階層（マルチレベル）データ構造の複雑な分析を可能にします。  data.frameまたは行列(data)のグループ変数(group)について、基本的な記述統計量(mean, sd, n)とグループ内相関(cors=TRUE)が各グループについて求められます。  全分散と比較したグループ化変数に関連する分散量は、第1種クラス内相関 (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))ここで npr は、各グループ内のケースの平均数です。群間差の信頼性は、平均が群内変動に関してどれだけ異なるかを反映するICC2によって求めることができる。  ICC2=(MSb-MSw)/MSb.平均2乗間は標本サイズに敏感なので，この推定も標本サイズを反映する．おそらく，statsByの最も有用な部分は，変数間の観察された相関を2つの部分に分解することである：グループ内相関とグループ間相関．これは、Pedazur (1997)やマルチレベル・パッケージのBlieseによって議論された、観測された相関をグループ内のプール相関(rwg)とグループ間の平均の重み付き相関に分解することに従います。* eta_{y_{wg}}* r_{xy_{wg}}  + eta_{x_{bg}} * eta_{x_{bgx_{bg}} * eta_{y_{bg}}* r_{xy_{bg}}ここで、r_{xy}は正規相関で、グループ内相関r_{xy_{wg}}とグループ間相関r_{xy_{bg}}に分解でき、etaはグループ内値、つまりグループ平均とのデータの相関である。  つまり、「生態学的相関」（グループ間相関）から低レベル（グループ内）相関を推論することは不適切である。  しかし、より高いレベルで推論を行うのであれば、これらのグループ間相関は依然として非常に意味のあるものである。  グループ間にプールされたグループ内相関を求めるには、実際には2つの方法がある。  すべてのグループ内の相関を求め、それらを標本サイズで重みづけして、このプール値（pooled）を報告することができます。  これはcorsオプションをTRUEに設定すると得られます。  これは論理的には、標本サイズで重み付けしたメタ分析相関を行うことと同じです。  もう1つの方法rwgは、各被験者のスコアがグループ平均からの偏差スコアとして与えられるとき、共分散、分散、したがって相関を考慮します。  四分位相関、多分位相関、または混合相関を発見した場合、これらの2つの推定値は異なります。weightsパラメータが指定された場合、プールされた相関は、標本サイズではなく、指定された重みでグループを重みづけすることによって求められます。r_{xy_{wg}}, pwgの信頼値と有意性は、グループ内のプールされたケース数を反映しますが、r_{xy_{bg}} , pbgは、グループ内のプールされたケース数を反映します。pbg} , グループの数。sim.multilevelは、マルチレベル構造のシミュレーションデータを生成します。statsBy.boot関数は、グループ化変数ntrialsを何回も無作為化して、statsBy出力を求めます。  これには長い時間がかかり、大量の出力が得られます。  この出力は、statsBy.boot.summary 関数で目的の変数を指定して、関連する変数について要約することができます。  この2つの関数は、単なるグループ分けが大きなグループ間相関をもたらすかどうかを調べるのに便利です。データを教育水準でグループ分けした場合のさまざまな能力テスト間の関係（statsBy(sat.act, "education"）、または影響度データを影響度操作内と操作間で分析した場合（statsBy(flat,group="Film")）を考えてみましょう。）faByはstatsByの出力を使って、各グループ内の相関行列の因子分析を行います。freeパラメータがFALSEの場合、各解は（可能な限り）グループ解に向かって回転されます。  出力は、各因子解のリストと、すべてのグループの負荷量と因子間相関の要約行列です。
マルチレベル・データは、心理学研究では一般的です。マルチレベル・データでは、オブザベーションは、ある上位レベルのグループ化変数に入れ子になっている被験者について取られます。  データは実験的（参加者が実験条件に入れ子になっている）かもしれないし、観察的（学生が教室に、学生が大学の専攻に入れ子になっている）かもしれない。この種のデータを分析するには、ランダム効果モデルや混合効果モデル、より一般的にはマルチレベル・モデルを使用します。  少なくとも2つの非常に強力なパッケージ（nlmeとmultilevel）があり、階層（マルチレベル）データ構造の複雑な分析を可能にします。  data.frameまたは行列(data)のグループ変数(group)について、基本的な記述統計量(mean, sd, n)とグループ内相関(cors=TRUE)が各グループについて求められます。  全分散と比較したグループ化変数に関連する分散量は、第1種クラス内相関 (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))ここで npr は、各グループ内のケースの平均数です。群間差の信頼性は、平均が群内変動に関してどれだけ異なるかを反映するICC2によって求めることができる。  ICC2=(MSb-MSw)/MSb.平均2乗間は標本サイズに敏感なので，この推定も標本サイズを反映する．おそらく，statsByの最も有用な部分は，変数間の観察された相関を2つの部分に分解することである：グループ内相関とグループ間相関．これは、Pedazur (1997)やマルチレベル・パッケージのBlieseによって議論された、観測された相関をグループ内のプール相関(rwg)とグループ間の平均の重み付き相関に分解することに従います。* eta_{y_{wg}}* r_{xy_{wg}}  + eta_{x_{bg}} * eta_{x_{bgx_{bg}} * eta_{y_{bg}}* r_{xy_{bg}}ここで、r_{xy}は正規相関で、グループ内相関r_{xy_{wg}}とグループ間相関r_{xy_{bg}}に分解でき、etaはグループ内値、つまりグループ平均とのデータの相関である。  つまり、「生態学的相関」（グループ間相関）から低レベル（グループ内）相関を推論することは不適切である。  しかし、より高いレベルで推論を行うのであれば、これらのグループ間相関は依然として非常に意味のあるものである。  グループ間にプールされたグループ内相関を求めるには、実際には2つの方法がある。  すべてのグループ内の相関を求め、それらを標本サイズで重みづけして、このプール値（pooled）を報告することができます。  これはcorsオプションをTRUEに設定すると得られます。  これは論理的には、標本サイズで重み付けしたメタ分析相関を行うことと同じです。  もう1つの方法rwgは、各被験者のスコアがグループ平均からの偏差スコアとして与えられるとき、共分散、分散、したがって相関を考慮します。  四分位相関、多分位相関、または混合相関を発見した場合、これらの2つの推定値は異なります。weightsパラメータが指定された場合、プールされた相関は、標本サイズではなく、指定された重みでグループを重み付けすることによって求められます。r_{xy_{wg}}, pwgの信頼値と有意性は、グループ内のプールされたケース数を反映しますが、r_{xy_{bg}} , pbgは、グループ内のプールされたケース数を反映します。pbg} , グループの数。sim.multilevelは、マルチレベル構造のシミュレーションデータを生成します。statsBy.boot関数は、グループ化変数ntrialsを何回も無作為化して、statsBy出力を求めます。  これには長い時間がかかり、大量の出力が得られます。  この出力は、statsBy.boot.summary 関数で目的の変数を指定して、関連する変数について要約することができます。  この2つの関数は、単なるグループ分けが大きなグループ間相関をもたらすかどうかを調べるのに便利です。データを教育水準でグループ分けした場合のさまざまな能力テスト間の関係（statsBy(sat.act, "education"）、または影響度データを影響度操作内と操作間で分析した場合（statsBy(flat,group="Film")）を考えてみましょう。）faByはstatsByの出力を使って、各グループ内の相関行列の因子分析を行います。freeパラメータがFALSEの場合、各解は（可能な限り）グループ解に向かって回転されます。  出力は、各因子解のリストと、すべてのグループの負荷量と因子間相関の要約行列です。
推奨される関数はstructure.diagramで、これはRgraphvizを使用しませんが、ドットコードも生成しません。  3つのstructure関数はすべて、semまたはlavaanパッケージで使用するのに適したコマンドの行列を返します。  (sem.diagramとsem.graphは、semパッケージで実行された単純なCFAからの出力を変換し、structure.diagramまたはstructure.graph.lavaan.diagramを使って描画します。diagramは、lavaanパッケージで行われた単純なCFAからの出力（フィット）を変換し、structure.diagramを使って描画します。X変数間の相関（Rxが指定された場合） X変数とそれらの潜在因子（fxが指定された場合） 潜在Xと潜在Y（Phiが指定された場合） 潜在Yと観測されたY（fyが指定された場合） Y変数間の相関（Ryが指定された場合） 確定因子モデルではfxとPhiだけが指定され、構造モデルではfx、Phi、fyが含まれます。  lavaan.diagramは、適合のクラスに応じてfa.diagram、omega.diagram、iclust.diagramを呼び出すdiagram関数から呼び出すことができます。  これらの関数はすべて、dia.rect、dia.ellipse、dia.arrow、dia.curve、dia.curved.arrow、dia.shape などのさまざまな dia 関数を使用します。
推奨関数はstructure.diagramで、これはRgraphvizを使用しないが、ドットコードも生成しない。  3つのstructure関数はすべて、semまたはlavaanパッケージで使用するのに適したコマンドの行列を返します。  (sem.diagramとsem.graphは、semパッケージで実行された単純なCFAからの出力を変換し、structure.diagramまたはstructure.graph.lavaan.diagramを使って描画します。diagramは、lavaanパッケージで行われた単純なCFAからの出力（フィット）を変換し、structure.diagramを使って描画します。X変数間の相関（Rxが指定された場合） X変数とそれらの潜在因子（fxが指定された場合） 潜在Xと潜在Y（Phiが指定された場合） 潜在Yと観測されたY（fyが指定された場合） Y変数間の相関（Ryが指定された場合） 確定因子モデルではfxとPhiだけが指定され、構造モデルではfx、Phi、fyが含まれます。  lavaan.diagramは、適合のクラスに応じてfa.diagram、omega.diagram、iclust.diagramを呼び出すdiagram関数から呼び出すことができます。  これらの関数はすべて、dia.rect、dia.ellipse、dia.arrow、dia.curve、dia.curved.arrow、dia.shapeなどのさまざまなdia関数を使用します。
これはほとんど自明である。  例を見てください。
推奨関数はstructure.diagramで、これはRgraphvizを使用しないが、ドットコードも生成しない。  3つのstructure関数はすべて、semまたはlavaanパッケージで使用するのに適したコマンドのマトリックスを返します。  (sem.diagramとsem.graphは、semパッケージで実行された単純なCFAからの出力を変換し、structure.diagramまたはstructure.graph.lavaan.diagramを使って描画します。diagramは、lavaanパッケージで行われた単純なCFAからの出力（フィット）を変換し、structure.diagramを使って描画します。X変数間の相関（Rxが指定された場合） X変数とそれらの潜在因子（fxが指定された場合） 潜在Xと潜在Y（Phiが指定された場合） 潜在Yと観測されたY（fyが指定された場合） Y変数間の相関（Ryが指定された場合） 確定因子モデルではfxとPhiだけが指定され、構造モデルではfx、Phi、fyが含まれます。  lavaan.diagramは、適合のクラスに応じてfa.diagram、omega.diagram、iclust.diagramを呼び出すdiagram関数から呼び出すことができます。  これらの関数はすべて、dia.rect、dia.ellipse、dia.arrow、dia.curve、dia.curved.arrow、dia.shape などのさまざまな dia 関数を使用します。
print.psychとsummary.psychは、ハイライトだけを表示する汎用的なメソッドです。  他に何が利用できるかを確認するには、特定のオブジェクトの構造を求めてください： (str(theobject) ).あるいは、完全な出力を得るには、unclass(theobject)してからそれを表示します。追加機能として、promax関数が因子負荷行列に適用された場合、通常の出力は回転行列を提供するだけです。(これはJohn FoxとUli KellerによるR-helpリストへの提案に従ったものです）。  もう1つの方法は，factanalオブジェクトに直接Promax関数を使用することです．
sim.structural、structure.graph、make.keysなどのいくつかの関数は、部分行列の集合から形成されると考えることができる行列を使用します。  特に、make.keysを使用して、項目の集合をスコアリングしたり（scoreItemsまたはscoreOverlap）、指定されたクラスタを形成したり（cluster.cor）する場合、異なる項目の集合に対して異なるスコアリングキーのセットを定義し、これらのスコアリングキーを1つのスーパーキーに結合することが便利です。  したがって、x = scoreOverlap からの尺度の相関、y = 尺度を形成するために使用される項目の相関、xy = 尺度と項目の相関とします。   
sim.structural、structure.graph、make.keysなどいくつかの関数は、部分行列の集合から形成されると考えることができる行列を使用します。  bestScalesを使用して尺度を開発し、項目を調査する場合、scoreOverlapから出力される行列を元の相関行列と組み合わせることが役に立つことがあります。  したがって、x = scoreOverlap からの尺度の相関、y = 尺度を形成するために使用される項目の相関、xy = 尺度と項目の相関とします。   
sim.structural、structure.graph、make.keysなどいくつかの関数は、部分行列の集合から形成されると考えることができる行列を使用します。  bestScalesを使用して尺度を開発し、項目を調査する場合、scoreOverlapから出力される行列を元の相関行列と組み合わせることが役に立つことがあります。  したがって、x = scoreOverlap からの尺度の相関、y = 尺度を形成するために使用される項目の相関、xy = 尺度と項目の相関とします。   
2つのグループがどのように異なるかを報告する方法はたくさんあります。  Cohenのd統計量は、プールされたグループ内標準偏差で表現された平均の差です。  これは標本サイズに影響されません。rは効果量の普遍的な尺度で、dの単純な関数ですが、-1 から 1の境界です。t 統計量は、単に d * sqrt(n)/2 であり、したがって標本サイズを反映します。   (M2- M1)/Sp ここで Sp はプールされた標準偏差。  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}.  }  Cohens dはプールされた平方和の除数としてNを使う。  Hedges gはN-2を用いる。  Cohensのdの信頼区間は、dをtに変換し、tの信頼区間を求め、それをdsに変換することで求めることができる。  cohen.dの結果はerror.dots関数を使って表示することができます。  これには、辞書で提供されたラベルが含まれる。  0（1標本の場合）との比較のためにcohen.d.ciを使用して）信頼区間を求める場合、n1を指定します。  これはd = t/sqrt(n1)をもたらしますが、2つの標本間の差の場合はd = 2*t/sqrt(n) (標本サイズが等しい場合 n = n1+ n2)、または標本サイズが等しくない場合はd = t/sqrt(1/n1 + 1/n2) となります。7/14/21まで、私はtとしたがってp値を推定するために合計nを使っていました。  問い合わせ（ニュース参照）に応えて、実際の標本サイズns（n1とn2）を使用し、ヘッジg値に基づいてtを求めることに切り替えました。  これは、t.testでvar.equal = TRUEオプションで報告されたt値を生成します。報告された様々な信頼区間は正規理論に基づいており、慎重に解釈されるべきであるというコメントは、おそらく有益でしょう。cohen.d.byは、group2で定義されたデータの各サブセットについて、グループのCohenのdを求めます。  出力の要約は、各グループの各変数のd値の簡略化されたリストを生成する。  d.robustはAlgina et al. 2005)に従い、トリム平均(trim =.2)とWinsorize分散(trim =.2)を求めます。  m2tは、2つのグループの平均、標準偏差、標本サイズが与えられたときのStudentのt.t.検定を報告します。  これは、推定値は提供されているが生データが入手できない統計量をチェックするときに便利である。  デフォルトでは、プールされた推定分散を与えますが、pooledがFALSEの場合、Welchの補正が適用されます。マハラノビス距離は、個々のdsを結合し、それらのユニークな寄与度で重み付けします：  D=√{d'R^{-1}d}。デフォルトでは、cohen.dは2つのグループ間のマハラノビス距離を求めます（複数のDVがある場合）。これはすべてのDVの相関を求める必要があり、いくつかのペアが存在しないため、その行列が可逆でない場合は失敗することがあります。  したがって、MD=FALSEに設定するとマハラノビスの計算ができなくなります。Marco del Giudice (2019)は、さまざまな重複係数の観点からdとMdを解釈する方法について議論している非常に有用な論文があります。これらは、d2OVL（1つの分布の重なり割合）、d2OVL2（共同分布の重なり割合）、d2CL（共通言語の効果量）、d2U3（上位群の割合が下位群の中央値を超える）を使用することで見つけることができます。OVL = 2 φ(-d/2) は重なり割合です（dが大きくなるほど小さくなります）。ここで Phi は正規分布の累積密度関数である.OVL_2 = OVL/(2-OVL)U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )最後の2つは、(abs (d))で大きくなる。  CohenのdとMahalanobis Dのグラフ表示については、scatterHistの例、またはpsychTools::GERASデータセットの例を参照してください。
NA
キュビット（腕の長さ）による身長のGalton (1888)のオリジナルは表形式である。これを相関や散布図として表示するには、表を2列の行列やデータフレームに変換するのが便利です。  この関数は、項目応答パターン表をデータ表に変換するのにも使える。  
キュビット（腕の長さ）による身長の元のGalton (1888)は表形式である。これを相関や散布図として表示するには、表を2列の行列やデータフレームに変換するのが便利です。  この関数は、項目応答パターン表をデータ表に変換するのにも使える。  
lowerCor は，列名を桁数 + 3 文字に省略し，桁数に丸められた対角行列の下側の行列を出力しますが，丸められない完全な行列も返します．  デフォルトでは、変数の対削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
Tal-Or et al. (2010)は、2つの実験的研究でメディアの推定効果を調べた。  これらのデータは研究2からのものである。砂糖の不足が予想されるというニュース記事の影響に関する知覚を、ニュース記事への曝露の知覚を操作することによって間接的に操作し、その結果、その記事から生じる行動意図を測定した。(p 801).
Tal-Orら（2010）は、2つの実験的研究でメディアの推定効果を検証した。  これらのデータは研究2のものである。砂糖の不足が予想されるというニュース記事の影響に関する認識を、ニュース記事への曝露の認知を操作することによって間接的に操作し、その結果、ニュース記事から生じる行動意図を測定した。(p 801).
これらの関数の中で最も有用な2つは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminであろう。もう1つはTargetQで、これはターゲットのNA値の欠損を許容する。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、その最も高い負荷量を持つグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷量とグループ因子がある純粋なバイファクターモデル）を扱いません。
これらの関数の中で最も有用なのは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminでしょう。2番目は TargetQ で、これはターゲット中の欠損NA値を可能にします。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、その最も高い負荷量を持つグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷量とグループ因子がある純粋なバイファクターモデル）を扱いません。
これらの関数の中で最も有用なのは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminでしょう。2番目は TargetQ で、これはターゲット中の欠損NA値を可能にします。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目は、その負荷が最も高いグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷とグループ因子がある純粋なバイファクターモデル）を扱うことはできません。
驚くことに、Spearman (1904)が信頼性の概念を心理学者に紹介してから1世紀以上経った今でも、信頼性を測定するための複数のアプローチがあります。非常にポピュラーですが、クロンバックのα (1951) はテストの信頼性を過小評価し、第1因子の飽和を過大評価します。16項目以下のテストにsplitHalfを使用すると、すべての可能な分割をかなり簡単に見つけることができます。  17項目以上のテストでは、n.sample分割がランダムに発見されます。したがって、16 個以下の項目の場合、上限値と下限値は正確です。  17項目以上の場合、それらは近いですが、おそらく最高信頼度をわずかに過小評価し、最低信頼度を過大評価するでしょう。  guttman関数は、Guttman (1945)が議論した6つの推定値、Berge and Zergers (1978)の10個のうちの4つ、およびsplitHalfを使ったRevelleのβ (1979)を含みます。コンパニオン関数の ω は、 ω hierarchical (ω_h) と ω total (ω_t) を計算します。Guttmanの最初の推定 λ_1 は，項目の分散がすべて誤差であると仮定します：λ 1= 1-tr(Vx)/Vxこれは明らかに過小評価です.2番目の境界 λ_2 は，対角を，対角から外れた要素の平方和の平方根の関数に置き換えます．  C_2 = ￢vec{1}( ￢vec{V}-diag(￢vec{V})^2 ￢vec{1}' とすると、λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x )事実上、これは対角線をn * 対角線外要素の平均二乗の平方根で置き換えています。  Guttmanの第3の下界、λ_3もλ_1を修正し、各項目の真の分散を項目間の平均共分散として推定し、もちろんCronbachのαと同じです。 λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx) = ((n)/(n-1))(Vx-tr(Vx)/Vx = αこれは対角要素を平均対角外要素に置き換えただけです。  λ_2 ≥ λ_3 で、共分散が同一でなければ λ_2 > λ_3 である。λ_3 と λ_2 はともに λ_1 の補正であり、この補正は連続的な改良の無限集合として一般化できる。(Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)ここで、p_h = sum(σ^2h, h = 0, 1, 2, ... r-1 andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978)。  μ_0＝λ_3＝α、μ_1＝λ_2であることは明らかである。μ_r≧μ_{r-1}≧・・・μ_1≧μ_0であるが、この系列は最初の2ステップ以降はあまり改善されない。Guttmanの第4の下界、λ_4はもともと任意のスピットハーフ信頼性として提案されたが、最大のスプリットハーフ信頼性と解釈されている。X}を2つの部分、相関r_{ab}で、 \vec{X}_a と \vec{X}_b に分割すると、λ 4 = 4rab/(Va + Vb + 2rabVaVb)となり、これは通常の分割半信頼性ですが、この場合は最も類似した分割の信頼性です。16以下の項目の場合、これはすべての可能な分割を試すことによって求められる。  Guttmanの第5の下界であるλ_5は、対角値を項目間共分散2乗和の最大値（項目全体）の平方根の2倍に置き換えますλ_5 = λ_1 +2/sqrt(平均(C_2)/V_X.)。λ_1より優れているが、λ_5は対角の補正を過小評価する。  より良い推定は、λ_3で使用された補正に類似している：λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman の最終的な境界は、他のすべての項目の線形回帰で説明できる各項目の分散の量（2乗重相関またはsmc）を考慮します、より正確には，誤差の分散 e_j^2 で，λ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx である．smcは全ての項目から求められる。  score.items関数で報告されるGuttmanのλ_6, λ_6*の修正は、選択された尺度の項目だけでなく、与えられた項目のプール全体からsmcを求めることです。  Guttmanのλ_4は最大のスプリットハーフ信頼性です。  もともとは3つの異なるアプローチからの出力を組み合わせることで求めていましたが、現在ではsplitHalfを使用して（16項目以下の場合）総当たりで最大値を求めるか、相当数のランダムな分割を行うことで置き換えられています。以前から試みられていたアルゴリズムには次のようなものがあります： a) 逆相関行列のICLUSTを行う。  ICLUSTは通常、最も明確なクラスタを形成する。  相関を逆にすることで、最も関連性の高いクラスターを見つける傾向がある。  b) あるいは、相関のkmeansクラスタリング（対角線を0に置き換えて擬似距離を作る）により、2つの類似したクラスタを生成することができる。c) 第1主因子上の順序に基づいて項目を2つのクラスタに割り当てることにより、クラスタを識別。  (Highest to cluster 1, next 2 to cluster 2, etc.) これら3つの手順は，項目を2つの分割に割り当てるためのキーベクトルを生成する．  スプリット・ハーフの信頼性の最大値は、これら3つのアプローチの最大値をとることで求められる。  ブルートフォース法とサンプリング法は、より安定で大きな推定値を提供するようである。splitHalf に実装されているもう1つの手続きは、可能なすべてのスプリットハーフ(n items <= 16の場合)を実際に形成するか、テスト長で補正された10,000(またはそれ以上)のスプリットハーフをサンプリングします。  この関数は、最良分割と最悪分割を項目キーとして返します。  24項目までは妥当な時間で処理できますが、24項目を超えるとかなり遅くなります。24 個の項目のすべての可能な分割を行うには、 1,352,078 個の分割を考える。  2.4GHzの8コアを搭載したMacProで24項目の問題を実行した場合のタイミングは、デフォルトの10,000サンプルで0.24秒、30,000サンプルで0.678秒、すべての可能性で22.58秒である。  これらのサンプルサイズでの最大分割の値は、デフォルトのサンプルサイズ10000の3回の複製で.799,/.804と.800、30,000の2セットで.805と.806、網羅的探索で.812でした。  1つは、glb で、最大のスプリット・ハーフ信頼度 λ_4 を求めます。これはテストを項目の集合とみなし、項目をどのように分割するのが最適かを検討する。他の2つ、glb.faとglb.algebraicは、行列の対角の重みづけの代替方法です。glb.faは、因子の数が正の固有値を持つ数である因子モデルから変数の共通性を推定します。  そして、信頼性は、glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/Vxで求められます。この推定は、Rcsdpパッケージのcsdpの呼び出しを使用するAndreas Moeltnerによって書かれたglb.algebraicによって求められるものとは少し異なります。彼のアルゴリズムは、JacksonとWoodhouseによるglbの記述により近いものであるが、サンプルサイズが小さい場合、正のバイアス（すなわち、いくつかの項目の信頼性を過剰に推定する；それらは=1と言われる）を持つようである。  この2つのアルゴリズムについては、現在さらに調査中である。glb.algebraicと比較すると、glb.faはサンプルサイズが小さい（n < 500）場合は（正の）バイアスが少ないようですが、サンプルサイズが大きい（> 1000）場合は大きくなります。これは変数の数と相互作用するので、等しいバイアスのサンプルサイズは変数の数の関数として異なります。  しかし，その差は小さい．サンプルサイズが大きくなると、glb.algebraicは母集団の値に収束するようですが、glb.faは正のバイアスを持ちます。
lowerCorは，対角行列の下側を丸め，列名を桁数+3文字に省略したものを出力しますが，丸めずに完全な行列も返します．  デフォルトでは、変数のペアワイズ削除が使用される。  この行列は、きれいな印刷を行うlowerMatを呼び出します。  必要なのはlowerMatだけなのにlowerCorを呼び出さないことを覚えておくことが重要です！ csはFrank HarrellによるHmiscパッケージのCs関数をそのままコピーしたものです。  Hmiscパッケージのオーバーヘッドを避けるためにpsychに追加された。
nvar変数のn.obsオブザベーション（0/1）が、ロジスティックまたは正規理論モデルを用いてシミュレートされます。  そして、多くの異なるスコアリングアルゴリズムが適用され、グラフで表示されます。  ltmスコアを比較するためにldmパッケージがインストールされている必要があります。
psych パッケージを修正する場合、あるコードを追加しても他のコードが壊れないことを確認するのが便利です。  test.psych関数は、様々な標準的なデータセットで主要な関数をテストします。  また、psychパッケージの多くの機能を披露しています：USArrests 米国州別凶悪犯罪率（4変数） attitude Chatterjee-Price態度データ Harman23.cor$cov Harman 例2.3 8身体的測定値 Harman74.cor$cov Harman 例7.4 24精神的測定値 ability.cov$cov 8能力と知能テスト また、psychのbfiと能力データセットも使用します。
信頼性を測定する方法はたくさんある。テスト-再テストは1つの方法である。  時間間隔が非常に短い（または即時の）場合、これは依存性相関として知られ、時間間隔が長い場合は安定性係数です。  いずれの場合も、これは異なる時点における2つの測定値間の相関である。  これらのデータの多段階性を考えると、個人、時間、項目、項目ごとの時間などに関連する分散成分を見つけることが可能である。  これは、信頼性のいくつかの異なる推定につながります（議論と参考文献については、multilevel.reliabilityを参照してください）。また、時間横断的な被験者信頼性を求めることも可能です（これは、各被験者の時間1と時間2の項目間の相関です）。  これは被験者の信頼性を示すものである（Wood et al, 2017）。  項目は、時間の経過とともに異なる量のテスト・リテスト信頼性を示す可能性がある。  残念ながら、項目間であまり差がない場合、個人内相関には問題がある。  すべての項目が同じキー方向で、同じ構成要素を測定している場合、個人の回答プロファイルは基本的に平坦になります。これは、再現性がほぼ完璧であっても、相関が実際には負になる可能性があることを意味します。  項目間の個人内距離(d2)は、各項目の差の2乗の平均です。  rqqスコアとは負の相関が高いが、これはランダムな回答者（dqqが高くrqqが低い）と分散が小さい一貫した回答者（dqqが低くrqqが低い）を区別する。いくつかの個々の統計量が score オブジェクトで報告されます。  これらは、pairs.panel を用いて、さまざまな尺度の関係や範囲をグラフィカルに表示することができます。項目がテスト内に入れ子になっているテストの分散を分解することを意図していますが、2つのテストが与えられているだけの場合、人と時間の分散成分も表示されます。  その結果得られる全分散に対する人の分散比は、2つのテスト間のクラス内相関です。  より一般的なケースについてはICCも参照．
四分位相関は，2変量正規性の仮定で，度数の2 x 2 表から潜在的なピアソン相関を推論する．  推定手順は2段階のML．  各組の項目のセル度数が求められる．テトラコリックスの場合、カウントがゼロのセルは、連続性の補正として0.5に置き換えられる（correct=TRUE）。データは通常、真/偽（4値法）または限定された回答数（多値法）のいずれかで採点されたアンケートへの回答の生データ行列になります。  どちらの場合も、限界度数は正規理論のしきい値に変換され、各項目ペアの結果の表は、観察されたセル度数と観察された限界度数を生成する（推測された）潜在ピアソン相関に変換されます。  (例としてdraw.tetraとdraw.corを参照)これは計算集約的な関数であり、マルチコアを使用し、並列パッケージを使用することでかなり高速化できる。  polychoricまたはtetrachoricを行う際に使用するコアの数はoptionsコマンドで指定することができる。最も速度が向上するのは1コアから2コアにすることで、約50％の節約になる。  4コアにすると66％、8コアにすると75％の節約になるようだ。  options("mc.cores"=4)は、コア数を4に設定します。tetrachoricとpolychoricは、x変数（列）とy変数（行）のセットの非対称相関行列を求めることができます。  これは、解を基本集合から別の集合に拡張する場合に便利である。  四分位相関は様々な文脈で使用され，1つはテスト得点の項目反応理論（IRT）分析で，もう1つは共存統計量の相関係数への変換で重要である．  この2番目の文脈で、セル度数に対する係数の感度の例が明らかになります。Kirk (1973)のテストデータセットを考えてみましょう。彼は、テトラコリック相関（例参照）に対するMLアルゴリズムの有効性を報告しています。  以前のバージョンはJohn Foxのpolychor関数を使用していましたが、現在はpolyc関数に置き換えられています。poly.matはpolycor関数の代替ラッパーでした。双列相関と多列相関は、観測された点-双列相関と点-多列相関（それ自体は単なるピアソン相関です）に相当する推定潜在相関です。polyserial 関数は、行列またはデータフレーム入力で動作し、応答頻度の全体的な（すべての観察されたケースの）確率で補正されたペアワイズ Pearson r を見つけることによって欠損データを扱います。  これは、大量の欠損データがあり、完全な症例がないSAPA手順（https://www.sapa-project.org/）（Revelle et al. 2010, 2016, 2020）に特に有用です。同様のデータについては、International Cognitive Ability Resource (https://www.icar-project.org/)も参照してください。能力検査や性格検査の行列は、通常、通常のピアソン相関を使用する場合よりも、テトラコロリック相関やポリコロリック相関を使用する場合の方が、よりきれいな構造を持ちます。バイセリアル相関（単なるピアソン相関であるポイント・バイセリアル相関と混同しないように）は，xとyの間の潜在相関で，yは連続，xは2値であるが，（観察されない）連続正規変数を表すと仮定される．p = xのレベル1の確率，q = 1 - pとする． zp = pに関連するz スコアの正規縦軸とする．MacCallumら, 2002によってうまく議論されているように，平均で人為的に2分すると，ポイント2等分線は2等分線の0.8になる（実際には0.798）．  同様に、ファイ係数（2分値データでのピアソン）は、実数値（rho）の2[arcsin(rho)/pi)になります。「アドホック」多列相関、rps は、単に r = r * sqrt(n-1)/n) σ y /∑(zpi) であり、ここで zpi は、項目回答間のカットポイント境界の正規等価での正規曲線の縦線です。(Olsson, 1982) これらはすべて、相関の正確なML推定に使用されるべきJohn Foxのpolychorパッケージからインスパイアされ（そして、そこから適応され）ました。  特に、polychorパッケージのhetcor関数を参照してください。  polychoricの結果は、correct=FALSE、global=FALSEを使用した場合、polychorの答えと少なくとも小数点以下5桁で一致します。特に欠損データのあるデータセットからのテトラコリック相関の場合、行列が正定値にならないことがあります。  さまざまな平滑化の選択肢がありますが、ここで行われるのは、相関行列の固有値分解を行い、すべての負の固有値を10 * .Machine$double.epsに設定し、正の固有値を変数の数の合計になるように正規化し、相関行列を再構成することです。   非常に小さなデータセットの場合、特にglobal=FALSEオプションを使用している場合、または一度に1つの相関だけを行う場合、多項式相関の連続性の補正が困難になる可能性があります。補正値を小さく設定する（つまり、correct =.1）ことで解決するようです。  John Uebersax (2015)は、ポリコーリック相関とテトラコーリック相関の両方が、過去の発見方法であるテトラコーリックやポリコーリックではなく、発見方法から潜在相関または潜在連続相関と呼ばれるべきであるという興味深い指摘をしています。つまり，人為的に2つ（四分位）またはそれ以上（多分位）の値に分割すると，観察された度数のn x n 表が得られる2つの潜在変数の間の相関は何かということである．  連続変数、カテゴリー変数、2値変数の組み合わせについては、mixed.corを参照してください。応答選択肢の数が可変のデータを使用する場合は、polychoricのglobal=FALSEオプションを使用する必要があります。  (この条件が検出された場合、自動的に設定されます).二分値データで比較的小さなサンプルの場合、いくつかのセルが空であったり、再サンプリングされた行列が正半定値でない場合、警告が出されます。これは、multi.cores(Macを使用している場合のデフォルト)を使用している場合、深刻な問題につながります。解決策は、multi.coresを使用しないことであるようです（例えば、options(mc.cores =1) mixedCorは、2.4GHz 8コアのIntel i9を搭載したMac book Pro上で、spiデータセットのN = 4,000、145アイテムに対してpolychoricを呼び出し、1コアで130秒、2コアで68秒、4コアで37秒、8コアで22秒、16コアで22.8秒かかりました。  ポリコレ相関（spi[11:145]）を見つけるだけで、4コアでは34秒、8コアでは22秒かかった。(2022年更新：M1 maxチップを使用) 145変数すべてに対するmixedCor：1コア=54、2コア=28、4コア=15.8秒、8コア=8.9秒。  ポリコレだけの場合は、4コア＝13.7秒、8コア＝7.4秒。     カテゴリの数が増えるにつれて、計算時間も増加する。  約8カテゴリー以上では、通常のピアソン相関とポリコレの差はあまり大きくない*。  しかし，必要であれば，カテゴリー数（num.cat）を大きな値に設定してもよい．  (バージョン2.0.6で追加).*FoldnesとGronnebergの最近の論文によると、カテゴリ数が大きくても、カテゴリデータにポリコレを使用することが適切であることが示唆されている。  これは特に、カテゴリー度数の分布が非常に非正規分布の場合である。
Louis L. Thurstoneは、心理測定理論と態度、興味、能力の測定のパイオニアである。  彼の多くの貢献の中に、比較判断のプロセスの体系的分析がある（thurstone, 1927）。  彼は、被験者に一対の物体を連続して比較させる場合を考えた。サーストンは、2つの物体の値の比較は、物体間の差の標準偏差と比較した各物体の平均値の差を表すものとして表現できると提案した。  基本的なモデルは、各項目は回答強度の正規分布を持ち、選択肢は2つの回答強度のうち強い方を表すというものである。  正規性の仮定を正当化する根拠は、各決定が多数の独立した入力の合計を表し、したがって中心極限定理によって正規分布することである。Thurstoneは、各項目の分散の等質性と独立性に関する5つの異なる仮定セットを検討した (Thurston, 1927)。Torgersonは、データ収集の3つのクラス（個体あり、個体間あり、個体内と個体間の混合）と3組の仮定（決定過程の共分散が等しい、相関が等しく分散差が小さい、分散が等しい）を掛け合わせて考えることによって、この分析を少し拡張した。  データは、嗜好のデータフレームの正方行列（列変数が行変数より選ばれる確率を持つ比率として）、または順位（1が2より優先されるなど）の行列またはデータフレームのいずれかである。  これらのデータは、選択肢の行列に変換され、スケーリングされます。  データが無意味であるにもかかわらず、適合度は実質的に完璧です。   これは、よりよい適合度検定を適用すべきことを示唆している。  
Holzinger and Swineford (1937)は、精神能力について2因子モデル（1つの一般因子といくつかのグループ因子）を導入した。  これは、ω関数またはsemを用いて分析することができる階層的因子構造のすばらしい実証データセットである。このようなデータを分析するには、いくつかの方法があります。  1つは、オメガ関数を使用して、Schmid-Leiman変換を使用して階層因数分解を行うことである。これは、探索的モデルとして、そしてomegaSemを用いた確認的モデルとして行うことができます。もう1つの方法は、通常の因子分析を行い、bifactor 回転またはbiquartimin 回転を使用することです。後者の2つの関数は、Jennrich and Bentler (2011) のバイファクター変換とバイクォーティミン変換を実装しています。  バイファクター回転は，局所最小の問題（Mansolf and Reise, 2016）に悩まされるので，探索的分析と確認的分析の混合が望ましいかもしれない．14の変数は、3つの空間テスト、3つのメンタル・スピード・テスト、4つの運動スピード・テスト、4つの言語テストを反映するように順序付けされている。  Holzingerからのもう1つのデータ集合（Holzinger.9）は、9つの認知能力（Holzinger, 1939）を表し、Karl Joreskog (2003)がMINRESアルゴリズムによる因子分析の例として使用し、LISRELのマニュアルにも例NPV.KMとして掲載されています。  このデータ集合は、Grant White中学校の9つのテストの得点です：  「t01_visperc""t02_cubes""t04_lozenges""t06_paracomp""t07_sentcomp""t09_wordmean""t10_addition""t12_countdot "および "t13_sccaps "で、lavaanパッケージの変数x1 ... x9（Grant-White校の場合）として表示されます。もう1つの古典的なデータ集合は、R. P. McDonald (1985, 1999)によって詳細に議論され、SASのPROC CALIS マニュアルと同様に sem パッケージで例として使用されている9変数のThurstone問題です。  これらの9つのテストは、ThurstoneとThurstone, 1941によって（他のデータに基づいて）3つの要因にグループ化された：Verbal Comprehension、Word Fluency、Reasoningである。元のデータはThurstone and Thurstone (1941)によるものだが、Bechthold (1961)によって再分析され、彼はデータセットを2つに分けた。サンプルサイズは213である。Thurstone (1933)に起因する9つの認知変数のもう1つのセットは、プリンストンのBrigham教授が大学入試委員会に報告した4,175人の学生のデータセットである。  Tucker (1958) は、Turstone and Thburstone (1941)の9変数を、彼のバッテリー間因子分析の例に使用しています。  二因子モデルの最近の応用は、心理的状態の測定である。Reiseのデータセットは、Consumer Assessment of Health Care Provider and Systems調査票に対する35,000を超える観察に基づく相関行列である。Reise, Morizot, and Hays (2007)は、1,000症例に基づく二因子解を記述している。    Reiseらによる5つの因子は、「迅速に治療を受ける」（1-3）、「医師がよくコミュニケーションをとる」（4-7）、「丁寧で親切なスタッフ」（8,9）、「必要な治療を受ける」（10-13）、「医療プランの顧客サービス」（14-16）である。2つのBechtoldtデータセットは、Thurstone and Thurstone (1941)からの2つのサンプルである。  これらは、17の変数を含み、そのうちの9つは、Thurstoneデータ集合を形成するためにMcDonaldによって使用されました。サンプルサイズはそれぞれ212と213である。提案された6つの因子は、記憶、言語、単語、空間、数、推論を反映し、暗記記憶因子を期待するすべての3つのマーカーがある。このセットから9変数がThurstoneデータセットに現れる。同様の構造を持つデータセットがHarmanデータセットにさらに2つある。  これには、Harman link{Harman.Holzinger}が使用したHolzingerの別の9変数（被験者696人）と、link{burt}の8感情変数が含まれます。二因子構造の検定のために調べる価値があるもう1つのデータ集合は、Keith Widamanによって提供されたHolzinger and Swineford (1939)のオリジナル・データを含むholzinger.swinefordデータ集合です。  これはpsychToolsパッケージに入っている。Bechtoldt.1: 能力検査の17 x 17相関行列、N = 212.Bechtoldt.2：能力検査の17 x 17相関行列，N = 213．Holzinger：能力検査の14 x 14相関行列，N = 355 Holzinger.9：能力検査の9 x 9相関行列，N = 145 Reise：健康満足度項目の16 x 16相関行列．  N = 35,000 Thurstone: 能力検査の9 x 9相関行列、N = 213 Thurstone.33: 能力検査の別の9 x 9相関行列、N = 4175 Thurstone:9：さらに別の9×9の能力テストの相関行列、N=710
HolzingerとSwineford（1937）は、精神能力について二因子モデル（1つの一般因子といくつかのグループ因子）を導入した。  これは、オメガ関数やsemを用いて分析できる階層的な因子構造のすばらしい実証データセットである。このようなデータを分析するには、いくつかの方法があります。  1つは、オメガ関数を使用して、Schmid-Leiman変換を使用して階層因数分解を行うことである。これは、探索的モデルとして、そしてomegaSemを用いた確認的モデルとして行うことができます。もう1つの方法は、通常の因子分析を行い、bifactor 回転またはbiquartimin 回転を使用することです。後者の2つの関数は、Jennrich and Bentler (2011) のバイファクター変換とバイクォーティミン変換を実装しています。  バイファクター回転は，局所最小の問題（Mansolf and Reise, 2016）に悩まされるので，探索的分析と確認的分析の混合が望ましいかもしれない．14の変数は、3つの空間テスト、3つのメンタル・スピード・テスト、4つの運動スピード・テスト、4つの言語テストを反映するように順序付けされている。  Holzingerからのもう1つのデータ集合（Holzinger.9）は、9つの認知能力（Holzinger, 1939）を表し、Karl Joreskog (2003)がMINRESアルゴリズムによる因子分析の例として使用し、LISRELのマニュアルにも例NPV.KMとして掲載されています。  このデータ集合は、Grant White中学校の9つのテストの得点です：  「t01_visperc""t02_cubes""t04_lozenges""t06_paracomp""t07_sentcomp""t09_wordmean""t10_addition""t12_countdot "および "t13_sccaps "で、lavaanパッケージの変数x1 ... x9（Grant-White校の場合）として表示されます。もう1つの古典的なデータ集合は、R. P. McDonald (1985, 1999)によって詳細に議論され、SASのPROC CALIS マニュアルと同様に sem パッケージで例として使用されている9変数のThurstone問題です。  これらの9つのテストは、ThurstoneとThurstone, 1941によって（他のデータに基づいて）3つの要因にグループ化された：Verbal Comprehension、Word Fluency、Reasoningである。元のデータはThurstone and Thurstone (1941)によるものだが、Bechthold (1961)によって再分析され、彼はデータセットを2つに分けた。サンプルサイズは213である。Thurstone (1933)に起因する9つの認知変数のもう1つのセットは、プリンストンのBrigham教授が大学入試委員会に報告した4,175人の学生のデータセットである。  Tucker (1958) は、Turstone and Thburstone (1941)の9変数を、彼のバッテリー間因子分析の例に使用しています。  二因子モデルの最近の応用は、心理的状態の測定である。Reiseのデータセットは、Consumer Assessment of Health Care Provider and Systems調査票に対する35,000を超える観察に基づく相関行列である。Reise, Morizot, and Hays (2007)は、1,000症例に基づく二因子解を記述している。    Reiseらによる5つの因子は、「迅速に治療を受ける」（1-3）、「医師がよくコミュニケーションをとる」（4-7）、「丁寧で親切なスタッフ」（8,9）、「必要な治療を受ける」（10-13）、「医療プランの顧客サービス」（14-16）である。2つのBechtoldtデータセットは、Thurstone and Thurstone (1941)からの2つのサンプルである。  これらは、17の変数を含み、そのうちの9つは、Thurstoneデータ集合を形成するためにMcDonaldによって使用されました。サンプルサイズはそれぞれ212と213である。提案された6つの因子は、記憶、言語、単語、空間、数、推論を反映し、暗記記憶因子を期待するすべての3つのマーカーがある。このセットから9変数がThurstoneデータセットに現れる。同様の構造を持つデータセットがHarmanデータセットにさらに2つある。  これには、Harman link{Harman.Holzinger}が使用したHolzingerの別の9変数（被験者696人）と、link{burt}の8感情変数が含まれます。二因子構造の検定のために調べる価値があるもう1つのデータ集合は、Keith Widamanによって提供されたHolzinger and Swineford (1939)のオリジナル・データを含むholzinger.swinefordデータ集合です。  これはpsychToolsパッケージに入っている。Bechtoldt.1: 能力検査の17 x 17相関行列、N = 212.Bechtoldt.2：能力検査の17 x 17相関行列，N = 213．Holzinger：能力検査の14 x 14相関行列，N = 355 Holzinger.9：能力検査の9 x 9相関行列，N = 145 Reise：健康満足度項目の16 x 16相関行列．  N = 35,000 Thurstone: 能力検査の9 x 9相関行列、N = 213 Thurstone.33: 能力検査の別の9 x 9相関行列、N = 4175 Thurstone:9：さらに別の9×9の能力テストの相関行列、N=710
HolzingerとSwineford（1937）は、精神能力について二因子モデル（1つの一般因子といくつかのグループ因子）を導入した。  これは、オメガ関数やsemを用いて分析できる階層的な因子構造のすばらしい実証データセットである。このようなデータを分析するには、いくつかの方法があります。  1つは、オメガ関数を使用して、Schmid-Leiman変換を使用して階層因数分解を行うことである。これは、探索的モデルとして、そしてomegaSemを用いた確認的モデルとして行うことができます。もう1つの方法は、通常の因子分析を行い、bifactor 回転またはbiquartimin 回転を使用することです。後者の2つの関数は、Jennrich and Bentler (2011) のバイファクター変換とバイクォーティミン変換を実装しています。  バイファクター回転は，局所最小の問題（Mansolf and Reise, 2016）に悩まされるので，探索的分析と確認的分析の混合が望ましいかもしれない．14の変数は、3つの空間テスト、3つのメンタル・スピード・テスト、4つの運動スピード・テスト、4つの言語テストを反映するように順序付けされている。  Holzingerからのもう1つのデータ集合（Holzinger.9）は、9つの認知能力（Holzinger, 1939）を表し、Karl Joreskog (2003)がMINRESアルゴリズムによる因子分析の例として使用し、LISRELのマニュアルにも例NPV.KMとして掲載されています。  このデータ集合は、Grant White中学校の9つのテストの得点です：  「t01_visperc""t02_cubes""t04_lozenges""t06_paracomp""t07_sentcomp""t09_wordmean""t10_addition""t12_countdot "および "t13_sccaps "で、lavaanパッケージの変数x1 ... x9（Grant-White校の場合）として表示されます。もう1つの古典的なデータ集合は、R. P. McDonald (1985, 1999)によって詳細に議論され、SASのPROC CALIS マニュアルと同様に sem パッケージで例として使用されている9変数のThurstone問題です。  これらの9つのテストは、ThurstoneとThurstone, 1941によって（他のデータに基づいて）3つの要因にグループ化された：Verbal Comprehension、Word Fluency、Reasoningである。元のデータはThurstone and Thurstone (1941)によるものだが、Bechthold (1961)によって再分析され、彼はデータセットを2つに分けた。サンプルサイズは213である。Thurstone (1933)に起因する9つの認知変数のもう1つのセットは、プリンストンのBrigham教授が大学入試委員会に報告した4,175人の学生のデータセットである。  Tucker (1958) は、Turstone and Thburstone (1941)の9変数を、彼のバッテリー間因子分析の例に使用しています。  二因子モデルの最近の応用は、心理的状態の測定である。Reiseのデータセットは、Consumer Assessment of Health Care Provider and Systems調査票に対する35,000以上の観察に基づく相関行列である。Reise, Morizot, and Hays (2007)は、1,000症例に基づく二因子解を記述している。    Reiseらによる5つの因子は、「迅速に治療を受ける」（1-3）、「医師がよくコミュニケーションをとる」（4-7）、「丁寧で親切なスタッフ」（8,9）、「必要な治療を受ける」（10-13）、「医療プランの顧客サービス」（14-16）である。2つのBechtoldtデータセットは、Thurstone and Thurstone (1941)からの2つのサンプルである。  これらは、17の変数を含み、そのうちの9つは、Thurstoneデータ集合を形成するためにMcDonaldによって使用されました。サンプルサイズはそれぞれ212と213である。提案された6つの因子は、記憶、言語、単語、空間、数、推論を反映し、暗記記憶因子を期待するすべての3つのマーカーがある。このセットから9変数がThurstoneデータセットに現れる。同様の構造を持つデータセットがHarmanデータセットにさらに2つある。  これには、Harman link{Harman.Holzinger}が使用したHolzingerの別の9変数（被験者696人）と、link{burt}の8感情変数が含まれます。二因子構造の検定のために調べる価値があるもう1つのデータ集合は、Keith Widamanによって提供されたHolzinger and Swineford (1939)のオリジナル・データを含むholzinger.swinefordデータ集合です。  これはpsychToolsパッケージに入っている。Bechtoldt.1: 能力検査の17 x 17相関行列、N = 212.Bechtoldt.2：能力検査の17 x 17相関行列，N = 213．Holzinger：能力検査の14 x 14相関行列，N = 355 Holzinger.9：能力検査の9 x 9相関行列，N = 145 Reise：健康満足度項目の16 x 16相関行列．  N = 35,000 Thurstone: 能力検査の9 x 9相関行列、N = 213 Thurstone.33: 能力検査の別の9 x 9相関行列、N = 4175 Thurstone:9：能力検査の9×9相関行列、N =710
NA
tr関数は様々な行列演算で使用され、行列の対角要素の和である。
Tucker (1958)の相関行列は、Tucker and Lewis (1973)でファクタリング信頼性のTucker-Lewis指数に使用された。
これはまだ開発中の探索的指標のセットである。  多くのテストケースから、uはデータが実際に一次元であるときは高い値を示し、そうでないときは低い値を示すことが示唆されている：  一次元性は、データの1因子モデルがデータの共分散に適合することを意味する。  もしそうであれば、因子モデルはR = FF' + U2 の残差が0になることを意味します。同様に、これは観測された相関がモデルと等しくなることも意味します。  したがって，観察された相関の合計（対角は共分散率に置き換えられる）は，因子モデルに一致するはずである．  これらの2つのモデルを比較する：  R - U2 対 FF'。  これがunidim.A推定値である。また、おそらくより良いであろうが、相関に対する1因子モデルの適合も報告される。これは単に残差相関の総和/元の相関の総和です。  因子モデルが完全な場合、これは1になります。これはよく働きますが，1因子が正しいにもかかわらず，負荷量のいくつかが非常に小さい場合，項目が1次元尺度を形成していると考えるのは，おそらくよい考えではないでしょう．  もう1つのモデル（av.r.fit 統計量）は，観察された相関から平均相関を引くことによって得られる残差を考慮する．  fa.fitとav.r.fitの積が、一次元性の尺度uである。
これらの関数の中で最も有用なものは、Jennrich and Bentler (2011)によって導入された斜めバイファクター回転を実装したbiquartiminの2つであろう。2番目はTargetQで、ターゲットのNA値の欠損を許容します。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、その最も高い負荷量を持つグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷量とグループ因子がある純粋なバイファクターモデル）を扱いません。
これらの関数の中で最も有用なのは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminの2つでしょう。2番目は TargetQ で、これはターゲット中の欠損NA値を可能にします。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、その最も高い負荷量を持つグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷量とグループ因子がある純粋なバイファクターモデル）を扱いません。
これらの関数の中で最も有用なのは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminの2つでしょう。2番目は TargetQ で、これはターゲット中の欠損NA値を可能にします。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造（すべての項目が、その最も高い負荷量を持つグループに割り当てられる）に回転することです。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷量とグループ因子がある純粋なバイファクターモデル）を扱いません。
これらの関数の中で最も有用なのは、Jennrich and Bentler (2011)によって導入された斜め2因子回転を実装するbiquartiminの2つでしょう。2番目は TargetQ で、これはターゲット中の欠損NA値を可能にします。次に優れているのは、直交の場合のbifactorである。  これらはいずれもGPArotationには（まだ）実装されていないようです。faRotateは、ここで追加されたものと同様に、様々なGPArotation関数を呼び出す便利な方法に過ぎません。biquartiminとbifactorの違いは、後者がJennrich and Bentler (2011)で文書化されている直交ケースであるということだけです。  Holzingerの24認知変数に対するbifactorの出力は、JennrichとBentlerの出力と一致し、fm="mle "が使用され、JennrichとBentlerの解が共分散から相関に再スケーリングされた場合のChenらの問題に対する出力も一致します。Promaxは、stats::promax関数を非常に直接的に適応したものです。  varimin は、Suitbert Ertl (2013)によって提案されたvarimin 基準を実装しています。  varimax 基準を最大化するのではなく、最小化する。  さらに、これらの関数は、factanal、fa、または以前の（factor.pa、factor.minres、またはprincipal）関数からの出力を受け取り、分析のために負荷行列だけを選択します。equamaxは、GPArotationのcFT関数（回転のCrawford Fergusonファミリー用）を呼び出すだけです。TargetQはMichael Browneのアルゴリズムを実装したもので、NA値の指定が可能です。Targetの入力はリストである（例参照）。  因子を定義する際に、因子が何でないかを指定することがいかに強力に働くかは興味深い。  target.rot関数は、Michael Browneの関数を適応したもので、任意のターゲット行列に対して回転を行うことができます。  Pat Shroutの提案によるものです。target.rotのデフォルトは、独立クラスター構造に回転します（すべての項目が、最も負荷の高いグループに割り当てられます）。target.rotは、線形従属性を持つターゲット（たとえば、すべての変数にg負荷とグループ因子がある純粋な2因子モデル）を扱いません。
バイオリンプロットを使ってデータを記述します。アルファ値を変更して陰影を変更します。  比較的小さなデータセット（500-1000未満）では、実際のデータポイントも表示するのが有益です。これはdots=TRUEオプションで行います。ジッター値は任意に0.05に設定されますが、これを大きくすると（例えば0.1や0.2）、より多くのポイントが表示されます。
バイオリンプロットを使ってデータを記述します。alphaを変更して陰影を変更します。  比較的小さいデータセット（< 500-1000）の場合、実際のデータポイントも表示すると有益です。これはdots=TRUEオプションで行います。ジッター値は任意に0.05に設定されますが、これを大きくすると（例えば0.1や0.2）、より多くのポイントが表示されます。
因子分析から最も解釈しやすい因子の数を決定することは、おそらく因子分析の最大の課題の1つです。  この問題には多くの解決策がありますが、どれも一様に最良とは言えません。  "因子数の問題を解くのは簡単だ、毎日朝食前にやっている"。  しかし、正しい解を知ることは難しい。(Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  3) 実データの固有値が、同じサイズのランダムなデータ集合の対応する固有値よりも小さくなるまで因子を抽出する（並列分析） fa.parallel.4連続する固有値の大きさをプロットし、scree検定（山の距骨の斜面をよじ登り、岩肌に近づいたときに見られる斜面の変化に類似した固有値の急激な低下）を適用する5) 固有値＜1になるまで主成分を抽出する6) 間隔がある限り因子を抽出する7) Very Simple Structure Criterion (VSS)を使用する8) Wayne VelicerのMinimum Average Partial (MAP)基準を使用する。それぞれの手続きには長所と短所がある。  カイ2乗検定または2乗変化検定は，もちろん被験者数に敏感で，多くの因子を見つけたい場合は，単に被験者を増やせばよいという無意味な状態になる．並列分析は，大きな標本ではランダム因子の固有値が非常に小さくなるという点で，標本サイズに部分的に敏感である．  scree検定は非常に魅力的ですが、screeがいつ "壊れる "かについての解釈の相違につながる可能性があります。解釈可能な因子を抽出することは、因子の数がデータよりも調査者の創造性を反映することを意味します。  VSSは、理解するのは非常に簡単ですが、データが因子的に非常に複雑な場合は、あまりうまく機能しません。(ほとんどの因子分析のユーザーは、すべての変数について最大の負荷量に注意を集中し、より小さな負荷量を無視することによって、因子出力を解釈する傾向があります。  非常に単純な構造は，オリジナルの相関行列を，オリジナルの因子行列(F)の単純化バージョン(S)によって再現されるものと比較することによって，この傾向を運用する．  R = SS' + U2.   S は，各変数の（絶対値で）最大 c 個の負荷量だけで構成される．  C （または複雑度）は，モデルのパラメータで，1から因子の数まで変化する．  VSS 基準は，元の相関に対する単純化モデルの適合を比較する．VSS = 1 -sumsquares(r*)/sumsquares(r) ここで R* は残差行列 R* = R - SS'であり、r* と r はそれぞれ R* と R の要素である。与えられた複雑度に対するVSSは、最適な（最も解釈しやすい）因子の数でピークに達する傾向がある(Revelle and Rocklin, 1979)。VSSはもともとメインフレームコンピュータ用のFortranで書かれたが、Pascalを使ってマイクロコンピュータ（例えばMacintosh OS 6-9）に適応されている。今回、VSSを計算するためのRコードを公開します。相関行列(例えばmy.matrix)を使って因子分析を行う場合、パラメータn.obsは因子分析のために指定する必要があることに注意してください：例えば、呼び出しはVSS(my.matrix,n.obs=500)です。  そうでない場合、デフォルトは1000になります。Wayne VelicerのMAP基準は、抽出する最適な成分数の追加テストとして追加されました。  nfactors関数は、VSSを行い、MAPを求め、他の多くの基準（例えば、BIC、複雑度、カイ2乗、...）を報告します。さまざまな回転オプションが利用できます。これらには、varimax、promax、obliminが含まれる。他にも追加可能です。  提案を歓迎する。
因子分析から最も解釈可能な因子の数を決定することは，おそらく因子分析における最大の課題の1つであろう．  この問題には多くの解決策がありますが，どれも一様に最良とは言えません．  "因子数の問題を解くのは簡単だ、毎日朝食前にやっている"。  しかし、正しい解を知ることは難しい。(Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  3) 実データの固有値が、同じサイズのランダムなデータ集合の対応する固有値よりも小さくなるまで因子を抽出する（並列分析） fa.parallel.4連続する固有値の大きさをプロットし、scree検定（山の距骨の斜面をよじ登り、岩肌に近づいたときに見られる斜面の変化に類似した固有値の急激な低下）を適用する5) 固有値＜1になるまで主成分を抽出する6) 間隔がある限り因子を抽出する7) Very Simple Structure Criterion (VSS)を使用する8) Wayne VelicerのMinimum Average Partial (MAP)基準を使用する。それぞれの手続きには長所と短所がある。  カイ2乗検定または2乗変化検定は，もちろん被験者数に敏感で，多くの因子を見つけたい場合は，単に被験者を増やせばよいという無意味な状態になる．並列分析は，大きな標本ではランダム因子の固有値が非常に小さくなるという点で，標本サイズに部分的に敏感である．  scree検定は非常に魅力的ですが、screeがいつ "壊れる "かについての解釈の相違につながる可能性があります。解釈可能な因子を抽出することは、因子の数がデータよりも調査者の創造性を反映することを意味します。  VSSは、理解するのは非常に簡単ですが、データが因子的に非常に複雑な場合は、あまりうまく機能しません。(ほとんどの因子分析のユーザーは、すべての変数について最大の負荷量に注意を集中し、より小さな負荷量を無視することによって、因子出力を解釈する傾向があります。  非常に単純な構造は，オリジナルの相関行列を，オリジナルの因子行列(F)の単純化バージョン(S)によって再現されるものと比較することによって，この傾向を運用する．  R = SS' + U2.   S は，各変数の（絶対値で）最大 c 個の負荷量だけで構成される．  C （または複雑度）は，モデルのパラメータで，1から因子の数まで変化する．  VSS 基準は，元の相関に対する単純化モデルの適合を比較する．VSS = 1 -sumsquares(r*)/sumsquares(r) ここで R* は残差行列 R* = R - SS'であり、r* と r はそれぞれ R* と R の要素である。与えられた複雑度に対するVSSは、最適な（最も解釈しやすい）因子の数でピークに達する傾向がある(Revelle and Rocklin, 1979)。VSSはもともとメインフレームコンピュータ用のFortranで書かれたが、Pascalを使ってマイクロコンピュータ（例えばMacintosh OS 6-9）に適応されている。今回、VSSを計算するためのRコードを公開します。相関行列(例えばmy.matrix)を使って因子分析を行う場合、パラメータn.obsは因子分析のために指定する必要があることに注意してください：例えば、呼び出しはVSS(my.matrix,n.obs=500)です。  そうでない場合、デフォルトは1000になります。Wayne VelicerのMAP基準は、抽出する最適な成分数の追加テストとして追加されました。  nfactors関数は、VSSを行い、MAPを求め、他の多くの基準（例えば、BIC、複雑度、カイ2乗、...）を報告します。さまざまな回転オプションが利用できます。これらには、varimax、promax、obliminが含まれる。他にも追加可能です。  提案を歓迎する。
NA
項目因子モデルは、その「複雑度」によって異なる。  複雑度1 は、項目に対する最大の（絶対）負荷以外は無視されることを意味する。基本的にはクラスター・モデル（例：ICLUST）。複雑度2は、最大の2つを除くすべてを意味する。  異なる複雑度は、抽出する因子の最適な数を示唆することができる。  パーソナリティ項目については、複雑度1と2がおそらく最も意味があります。非常に単純な構造基準は、与えられた複雑さのレベルで最も解釈可能な因子の数でピークに達する傾向があります。  問題によっては、最も解釈可能な因子の数が複雑さの関数として異なることに注意してください。  例えば、ハーマン24の心理変数の問題を解くとき、複雑度1の回転していない解は1因子(g)を示唆しますが、複雑度2の解は4因子の解が最も適切であることを示唆します。  この後者は、おそらく2因子構造を反映していると思われます。  VSS.plot出力の例については、https://personality-project.org/r/r.vss.html を参照。
最適な因子数を選択する多くの方法の中に、scree検定があります。  screeを表示するだけでなく、ランダムに並列化された解と比較するためのより良い関数がfa.parallelにあります。
NA
NA
 Schmid and Leiman (1957)の2つの人工相関行列。Chen et al. (2006)からの実共分散行列と人工共分散行列。  Schmid: Schmid-Leiman変換を示すために作成された12 x 12の人工相関行列：対角上に共分散を持つ12 x 12の行列。  これを共分散行列として扱うと，6 x 6 因子解を示す Chen: Chen ら (2006)の健康関連QOL項目の18 x 18 共分散行列．オブザベーションの数 = 403.  最初の項目は、生活の質の尺度である。  残りの17項目は4つのサブファクターを形成する：項目は以下の通りである：「推論や問題解決が困難か？  "言われたことやされたことへの反応が遅い"、"混乱して一度にいくつもの行動を始める"。  "物や約束をどこに置いたか忘れるか？"; "集中するのが難しいか？" (b) 活力下位尺度：「疲れを感じるか？  「(R)「疲れていると感じますか？(右) "疲れを感じるか"、"活力に満ちているか"。(R).(c）メンタルヘルス下位尺度：c）精神的健康下位尺度：「穏やかで平和な気分ですか」（R）、「落ち込んでブルーな気分ですか」（R）、「とても幸せな気分ですか」（R）、「とても神経質な気分ですか」（R）、「何も元気が出ないほど落ち込んでいますか」（R）。 d）疾病の心配下位尺度：(d)病気の心配下位尺度：「健康のことで恐れていましたか」；「健康についてイライラしていましたか」；「健康は生活の心配事でしたか」。西Chen et al. (2006)による16 x 16の人工共分散行列。
中心傾向の多くのロバスト推定値の中で、Winsorized平均を推奨するものがある。  最上位と最下位のトリム・パーセントを削除するのではなく、これらの極端な値をトリム分位点と1-トリム分位点の値で置き換える。
中心傾向の多くのロバスト推定値の中で、ウィンソライズ平均を推奨するものがある。  単に上下のトリム・パーセントを削除するのではなく、これらの極端な値をトリム分位点および1-トリム分位点の値に置き換える。
中心傾向の多くのロバスト推定値の中で、ウィンソライズ平均を推奨するものがある。  単に上下のトリム・パーセントを削除するのではなく、これらの極端な値をトリム分位点および1-トリム分位点の値に置き換える。
中心傾向の多くのロバスト推定値の中で、ウィンソライズ平均を推奨するものがある。  単に上下のトリム・パーセントを削除するのではなく、これらの極端な値をトリム分位点および1-トリム分位点の値に置き換える。
中心傾向の多くのロバスト推定値の中で、ウィンソライズ平均を推奨するものがある。  上位と下位のトリム・パーセントを削除するのではなく、これらの極端値をトリム分位点と1-トリム分位点の値に置き換える。
異なる自然グループ（例えば、民族、年齢、性別、大学の専攻、国など）に属する個人間の相関は、これらのグループの平均の相関と同様に、各グループ内のプールされた相関の未知の混合を反映する。これら2つの相関は独立しており、1つのレベル（グループ）からもう1つのレベル（個人）への推論はできません。  このデータ・セットはこの独立性を示している。  9変数間のグループ内相関は、1, 0, -1に設定され、グループ間相関も1, 0, -1に設定されます。 V1, V4, V7は、グループ内相関が1であり、V2, V5, V8、V3, V6, V9もそうであるように、相関のこれらの2つのセットは交差しています。  V1はV2、V5、V8とのグループ内相関が0、V3、V6、V9とのグループ内相関が-1。  V1、V2、V3は、V4、V5、V6、V7、V8、V9と同様に群間相関が1である。  statsByでは、観測された相関を群間相関と群内相関に分解することができ、sim.multilevelでも同様のデータが得られる。
2つのカテゴリーでカテゴリー判断が行われる場合、関係の尺度はファイ係数である。  しかし、カテゴリー判断の中には、2つ以上の結果を用いてなされるものもある。  例えば、2人の診断士が患者を3通りに分類するよう求められたり（例えば、人格障害、神経症、精神病）、病気の病期を分類するよう求められたりする。  塩基率が2×2の表で観察されるセルの度数に影響するように、n-way表でも考慮する必要がある(Cohen, 1960)。Kappaは主対角線上のマッチを考慮する。  対角線から外れた一致にはペナルティ関数（重み）を適用できる。  重みが対角からの距離の2乗で増加する場合，重み付きκはクラス内相関（ICC）に似ている．重み付きκの導出は，類似度で表現されることもあれば，非類似度で表現されることもある．後者の場合、対角線上の重みは1で、対角線から外れた重みは1より小さい。この場合、重みが1-対角からの距離の2乗/kであれば、結果はICCに類似している（任意の正のkの場合）。 cohen.kappaは、様々な公表された例と一致させるために、類似度重み付け（対角=0）または非類似度重み付け（対角=1）のいずれかを使用することができる。入力は2列のdata.frameまたは行列で、列は2人の審査員を表し、行は評価される被験者を表す。あるいは、入力は正方形の n x n 行列で、マッチの回数または割合を表すこともできる。  比率が使用される場合、信頼区間を正しく求めるためにオブザベーションの数 (n.obs)を指定することが必要です。信頼区間は、Cohen (1968)とBlashfieldの式を修正したFleiss, Cohen, and Everittによって議論された分散推定に基づいています。いくつかのデータ集合は、いくつかのカテゴリ値が完全に欠損している数値カテゴリを持つデータを含みます。  κがカテゴリー関係の尺度であるという意味では、これは問題ではないはずである。  しかし、重み付きκを求めるとき、重み付けされたカテゴリの数は、データ中の潜在的なカテゴリの数より少なくなる。  これはlevelsパラメータを指定することで改善できる。  これは、データ中に潜在的に存在するレベルのベクトルです（欠落しているものもあります）。   2人以上の評価者がいる場合は、すべての評価者の平均がLight's kappaとして知られている(Conger, 1980)．(Conger, 1980).  
Yule は，2対2の表について2つの関連尺度を開発した．  両方ともオッズ比の関数である
Yule は，2×2表について2つの関連尺度を開発した．  両方ともオッズ比の関数である
Yule は2×2表について2つの関連尺度を開発した．  どちらもオッズ比の関数である
これらの関数は，行列の各セルに対して Yule2poly，Yule2phi，または phi2poly を呼び出します．詳細はこれらの関数を参照してください。  例については phi.demo を参照。
Yuleは2×2の表に対して2つの関連尺度を開発しました。  どちらもオッズ比の関数です
これらの関数は，行列の各セルに対して Yule2poly，Yule2phi，または phi2poly を呼び出します。詳細はこれらの関数を参照してください。  例については phi.demo を参照。
Yuleは2×2の表に対して2つの関連尺度を開発しました。  どちらもオッズ比の関数です
Yule は2×2の表について2つの関連尺度を開発した。  どちらもオッズ比の関数
Yuleは2×2の表について2つの関連尺度を開発した。  両方ともオッズ比の関数である
type = "correlation" および "covariance" の場合，推定は標本の共分散に基づく．(デフォルトでは，欠損値は許されない．  (na.passのように)na.action関数が欠損値を通過する場合、共分散は完全なケースから計算されます。  これは、計算された推定値が有効な自己相関列ではない可能性があり、欠損値を含む可能性があることを意味します。  偏相関係数は、lag.maxまでの高次の自己回帰モデルに適合させることによって推定される。ジェネリック関数plotには、クラス "acf "のオブジェクト用のメソッドがある。
NA
factor.scopeは、ユーザーが直接呼び出すことを意図していない。
drop1 メソッドでは、スコープがないと、モデル内のすべての項とみなされる。lmとglmのメソッドは、モデル行列を再計算せず、fitメソッドを直接呼び出すという点でより効率的です。デフォルトの出力表は、対数尤度のマイナス2倍＋2p（pはモデルのランク（有効パラメータの数））と定義されたAICを示します。  これは(対数尤度のように)加法定数までしか定義されません。  glm "手法のF検定はデビアンス分析検定に基づくので、分散が推定される場合は、anova.glmのF検定とは異なり、残留デビアンスに基づく。
マージンを形成するために使用される関数が可換でない場合、結果はマージンが計算される順序に依存します。  マージンのアノテーションはFUNリストに名前を付けることで行う。
デフォルトのメソッドであるaggregate.defaultは、xが時系列の場合はtime seriesメソッドを使用し、そうでない場合はxをデータフレームに変換してデータフレームメソッドを呼び出します。  xがデータフレームでない場合は、データフレームに強制され、そのデータフレームは0行であってはならない。  そして、xの各変数（列）は、byの各要素の同一の組み合わせのケース（行）のサブセットに分割され、FUNは、さらに...の引数が渡されたそのようなサブセットに適用されます。  結果は、by と x の変数を含むデータフレームに再フォーマットされる。by から生じるものには、部分集合を決定するために使用されたグループ化値の一意な組み合わせが含まれ、x から生じるものには、x のそれぞれの変数の部分集合に対応する要約が含まれる。simplify が真である場合、要約は、それぞれ長さが1以上であれば、ベクトルまたは行列に単純化される。  それ以外の場合は、部分集合に従った要約結果のリストが得られる。  (aggregate.formulaは、aggregate.data.frameに対する標準的な数式インターフェースです。aggregate.tsは時系列メソッドで、FUNがスカラー関数であることを必要とします。  xが時系列でない場合、時系列に強制される。  そして、xの変数がfrequency(x) / nfrequencyの長さの適切なブロックに分割され、FUNがそのようなブロックごとに適用されます。  FUNはmatch.funに渡されるため、関数であることも、関数を示す記号や文字列であることもある。
デフォルトのメソッドであるaggregate.defaultは、xが時系列の場合はtime seriesメソッドを使用し、そうでない場合はxをデータフレームに強制してデータフレームメソッドを呼び出します。  xがデータフレームでない場合は、データフレームに強制され、そのデータフレームは0行であってはならない。  そして、xの各変数（列）は、byの各要素の同一の組み合わせのケース（行）のサブセットに分割され、FUNは、さらに...の引数が渡されたそのようなサブセットに適用されます。  結果は、by と x の変数を含むデータフレームに再フォーマットされる。by から生じるものには、部分集合を決定するために使用されたグループ化値の一意な組み合わせが含まれ、x から生じるものには、x のそれぞれの変数の部分集合に対応する要約が含まれる。simplify が真である場合、要約は、それぞれ長さが1以上であれば、ベクトルまたは行列に単純化される。  それ以外の場合は、部分集合に従った要約結果のリストが得られる。  (aggregate.formulaは、aggregate.data.frameに対する標準的な数式インターフェースです。aggregate.tsは時系列メソッドで、FUNがスカラー関数であることを必要とします。  xが時系列でない場合、時系列に強制される。  そして、xの変数がfrequency(x) / nfrequencyの長さの適切なブロックに分割され、FUNがそのようなブロックごとに適用されます。  FUNはmatch.funに渡されるため、関数であることも、関数を示す記号や文字列であることもある。
デフォルトのメソッドであるaggregate.defaultは、xが時系列の場合はtime seriesメソッドを使用し、そうでない場合はxをデータフレームに強制してデータフレームメソッドを呼び出します。  xがデータフレームでない場合は、データフレームに強制され、そのデータフレームは0行であってはならない。  そして、xの各変数（列）は、byの各要素の同一の組み合わせのケース（行）のサブセットに分割され、FUNは、さらに...の引数が渡されたそのようなサブセットに適用されます。  結果は、by と x の変数を含むデータフレームに再フォーマットされる。by から生じるものには、部分集合を決定するために使用されたグループ化値の一意な組み合わせが含まれ、x から生じるものには、x のそれぞれの変数の部分集合に対応する要約が含まれる。simplify が真である場合、要約は、それぞれ長さが1以上であれば、ベクトルまたは行列に単純化される。  それ以外の場合は、部分集合に従った要約結果のリストが得られる。  (aggregate.formulaは、aggregate.data.frameに対する標準的な数式インターフェースです。aggregate.tsは時系列メソッドで、FUNがスカラー関数であることを必要とします。  xが時系列でない場合、時系列に強制される。  そして、xの変数がfrequency(x) / nfrequencyの長さの適切なブロックに分割され、FUNがそのようなブロックごとに適用されます。  FUNはmatch.funに渡されるため、関数であることも、関数を示す記号や文字列であることもある。
AICの理論では、対数尤度が最大化されていることが必要です。最尤法によらないモデルでもAICは計算できますが、それらのAIC値を比較すべきではありません。これらの関数はジェネリック関数です（S4ジェネリック関数はpackagestats4で定義されています）：しかし、メソッドはこれらの関数ではなく、対数尤度関数logLikに対して定義されるべきです：デフォルトのメソッドの動作は、与えられたすべてのオブジェクトに対してlogLikを呼び出し、結果をまとめることです。  対数尤度、ひいてはAIC/BICは加法定数までしか定義されません。  従来、異なる定数が異なる目的で使用されてきたため、extractAICとAICは異なる値を与える可能性があります（クラス "lm "のモデルではそうです：extractAICのヘルプを参照してください）。  BICは、AIC(object, ..., k = log(nobs(object)))として定義されます。これは、オブザベーションの数が既知である必要があります：デフォルトのメソッドは、まずlogLikメソッドからの返り値で "nobs "属性を探し、次にnobsジェネリックを試し、どちらも成功しなければBICをNAとして返します。
完全エイリアシング（Complete aliasing）とは、線形モデルにおける効果で、モデルの初期に現れる項とは独立に推定できないため、その係数がフィットから省略されることをいう。部分的エイリアシングとは、デザインによって誘発される相関のために、より正確には推定できない効果を指します。"lm "メソッドのいくつかの部分は、推奨パッケージMASSがインストールされている必要があります。
NA
xとyがそれぞれ密度f((t-m)/s)/sとf(t-m)を持つ分布からの独立標本であり、mが未知の厄介なパラメータ、sが注目するパラメータであるとする。  アンサリ・ブラドレー検定は、sが1に等しいという帰無値を検定するために使用され、両側対立選択肢は、s != 1（分布は分散のみが異なる）であり、片側対立選択肢は、s > 1（xの基礎となる分布は大きな分散を持つ、"より大きい"）またはs < 1（"より小さい"）である。デフォルトでは（正確が指定されていない場合）、両方の標本が50未満の有限値を含み、同値がない場合、正確なp値が計算される。  オプションで、ノンパラメトリック信頼区間とforsの推定値が計算されます。  正確なp-値が利用可能な場合，Bauer(1972)に記述されたアルゴリズムによって正確な信頼区間が求められ，Hodges-Lehmann推定量が採用される．  それ以外の場合は，正規近似に基づいて信頼区間と点推定値が求められる．同点の場合は，Hollander & Wolfe (1973)で採用されている平均点ではなく，中間順位が使用されることに注意されたい．  詳細は、Hajek,Sidak and Sen (1999)の131ffページを参照。
lmとの主な違いは、print,summaryなどが適合を処理する方法です：これは、線形モデルではなく、伝統的な分散分析の言語で表現されます。式が1つのError項を含む場合、これは誤差層の指定に使用され、各誤差層で適切なモデルがフィットされます、モデル.tablesではサポートされていない)。
(na.rmがtrueの場合、つまりデフォルトの場合)入力には欠損値が含まれることがあり、少なくとも2つの完全な(x, y)ペアが必要である(method = "linear "の場合、それ以外は1つ)。  ties="ordered "の場合、x値はすでに順序付けされている（そして一意である）と仮定され、同値はチェックされませんが、存在する場合は保持されます。tiesが長さ2のリストの場合、ties[[2]]はtiesに適用される関数でなければならないが、ties[[1]]が "ordered "と同じ場合、x値はソートされていると仮定され、同値のチェックのみが行われる。  その結果、ties = list("ordered", mean)の方が、デフォルトのties = meanよりも若干効率的である。
入力には欠損値が含まれることがあり、（na.rmがtrueの場合、つまりデフォルトの場合）欠損値は削除されるので、少なくとも2組の完全な（x, y）組が必要である（method = "linear "の場合は2組、それ以外の場合は1組）。  ties="ordered "の場合、x値はすでに順序付けされている（そして一意である）と仮定され、同値はチェックされませんが、存在する場合は保持されます。tiesが長さ2のリストの場合、ties[[2]]はtiesに適用される関数でなければならないが、ties[[1]]が "ordered "と同じ場合、x値はソートされていると仮定され、同値のチェックのみが行われる。  その結果、ties = list("ordered", mean)の方が、デフォルトのties = meanよりも若干効率的である。
AR係数はx[t] - m = a[1]*(x[t-1] - m) + ... + a[p]*(x[t-p] - m) + e[t]arは関数ar.yw,ar.burg,ar.ols,ar.mleのラッパーに過ぎない。ar.mleだけが真の最尤推定を行うので、これは問題である。AICは、分散推定がMLEであるかのように計算され、尤度から行列式項を省略します。これは推定されたパラメータ値で評価されるガウス尤度とは異なることに注意してください。  ar.ywでは、イノベー ションの分散行列は、フィットされた係数とx.ar.burgの自己共分散から計算されます。方法1は、Levinson-Durbin再帰式(Brockwell and Davis, 1991, (8.2.6)on page 242)で与えられる更新を使用し、S-PLUSに従う。方法2は、前方予測誤差と後方予測誤差の2乗和の平均(Brockwell and Davis, 1996, page 145と同様)です。Percival and Walden(1998)は両方について議論しています。多変量の場合、推定係数は分散推定方法に(少し)依存します。arは、ARモデルをフィットする前にxの全体平均を除去するか、(ar.mle)減算する定数を推定することによって、デフォルトでモデルに定数を含むことを覚えておいてください。
arはar.yw,ar.burg,ar.ols,ar.mle関数のラッパーである。ar.mleだけが真の最尤推定を行うので、これは問題である。AICは、分散推定がMLEであるかのように計算され、尤度から行列式項を省略します。これは推定されたパラメータ値で評価されるガウス尤度とは異なることに注意してください。  ar.ywでは、イノベー ションの分散行列は、フィットされた係数とx.ar.burgの自己共分散から計算されます。方法1は、Levinson-Durbin再帰式(Brockwell and Davis, 1991, (8.2.6)on page 242)で与えられる更新を使用し、S-PLUSに従う。方法2は、前方予測誤差と後方予測誤差の2乗和の平均(Brockwell and Davis, 1996, page 145と同様)です。Percival and Walden(1998)は両方について議論しています。多変量の場合、推定係数は分散推定方法に(少し)依存します。arは、ARモデルをフィットする前にxの全体平均を除去するか、(ar.mle)減算する定数を推定することによって、デフォルトでモデルに定数を含むことを覚えておいてください。
arはar.yw,ar.burg,ar.ols,ar.mle関数のラッパーである。ar.mleだけが真の最尤推定を行うので、これは問題である。AICは、分散推定がMLEであるかのように計算され、尤度から行列式項を省略します。これは推定されたパラメータ値で評価されるガウス尤度とは異なることに注意してください。  ar.ywでは、イノベー ションの分散行列は、フィットされた係数とx.ar.burgの自己共分散から計算されます。方法1は、Levinson-Durbin再帰式(Brockwell and Davis, 1991, (8.2.6)on page 242)で与えられる更新を使用し、S-PLUSに従う。方法2は、前方予測誤差と後方予測誤差の2乗和の平均です(Brockwell and Davis, 1996, page 145と同様)。Percival and Walden(1998)は両方について議論しています。多変量の場合、推定係数は分散推定方法に(少し)依存します。arは、ARモデルをフィットする前にxの全体平均を取り除くか、(ar.mle)差し引く定数を推定することによって、デフォルトでモデルに定数を含むことを覚えておいてください。
ar.olsは、系列xの非定常および/または多変量システムに一般的なARモデルをフィットします。結果の非制約最小二乗推定値は、系列の一部が非定常および/または共積分であっても一貫しています。ここで、a[0]は、intercept が真でない限りゼロであり、m は、demean が真であれば標本平均であり、そうでなければゼロである。ar.olsは真の最尤推定を行わないので、これは問題である。AICは、（残差の分散行列から計算された）分散推定値がMLEであるかのように計算され、尤度から行列式項を省略します。これは、推定されたパラメータ値で評価されるガウス尤度とは異なることに注意してください。これを使用するのは、系列がほぼゼロを中心としている場合のみです。そうでない場合は、計算が不正確になるか、完全に失敗する可能性があります。
AR係数はx[t] - m = a[1]*(x[t-1] - m) + ... + a[p]*(x[t-p] - m) + e[t]arは関数ar.yw,ar.burg,ar.ols,ar.mleのラッパーです。ar.mleだけが真の最尤推定を行うので、これは問題である。AICは、分散推定がMLEであるかのように計算され、尤度から行列式項を省略します。これは推定されたパラメータ値で評価されるガウス尤度とは異なることに注意してください。  ar.ywでは、イノベー ションの分散行列は、フィットされた係数とx.ar.burgの自己共分散から計算されます。方法1は、Levinson-Durbin再帰式(Brockwell and Davis, 1991, (8.2.6)on page 242)で与えられる更新を使用し、S-PLUSに従う。方法2は、前方予測誤差と後方予測誤差の2乗和の平均です(Brockwell and Davis, 1996, page 145と同様)。Percival and Walden(1998)は両方について議論しています。多変量の場合、推定係数は分散推定方法に(少し)依存します。arは、ARモデルをフィットする前にxの全体平均を除去するか、(ar.mle)減算する定数を推定することによって、デフォルトでモデルに定数を含むことを覚えておいてください。
ARMAモデルの定義によって、AR係数やMA係数の符号が異なります。  ここで使用されている定義は、X[t] = a[1]X[t-1] + ... + a[p]X[t-p] + e[t] + b[1]e[t-1] + ... + b[q]e[t-q] であり、MA係数の符号はS-PLUSのものとは異なります。meanがtrue（ARMAモデルのデフォルト）である場合、この式はXではなくX - mに適用されます。差分を取るARIMAモデルの場合、差分された系列はゼロ平均ARMAモデルに従います。推定値の分散行列は対数尤度のヘシアンから求められるので、大まかな目安にしかならないかもしれません。  最適化はoptimによって行われる。optimはxregの列がゼロ平均、単位分散に大まかにスケーリングされている場合に最適に働くが、適切なスケーリングを推定しようとする。
ARIMAモデルの正確な定義についてはarimaを参照のこと。ARMAモデルは定常性をチェックされる。ARIMAモデルはarimaと同じようにmodelのorderコンポーネントで指定される。  次数成分の他の側面は無視されるが、MAとARの次数の矛盾した指定は検出される。  非微分化は以前の値をゼロと仮定し、このことをユーザーに思い出させるために、それらの値が返されます。「バーンイン」期間のランダム入力は、rand.genを呼び出して生成されます。
ARMAモデルの定義によって、AR係数やMA係数の符号が異なります。ここでの定義は、X[t] = a[1]X[t-1] + ... + a[p]X[t-p] + e[t] + b[1]e[t-1] + ... + b[q]e[t-q]であり、MA係数はS-PLUSの係数とは符号が異なる。  さらに、include.meanが真の場合、この式はXではなくX-mに適用されます。ARIMAモデルの場合、差分された系列はゼロ平均ARMAモデルに従います。推定値の分散行列は対数尤度のヘシアンから求められるので、特に反転可能性の境界に近いフィットについては、大まかな目安にしかならないかもしれません。これはxregの列がおおよそゼロ平均と単位分散にスケーリングされている場合に最も効果的ですが、適切なスケーリングを推定しようとします。これは、フィットのMA部分が反転可能である場合にのみ統計的に効率的であり、sopredict.arima0は反転不可能なMAモデルに対して警告を出します。
chisq()関数はnon-centrality引数を取るようになったので、*nchisq()関数は不要になりました。  arima0.diagはtsdiagに置き換えられました。arima0.plot.mtsは削除され、plot.tsが同じ機能を持つようになりました。lm.fit.nullとlm.wfit.nullは、lm.fitとlm.wfit.nullに取って代わられました。同様に、glm.fit.nullは、glm.fitに取って代わられました。mauchley.testは、Mauchlyの名前のスペルミスでしたが、mauchly.testの導入により修正されました。clearNamesは、unnameとほぼ同時に導入されましたが、より一般的でなく、ほとんど使われていません。plclustは、デンドログラム（"cluster - trees"）を描いていましたが、"dendrogram "クラスのplot()メソッドに取って代わられました。
使用される手法は、Brockwell & Davis (1991, section 3.3)に従う。  式(3.3.8)は、ラグ0, ..., max(p, q+1)での自己共分散について解かれ、残りの自己相関は再帰フィルタによって与えられる。
NA
デンドログラムは，各成分が木の枝に対応する入れ子リストとして直接表現される．  したがって，木zの最初の枝はz[[1]]であり，対応する部分木の2番目の枝はz[[1]][[2]]，またはより短いz[[c(1,2)]]などである．  木の各ノードは，効率的なプロットや切断に必要ないくつかの情報を属性として持ち，そのうち，メンバ，高さ，葉の葉だけが必須である：枝の葉の総数ノードをプロットする高さの数値（非負）枝の左境界（左端の葉）からのノードの水平距離の数値（すべての葉の間の単位1）。  これは、plot(*, center = FALSE) で使用されます。cut()$upper のノードのラベル、前のメンバの数。より一般的には、'horizontal'(horizontal = FALSE の場合、それ以外は 'vertical')整列に使用されるメンバ・コンポーネントの代用です。文字; ノードにつながる辺のラベル。点描画のためのノード固有の属性を指定する名前付きリスト（長さ-1個の構成要素）。cut.dendrogram()は、成分$upperと$lowerを持つリストを返す。前者は、元の木を切り詰めたもので、これもdendrogramクラスである、クラス "hclust "のオブジェクトは、メソッド as.dendrogram()を用いてクラス "dendrogram "に変換できる。reorder.dendrogramも参照してください。merge(x, y, ...)メソッドは、2つ以上のデンドログラムを、xとy（およびオプションのさらなる引数）を枝とする新しいデンドログラムにマージします。  is.leaf(object) は、オブジェクトが aleaf（最も単純なデンドログラム）であるかどうかを示す論理値を返します。plotNode() と plotNodeLimit() はヘルパー関数です。
利用可能な距離尺度は以下の通り (2つのベクトル x と y に対して記述):2つのベクトル間の通常の距離 (2norm 別名 L_2), sqrt(sum((x_i - y_i)^2)).x と y の2つの成分間の最大距離 (supremum norm).2つのベクトル間の絶対距離 (1 norm 別名 L_1).sum(|x_i-y_i|/(|x_i| + |y_i|)).分子と分母がゼロの項はsumから省略され、値がないものとして扱われる、元々、Rはx_i + y_iを使用していたが、1998年から2017年までは｜x_i + y_i｜を使用し、その後、正しい｜x_i｜ +｜y_i｜（別名：非対称バイナリ）を使用している：）ベクトルは2進数のビットとみなされ、0以外の要素は「オン」、0の要素は「オフ」である。  距離は，少なくとも1つがオンであるビットのうち，1つだけがオンであるビットの割合である．ユークリッド距離、マンハッタン距離、キャンベラ距離、ミンコフスキー距離の計算において、いくつかの列が除外される場合、合計は使用される列の数に比例してスケールアップされる。  as.matrix()およびas.dist()の "dist "メソッドは、"dist "クラスのオブジェクトと従来の距離行列との間の変換に使用できます。  as.dist() は汎用関数であり、デフォルトのメソッドは、クラス "dist" を継承するオブジェクト、または as.matrix() を使用する行列と互換性のあるオブジェクトを扱います。  as.matrix()、またはより直接的には、そのようなクラスに対するas.distメソッドを提供することで、距離（非類似度としても知られている）を表現するクラスのサポートを追加することができます。
例えばこのようなモデルは、+演算子で区切られた一連の項から構成されます。項自体は、 : 演算子で区切られた変数名と因子名から構成されます。  演算子 * は因子の交差を表す： a*b は a+b+a:b と解釈される。  演算子は指定された次数の交差を示す。  例えば(a+b+c)^2は(a+b+c)*(a+b+c)と同じで、a,b,cの主効果とそれらの2次相互作用を含む式に展開される。 %in%演算子は、その左側の項が右側の項の中で入れ子になっていることを示す。  たとえば、a + b %in% a は、式 a + a:b に展開される。  演算子 - は指定された項を削除するので、(a+b+c)^2 - a:b は a + b + c + b:c + a:c と同じになる。  これは切片項を削除するのにも使える：線形モデルy ~ x - 1をフィットするとき、原点を通る直線を指定する。  切片のないモデルは、y ~ x + 0 または y ~ 0 + x と指定することもできる。式は通常、変数名とファクト名だけを含むが、算術式を含むこともある。log(y) ~ a + log(x)という式はかなり合法的である。このような算術式に、モデル式で記号的にも使用される演算子が含まれる場合、算術演算子と記号演算子の使用が混同されることがあります。この混同を避けるために、関数I()を使用して、演算子が算術的な意味で使用されるモデル式の部分を括ることができます。  例えば、ay ~ a + I(b+c)という式では、b+cという項はbとcの和と解釈される。変数名は、このような非構文的な名前を使用するすべてのコードが受け入れるという保証はないが、このようなインフォミュレータのようにバックスティックで引用することができる。  関数によっては、strataやclusterのような他の'特別な'名前を受け付けるものもあります（terms.formulaのspecials引数を参照）。式中の.には2つの特別な解釈があります。  通常のものは、モデフィッティング関数のデータ引数の文脈におけるもので、「式に含まれないすべての列」を意味します。  update.formulaのコンテキストでは、'以前に式のこの部分に含まれていたもの'という意味になります。formulaが適合モデルオブジェクト上で呼び出される場合、特定のメソッド（クラス "nls "のメソッドなど）が使用されるか、デフォルトのメソッドが使用されます。  デフォルトでは、まずオブジェクトの "formula "コンポーネントを探し（そしてそれを評価し）、次に "terms "コンポーネントを探し、次に呼び出しのformulaパラメータを探し（そしてその値を評価し）、最後に "formula "属性を探します。  例えば、amodel.frame() のように、"terms" 属性に数式が含まれている場合、その数式が返されます。  もし、以前の (R <= 3.5.x) 挙動を望むのであれば、"terms" 属性を考慮しない補助DF2formula() を使用する。  列数が多い場合は、最初の列が数式のLHSとなり、+で区切られた残りの列がRHSを形成する。
現在のところ、パッケージclusterの関数dianaとagnesが生成するクラス "twins "のオブジェクトの変換のみがサポートされています。  デフォルトのメソッドは、"hclust "オブジェクトが渡されない限りエラーをスローします。
NA
ts関数は、時系列オブジェクトを作成するために使用されます。  これは "ts "のクラス(および追加属性)を持つベクトルまたは行列であり、時間的に等しい間隔でサンプリングされたデータを表します。  行列の場合、行列データの各列は、1つの（単変量の）時系列を含むと仮定されます。時系列は、少なくとも1つのオブザベーションを持つ必要があり、数値である必要はありませんが、非数値系列のサポートは非常に限られています。  特に、算術演算は時間軸の整列を試み、系列の部分集合を抽出するための部分集合が使用できる（例えば、EuStockMarkets[, "DAX"]）。  ただし、最初の (または唯一の) 次元のサブセットは、行列のサブセットと同様、行列またはベクトルを返します。  tのメソッドには、系列を行列（ベクトルの場合は1列の行列）として転置するメソッドがあり、クラス "ts "を継承しない結果を返します。引数frequencyは時系列のサンプリング頻度を示し、デフォルト値1は単位時間間隔ごとに1回のサンプリングを示します。  例えば、データが毎日サンプリングされ、自然な期間が1週間である場合、frequencyに7を使用することができ、データが毎月サンプリングされ、自然な期間が1年である場合、frequencyに12を使用することができます。  4と12の値は、それぞれ四半期系列と月系列を意味する（例えば）印刷法では仮定される。  R 4.0.0からは、frequencyは整数である必要はない。  例えば、frequency = 0.2は、5つの時間単位ごとに1回サンプリングすることを意味します。  デフォルトのメソッドは、オブジェクトにtsp属性があればそれを使って開始時刻と終了時刻を設定し、frequency.is.tsはオブジェクトが時系列かどうかをテストします。  これは汎用的なもので、特定のクラスのオブジェクトを扱うメソッドを書くことができます。
NA
NA
カーネルは、一般的なカーネルまたは指定されたカーネルを構築するために使用されます。  正規化は fork <- kernel(*), sum(k[ -k$m : k$m ]) が1になるように行われる。kernel は Brockwell and Davis (1991), page362 で定義されている平滑化カーネルの「等価な自由度」を返し， bandwidth.kernel は Bloomfield (1976), p. 201 で定義されている等価な帯域幅に連続性補正を加えたものを返す．
x がリストの場合，その要素は，分散の同質性について比較される標本または適合線形モデルとみなされる．  この場合、要素はすべて数値データ・ベクトルかfittedlinearモデル・オブジェクトでなければならず、gは無視され、単純にbartlett.test(x)を使って検定を行うことができます。  サンプルがまだリストに含まれていない場合は、bartlett.test(list(x, ...)) を使用します。そうでない場合は、xは数値データベクトルでなければならず、gはxの対応する要素のグループを与えるxと同じ長さのベクトルまたは因子オブジェクトでなければなりません。
AICの理論では、対数尤度が最大化されていることが必要です：AICは最尤法によらないモデルでも計算できますが、それらのAIC値を比較すべきではありません。これらの関数はジェネリック関数です（S4ジェネリック関数はpackagestats4で定義されています）：しかし、メソッドはこれらの関数ではなく、対数尤度関数logLikに対して定義されるべきです：デフォルトのメソッドの動作は、与えられたすべてのオブジェクトに対してlogLikを呼び出し、結果をまとめることです。  対数尤度、ひいてはAIC/BICは加法定数までしか定義されません。  従来、異なる定数が異なる目的で使用されてきたため、extractAICとAICは異なる値を与える可能性があります（クラス "lm "のモデルではそうです：extractAICのヘルプを参照してください）。  BICは、AIC(object, ..., k = log(nobs(object)))として定義されます。これは、オブザベーションの数が既知である必要があります：デフォルトの手法は、まずlogLikメソッドからの返り値で "nobs "属性を探し、次にnobsジェネリックを試し、どちらも成功しなければBICをNAとして返します。
信頼区間は，Clopper and Pearson (1934)で最初に与えられた手順で得られる．  これは、信頼水準が少なくともconf.levelであることを保証するが、一般的には、最短の信頼区間を与えない。
familyは "glm "と "lm "のクラスのメソッドを持つ汎用関数です（後者はgaussian()を返します）。二項族と準二項族の場合、応答は3つの方法のいずれかで指定できます：因子として： 'success'は、第1水準を持たない（したがって、通常は第2水準を持つ）因子として解釈される。0と1の間の値を持つ数値ベクトルとして：成功ケースの比率として解釈される（ケースの総数は重みで与えられる）．準2項族および準ポアソン族は、分散パラメータが1に固定されていない点でのみ2項族およびポアソン族と異なり、過分散をモデルすることができる。  二項の場合は、McCullagh and Nelder(1989, pp.124-8)を参照。  彼らは、(いくつかの制限のもとで)準2項モデルのように平均に比例する分散を持つモデルが存在することを示しているが、glmがそのモデルにおいて最尤推定値を計算しないことに注意してほしい。  Sの振る舞いは準変数に近い。
バイプロットは、多変量データの行列のオブザベーションと変数の両方を同じプロット上に表現することを目的としたプロットです。biplot.default関数は、単に2組の変数を同じ図にプロットするための基本的なコードを提供するだけである。
これらの検定は、ARMA(p, q)フィットの残差に適用されることもあり、その場合、lag > fitdfであれば、fitdf = p+qとすることで、ヌル仮説分布により近似した結果が得られることが示唆されている。
bw.nrd0は、ガウスカーネル密度推定量の帯域幅を選択するための経験則を実装しています。デフォルトは、標準偏差と四分位範囲の最小値を1で割った値の0.9倍です。標本サイズの34倍の負の5乗（= Silvermanの「経験則」、Silverman (1986, page 48, eqn (3.31)）ただし、四分位が一致する場合は、正の結果が保証される。bw.SJは、微分のパイロット推定を用いて帯域幅を選択するSheather & Jones (1991)の方法を実装している。ste "手法のアルゴリズムは、（unirootを介して）方程式を解き、そのため、境界がユーザー指定されておらず、ルートを括らない場合、区間c(lower, upper)を拡大する。最後の3つの手法は、すべてのペアワイズビニング距離を使用する。  ビニングのため、xが変換されたり符号反転されたりすると、結果はわずかに異なる。
bw.nrd0は、ガウシアンカーネル密度推定量の帯域幅を選択するための経験則を実装している。デフォルトは、標準偏差と四分位範囲間の最小値を1で割った値の0.9倍である。標本サイズの34倍の負の5乗（= Silvermanの「経験則」、Silverman (1986, page 48, eqn (3.31)）ただし、四分位が一致する場合は、正の結果が保証される。bw.SJは、微分のパイロット推定を用いて帯域幅を選択するSheather & Jones (1991)の方法を実装している。ste "手法のアルゴリズムは、（unirootを介して）方程式を解き、そのため、境界がユーザー指定されておらず、ルートを括らない場合、区間c(lower, upper)を拡大する。最後の3つの手法は、すべてのペアワイズビニング距離を使用する。  ビニングのため、xが変換されたり符号反転されたりすると、結果はわずかに異なる。
bw.nrd0は、ガウシアンカーネル密度推定量の帯域幅を選択するための経験則を実装している。デフォルトは、標準偏差と四分位範囲間の最小値を1で割った値の0.9倍である。標本サイズの34倍の負の5乗（= Silvermanの「経験則」、Silverman (1986, page 48, eqn (3.31)）ただし、四分位が一致する場合は、正の結果が保証される。bw.SJは、微分のパイロット推定を用いて帯域幅を選択するSheather & Jones (1991)の方法を実装している。ste "手法のアルゴリズムは、（unirootを介して）方程式を解き、そのため、境界がユーザー指定されておらず、ルートを括らない場合、区間c(lower, upper)を拡大する。最後の3つの手法は、すべてのペアワイズビニング距離を使用する：これらは、n = nb/2までは複雑度O(n^2)、それ以降はO(n)である。  ビニングのため、xが変換されたり符号反転されたりすると、結果はわずかに異なる。
bw.nrd0は、ガウシアンカーネル密度推定量の帯域幅を選択するための経験則を実装している。デフォルトは、標準偏差と四分位範囲間の最小値を1で割った値の0.9倍である。標本サイズの34倍の負の5乗（= Silvermanの「経験則」、Silverman (1986, page 48, eqn (3.31)）ただし、四分位が一致する場合は、正の結果が保証される。bw.SJは、微分のパイロット推定を用いて帯域幅を選択するSheather & Jones (1991)の方法を実装している。ste "手法のアルゴリズムは、（unirootを介して）方程式を解き、そのため、境界がユーザー指定されておらず、ルートを括らない場合、区間c(lower, upper)を拡大する。最後の3つの手法は、すべてのペアワイズビニング距離を使用する：これらは、n = nb/2までは複雑度O(n^2)、それ以降はO(n)である。  ビニングのため、xが変換されたり符号反転されたりすると、結果はわずかに異なる。
bw.nrd0は、ガウシアンカーネル密度推定量の帯域幅を選択するための経験則を実装している。デフォルトは、標準偏差と四分位範囲間の最小値を1で割った値の0.9倍である。標本サイズの34倍の負の5乗（= Silvermanの「経験則」、Silverman (1986, page 48, eqn (3.31)）ただし、四分位が一致する場合は、正の結果が保証される。bw.SJは、微分のパイロット推定を用いて帯域幅を選択するSheather & Jones (1991)の方法を実装している。ste "手法のアルゴリズムは、（unirootを介して）方程式を解き、そのため、境界がユーザー指定されておらず、ルートを括らない場合、区間c(lower, upper)を拡大する。最後の3つの手法は、すべてのペアワイズビニング距離を使用する。  ビニングしているため、xが平行移動したり符号反転したりすると結果は若干異なる。
Sとの互換性のため、contr.treatmentなどの省略形として、contrはtreatment,helmert,sum,poly（引用符なし）とすることができる。
正準相関分析は，x変数の線形結合によってよく説明されるthey変数の線形結合を求める．よく説明される」は相関によって測定されるので、関係は対称的である。
NA
タイプ = "相関" および "共分散" の場合、推定は標本の共分散に基づく。(デフォルトでは、欠損値は許されない。  (na.passのように)na.action関数が欠損値を通過する場合、共分散は完全なケースから計算されます。  これは、計算された推定値が有効な自己相関列ではない可能性があり、欠損値を含む可能性があることを意味します。  偏相関係数は、lag.maxまでの高次の自己回帰モデルに適合させることによって推定されます。ジェネリック関数plotには、クラス "acf "のオブジェクト用のメソッドがあります。
xが1行または1列の行列の場合、またはxがベクトルでyが与えられていない場合、適合度検定が実行される（xは1次元分割表として扱われる）。  xの項目は非負整数でなければならない。  この場合，検定される仮説は，母集団確率がpの確率と等しいか，またはpが与えられない場合はすべて等しいかである．xが少なくとも2行と列を持つ行列の場合は，2次元分割表として扱われる： xの項目は非負整数でなければならない．  それ以外の場合， x と y は同じ長さのベクトルまたは因子でなければならない．欠損値のあるケースは除去され，オブジェクトは因子に強制され，分割表はこれらから計算される．  simulate.p.valueがFALSEの場合、p-値は検定統計量の漸近カイ2乗分布から計算されます; 連続性補正は2-by-2の場合にのみ使用されます（correctがTRUEの場合、デフォルト）。  分割表の場合のシミュレーションは、与えられたマージンを持つすべての分割表の集合から無作為にサンプリングすることによって行われ、マージンが厳密に正である場合にのみ機能する。  連続性補正は使用されず、統計量は補正なしで引用される。  適合度の場合のシミュレーションは、pで指定された離散分布からのランダム・サンプリングによって行われ、各標本のサイズはn = sum(x)です。  このシミュレーションはRで行われ、遅いかもしれない。
多次元スケーリングは、非類似度の集合を取り、点間の距離が非類似度とほぼ等しくなるような点の集合を返す。  (cmdscaleは、Mardia (1978)の分析に従い、最も適合するk次元の表現を返すが、kは引数kより小さくてもよい。  add=TRUEの場合、非類似度d[i,j] + c*がユークリッドになるような最小の加法定数c*が計算され、n - 1次元で表現される。  S (Becker et al, 1988) がTorgersonによって提案された近似を用いてこの定数を計算するのに対して，Rは Cailliez (1983) の解析解を用いる。
aov "メソッドは、complete = FALSEの場合、エイリアスされた係数(aliasを参照)を報告しない(complete引数もvcovメソッドとの互換性のために存在する)。  これにより、p <- length(coef(obj, complete = TF)),dim(vcov(obj, complete = TF)) == c(p,p)はcomplete設定とデフォルトの両方で満たされます。
complete引数はvcovメソッドとの互換性のために存在し、他のクラスのcoefメソッドやaovメソッドも通常complete = *の挙動を維持する必要があります。  これにより、p <- length(coef(obj, complete = TF)),dim(vcov(obj, complete = TF)) == c(p,p)は、complete設定とデフォルトの両方で満たされます。
NA
confintは汎用関数です。  デフォルトのメソッドは正規性を仮定しており、適切な coef および vcov メソッドが利用可能である必要があります。  クラス "lm "のオブジェクトに対しては、t値に基づく直接式が使用されます。クラス "glm "と "nls "用のスタブメソッドがパッケージstatsにあり、パッケージMASS(インストールされている場合)のメソッドを呼び出します: MASS名前空間がロードされている場合、そのメソッドが直接使用されます。  (これらのメソッドはプロファイル尤度に基づいている)。
confintは汎用関数です。  デフォルトのメソッドは正規性を仮定しており、適切な coef と vcov メソッドが利用可能である必要があります。  デフォルトのメソッドは、他のメソッドと比較するために直接呼び出すことができます。クラス "lm "のオブジェクトについては、t値に基づく直接の計算式が使用されます。クラス "glm "と "nls "用のスタブメソッドがstatsパッケージにあり、パッケージMASS(インストールされている場合)のメソッドを呼び出します。  (これらのメソッドはプロファイル尤度に基づいている)。
confintは汎用関数です。  デフォルトのメソッドは正規性を仮定しており、適切な coef と vcov メソッドが利用可能である必要があります。  デフォルトのメソッドは、他のメソッドと比較するために直接呼び出すことができます。クラス "lm "のオブジェクトについては、t値に基づく直接の計算式が使用されます。クラス "glm "と "nls "用のスタブメソッドがstatsパッケージにあり、パッケージMASS(インストールされている場合)のメソッドを呼び出します。  (これらのメソッドはプロファイル尤度に基づく)。
実行可能領域は、ui %*% theta - ci >= 0で定義される。開始値は実行可能領域の内部になければならないが、最小値は境界にあってもよい。バリア関数は、外側の反復ごとに目的関数が減少するように選択される。実行可能領域の内部での最小値は、通常、非常に早く見つかりますが、境界での最小値には、かなりの回数の外部反復が必要になる場合があります。チューニングパラメータmuは、バリア項を乗算します。その正確な値は比較的重要でないことが多い。muが増加するにつれて、拡張された目的関数は元の目的関数に近くなりますが、実行可能領域の境界付近では滑らかさも減少します。目的関数に無限大の値を許容するoptim法であれば、どのようなものでも使用することができます(現在のところ、"L-BFGS-B "以外はすべて)。目的関数fは、最小化が行われるパラメータのベクトルを最初の引数としてとります。  これはスカラー結果を返す。オプションの引数...はoptimに渡され、(optimが使用しない場合は)fに渡されます。勾配関数gradは、method = "Nelder-Mead "以外を与えなければならない。  grad関数はfと同じ引数をとり、勾配を含むベクトルを返す。
これらの関数は、分散分析および回帰モデルに使用するためのコントラスト行列を作成するために使用されます。  結果として得られる行列の列には、n個の水準を持つ因子を符号化するために使用できる対比が含まれます。  返される値には、計算された対比が含まれます。  contr.helmertはHelmert対照を返し、これは2番目の水準と1番目の水準、3番目の水準と最初の2つの水準の平均を対照にします。  contr.polyは、直交多項式に基づく対比を返します。contr.sumは、'sum to zerocontrasts'を使用します。contr.treatmentは、各レベルをベースラインレベル（baseで指定）と対比します：ベースラインレベルは省略されます。  contr.SASはcontr.treatmentのラッパーで、ベースレベルを因子の最後のレベルに設定します。  一貫性を保つために、sparseはすべてのcontrast関数の引数ですが、contr.polyのsparse = TRUEは通常無意味で、contr.helmertではほとんど役に立ちません。
これらの関数は、分散分析や回帰モデルで使用するコントラスト行列を作成するために使用されます。  結果の行列の列には、n個の水準を持つ因子のコーディングに使用できる対比が含まれます。  返される値には、計算された対比が含まれます。  contr.helmertはHelmert対照を返し、これは2番目の水準と1番目の水準、3番目の水準と最初の2つの水準の平均を対照にします。  contr.polyは、直交多項式に基づく対比を返します。contr.sumは、'sum to zerocontrasts'を使用します。contr.treatmentは、各レベルをベースラインレベル（baseで指定）と対比します：ベースラインレベルは省略されます。  contr.SASはcontr.treatmentのラッパーで、ベースレベルを因子の最後のレベルに設定します。  一貫性を保つために、sparseはすべてのcontrast関数の引数ですが、contr.polyのsparse = TRUEは通常無意味で、contr.helmertではほとんど役に立ちません。
これらの関数は、分散分析や回帰モデルで使用するコントラスト行列を作成するために使用されます。  結果の行列の列には、n個の水準を持つ因子のコーディングに使用できる対比が含まれます。  返される値には、計算された対比が含まれます。  contr.helmertはHelmert対比を返し、これは2番目の水準と1番目の水準、3番目の水準と最初の2つの水準の平均を対比します。  contr.polyは、直交多項式に基づく対比を返します。contr.sumは、'sum to zerocontrasts'を使用します。contr.treatmentは、各レベルをベースラインレベル（baseで指定）と対比します：ベースラインレベルは省略されます。  contr.SASはcontr.treatmentのラッパーで、ベースレベルを因子の最後のレベルに設定します。  一貫性を保つために、sparseはすべてのcontrast関数の引数ですが、contr.polyのsparse = TRUEは通常無意味で、contr.helmertではほとんど役に立ちません。
これらの関数は、分散分析や回帰モデルで使用するコントラスト行列を作成するために使用されます。  結果の行列の列には、n個の水準を持つ因子のコーディングに使用できる対比が含まれます。  返される値には、計算された対比が含まれます。  contr.helmertはHelmert対比を返し、これは2番目の水準と1番目の水準、3番目の水準と最初の2つの水準の平均を対比します。  contr.polyは、直交多項式に基づく対比を返します。contr.sumは、'sum to zerocontrasts'を使用します。contr.treatmentは、各レベルをベースラインレベル（baseで指定）と対比します：ベースラインレベルは省略されます。  contr.SASはcontr.treatmentのラッパーで、ベースレベルを因子の最後のレベルに設定します。  一貫性を保つために、sparseはすべてのcontrast関数の引数ですが、contr.polyのsparse = TRUEは通常無意味で、contr.helmertではほとんど役に立ちません。
これらの関数は、分散分析や回帰モデルで使用するコントラスト行列を作成するために使用されます。  結果の行列の列には、n個の水準を持つ因子のコーディングに使用できる対比が含まれます。  返される値には、計算された対比が含まれます。  contr.helmertはHelmert対比を返し、これは2番目の水準と1番目の水準、3番目の水準と最初の2つの水準の平均を対比します。  contr.polyは、直交多項式に基づく対比を返します。contr.sumは、'sum to zerocontrasts'を使用します。contr.treatmentは、各レベルをベースラインレベル（baseで指定）と対比します：ベースラインレベルは省略されます。  contr.SASはcontr.treatmentのラッパーで、ベースレベルを因子の最後のレベルに設定します。  一貫性を保つために、sparseはすべてのcontrast関数の引数ですが、contr.polyのsparse = TRUEは通常無意味で、contr.helmertではほとんど役に立ちません。
論理ベクトルxは、レベルc(FALSE, TRUE)を持つ2レベル因子に変換されます(どのレベルが変数に現れるかは関係ありません).xにmatrixcontrasts属性が設定されている場合、引数contrastsは無視されます。  さもなければ、contrasts = TRUEの場合はcontr.treatmentのようなcontrasts関数に渡され、contrasts = FALSEの場合は等値行列が返されます。  適切な関数は、レベルの文字ベクトルである最初の引数、名前付き引数contrasts（常にcontrasts = TRUEで呼び出される）、およびオプションで論理引数sparseを持つ。valueが何個以上のコントラストを指定した場合、最初の何個が使用される。  valueが与えるコントラストが少なすぎる場合、列が（定数項と直交する）コントラストであり、かつコリニアでないことを確認した後、valueを拡張して適切なコントラスト行列が作成される。
論理ベクトルxは、レベルc(FALSE, TRUE)を持つ2レベル因子に変換されます(どのレベルが変数に現れるかは関係ありません)。xにmatrixcontrasts属性が設定されている場合、引数contrastsは無視されます。  さもなければ、contrasts = TRUEの場合はcontr.treatmentのようなcontrasts関数に渡され、contrasts = FALSEの場合は等値行列が返されます。  適切な関数は、レベルの文字ベクトルである最初の引数、名前付き引数contrasts（常にcontrasts = TRUEで呼び出される）、およびオプションで論理引数sparseを持つ。valueが何個以上のコントラストを指定した場合、最初の何個が使用される。  valueが与えるコントラストが少なすぎる場合、列がコントラスト（定数項に対して直交）であり、かつ非直交であることを確認した後、valueを拡張して適切なコントラスト行列が作成されます。
入力シーケンスxとyは、circularが真の場合、同じ長さでなければならない。2つのシーケンスxとyの畳み込みの通常の定義は、convolve(x, rev(y), type = "o")で与えられることに注意。
主な高レベル関数は、influence.measuresで、各モデル変数のDFBETAS、DFFITS、共分散比、クックの距離、ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を生成します。  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロス・バリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供します。 関数hat()は、主にS（バージョン2）との互換性のために存在します。
クラスタ化された2つのオブザベーション間のコフェネティック距離は、2つのオブザベーションが最初に1つのクラスタに結合されるグループ間非類似度であると定義される。  copheneticは汎用関数である。  クラス "dendrogram "のオブジェクトに対するメソッドは、dendrogramオブジェクトのすべての葉が非NULLラベルを持つことを必要とします。
入力は数値でなければなりません（is.numericによって決定されます：歴史的な互換性のために論理値も許可されます）："kendall "および "spearman "メソッドは、順序付けられた入力に対して意味を持ちますが、xtfrmは数値への適切な事前変換を見つけるために使用することができます。varはcovへの単なるインタフェースであり、rena.rmはそれが指定されていない場合に使用するデフォルトを決定するために使用されます。  na.rmがTRUEであれば、完全な観測（行）が分散の計算に使用されます（use = "na.or.complete"）。  useが "everything "の場合、NAは概念的に伝播する。つまり、寄与するオブザベーションの1つがNAの場合、結果の値はNAになる。useが "all.obs "の場合、欠損オブザベーションがあるとエラーになる。  useが "complete.obs "であれば、欠損値はケースワイズド削除によって処理される（そして、完全なケースがない場合はエラーになる）。"na.or.complete "は、完全なケースがない限り同じで、NAになる。この結果、共分散行列または相関行列が正半定値にならないことがあり、またその変数の組に完全な組がない場合は、NAのエントリが得られます。   covとvarの場合、"pairwise.complete.obs "は "pearson "法でのみ動作します。(等価な) var(double(0), use = *) は、use = "everything "と "na.or.complete "の場合はNAを与え、その他の場合はエラーを与えることに注意してください。cor()では、methodが "kendall "または "spearman "の場合、KendallのtauまたはSpearmanのrho統計量を使用して、順位ベースの関連尺度を推定します。  これらはよりロバストであり、データが必ずしも二変量正規分布に由来しない場合に推奨されている。  spearman" は、基本的に cor(R(x), R(y)) (または cov(., .)) を計算することに注意してください。欠損値の場合，完全なオブザベーションに基づくか，各ペアの順位付けを伴うペアごとの完全性に基づくか，使用値によって順位が計算される．同値の場合， Kendall (1945)によって提案された Kendall の tau_b が計算される．共分散行列を相関行列にスケーリングする方法は多数あり，数学的には，左右の対角行列との乗算，またはweep(., FUN = "/") を2回使用することでより効率的に行うことができる．  cov2cor関数はさらに少し効率的で、主に教育的な理由から提供されている。
3つの手法は、それぞれ対になった標本間の関連を推定し、その値がゼロであることの検定を計算します。  これらはそれぞれ異なる関連性の尺度を用いており、すべて[-1, 1]の範囲にあり、0は関連性がないことを示す。  手法が "pearson "の場合、検定統計量はPearsonの積モーメント相関係数cor(x, y)に基づき、標本が独立な正規分布に従う場合は自由度長(x)-2のt分布に従う。  手法が "kendall "または "spearman "の場合は、Kendall'stau統計量またはSpearman's rho統計量が、関連性の順位ベースの尺度を推定するために使用される。  Kendallの検定では、デフォルトでは（正確度がNULLの場合）、有限値を含む対の標本が50個未満で同値がない場合、正確度のp値が計算されます。  Spearmanの検定では、n < 1290かつ exact = TRUEの場合はアルゴリズムAS 89を用いてp-値が計算され、それ以外の場合は漸近近似を用いて計算されます。  これらは、n < 10では「正確」であり、より大きなサンプルサイズではEdgeworth級数近似を使用することに注意してください（カットオフは元の論文から変更されています）。
入力は数値でなければなりません（is.numericによって決定されます：歴史的な互換性のために論理値も許可されます）："kendall "と "spearman "メソッドは、順序付けられた入力に対して意味を持ちますが、xtfrmは数値への適切な事前変換を見つけるために使用することができます。varはcovへの単なるインターフェイスであり、rena.rmはそれが指定されていない場合に使用するデフォルトを決定するために使用されます。  na.rmがTRUEであれば、完全な観測（行）が分散の計算に使用されます（use = "na.or.complete"）。  useが "everything "の場合、NAは概念的に伝播する。つまり、寄与するオブザベーションの1つがNAの場合、結果の値はNAになる。useが "all.obs "の場合、欠損オブザベーションがあるとエラーになる。  useが "complete.obs "であれば、欠損値はケースワイズド削除によって処理される（そして、完全なケースがない場合はエラーになる）。"na.or.complete "は、完全なケースがない限り同じで、NAになる。この結果、共分散行列や相関行列が正半定値にならないことがあり、その変数の組に完全な組がない場合はNAのエントリが得られます。   covとvarの場合、"pairwise.complete.obs "は "pearson "法でのみ動作します。(等価な) var(double(0), use = *) は、use = "everything "と "na.or.complete "の場合はNAを与え、それ以外の場合はエラーを与えることに注意してください。cor()では、methodが "kendall "または "spearman "の場合、KendallのtauまたはSpearmanのrho統計量を使用して、順位ベースの関連尺度を推定します。  これらはよりロバストであり、データが必ずしも二変量正規分布に由来しない場合に推奨されている。  spearman" は、基本的に cor(R(x), R(y)) (または cov(., .)) を計算することに注意してください。欠損値の場合，完全なオブザベーションに基づくか，各ペアの順位付けを伴うペアごとの完全性に基づくか，使用値によって順位が計算される．同値の場合， Kendall (1945)によって提案された Kendall の tau_b が計算される．共分散行列を相関行列にスケーリングする方法は多数あり，数学的には，左右の対角行列との乗算，またはweep(., FUN = "/") を2回使用することでより効率的に行うことができる．  cov2cor関数はさらに少し効率的で、主に教育的な理由から提供されている。
デフォルトでは、method = "unbiased", 共分散行列は重み付けの2乗和を差し引いた値で除算されるので、重みがデフォルト（1/n）の場合、除数（n - 1）を持つ共分散行列の従来の不偏推定値が得られる。  これは、method = "ML "に対応し、除算を行わないS-PLUSの動作とは異なります。
入力は数値でなければなりません（is.numericによって決定されます：歴史的な互換性のために論理値も許可されます）："kendall "と "spearman "メソッドは、順序付けられた入力に対して意味を持ちますが、xtfrmは数値への適切な事前変換を見つけるために使用することができます。varはcovへの単なるインターフェイスであり、rena.rmはそれが指定されていない場合に使用するデフォルトを決定するために使用されます。  na.rmがTRUEであれば、完全な観測（行）が分散の計算に使用されます（use = "na.or.complete"）。  useが "everything "の場合、NAは概念的に伝播する。つまり、寄与するオブザベーションの1つがNAの場合、結果の値はNAになる。useが "all.obs "の場合、欠損オブザベーションがあるとエラーになる。  useが "complete.obs "であれば、欠損値はケースワイズド削除で処理される（そして、完全なケースがない場合はエラーになる）。"na.or.complete "は、完全なケースがない限り同じで、NAになる。この結果、共分散行列または相関行列が正半定値にならないことがあり、またその変数の組に完全な組がない場合は、NAのエントリが得られます。   covとvarの場合、"pairwise.complete.obs "は "pearson "法でのみ動作します。(等価な) var(double(0), use = *) は、use = "everything "と "na.or.complete "の場合はNAを与え、その他の場合はエラーを与えることに注意してください。cor()では、methodが "kendall "または "spearman "の場合、KendallのtauまたはSpearmanのrho統計量を使用して、順位ベースの関連尺度を推定します。  これらはよりロバストであり、データが必ずしも二変量正規分布に由来しない場合に推奨されている。  spearman" は、基本的に cor(R(x), R(y)) (または cov(., .)) を計算することに注意してください。欠損値の場合，完全なオブザベーションに基づくか，各ペアの順位付けを伴うペアごとの完全性に基づくか，使用値によって順位が計算される．同値の場合， Kendall (1945)によって提案された Kendall の tau_b が計算される．共分散行列を相関行列にスケーリングする方法は多数あり，数学的には，左右の対角行列との乗算，またはweep(., FUN = "/") を2回使用することでより効率的に行うことができる．  cov2cor関数はさらに少し効率的で、主に教育的な理由から提供されている。
主な高レベル関数はinfluence.measuresで、各モデル変数のDFBETAS、DFFITS、共分散比、クックの距離、ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を生成します。  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロス・バリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供します。関数hat()は、主にS（バージョン2）との互換性のために存在します。
NA
与えられた高さで木を切断することは、ウルトラメトリック木（単調なクラスタリング高さを持つ）に対してのみ可能である。
これらはすべて汎用関数で、xのtsp属性が存在すればそれを使用します。timeとcycleは、結果をそのクラスに強制するクラスtsのメソッドを持っています。
derivは、デフォルトと式メソッドを持つジェネリック関数です。  exprとその（部分）導関数を同時に計算するための呼び出しを返します。  いわゆるアルゴリズム微分を使用します。  現在のところ、deriv.formulaは~.deriv3の右側の式を抽出した後にderiv.defaultを呼び出すだけであり、deriv3とそのメソッドは、hessianのデフォルトがderiv3のTRUEであることを除けば、derivとそのメソッドと同等です。内部コードは、算術演算子 +, -, *, /, ^ と単一変数関数 exp, log, sin, cos, tan, sinh, cosh, sqrt, pnorm, dnorm, asin, acos, atan, gamma, lgamma, digamma, trigamma と、1つまたは2つの引数に対する psigamma (ただし微分は最初の引数に対してのみ) について知っています。(R 3.4.0以降では、単一変数関数log1p,expm1,log2,log10,cospi,sinpi,tanpi,factial,lfactorialもサポートされています。
パラメータ shape1 = a とshape2 = b のベータ分布は密度Γ(a+b)/(Γ(a)Γ(b))x^(a-1)(1-x)^(b-1)を a > 0, b > 0, 0 ≤ x ≤ 1 で持ち、x=0 または x=1 での境界値は連続(極限)で定義されます。平均は a/(a+b) で、分散は ab/((a+b)^2 (a+b+1)) である。これらのモーメントとすべての分布特性は、abがゼロまたは無限大のとき、極限として定義することができ（0、1/2、または1の点質量につながる）、対応する[dpqr]beta()関数がそれに応じて定義される。  6.6.1B_x(a,b) = integral_0^x t^(a-1) (1-t)^(b-1) dt, 6.6.2 I_x(a,b) = B_x(a,b) / B(a,b)ここでB(a,b) = B_1(a,b)はベータ関数(beta)である。I_x(a,b)はpbeta(x, a, b)である。非心ベータ分布(ncp = λ)はX/(X+Y)の分布として定義され(Johnson et al, 1995, pp. 502)、ここでX ~ chi^2_2a(λ)，Y ~ chi^2_2bである。
size = n, prob = pの二項分布は、x = 0, ..., nに対して密度p(x) = choose(n, x) p^x (1-p)^(n-x) を持つ。p(x)はLoaderのアルゴリズムを用いて計算される。分位数は、F(x)≧pとなる最小の値xとして定義される。
位置lとスケールsを持つコーシー分布は，すべてのxに対して密度f(x) = 1 / (π s (1 + ((x-l)/s)^2)) を持つ．
自由度 df= n ≥ 0 のカイ2乗分布は x > 0 のとき密度 f_n(x) = 1 / (2^(n/2) Γ(n/2)) x^(n/2-1) e^(-x/2)ここで f_0(x) := \lim_{n Γ to 0} f_n(x) = δ_0(x)、ゼロの点質量は密度関数ではなく「δ分布」です。平均と分散はnと2nである。自由度df= n度、非心パラメータncp= λの非心カイ2乗分布は、x≧0の時、密度f(x) = exp(-λ/2) SUM_{r=0}^∞ ((λ/2)^r / r!) dchisq(x, df + 2r)を持つ。  整数nの場合、これは分散1、λは正規平均の平方和を持つn個の正規分布の平方和の分布であり、E(X) = n + λ、Var(X) = 2(n + 2*λ) 、E((X - E(X))^3) = 8(n + 3*λ) となる。自由度 df= n, は整数であり、非中心性λ > 0に関係するn = 0でもあることに注意。この（非心、ゼロdf）の場合、分布はx = 0での点質量（サイズpchisq(0, df=0, ncp=ncp)）と連続部分の混合であり、dchisq()はその混合測度に関する密度ではなく、むしろdf -> 0での密度の極限である。
使用される加法モデルは:Y[t] = T[t] + S[t] + e[t]使用される乗法モデルは:Y[t] = T[t] * S[t] * e[t]この関数は、まず移動平均(filterがNULLの場合、重みが等しい対称窓が使用される)を使用してトレンド成分を決定し、時系列からそれを除去する。  次に、各時間単位で全期間を平均することにより、季節的な数値が計算される。  そして、季節数値はセンタリングされる。   最後に、元の時系列からトレンドと季節的な数値（必要に応じてリサイクルされる）を取り除くことにより、誤差成分が決定される。
NA
これらはすべて汎用関数であり、xのtsp属性が存在すればそれを使用する。timeとcycleには、結果をそのクラスに強制するクラスts用のメソッドがある。
NA
density.defaultで使用されるアルゴリズムは、経験分布関数の質量を少なくとも512点の規則的なグリッド上に分散させ、高速フーリエ変換を使用してこの近似値をカーネルの離散化バージョンと畳み込み、指定された点での密度を評価するために線形近似を使用します。カーネルの統計的特性は、sig^2 (K) = int(t^2 K(t) dt)によって決定され、これは我々のカーネルでは常に = 1である（したがって、帯域幅bwはカーネルの標準偏差である）、およびR(K) = int(K^2(t) dt)によって決定される。  この値は、give.Rkern = TRUE のときに返されます。  xの無限値は+/-Infにおける点質量に対応すると仮定され, 密度推定は(-Inf, +Inf)上の部分密度になります.
density.defaultで使用されるアルゴリズムは、経験分布関数の質量を少なくとも512点の規則的なグリッド上に分散させ、高速フーリエ変換を使用して、この近似をカーネルの離散化バージョンと畳み込み、指定された点での密度を評価するために線形近似を使用します。カーネルの統計的特性は、sig^2 (K) = int(t^2 K(t) dt)によって決定され、これは我々のカーネルでは常に = 1である（したがって、帯域幅bwはカーネルの標準偏差である）、およびR(K) = int(K^2(t) dt)によって決定される。  この値は、give.Rkern = TRUE のときに返されます。  xの無限値は+/-Infにおける点質量に対応すると仮定され, 密度推定は(-Inf, +Inf)上の部分密度になります.
Dは、単純な記号導関数を取るためのSに倣ってモデル化されています。  exprとその(偏)導関数を同時に計算するための呼び出しを返します。  いわゆるアルゴリズム微分を使用します。  現在のところ、deriv.formulaは~.deriv3の右側の式を抽出した後にderiv.defaultを呼び出すだけであり、deriv3とそのメソッドは、hessianのデフォルトがderiv3のTRUEであることを除けば、derivとそのメソッドと同等です。内部コードは、算術演算子 +, -, *, /, ^ と単一変数関数 exp, log, sin, cos, tan, sinh, cosh, sqrt, pnorm, dnorm, asin, acos, atan, gamma, lgamma, digamma, trigamma と、1つまたは2つの引数に対する psigamma (ただし微分は最初の引数に対してのみ) について知っています。(R 3.4.0以降、1変数関数log1p,expm1,log2,log10,cospi,sinpi,tanpi,factial,lfactorialもサポートされています。
derivは、デフォルトと数式メソッドを持つ汎用関数です。  exprとその（部分）導関数を同時に計算するための呼び出しを返します。  いわゆるアルゴリズム微分を使用します。  現在のところ、deriv.formulaは~.deriv3の右側の式を抽出した後にderiv.defaultを呼び出すだけであり、deriv3とそのメソッドは、hessianのデフォルトがderiv3のTRUEであることを除けば、derivとそのメソッドと同等です。内部コードは、算術演算子 +, -, *, /, ^ と単一変数関数 exp, log, sin, cos, tan, sinh, cosh, sqrt, pnorm, dnorm, asin, acos, atan, gamma, lgamma, digamma, trigamma と、1つまたは2つの引数に対する psigamma (ただし微分は最初の引数に対してのみ) について知っています。(R 3.4.0以降では、1変数関数log1p,expm1,log2,log10,cospi,sinpi,tanpi,factial,lfactorialもサポートされています。
これは、フィットしたモデルの偏差を抽出するために使用できる汎用関数です。  この関数の使用方法の詳細については、各モデリング関数を参照してください。
率λを持つ指数分布は、x≥0に対して密度f(x) = λ {e}^{-λ x}を持ちます。
自由度 df1 = n1 と df2 =n2 の F 分布は、密度 f(x) = Γ((n1 + n2)/2) を持ちます。/ n1 と n2 の独立した標準正規分布の平均2乗の比の分布であり、2つの独立したカイ2乗変量の比を自由度で割った分布です。  正規分布とm個の独立した正規分布の平方根の比はスチューデントのt_m分布を持つので、t_m変量の2乗は自由度1とmでF分布を持つ。非心F分布は、やはり単位分散の独立した正規分布の2乗平均の比であるが、分子のものは0でない平均を持つことが許され、ncpは平均の2乗和である。  非心分布の詳細については二乗を参照のこと。
カーネルは、一般的なカーネルまたは指定されたカーネルを構築するために使用される。  修正Daniellカーネルは(S-PLUSで使用されるように)端係数を半分にします。[ メソッドは(-m) : mのインデックスを持つカーネルオブジェクトの自然なインデックス付けを可能にします。kernelは，Brockwell and Davis (1991), page362で定義されている平滑化カーネルの「等価自由度」を返し，bandwidth.kernelは，Bloomfield (1976), p. 201で定義されている等価帯域幅に連続性補正を加えたものを返す．
これは、フィットしたモデルの残差自由度を抽出するために使用できる汎用関数です。  デフォルトの方法では、df.residual成分を抽出するだけです。
例えばこのようなモデルは、+演算子で区切られた一連の項から構成されます。項自体は、 : 演算子で区切られた変数名と因子名から構成されます。  演算子 * は因子の交差を表す： a*b は a+b+a:b と解釈される。  演算子は指定された次数の交差を示す。  例えば(a+b+c)^2は(a+b+c)*(a+b+c)と同じで、a,b,cの主効果とそれらの2次相互作用を含む式に展開される。  たとえば、a + b %in% a は、式 a + a:b に展開される。  演算子 - は指定された項を削除するので、(a+b+c)^2 - a:b は a + b + c + b:c + a:c と同じになる。  これは切片項を削除するのにも使える：線形モデルy ~ x - 1をフィットするとき、原点を通る直線を指定する。  切片のないモデルは、y ~ x + 0 または y ~ 0 + x と指定することもできる。式は通常、変数名とファクト名だけを含むが、算術式を含むこともある。log(y) ~ a + log(x)という式はかなり合法的である。このような算術式に、モデル式で記号的にも使用される演算子が含まれる場合、算術演算子と記号演算子の使用が混同されることがあります。この混同を避けるために、関数I()を使用して、演算子が算術的な意味で使用されるモデル式の部分を括ることができます。  例えば、ay ~ a + I(b+c)という式では、b+cという項はbとcの和と解釈されます。変数名は、このような非構文的な名前を使用するすべてのコードが受け入れるという保証はありませんが、このようなインフォミュレータのようにバックスティックで引用することができます。ほとんどのモデルフィッティング関数は、右辺が1の固定係数を持つ項を示す関数offsetを含む式を受け入れます。  関数によっては、strataやclusterのような他の'特別な'名前を受け付けるものもあります（terms.formulaのspecials引数を参照）。  通常のものは、モデフィッティング関数のデータ引数の文脈におけるもので、「式に含まれないすべての列」を意味します。  update.formulaのコンテキストでは、'以前に式のこの部分に含まれていたもの'という意味になります。formulaが適合モデルオブジェクト上で呼び出される場合、特定のメソッド（クラス "nls "のメソッドなど）が使用されるか、デフォルトのメソッドが使用されます。  デフォルトでは、まずオブジェクトの "formula "コンポーネントを探し（そしてそれを評価し）、次に "terms "コンポーネントを探し、次に呼び出しのformulaパラメータを探し（そしてその値を評価し）、最後に "formula "属性を探します。  例えば、amodel.frame() のように、"terms" 属性に数式が含まれている場合、その数式が返されます。  もし、以前の (R <= 3.5.x) 挙動を望むなら、"terms" 属性を考慮しない補助DF2formula() を使用する。  列数が多い場合は、最初の列が式のLHSとなり、+で区切られた残りの列がRHSを形成する。
主な高レベル関数は、influence.measuresで、各モデル変数のDFBETAS、DFFITS、共分散比、クックの距離、ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を生成します。  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロスバリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供します。 関数hat()は、主にS（バージョン2）との互換性のために存在します。
主要な高レベル関数は，influence.measures で，これは，各モデル変数のDFBETAS，DFFITS，共分散比，クックの距離，ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を生成します．  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロスバリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供します。 関数hat()は、主にS（バージョン2）との互換性のために存在します。
主要な高レベル関数は，influence.measures で，これは，各モデル変数のDFBETAS，DFFITS，共分散比，クックの距離，ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を生成します．  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロスバリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供する。
パラメータshape = a、scale = sのガンマ分布は、密度f(x)=1/(s^a Gamma(a)) x^(a-1) e^-(x/s)で、x≥0、a > 0、s > 0となる(ここでGamma(a)はRのgamma()で実装され、ヘルプで定義されている関数である。  平均と分散は、E(X) = a*s とVar(X) = a*s^2です。累積ハザードH(t) = - log(1 - F(t))は、shape(およびmoderatecale)の小さな値の場合、ガンマ分布の質量の大部分は、コンピュータの演算でゼロとして表現されるほどゼロに近いxの値であることに注意してください。  だからrgammaはゼロとして表現される値を返すかもしれない。  (実際の生成はscale = 1で行われるため、これは非常に大きなscaleの値でも起こります)。
確率=pの幾何分布は、密度p(x) = p (1-p)^x for x = 0, 1, 2, ..., 0 < p ≤ 1.を持つ。xの要素が整数でない場合、dgeomの結果は警告とともにゼロとなる。分位数は、F(x) ≥ pとなる最小の値xとして定義される。
超幾何分布は，置換なしのサンプリングに使用される．  パラメータm, n, k (以下の文献ではそれぞれNp, N-Np, nと命名。N := m+n は他の文献でも使われている)を持つこの分布の密度は，x = 0, ..., kに対して byp(x) = choose(m, x) choose(n, k-x) / choose(m+n, k)で与えられる．p(x)が0でないのは，max(0, k-n) <= x <= min(k, m)のときだけであることに注意．p := m/(m+n) (従ってNp = N ⅹtimes p in thereference's notation)とすると、最初の2つのモーメントは平均E[X] = μ = k p と分散Var(X) = k p (1 - p) * (m+n-k)/(m+n-1) であり、これはBinomial(k,p)（k = 1でない限り超幾何の方が分散が小さい）に近いことを示している。rhyper()では、m,n,kのいずれかが.Machine$integer.maxを超える場合、現在はqhyper(runif(nn), m,n,k) と同等のものが使用されます。
diffinvは、クラス "ts "用のメソッドと、ベクトルと行列用のデフォルトのメソッドを持つ汎用関数です。
利用可能な距離尺度は (2つのベクトル x と y に対して記述される):2つのベクトル間の通常の距離 (2ノルム、別名 L_2), sqrt(sum((x_i - y_i)^2)).x と y の2つの成分間の最大距離 (上値ノルム).2つのベクトル間の絶対距離 (1ノルム、別名 L_1).sum(|x_i-y_i|/(|x_i| + |y_i|)).分子と分母がゼロの項はsumから省略され、値がないものとして扱われる、元々、Rはx_i + y_iを使用していたが、1998年から2017年までは｜x_i + y_i｜を使用し、その後、正しい｜x_i｜ +｜y_i｜（別名：非対称バイナリ）を使用している：）ベクトルは2進数のビットとみなされ、0以外の要素は「オン」、0の要素は「オフ」である。  距離は，少なくとも1つがオンであるビットのうち，1つだけがオンであるビットの割合である．ユークリッド距離、マンハッタン距離、キャンベラ距離、ミンコフスキー距離の計算において、いくつかの列が除外される場合、合計は使用される列の数に比例してスケールアップされる。  as.matrix()およびas.dist()の "dist "メソッドは、"dist "クラスのオブジェクトと従来の距離行列との間の変換に使用できます。  as.dist() は汎用関数であり、デフォルトのメソッドは、クラス "dist" を継承するオブジェクト、または as.matrix() を使用する行列と互換性のあるオブジェクトを扱います。  as.matrix()、またはより直接的には、そのようなクラスに対するas.distメソッドを提供することで、距離（非類似度としても知られている）を表すクラスのサポートを追加することができます。
対数正規分布は密度f(x) = 1/(√(2 π) σ x) e^-((log x - μ)^2 / (2 σ^2))を持ちます。平均はE(X) = exp(μ + 1/2 σ^2)、中央値はmed(X) = exp(μ)、分散はVar(X) = exp(2*μ + σ^2)*(exp(σ^2)-1)、したがって変動係数はsqrt(exp(σ^2)-1)となり、これが小さいとき(例えば、σ < 1/2).
location = m andscale = s のロジスティック分布は、分布関数F(x) = 1 / (1 + exp(-(x-m)/s))、密度f(x) = 1/s exp((x-m)/s) (1 + exp((x-m)/s))^-2 を持ち、平均m、分散π^2 /3 s^2のロングテール分布である。
xがK成分のベクトルであれば、dmultinom(x, prob)は確率P(X[1]=x[1], ... , X[K]=x[k]) = C * prod(j=1 , ..., K) p[j]^x[j]であり、Cは「多項係数」C = Nである！/ (x[1]!*・・・* x[K]!)そして N = sum(j=1, ..., K) x[j].定義により, 各成分X[j]は, j = 1, ..., Kに対してBin(size, prob[j])として2項分布する.rmultinom() アルゴリズムは、Bin(n[j], P[j])から2項X[j]を順次に引く：= P[1] = p[1] (pは確率を和1にスケーリング), そして, j ≥ 2, recursively, n[j] = N - sum(k=1, ..., j-1) X[k]andP[j] = p[j] / (1 - sum(p[1:(j-1)]).
サイズ＝n、prob＝pの負の2項分布は密度Γ(x+n)/(Γ(n) x!) p^n (1-p)^xx for x = 0, 1, 2, ..., n > 0, 0 < p ≤ 1.これは成功の目標数に達する前に一連のベルヌーイ試行で発生する失敗の数を表します。負の2項分布は、スケールパラメータ(1 - prob)/プロバンド形状パラメータsizeを持つガンマ分布(pgammaを参照)として分布する平均を持つポアソン分布の混合として生じることもある。  (この定義では，サイズの非整数値を許す)別のパラメタリゼーション(生態学でよく使われる)は，テーマmu (上記参照)と分散パラメタsizeで，prob = size/(size+mu)である。  このパラメトリゼーションでは、分散はmu + mu^2/sizeとなります。xの要素が整数でない場合、dnbinomの結果は0となり、警告が表示されます。size == 0の場合は、0に集中した分布となります。これは、probではなくmuが一定であっても、sizeが0に近づいた場合の限界分布です。  ただし、極限分布の平均は、muの値が何であれ0であることに注意してください。分位は、F(x) ≥ pとなる最小の値xとして定義されます。
正規分布は密度f(x) = 1/(√(2 π) σ) e^-((x - μ)^2/(2 σ^2))を持ち、ここでμは分布の平均、σは標準偏差である。
ポアソン分布は密度p(x) = λ^x exp(-λ)/x!for x = 0, 1, 2, ... .平均と分散はE(X) = Var(X) = λ.λ=0は極限ケース(0^0 = 1とする)であることに注意。p(x)はローダーのアルゴリズムを使って計算されます。indbinomのリファレンスを参照してください。分位は右連続です：qpois(p, lambda)は、P(X ≤ x) ≥ p.lower.tail=FALSEを設定することで、デフォルトのlower.tail=TRUEが1を返す場合に、より正確な結果を得ることができます。
factor.scopeは、ユーザーが直接呼び出すことを意図していない。
NA
drop1 メソッドでは、スコープがない場合、モデル内のすべての項がスコープとみなされます。lmとglmのメソッドは、モデル行列を再計算せず、fitメソッドを直接呼び出すという点で、より効率的です。デフォルトの出力表は、対数尤度のマイナス2倍＋2p（pはモデルのランク（有効パラメータの数））と定義されたAICを示します。  これは(対数尤度のように)加法定数までしか定義されません。  固定スケールを持つ線形ガウス・モデルの場合、定数はMallowsのCp,RSS/scale + 2p - nを与えるように選択されます。Cpが使用される場合、列はAICではなくCpとラベル付けされます。"glm "手法のF検定は、デビアンス分析検定に基づいているので、分散が推定される場合は、anova.glmのF検定とは異なり、残留デビアンスに基づいています。
この分布は次のようにして得られる。  xを原点に対して対称な連続分布からのサイズnの標本とする。  するとウィルコクソンの符号付き順位統計量は，x[i]が正である絶対値x[i]の順位の和となる．  この統計量は0からn(n+1)/2の間の値をとり、その平均と分散はそれぞれn(n+1)/4とn(n+1)(2n+1)/24である。最初の2つの引数のどちらかがベクトルである場合、長い方のベクトルの長さまでの2つのすべての組み合わせについて計算を行うためにリサイクル則が使用される。
df=n自由度を持つt分布は密度f(x) = Γ((n+1)/2)/ 平均 0 (n > 1) と分散 n/(n-2) (n > 2) を持ちます。パラメータ(df, Del) = (df, ncp)を持つ一般的な非心2次分布は，T(df, Del) := (U + Del) / √(V/df) の分布として定義され，ここでUとVは独立な確率変数，U ~ N(0,1)，V ~ χ^2(df)である(Chisquareを参照)．T=(mX-m0)/(S/sqrt(n))ここでmXは平均、Sはi.i.d. N(μ,σ^2)であるX_1,X_2,・・・,X_nの標本標準偏差(sd)である)そしてTは、自由度f= n - 1degree、非心パラメータerncp = (μ - m0) * sqrt(n)/σとして分布する。
適合線形モデルは，因子項の対比の係数を持ち，通常，水準数より1つ少ない．  この関数は、元のコーディングで係数を再表現します。係数は、縮小された基底でフィットされているので、暗黙の制約（例えば、contr.helmertやcontr.sumのゼロ和）は尊重されます。  contr.treatmentの対比にdummy.coefを使用することは、欠落した係数が定義上ゼロであるため、ほとんど意味がない。  しかし、主目的であるaovモデルには十分である。
フィットされた線形モデルは、因子項の対比の係数を持ち、通常、水準数より1つ少ない数である。  この関数は、元のコーディングで係数を再表現します。係数は、縮小ベースでフィットされているので、暗黙の制約（例えば、contr.helmertやcontr.sumのゼロ和）は尊重されます。  contr.treatmentの対比にdummy.coefを使用することは、欠落した係数が定義上ゼロであるため、ほとんど意味がない。  しかし、主目的であるaovモデルには十分である。
一様分布は、min ≤ x ≤ maxの場合、密度f(x) = 1/(max-min) を持つ。u := min == maxの場合、X == uの極限の場合が想定されるが、この場合密度はなく、dunifはNaN（エラー条件）を返す。runifは、max = minまたはmax-minがminに比べて小さくない限り、極値のどちらも生成せず、特にデフォルトの引数の場合は生成しない。
形状パラメータa、スケールパラメータbを持つワイブル分布は、x > 0に対してf(x) = (a/b) (x/b)^(a-1) exp(-(x/b)^a)で与えられる密度を持つ。累積分布関数は x > 0 で F(x) = 1 - exp(- (x/b)^a)，themean は E(X) = b Γ(1 + 1/a)，Var(X) = b^2 * (Γ(1 + 2/a) - (Γ(1 + 1/a))^2)である．
この分布は次のように得られる。  x と y を大きさ m と n の2つの無作為独立標本とすると、ウィルコクソン順位和統計量は、y[j]がx[i]より大きくないすべての組(x[i], y[j])の数です。  この統計量は0からm * nの間の値をとり、その平均と分散はそれぞれm * n / 2とm * n * (m + n + 1) / 12である。最初の3つの引数のいずれかがベクトルである場合、最長ベクトルの長さまでの3つのすべての組み合わせの計算を行うためにリサイクルルールが使用される。
e.c.d.f.(経験的累積分布関数)Fn は、観測値でのジャンプi/nを持つステップ関数で、iはその値での同点のオブザベーションの数である。  欠測値は無視されます。オブザベーションx= (x1,x2, ... xn)に対して、Fnはt以下のオブザベーションの割合、つまり、Fn(t) = #{xi <= t}/n = 1/n sum(i=1,n) Indicator(xi <= t)です。ecdfオブジェクトのplotメソッドを実装する関数plot.ecdfは、plot.stepfunの呼び出しによって実装されています。
多層を持つ分散分析モデルの固定効果項は、2つ以上の層で推定可能である場合があり、その場合、それぞれの層での情報が完全でないことがある。  ある項の効率は、その層だけで推定することによって得られる最大可能精度（逆分散）の割合である。  この関数は、model.tables.aovlistとse.contrast.aovlistで項を推定する層を選ぶのに使用されます。多くの場合、項は1つの層でしか出現せず、その場合すべての効率が1になります。
lmまたはaovによってフィットされた線形モデルの場合、効果は、フィッティングプロセス中にQR分解によって生成された連続する直交部分空間にデータを投影することによって得られる無相関の1自由度値である。最初のr（モデルのランク）は係数に関連し、残りは残差の空間にまたがる（ただし、特定の残差には関連しない）。
結果の行列の各行は，シーケンスx[t]，x[t-1]，...，x[t-dimension+1]からなり，ここでtはxの元のインデックスである．xが行列の場合，すなわちxが複数の変数を含む場合，x[t]は各変数のt番目のオブザベーションからなる．
これらはジェネリック関数で、xのtsp属性が存在すればそれを使用します。デフォルトのメソッドは、元の時間単位から開始時間をデコードするので、月系列1995.5はc(1995, 7)と表現されます。頻度fの系列では、timen+i/fはc(n, i+1)と表現される（i = 0, f = 1の場合でも）。
NA
na.expand=FALSEの場合、余分な変数のNA値はモデルで使用されるna.action関数に渡されます。  その結果、データフレームが短くなったり（na.omitの場合）、エラーになったり（na.failの場合）します。  na.expand=TRUEの場合、返されるデータフレームはmodel.frame(model)と全く同じ行を持ちますが、余分な変数に対応する列にはNAが含まれる可能性があります。
これは汎用の関数で、"aov"、"glm"、"lm"、"negbin" (MASSパッケージ)、"coxph"、"survreg" (survivalパッケージ)のメソッドがあります、未知のスケールを持つ線形モデル（つまりlmとaov）の場合、-2 log Lはデビアンスから計算され、logLikとは異なる加法定数を使用する。  RSSが(重み付き)残差平方和を表すとすると、-2 log Lに対してextractAICは、既知スケールsの場合はRSS/s - n (Mallows' Cpに対応)、未知スケールの場合はn log (RSS/n)という式を用います。AICは未知スケールのみを扱い、an*log(RSS/n) + n + n*log 2pi - sum(log w)という式を用います(wは重み)。  k = 2は伝統的なAICに対応し、k = log(n)を使用すると、代わりにBIC (Bayesian IC)が得られます。この関数のメソッドは、AICのメソッドとは前提条件が異なる場合があることに注意してください（通常はlogLikのメソッドを使用します）。  我々はすでに推定スケールを持つ "lm "モデルのケースについて述べた。"glm "および "negbin "メソッドにおいても、分散パラメータが "自由 "とみなされるかどうかという同様の問題がある。  extractAICは、同じクラスのモデルを比較するためにのみ使用されるので、これは重要ではありません（AIC値の差のみが考慮されます）。
因子分析モデルは、p要素ベクトルx、負荷量のp×km行列Λ、スコアのk要素ベクトルf、エラーのp要素ベクトルeについて、x = Λ f + eである。  x以外の成分は観測されないが，主な制約は，得点が無相関で単位分散であること，誤差が分散Psiで独立であること，一意性であることである．  したがって因子分析は、本質的にはxの相関行列Σ = Λ Λ' + Ψのモデルであり、Λを任意の直交行列GのG Λに置き換えても変わらないので、モデルにはまだ不確定性がある。  このような行列Gは回転行列として知られている（ただし、この用語は非直交可逆行列にも適用される）。  covmatが提供された場合，それが使用される．そうでない場合，xが行列であればxが使用され，式xがデータと共に使用されてモデル行列が構成され，それが共分散行列を構成するために使用される．  (式が応答を持つことは意味がなく、変数はすべて数値でなければならない)。  共分散行列が見つかるか、x から計算されると、分析のために相関行列に変換される。  相関行列は、結果の成分相関として返されます。適合は、一意性に対する多変量正規性を仮定して対数尤度を最適化することによって行われます。  (与えられた一意性に対する最大化負荷量は、分析的に見つけることができる：Lawley & Maxwell (1971,p. 27))。  startに与えられたすべての開始値が順番に試され、得られた最良の適合が使用される。  start=NULLの場合、最初の適合は、Jöreskog(1963)が提案し、Lawley & Maxwell(1971,p.31)が与えた値で開始され、次に、制御$nstart - 1の他の値が試され、一意性の等しい値としてランダムに選択される.一意性は、技術的には[0, 1]にあるように拘束されるが、ゼロに近い値は問題であり、最適化は、制御$lowerの下界、デフォルト0.005(Lawley & Maxwell, 1971, p. 32)で行われる.スコアは、制御$nstart - 1の値のみである。Thomsonの方法は（母集団において）未知のfをxに回帰してf = Λ' Σ^-1 xを得、右辺の量の標本推定値を代入する。  Bartlettの方法は、(適合した)Λが与えられ、fの選択に対する標準化誤差の2乗和を最小化します。xが数式である場合、標準的なNAハンドリングがスコアに適用されます(要求された場合): napredictを参照。printメソッド(loadingsの下で文書化)は、結果のパターンに注意を引くという因子分析の慣例に従っているので、デフォルトの精度は小数点以下3桁で、小さな負荷量は抑制されます。
factor.scopeは、ユーザーが直接呼び出すことを意図していません。
familyは、クラス "glm "と "lm"（後者はgaussian()を返す）のメソッドを持つ汎用関数です。二項族と準二項族の場合、応答は3つの方法のいずれかで指定できます：因子として： 'success'は、第1水準を持たない（したがって、通常は第2水準を持つ）因子として解釈される。0と1の間の値を持つ数値ベクトルとして：成功ケースの比率として解釈される（ケースの総数は重みで与えられる）．準2項族および準ポアソン族は、分散パラメータが1に固定されていない点でのみ2項族およびポアソン族と異なり、過分散をモデルすることができる。  二項の場合は、McCullagh and Nelder(1989, pp.124-8)を参照。  彼らは、(いくつかの制限のもとで)準2項モデルのように平均に比例する分散を持つモデルが存在することを示しているが、glmがそのモデルにおいて最尤推定値を計算しないことに注意してほしい。  Sの振る舞いは準変数に近い。
NA
欠測値はxでは許されるが、filterでは許されない（出力中のいたるところで欠測値になる）。再帰フィルタではラグ0に暗黙の係数1があり、y[i] = x[i] + f[1]*y[i-1] + ... + f[p]*y[i-p]となることに注意。再帰フィルタが可逆かどうかのチェックは行われない。畳み込みフィルタは y[i] = f[1]*x[i+o] + ... + f[p]*x[i+o-(p-1)]である。
xが行列の場合，それは2次元分割表とみなされるので，そのエントリは非負の整数でなければならない．  不完全なケースは除去され，ベクトルは因子オブジェクトに強制され，分割表はこれらから計算される．それ以外の計算は、MehtaとPatel (1983, 1986)によって開発され、Clarkson, Fan and Joe (1993)によって改良されたネットワークを実装するFORTRANサブルーチンFEXACTのCバージョンに基づいている。FORTRANコードはhttps://www.netlib.org/toms/643。  テーブルのエントリが大きすぎるとエラーになる。  (必要であれば、列より行が多くならないようにテーブルを転送する。  2×2の表では、条件付き独立性の帰無は、オッズ比が1に等しいという仮説と等価である。  厳密な」推論は，一般に，すべてのマージン合計が固定されているとすると，分割表の第1要素は，オッズ比によって与えられる非心パラメータを持つ非心超幾何分布を持っていることを観察することに基づくことができる(Fisher, 1935)．  片側検定の代替は，オッズ比に基づくので，代替 = "greater" は，オッズ比がより大きいかどうかの検定である．両側検定は，表の確率に基づき，確率が観察された表の確率よりも小さいか等しいすべての表を「より極端な」表とし，p値は，そのような確率の合計である．2×2より大きい表でhybrid = TRUEの場合、漸近カイ2乗確率は、hybridPars = c(expect = 5, percent = 80, Emin = 1)で指定される「コクラン条件」（またはその修正版）が満たされる場合、つまり、期待カウント数が1（= Emin）より小さいセルがなく、期待カウント数が5（= expect）以上のセルが80%（= percent）以上ある場合にのみ使用され、それ以外の場合は正確な計算が使用される。  偶発的に、Rは3.0.0から3.4.1までのRのバージョンで80 aspercentの代わりに180、つまりhybridPars[2]を使用していた、その結果、これらのバージョンのRでは、hybrid=TRUEが違いを生むことはありませんでした。r > 2またはc > 2のr x cの場合、内部テーブルが大きくなりすぎて正確なテストができなくなることがあり、その場合はエラーが通知されます。  ワークスペースを十分に増加させ、実行時間が非常に長くなることを除けば、simulate.p.value = TRUEを使用することで十分であることが多く、それゆえ推奨される。シミュレーションは行と列のマージンを条件として行われ、マージンが厳密に正である場合にのみ機能する。  (Patefield(1981)のアルゴリズムのC翻訳が使用されている)
NA
NA
NA
xがリストの場合、その要素は分散の均質性を比較する標本とみなされ、したがって数値データベクトルでなければならない。  この場合、gは無視され、単純にfligner.test(x)を使って検定を行うことができる。  サンプルがまだリストに含まれていない場合は、fligner.test(list(x, ...)) を使用します。そうでない場合は、xは数値データベクトルでなければならず、gはxの対応する要素に対するグループを与えるxと同じ長さのベクトルまたは因子オブジェクトでなければなりません。  これはk標本の単純線形順位で，中心化された標本の絶対値の順位と重みa(i) = qnorm((1 + i/(n+1))/2) を用いる．  ここで実装されているバージョンは、各標本の中央値センタリングを使用する（参考文献ではF-K:med X^2）。
例えばこのようなモデルは， +演算子で区切られた一連の項から構成される．項自体は， : 演算子で区切られた変数名と因子名から構成され，このような項は，項に現れるすべての変数と因子の相互作用として解釈される．  演算子 * は因子の交差を表す： a*b は a+b+a:b と解釈される。  演算子は指定された次数の交差を示す。  例えば、(a+b+c)^2 は、(a+b+c)*(a+b+c)と同じで、a,b,c の主効果とそれらの2次相互作用を含む式に展開される。  たとえば、a + b %in% a は、式 a + a:b に展開される。  演算子 - は指定された項を削除するので、(a+b+c)^2 - a:b は a + b + c + b:c + a:c と同じになる。  これは切片項を削除するのにも使える：線形モデルy ~ x - 1をフィットするとき、原点を通る直線を指定する。  切片のないモデルは、y ~ x + 0 または y ~ 0 + x と指定することもできる。式は通常、変数名とファクト名だけを含むが、算術式を含むこともある。log(y) ~ a + log(x)という式はかなり合法的である。このような算術式に、モデル式で記号的にも使用される演算子が含まれる場合、算術演算子と記号演算子の使用が混同されることがあります。この混同を避けるために、関数I()を使用して、演算子が算術的な意味で使用されるモデル式の部分を括ることができます。  例えば、ay ~ a + I(b+c)という式では、b+cという項はbとcの和と解釈されます。変数名は、このような非構文的な名前を使用するすべてのコードが受け入れるという保証はありませんが、このようなインフォミュレータのようにバックスティックで引用することができます。  関数によっては、strataやclusterのような他の'特別な'名前を受け付けるものもあります（terms.formulaのspecials引数を参照）。  通常のものは、モデフィッティング関数のデータ引数の文脈におけるもので、「式に含まれないすべての列」を意味します。  update.formulaのコンテキストでは、'以前に式のこの部分に含まれていたもの'という意味になります。formulaが適合モデルオブジェクト上で呼び出される場合、特定のメソッド（クラス "nls "のメソッドなど）が使用されるか、デフォルトのメソッドが使用されます。  デフォルトでは、まずオブジェクトの "formula "コンポーネントを探し（そしてそれを評価し）、次に "terms "コンポーネントを探し、次に呼び出しのformulaパラメータを探し（そしてその値を評価し）、最後に "formula "属性を探します。  例えば、amodel.frame() のように、"terms" 属性に数式が含まれている場合、その数式が返されます。  もし、以前の (R <= 3.5.x) 挙動を望むのであれば、"terms" 属性を考慮しない補助DF2formula() を使用する。  列数が多い場合は、最初の列が式のLHSとなり、+で区切られた残りの列がRHSを形成する。
これらはすべて汎用関数であり、xのtsp属性が存在すればそれを使用する。timeとcycleには、結果をそのクラスに強制するクラスts用のメソッドがある。
friedman.testは、正規性の仮定に違反する可能性のある、再現性のない完全ブロックデザイン（つまり、グループとブロックの各水準の組み合わせに対して、yにちょうど1つのオブザベーションがある）の分析に使用することができます。  yがNAを含む場合、対応するブロックは削除される。
ftableは「平坦な」分割表を作成する。  通常の分割表と同様，これらは，関係する変数（因子）のレベルの各組み合わせのカウントを含む．  そして、この情報は、行と列が（それぞれrow.varsとcol.varsで指定された）行変数と列変数のレベルの組み合わせに対応する行列として再整理されます。  組み合わせは、変数を逆順に（左端の変数のレベルが最も遅く変化するように）ループすることによって作成される。  この平坦な行列形式で分割表を表示する（クラス "ftable "のオブジェクトのprintメソッドであるprint.ftableによって）ことは、高次元配列として表示するよりも好ましい場合が多い。  そのデフォルトメソッドであるftable.defaultは、まずrow.varsとcol.vars以外のすべての引数から配列形式の分割表を作成します。最初の引数がクラス "table "であれば、分割表を表し、そのまま使用されます。クラス "ftable "のフラットテーブルであれば、as.ftableを使用して、それが含む情報を通常の配列表現に変換します。  そうでなければ、引数は（文字列を含む）因子として解釈できるRオブジェクトか、そのように解釈できる構成要素を持つリスト（またはデータフレーム）でなければならず、それらはtable.The引数row.varsとcol.varsを使用して分割表を平坦な形式に折りたたむ。  この2つのどちらも与えられない場合は、最後の変数が列に使われる。  引数が因子として解釈されるR式の場合、変数名の表示方法を制御するために、追加の引数がtableに渡されます。関数ftable.formulaは、フラットな分割表を作成するための式メソッドを提供します。
familyは、クラス "glm "と "lm"（後者はgaussian()を返す）のメソッドを持つ汎用関数です。二項族と準二項族の場合、応答は3つの方法のいずれかで指定できます：因子として： 'success'は、第1水準を持たない（したがって、通常は第2水準を持つ）因子として解釈される。0と1の間の値を持つ数値ベクトルとして：成功ケースの比率として解釈される（ケースの総数は重みで与えられる）．準2項族および準ポアソン族は、分散パラメータが1に固定されていない点でのみ2項族およびポアソン族と異なり、過分散をモデルすることができる。  二項の場合は、McCullagh and Nelder(1989, pp.124-8)を参照。  彼らは、(いくつかの制約のもとで)準2項モデルのように平均に比例する分散を持つモデルが存在することを示しているが、glmがそのモデルにおいて最尤推定値を計算しないことに注意してほしい。  Sの振る舞いは準2項モデルに近い。
family は、"glm "と "lm"（後者はgaussian()を返す）クラスのメソッドを持つ汎用関数です。二項および準二項モデルでは、応答は3つの方法のいずれかで指定できます：因子として： 'success'は、第1水準を持たない（したがって、通常は第2水準を持つ）因子として解釈される。0と1の間の値を持つ数値ベクトルとして：成功ケースの比率として解釈される（ケースの総数は重みで与えられる）．準2項族および準ポアソン族は、分散パラメータが1に固定されていない点でのみ2項族およびポアソン族と異なり、過分散をモデルすることができる。  二項の場合は、McCullagh and Nelder(1989, pp.124-8)を参照。  彼らは、(いくつかの制約のもとで)準2項モデルのように平均に比例する分散を持つモデルが存在することを示しているが、glmがそのモデルにおいて最尤推定値を計算しないことに注意してほしい。  Sの振る舞いは準変数に近い。
何が起こるかは、オブジェクトのクラスと属性に依存します。  これが "lm "のようなfitted-modelクラスのオブジェクトである場合、このメソッドは、モデルのフィットの際に使用された保存されたモデルフレームを返すか（もしあれば、多くの場合、引数model = TRUEで選択されます）、フィットの際に使用されたコールをdefaultメソッドに渡します。  defaultメソッド自体は、他の引数が与えられない場合、MASSパッケージの "lqs "クラスのような標準的なモデルオブジェクトに対応することができます。  formulaがtermsオブジェクトでない限り、as.formulaの後にtermsが呼び出されます。  (terms.formulaのkeep.order引数を使用したい場合は、formulaではなくtermsオブジェクトを渡します。)モデルフレームの行名は、引数dataがある場合はそこから取得され、formulaがある場合は、formulaのresponseの名前(行列の場合はrownames)から取得されます。formula、subset、...のすべての変数は、まずdataで検索され、次にformulaの環境で検索され(詳細はformula()のヘルプを参照)、データフレームに収集されます。  次に、サブセット式が評価され、データ・フレームの行インデックスとして使用されます。  その後、na.action 関数がデータフレームに適用されます（属性が追加されることもあります）。  na.action=NULLでない限り、時系列属性は検出された変数から取り除かれます（NAが取り除かれると間違ってしまうため）。get_all_varsは、計算式で使用された変数に加え、...で指定された変数を含むデータフレームを返します。
NA
NA
典型的な予測変数はresponse ~ termsの形式を持ち、responseは（数値）応答ベクトルで、 termsはresponseの線形予測変数を指定する項の系列です。  二項および準二項族では，応答は因子（第1水準が失敗を表し，それ以外が成功を表す場合）または列が成功と失敗の数を与える2列の行列としても指定できる．  first + second の形の項指定は、first の全項と second の全項の重複を取り除いたものを示す。first:second の形の項は、first の全項と second の全項の相互作用を取ることによって得られる項の集合を示す。  first*secondの指定は、firstとsecondの交差を示す。  これは、first + second + first:secondと同じである。式中の項は、主効果が最初に来て、交互作用、すべての2次、すべての3次と続くように並べ替えられる：これを避けるには、式としてtermsオブジェクトを渡す。NULLでない重みは，異なるオブザベーションが異なる分散を持つ（重みの値が分散に反比例する）ことを示すために使用できる．同様に，重みの要素が正の整数 w_i であるとき，各回答 y_i は，w_i 個の単位重みのオブザベーションの平均である．  二項GLMでは、事前重み付けは、応答が成功の割合である場合の試行回数を与えるために使用されます：ポアソンGLMではほとんど使用されません。  weights、subset、offset、etastart、mustartはすべてinformulaの変数と同じように評価されます。二項GLMの「適合確率が数値的に0または1になった」という警告メッセージの背景については、Venables &Ripley (2002, pp. 197-8)を参照してください。
εが小さい（1e-10以下）場合は、最小二乗法解における共線性の検出の許容誤差としても使用されます。traceがtrueの場合、catの呼び出しは、各IWLS反復の出力を生成します。  したがって、オプション(digits = *)が精度を上げるために使用できます。
典型的な予測変数は，response ~ termsの形式を持ち，ここでresponseは（数値）応答ベクトルで， termsはresponseの線形予測変数を指定する項の系列である．  2項および準2項族では，応答は因子（第1水準が失敗を表し，それ以外が成功を表す場合）として，または列が成功と失敗の数を与える2列の行列として指定することもできる．  first + second の形の項指定は、first の全項と second の全項の重複を取り除いたものを示す。first:second の形の項は、first の全項と second の全項の相互作用を取ることによって得られる項の集合を示す。  first*secondの指定は、firstとsecondの交差を示す。  これは、first + second + first:secondと同じである。式中の項は、主効果が最初に来て、交互作用、すべての2次、すべての3次と続くように並べ替えられる：これを避けるには、式としてtermsオブジェクトを渡す。NULLでない重みは，異なるオブザベーションが異なる分散を持つ（重みの値が分散に反比例する）ことを示すために使用できる．同様に，重みの要素が正の整数 w_i であるとき，各回答 y_i は，w_i 個の単位重みのオブザベーションの平均である．  二項GLMでは、事前重み付けは、応答が成功の割合である場合の試行回数を与えるために使用されます：ポアソンGLMではほとんど使用されません。  weights、subset、offset、etastart、mustartはすべて、informulaの変数と同じ方法で評価されます。つまり、まずデータで評価され、次にformulaの環境で評価されます。二項GLMの「適合確率が数値的に0または1になった」という警告メッセージの背景については、Venables &Ripley (2002, pp. 197-8)を参照してください。
tsp属性は、時間単位での開始時間、終了時間、および頻度（時間単位あたりのオブザベーションの数、たとえば月系列では12）を与えます。
この関数は、各モデル変数の DFBETAS、DFFITS、共分散比、クックの距離、ハット行列の対角要素を示すクラス "infl" オブジェクトの表形式表示を生成します。  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロスバリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供します。 関数hat()は、主にS（バージョン2）との互換性のために存在します。
主要な高レベル関数は，influence.measures で，これは，各モデル変数のDFBETAS，DFFITS，共分散比，クックの距離，ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を生成します．  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロスバリデーション残差を提供し、モデル・モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供する。
この関数は、クラスタ化される n 個のオブジェクトの非類似度の集合を用いて、階層的クラスタ分析を実行します。  各段階で、クラスタ間の距離は、使用される特定のクラスタリング手法に従って、 Lance-Williams の非類似度更新式によって再計算されます。  Wardの最小分散法は、コンパクトで球状のクラスタを見つけることを目的とする。単一連結法（これは最小分散木に密接に関連している）は、「友達の友達」クラスタリング戦略を採用する。  他の方法は、単一連結法と完全連結法の中間の特徴を持つクラスタを目指しているとみなすことができる。  しかし、"中央値 "と "重心 "の手法は、単調な距離測定につながらないこと、あるいは、結果として得られるデンドログラムは、いわゆる反転や逆転を持つことがあり、これは解釈が難しいが、Legendre and Legendre (2012)の三分法に注意してほしい。ward.D "オプション（Rのバージョン <= 3.0.3では唯一のWardオプション "ward "と等価）で使用されるものは、Ward (1963) のクラスタリング基準を実装していないが、"ward.D2 "オプションは、その基準を実装している(Murtagh and Legendre 2014)。  後者では、クラスタ更新の前に非類似度が2乗されます。agnes(*, method="ward")は、hclust(*, "ward.D2")に対応することに注意してください。members != NULLの場合、dは、シングルトン間の非類似度の代わりにクラスタ間の非類似度行列とみなされ、membersは、クラスタごとのオブザベーション数を与えます。  クラスタ間の非類似度は，限られた数の距離/連結の組み合わせに対してのみ（つまり，hclust自体なしで）効率的に計算でき，最も単純なものは，2乗ユークリッド距離とセントロイド連結である．  この場合，クラスタ間の非類似度は，クラスタ平均間の2乗ユークリッドと距離である．階層クラスタ表示では，どのサブツリーが左に行き，どのサブツリーが右に行くべきかを決定するために，各マージで決定が必要である．n 個のオブザベーションに対して、n-1個のマージがあるので、クラスタ木（またはデンドログラム）の葉の可能な順序付けは2^{(n-1)}個あります。hclustで使用されるアルゴリズムは、よりタイトなクラスタが左側になるようにサブツリーを順序付けることです（最後の、つまり、単一のオブザベーションは、可能な限り最もタイトなクラスタで、2つのオブザベーションを含むマージは、オブザベーションのシーケンス番号でそれらを順番に配置します。
Rowv または Colv のいずれかがデンドログラムの場合、それらは尊重される（そして並べ替えられない）。  そうでない場合、デンドログラムは、次のように計算されます。dd <- as.dendrogram(hclustfun(distfun(X))) ここで、Xは、xまたはt(x)のいずれかです。どちらかが（「重み」の）ベクトルである場合、適切なデンドログラムは、行の場合、reorder(dd, Rowv)によって、デンドログラムによって課される制約に従って、与えられた値に従って並べ替えられます。デフォルトのように、どちらかが欠けている場合、対応するデンドログラムの順序付けは、行/列の平均値によって行われる、デフォルト（scale = "row"）では、行は平均0、標準偏差1にスケールされる。  デフォルトの色はきれいではありません。  RColorBrewerパッケージのような拡張機能の使用を検討してください。
加法的Holt-Winters予測関数（周期長pの時系列に対して）は、Yhat[t+h] = a[t] + h * b[t] + s[t - p + 1 + (h - 1) mod p]であり、ここでa[t]、a[t] = α (Y[t] - s[t-p]) + (1-α) (a[t-1] + b[t-1]) b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1] s[t] = γ (Y[t] - a[t]) + (1-γ) s[t-] で与えられる。p]The multiplicative Holt-Winters prediction function (for time series with period length p) is Yhat[t+h] = (a[t] + h * b[t]) * s[t - p + 1 + (h - 1) mod p]、ここでa[t]はb[t] および s[t] は a[t] = α (Y[t] / s[t-p]) + (1-α) (a[t-1] + b[t-1]) b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1] s[t] = γ (Y[t] / a[t]) + (1-γ) s[t-p]乗法モデルの場合、x のデータは非ゼロである必要があります、であることが必要であるが、すべて正であれば最も理にかなっている。この関数は、αおよび/またはβおよび/またはγがNULL（デフォルト）の場合、2乗1段階予測誤差を最小化することによって、αおよび/またはβおよび/またはγの最適値を見つけようとします。季節モデルの場合、a、bおよびsの開始値は、開始.期間の最初の期間に対して移動平均（関数decomposeを参照）を使用して、トレンド成分と季節成分の単純な分解を実行することによって推論されます（トレンド成分に対する単純な線形回帰は、レベルとトレンドの開始に対して使用されます）。水準/トレンド・モデル（季節成分なし）の場合、a と b の開始値は、それぞれ x[2] と x[2] - x[1] です。水準のみのモデル（通常の指数平滑化）の場合、a の開始値は x[1] です。
影響度.measures()および「参照」に列挙されている他の関数は、さまざまな回帰診断を計算する、よりユーザー指向の方法を提供します。  これらはすべてlm.influenceをベースにしています。  GLM(identityリンクを持つGaussianファミリー以外)では、これらは1ステップ近似に基づいていることに注意してください。  (このようなケースを削除すると、通常変数が削除されるので、単純なドロップ・ワン診断はできない。)naresidは結果に適用されるので、フィットがna.action = na.excludeであった場合、NAで埋められる。
主な高レベル関数はinfluence.measuresで、各モデル変数のDFBETAS、DFFITS、共分散比、クックの距離、ハット行列の対角要素を示すクラス "infl "オブジェクトの表形式表示を作成します。  dfbetas、dffits、covratio、cooks.distance関数は、対応する診断量への直接アクセスを提供します。  関数rstandardとrstudentは、それぞれ標準化残差とスチューデント化残差を与える。(一般化線形モデルの値は、Williams (1987)で説明されているように近似値です（ただし、Cooks.distanceはカイ2乗値ではなくFとしてスケーリングされます）。  オプションのinfl、res、sd引数は、例えば、以下のような状況で、これらの直接アクセス関数の使用を奨励するためにある、重み==0のケースは、これらの関数すべてから除外されますが、na.action = na.excludeで線形モデルがフィットされた場合、フィット中に除外されたケースに対して適切な値が入力されることに注意してください。線形モデルの場合、rstandard(*, type = "predictive")は、1つ置きのクロスバリデーション残差を提供し、モデルの "PRESS "統計量（PREdictive Sum of Squares、CVスコアと同じ）を提供します。
有限区間の場合、WynnのEpsilonアルゴリズムによる外挿と関連して、グローバルに適応的な区間細分割が使用されます。Machine$double.eps、0.5e-28) if abs.tol <= 0.なお、'<R>/src/appl/integrate.c'のCソースコードのコメントには、より詳細な情報、特に失敗の理由(内部エラーコード ier >= 1)が記載されています。Rのバージョン <= 3.2.xでは、lowerとupperの最初のエントリが使用されていましたが、長さが1でない場合はエラーとなります。
デフォルトでは、x.factorの水準は与えられた順序でx軸上にプロットされ、凡例(指定された場合)のために右側に余分なスペースが残される。x.factorが順序付き因子で、水準が数値の場合、これらの数値がx軸に使用されます。その場合、欠損値および欠損値を結ぶ線分はプロットから省略されます（これは多少不愉快になることがあります）。グラフィックスパラメータxlab、ylab、ylm、ty、col、pchには、適切なデフォルトが与えられています（そして、xlimとxaxsは設定されており、上書きすることはできません）。デフォルトは、線種を循環させ、前景色を使用し、記号1:9、0、および大文字を使用してトレースをプロットします。
familyは "glm "と "lm"（後者はgaussian()を返す）クラスのメソッドを持つ汎用関数である：因子として： 'success'は、第1水準を持たない（したがって、通常は第2水準を持つ）因子として解釈される。0と1の間の値を持つ数値ベクトルとして：成功ケースの比率として解釈される（ケースの総数は重みで与えられる）．準2項族および準ポアソン族は、分散パラメータが1に固定されていない点でのみ2項族およびポアソン族と異なり、過分散をモデルすることができる。  二項の場合は、McCullagh and Nelder(1989, pp.124-8)を参照。  彼らは、(いくつかの制限のもとで)準2項モデルのように平均に比例する分散を持つモデルが存在することを示しているが、glmがそのモデルにおいて最尤推定値を計算しないことに注意してほしい。  Sの振る舞いは準変数に近い。
つまり、IQR(x) = quantile(x, 3/4) - quantile(x, 1/4) です。正規分布N(m,1)のXの場合、IQR(X)の期待値は2*qnorm(3/4) = 1.3490、つまり標準偏差を正規分布に一致させて推定するには、IQR(x) / 1.349を使います。
NA
デンドログラムは，各成分が木の枝に対応する入れ子リストとして直接表現される．  したがって，木zの最初の枝はz[[1]]であり，対応する部分木の2番目の枝はz[[1]][[2]]，またはより短いz[[c(1,2)]]などである．  木の各ノードは，効率的なプロットや切断に必要ないくつかの情報を属性として持ち，そのうち，メンバ，高さ，葉の葉だけが必須である：枝の葉の総数ノードをプロットする高さの数値（非負）枝の左境界（左端の葉）からのノードの水平距離の数値（すべての葉の間の単位1）。  これは、plot(*, center = FALSE) で使用されます。cut()$upper のノードのラベル、前のメンバの数。より一般的には、'horizontal'(horizontal = FALSE の場合、それ以外は 'vertical')整列に使用されるメンバ・コンポーネントの代用です。文字; ノードにつながる辺のラベル。点描画のためのノード固有の属性を指定する名前付きリスト（長さ-1個の構成要素）。cut.dendrogram()は、成分$upperと$lowerを持つリストを返す。前者は、元の木を切り詰めたもので、これもdendrogramクラスである、クラス "hclust "のオブジェクトは、メソッド as.dendrogram()を用いてクラス "dendrogram "に変換することができる。reorder.dendrogramも参照してください。merge(x, y, ...)メソッドは、2つ以上のデンドログラムを、xとy（およびオプションのさらなる引数）を枝とする新しいデンドログラムにマージします。  is.leaf(object) は、オブジェクトが aleaf（最も単純なデンドログラム）であるかどうかを示す論理値を返します。plotNode() と plotNodeLimit() はヘルパー関数です。
ts関数は、時系列オブジェクトを作成するために使用されます。  これらのオブジェクトは、"ts "のクラス（および追加の属性）を持つベクトルまたは行列であり、時間的に等しい間隔でサンプリングされたデータを表現します。  行列の場合、行列データの各列は、1つの（単変量の）時系列を含むと仮定されます。時系列は、少なくとも1つのオブザベーションを持つ必要があり、数値である必要はありませんが、非数値系列のサポートは非常に限られています。  特に、算術演算は時間軸の整列を試み、系列の部分集合を抽出するための部分集合が使用できる（例えば、EuStockMarkets[, "DAX"]）。  ただし、最初の (または唯一の) 次元のサブセットは、行列のサブセットと同様、行列またはベクトルを返します。  tのメソッドには、系列を行列（ベクトルの場合は1列の行列）として転置するメソッドがあり、クラス "ts "を継承しない結果を返します。引数frequencyは時系列のサンプリング頻度を示し、デフォルト値1は単位時間間隔ごとに1回のサンプリングを示します。  例えば、データが毎日サンプリングされ、自然な期間が1週間である場合、frequencyに7を使用することができ、データが毎月サンプリングされ、自然な期間が1年である場合、frequencyに12を使用することができます。  4と12の値は、それぞれ四半期系列と月系列を意味する（例えば）印刷法では仮定される。  R 4.0.0からは、frequencyは整数である必要はない。  例えば、frequency = 0.2は、5つの時間単位ごとに1回サンプリングすることを意味します。  デフォルトのメソッドは、オブジェクトにtsp属性があればそれを使って開始時刻と終了時刻を設定し、frequency.is.tsはオブジェクトが時系列かどうかをテストします。  これは汎用的なもので、特定のクラスのオブジェクトを扱うメソッドを書くことができます。
NA
ts関数は、時系列オブジェクトを作成するために使用されます。  これは、"ts "のクラス（および追加属性）を持つベクトルまたは行列であり、時間的に等しい間隔でサンプリングされたデータを表します。  行列の場合、行列データの各列は、1つの（単変量の）時系列を含むと仮定されます。時系列は、少なくとも1つのオブザベーションを持つ必要があり、数値である必要はありませんが、非数値系列のサポートは非常に限られています。  特に、算術演算は時間軸の整列を試み、系列の部分集合を抽出するための部分集合が使用できる（例えば、EuStockMarkets[, "DAX"]）。  ただし、最初の (または唯一の) 次元のサブセットは、行列のサブセットと同様、行列またはベクトルを返します。  tのメソッドには、系列を行列（ベクトルの場合は1列の行列）として転置するメソッドがあり、クラス "ts "を継承しない結果を返します。引数frequencyは時系列のサンプリング頻度を示し、デフォルト値1は単位時間間隔ごとに1回のサンプリングを示します。  例えば、データが毎日サンプリングされ、自然な期間が1週間である場合、frequencyに7を使用することができ、データが毎月サンプリングされ、自然な期間が1年である場合、frequencyに12を使用することができます。  4と12の値は、それぞれ四半期系列と月系列を意味する（例えば）印刷法では仮定される。  R 4.0.0からは、frequencyは整数である必要はない。  例えば、frequency = 0.2は、5つの時間単位ごとに1回サンプリングすることを意味します。  デフォルトのメソッドは、オブジェクトにtsp属性があればそれを使って開始時刻と終了時刻を設定し、frequency.is.tsはオブジェクトが時系列かどうかをテストします。  オブジェクトの特定のクラスを処理するメソッドを書くことができます。
kernelは、一般的なカーネルや指定されたカーネルを構築するために使用されます。  修正Daniellカーネルは(S-PLUSで使用されるように)端係数を半分にします。[ メソッドは(-m) : mのインデックスを持つカーネルオブジェクトの自然なインデックス付けを可能にします。kernel は，Brockwell and Davis (1991), page362 で定義されている平滑化カーネルの「等価自由度」を返し， bandwidth.kernel は，Bloomfield (1976), p. 201 で定義されている等価帯域幅に連続性補正を加えたものを返す．
このアルゴリズムは, 累積データ(すなわち, cumsum(y))の凸小線型m(x)を決定し, その結果は m'(x), 凸m(x)が累積データの多角形に接し勾配が変化する位置でのレベル変化を持つステップ関数である.as.stepfun() は, より簡潔なstepfunオブジェクトを返す.
これらの関数は、状態ベクトルa、遷移a <- T a + R e,e ~ N(0, kappa Q)、観測方程式y = Z'a + eta,eta ~ N(0, kappa h)を持つ一般的な一変量状態空間モデルで動作します。モデルは、遷移行列、観測係数、観測分散RQR'、現在の状態推定値、状態不確定性行列Qの現在の推定値、状態不確定性行列Qの時刻t-1での推定値（KalmanForecastでは更新されない）を少なくとも含むリストとして指定されます。KalmanSmoothはtsSmoothの主力関数である.makeARIMAはARIMAモデルの状態空間モデルを構築する.arimaも参照.状態空間の初期化はGardnerらの方法(SSinit = "Gardner1980")を長年唯一の方法として使用してきた.  しかし、この方法は非定常性に近い場合、時々欠陥に悩まされる。このため、将来的にはデフォルトとして置き換えられ、再現性のためだけに維持されるかもしれない。  Rossignol2011 "メソッドは、2011-09-20にグルノーブル大学のRaphael Rossignolによって提案され、部分的に文書化されています（以下のPR#14682を参照）。これは(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})の共分散行列を差分方程式法(Brockwell and Davisの93ページ)で計算するもので、Gardnerらのレフェリーから提案されたものらしい(彼らの論文の314ページ参照)参照）。
これらの関数は、状態ベクトル a、遷移 a <- T a + R e,e ~ N(0, kappa Q)、観測方程式 y = Z'a + eta,eta ~ N(0, kappa h)を持つ一般的な一変量状態空間モデルで動作する。モデルは、遷移行列、観測係数、観測分散RQR'、現在の状態推定値、状態不確定性行列Qの現在の推定値、状態不確定性行列Qの時刻t-1での推定値（KalmanForecastでは更新されない）を少なくとも含むリストとして指定されます。KalmanSmoothはtsSmoothの主力関数である.makeARIMAはARIMAモデルの状態空間モデルを構築する.arimaも参照.状態空間の初期化はGardnerらの方法(SSinit = "Gardner1980")を長年唯一の方法として使用してきた.  しかし、この方法は非定常性に近い場合、時々欠陥に悩まされる。このため、将来的にはデフォルトとして置き換えられ、再現性のためだけに維持されるかもしれない。  Rossignol2011 "メソッドは、2011-09-20にグルノーブル大学のRaphael Rossignolによって提案され、部分的に文書化されています（以下のPR#14682を参照）。これは(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})の共分散行列を差分方程式法(Brockwell and Davisの93ページ)で計算するもので、Gardnerらのレフェリーから提案されたものらしい(彼らの論文の314ページ参照)参照）。
これらの関数は、状態ベクトル a、遷移 a <- T a + R e,e ~ N(0, kappa Q)、観測方程式 y = Z'a + eta,eta ~ N(0, kappa h)を持つ一般的な一変量状態空間モデルで動作する。モデルは、遷移行列、観測係数、観測分散RQR'、現在の状態推定値、状態不確定性行列Qの現在の推定値、状態不確定性行列Qの時刻t-1での推定値（KalmanForecastでは更新されない）を少なくとも含むリストとして指定されます。KalmanSmoothはtsSmoothの主力関数である.makeARIMAはARIMAモデルの状態空間モデルを構築する.arimaも参照.状態空間の初期化はGardnerらの方法(SSinit = "Gardner1980")を長年唯一の方法として使用してきた.  しかし、この方法は非定常性に近い場合、時々欠陥に悩まされる。このため、将来的にはデフォルトとして置き換えられ、再現性のためだけに維持されるかもしれない。  Rossignol2011 "メソッドは、2011-09-20にグルノーブル大学のRaphael Rossignolによって提案され、部分的に文書化されています（以下のPR#14682を参照）。これは(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})の共分散行列を差分方程式法(Brockwell and Davisの93ページ)で計算するもので、Gardnerらのレフェリーから提案されたものらしい(彼らの論文の314ページ参照)参照）。
これらの関数は、状態ベクトル a、遷移 a <- T a + R e,e ~ N(0, kappa Q)、観測方程式 y = Z'a + eta,eta ~ N(0, kappa h)を持つ一般的な一変量状態空間モデルで動作する。モデルは、遷移行列、観測係数、観測分散RQR'、現在の状態推定値、状態不確定性行列Qの現在の推定値、状態不確定性行列Qの時刻t-1での推定値（KalmanForecastでは更新されない）を少なくとも含むリストとして指定されます。KalmanSmoothはtsSmoothの主力関数である.makeARIMAはARIMAモデルの状態空間モデルを構築する.arimaも参照.状態空間の初期化はGardnerらの方法(SSinit = "Gardner1980")を長年唯一の方法として使用してきた.  しかし、この方法は非定常性に近い場合、時々欠陥に悩まされる。このため、将来的にはデフォルトとして置き換えられ、再現性のためだけに維持されるかもしれない。  Rossignol2011 "メソッドは、2011-09-20にグルノーブル大学のRaphael Rossignolによって提案され、部分的に文書化されています（以下のPR#14682を参照）。これは(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})の共分散行列を差分方程式法(Brockwell and Davisの93ページ)で計算するもので、Gardnerらのレフェリーから提案されたものらしい(彼らの論文の314ページ参照)参照）。
NA
カーネルは、一般的なカーネルまたは指定されたカーネルを構築するために使用される。  修正Daniellカーネルは末端係数を半分にする(S-PLUSで使用)。[ メソッドは(-m) : mのインデックスを持つカーネルオブジェクトの自然なインデックス付けを可能にする。kernelは，Brockwell and Davis (1991), page362で定義されている平滑化カーネルの「等価自由度」を返し，bandwidth.kernelは，Bloomfield (1976), p. 201で定義されている等価帯域幅に連続性補正を加えたものを返す．
これは、点から割り当てられたクラスタ中心までの2乗和が最小になるように、点をk個のグループに分割することを目的とするものである。  最も一般的なのはMacQueen (1967)のアルゴリズムであるが，Lloyd (1957)やForgy(1965)のアルゴリズムであることもある．  Hartigan-Wongアルゴリズムは、一般に、これらのどちらよりも良い仕事をするが、いくつかのランダムスタート(nstart> 1)を試すことがしばしば推奨される。  まれに, いくつかの点(xの行)が極端に近い場合, アルゴリズムは "Quick-Transfer "段階で収束しないことがあり, 警告を発する(そして, ifault = 4を返す)。  プログラムの探索を簡単にするために、k=1が許され、特に中心とwithinssが返されます。Lloyd-Forgy法を除いて、数が指定されると常にk個のクラスタが返されます。中心の初期行列が与えられると、1つまたは複数の中心に最も近い点がない可能性があり、これは現在Hartigan-Wong法ではエラーとなります。

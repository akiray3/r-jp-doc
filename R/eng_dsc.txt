x
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
Operators for the <code>"Date"</code> class.

There is an <code>Ops</code> method and specificmethods for <code>+</code> and <code>-</code> for the <code>Date</code> class.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Generate regular sequences.
Accessing exported and internal variables, i.e. <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects(including lazy loaded data sets) in a namespace.
Accessing exported and internal variables, i.e. <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects(including lazy loaded data sets) in a namespace.
These operators act on raw, logical and number-like vectors.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
Binary operators which allow the comparison of values in atomic vectors.
Open parenthesis, <code>(</code>, and open brace, <code>{</code>, are<code>.Primitive</code> functions in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.

Effectively, <code>(</code> is semantically equivalent to the identity<code>function(x) x</code>, whereas <code>{</code> is slightly more interesting,see examples.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
Change the class of an object to indicate that it should be treated‘as is’.
Extract or replace subsets of data frames.
Description of the class <code>"Date"</code> representing calendar dates.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
This function provides a way to get a list of all the DLLs (see<code>dyn.load</code>) that are currently loaded in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
Extract or replace subsets of factors.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
Print character strings without quotes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
Extract or replace subsets of data frames.
Description of the class <code>"Date"</code> representing calendar dates.
Extract or replace subsets of factors.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
Extract or replace subsets of data frames.
Extract or replace subsets of factors.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
Extract or replace subsets of data frames.
Description of the class <code>"Date"</code> representing calendar dates.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Extract or replace subsets of factors.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Open parenthesis, <code>(</code>, and open brace, <code>{</code>, are<code>.Primitive</code> functions in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.

Effectively, <code>(</code> is semantically equivalent to the identity<code>function(x) x</code>, whereas <code>{</code> is slightly more interesting,see examples.
Extract or replace the contents of a slot in a object with aformal (S4) class structure.
Extract or replace the contents of a slot in a object with aformal (S4) class structure.
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
These operators act on raw, logical and number-like vectors.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
These operators act on raw, logical and number-like vectors.
Multiplies two matrices, if they are conformable.  If one argument isa vector, it will be promoted to either a row or column matrix to makethe two arguments conformable.  If both are vectors of the samelength, it will return the inner product (as a matrix).
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
<code>match</code> returns a vector of the positions of (first) matches ofits first argument in its second.

<code>%in%</code> is a more intuitive interface as a binary operator,which returns a logical vector indicating if there is a match or notfor its left operand.
The outer product of the arrays <code>X</code> and <code>Y</code> is the array<code>A</code> with dimension <code>c(dim(X), dim(Y))</code> where element<code>A[c(arrayindex.x, arrayindex.y)]    = FUN(X[arrayindex.x], Y[arrayindex.y], ...)</code>.
Computes the generalised kronecker product of two arrays,<code>X</code> and <code>Y</code>.
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
These unary and binary operators perform arithmetic on numeric orcomplex vectors (or objects which can be coerced to them).
Operators for the <code>"Date"</code> class.

There is an <code>Ops</code> method and specificmethods for <code>+</code> and <code>-</code> for the <code>Date</code> class.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Binary operators which allow the comparison of values in atomic vectors.
Assign a value to a name.
Assign a value to a name.
Binary operators which allow the comparison of values in atomic vectors.
Assign a value to a name.
Binary operators which allow the comparison of values in atomic vectors.
Binary operators which allow the comparison of values in atomic vectors.
Binary operators which allow the comparison of values in atomic vectors.
These operators act on raw, logical and number-like vectors.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
These operators act on raw, logical and number-like vectors.
Tilde is used to separate the left- and right-hand sides in a model formula.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
This function provides a way to get a list of all the DLLs (see<code>dyn.load</code>) that are currently loaded in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
Extract or replace subsets of data frames.
Abbreviate strings to at least <code>minlength</code> characters,such that they remain <em>unique</em> (if they were),unless <code>strict = TRUE</code>.
<code>abs(x)</code> computes the absolute value of x, <code>sqrt(x)</code> computes the(principal) square root of x, <i>√{x}</i>.

The naming follows the standard for computer languages such as C or Fortran.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
These functions give the obvious hyperbolic functions.  Theyrespectively compute the hyperbolic cosine, sine, tangent, and theirinverses, arc-cosine, arc-sine, arc-tangent (or ‘<em>area cosine</em>’,etc).
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
<code>addTaskCallback</code> registers an R functionthat is to be called each time a top-level taskis completed.

<code>removeTaskCallback</code> un-registers a functionthat was registered earlier via <code>addTaskCallback</code>.

These provide low-level access to the internal/nativemechanism for managing task-completion actions.One can use <code>taskCallbackManager</code>at the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>-language level to manage <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> functionsthat are called at the completion of each task.This is easier and more direct.
Searches for approximate matches to <code>pattern</code> (the first argument)within each element of the string <code>x</code> (the second argument) usingthe generalized Levenshtein edit distance (the minimal possiblyweighted number of insertions, deletions and substitutions needed totransform one string into another).
Searches for approximate matches to <code>pattern</code> (the first argument)within each element of the string <code>x</code> (the second argument) usingthe generalized Levenshtein edit distance (the minimal possiblyweighted number of insertions, deletions and substitutions needed totransform one string into another).
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Given a set of logical vectors, are all of the values true?
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
Return a character vector containing all the names which occur in anexpression or call.
Return a character vector containing all the names which occur in anexpression or call.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Given a set of logical vectors, is at least one of the values true?
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>NA</code> is a logical constant of length 1 which contains a missingvalue indicator.  <code>NA</code> can be coerced to any other vectortype except raw.  There are also constants <code>NA_integer_</code>,<code>NA_real_</code>, <code>NA_complex_</code> and <code>NA_character_</code> of theother atomic vector types which support missing values: all of theseare reserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.

The generic function <code>is.na</code> indicates which elements are missing.

The generic function <code>is.na&lt;-</code> sets elements to <code>NA</code>.

The generic function <code>anyNA</code> implements <code>any(is.na(x))</code> in apossibly faster way (especially for atomic vectors).
<code>NA</code> is a logical constant of length 1 which contains a missingvalue indicator.  <code>NA</code> can be coerced to any other vectortype except raw.  There are also constants <code>NA_integer_</code>,<code>NA_real_</code>, <code>NA_complex_</code> and <code>NA_character_</code> of theother atomic vector types which support missing values: all of theseare reserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.

The generic function <code>is.na</code> indicates which elements are missing.

The generic function <code>is.na&lt;-</code> sets elements to <code>NA</code>.

The generic function <code>anyNA</code> implements <code>any(is.na(x))</code> in apossibly faster way (especially for atomic vectors).
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Transpose an array by permuting its dimensions and optionally resizingit.
Transpose an array by permuting its dimensions and optionally resizingit.
Transpose an array by permuting its dimensions and optionally resizingit.
Add elements to a vector.
Returns a vector or array or list of values obtained by applying afunction to margins of an array or matrix.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
Displays the argument names and corresponding default values of afunction or primitive.
Creates or tests for arrays.
Give the <code>TRUE</code> indices of a logical object, allowing for arrayindices.
Creates or tests for arrays.
Creates or tests for arrays.
Create or test for objects of <code>mode</code> <code>"call"</code> (or<code>"("</code>, see Details).
Create or test for objects of type <code>"character"</code>.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Create or test for objects of type <code>"character"</code>.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Create or test for objects of type <code>"character"</code>.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
Functions to convert between character representations and objects ofclasses <code>"POSIXlt"</code> and <code>"POSIXct"</code> representing calendardates and times.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Description of the class <code>"Date"</code> representing calendar dates.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Functions to check if an object is a data frame, or coerce it if possible.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Functions to check if an object is a data frame, or coerce it if possible.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to check if an object is a data frame, or coerce it if possible.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Create, coerce to or test for a double-precision vector.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
A generic function coercing an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object to an<code>environment</code>.  A number or a character string isconverted to the corresponding environment on the search path.
Creates or tests for objects of mode <code>"expression"</code>.
Creates or tests for objects of mode <code>"expression"</code>.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
<code>as.function</code> is a generic function which is used to convertobjects to functions.

<code>as.function.default</code> works on a list <code>x</code>, which should contain theconcatenation of a formal argument list and an expression or anobject of mode <code>"call"</code> which will become the function body.The function will be defined in a specified environment, by defaultthat of the caller.
<code>as.function</code> is a generic function which is used to convertobjects to functions.

<code>as.function.default</code> works on a list <code>x</code>, which should contain theconcatenation of a formal argument list and an expression or anobject of mode <code>"call"</code> which will become the function body.The function will be defined in a specified environment, by defaultthat of the caller.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
Creates or tests for objects of type <code>"integer"</code>.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Description of the class <code>"Date"</code> representing calendar dates.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Create or test for objects of type <code>"logical"</code>, and the basiclogical constants.
Create or test for objects of type <code>"logical"</code>, and the basiclogical constants.
<code>matrix</code> creates a matrix from the given set of values.

<code>as.matrix</code> attempts to turn its argument into a matrix.

<code>is.matrix</code> tests if its argument is a (strict) matrix.
<code>matrix</code> creates a matrix from the given set of values.

<code>as.matrix</code> attempts to turn its argument into a matrix.

<code>is.matrix</code> tests if its argument is a (strict) matrix.
<code>matrix</code> creates a matrix from the given set of values.

<code>as.matrix</code> attempts to turn its argument into a matrix.

<code>is.matrix</code> tests if its argument is a (strict) matrix.
Print character strings without quotes.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
A ‘name’ (also known as a ‘symbol’) is a way to refer to<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects by name (rather than the value of the object, if any, boundto that name).

<code>as.name</code> and <code>as.symbol</code> are identical: they attempt tocoerce the argument to a name.

<code>is.symbol</code> and the identical <code>is.name</code> return <code>TRUE</code>or <code>FALSE</code> depending on whether the argument is a name or not.
<code>NULL</code> represents the null object in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>: it is a reservedword.  <code>NULL</code> is often returned by expressions and functionswhose value is undefined.
<code>NULL</code> represents the null object in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>: it is a reservedword.  <code>NULL</code> is often returned by expressions and functionswhose value is undefined.
Creates or coerces objects of type <code>"numeric"</code>.<code>is.numeric</code> is a more general test of an object beinginterpretable as numbers.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
Functions to manipulate objects of classes <code>"POSIXlt"</code> and<code>"POSIXct"</code> representing calendar dates and times.
<code>qr</code> computes the QR decomposition of a matrix.
Creates or tests for objects of type <code>"raw"</code>.
Create, coerce to or test for a double-precision vector.
Create, coerce to or test for a double-precision vector.
A ‘name’ (also known as a ‘symbol’) is a way to refer to<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects by name (rather than the value of the object, if any, boundto that name).

<code>as.name</code> and <code>as.symbol</code> are identical: they attempt tocoerce the argument to a name.

<code>is.symbol</code> and the identical <code>is.name</code> return <code>TRUE</code>or <code>FALSE</code> depending on whether the argument is a name or not.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
<code>vector</code> produces a vector of the given length and mode.

<code>as.vector</code>, a generic, attempts to coerce its argument into avector of mode <code>mode</code> (the default is to coerce to whichevervector mode is most convenient): if the result is atomic allattributes are removed.

<code>is.vector</code> returns <code>TRUE</code> if <code>x</code> is a vector of thespecified mode having no attributes <em>other than names</em>.  It returns<code>FALSE</code> otherwise.
<code>vector</code> produces a vector of the given length and mode.

<code>as.vector</code>, a generic, attempts to coerce its argument into avector of mode <code>mode</code> (the default is to coerce to whichevervector mode is most convenient): if the result is atomic allattributes are removed.

<code>is.vector</code> returns <code>TRUE</code> if <code>x</code> is a vector of thespecified mode having no attributes <em>other than names</em>.  It returns<code>FALSE</code> otherwise.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
These functions give the obvious hyperbolic functions.  Theyrespectively compute the hyperbolic cosine, sine, tangent, and theirinverses, arc-cosine, arc-sine, arc-tangent (or ‘<em>area cosine</em>’,etc).
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Split an array or matrix by its margins.
Tests whether the object is an instance of an S4 class.
Tests whether the object is an instance of an S4 class.
Assign a value to a name in an environment.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
These functions give the obvious hyperbolic functions.  Theyrespectively compute the hyperbolic cosine, sine, tangent, and theirinverses, arc-cosine, arc-sine, arc-tangent (or ‘<em>area cosine</em>’,etc).
The database is attached to the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> search path.  This means that thedatabase is searched by <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> when evaluating a variable, so objects inthe database can be accessed by simply giving their names.
Functions to load and unload name spaces.
Get or set specific attributes of an object.
<code>all.equal(x, y)</code> is a utility to compare <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects <code>x</code>and <code>y</code> testing ‘near equality’.  If they are different,comparison is still made to some extent, and a report of thedifferences is returned.    Do not use <code>all.equal</code> directly in<code>if</code> expressions—either use <code>isTRUE(all.equal(....))</code> or<code>identical</code> if appropriate.
Get or set specific attributes of an object.
These functions access an object's attributes.The first form below returns the object's attribute list.The replacement forms uses the list on the right-handside of the assignment as the object's attributes (if appropriate).
These functions access an object's attributes.The first form below returns the object's attribute list.The replacement forms uses the list on the right-handside of the assignment as the object's attributes (if appropriate).
<code>autoload</code> creates a promise-to-evaluate <code>autoloader</code> andstores it with name <code>name</code> in <code>.AutoloadEnv</code> environment.When <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> attempts to evaluate <code>name</code>, <code>autoloader</code> is run,the package is loaded and <code>name</code> is re-evaluated in the newpackage's environment.  The result is that <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> behaves as if<code>package</code> was loaded but it does not occupy memory.

<code>.Autoloaded</code> contains the names of the packages forwhich autoloading has been promised.
<code>autoload</code> creates a promise-to-evaluate <code>autoloader</code> andstores it with name <code>name</code> in <code>.AutoloadEnv</code> environment.When <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> attempts to evaluate <code>name</code>, <code>autoloader</code> is run,the package is loaded and <code>name</code> is re-evaluated in the newpackage's environment.  The result is that <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> behaves as if<code>package</code> was loaded but it does not occupy memory.

<code>.Autoloaded</code> contains the names of the packages forwhich autoloading has been promised.
Solves a triangular system of linear equations.
Get, set, test for and create environments.
<code>basename</code> removes all of the path up to and including the lastpath separator (if any).

<code>dirname</code> returns the part of the <code>path</code> up to butexcluding the last path separator, or <code>"."</code> if there is no pathseparator.
Bessel Functions of integer and fractional order, of firstand second kind, <i>J(nu)</i> and <i>Y(nu)</i>, andModified Bessel functions (of first and third kind),<i>I(nu)</i> and <i>K(nu)</i>.
Bessel Functions of integer and fractional order, of firstand second kind, <i>J(nu)</i> and <i>Y(nu)</i>, andModified Bessel functions (of first and third kind),<i>I(nu)</i> and <i>K(nu)</i>.
Bessel Functions of integer and fractional order, of firstand second kind, <i>J(nu)</i> and <i>Y(nu)</i>, andModified Bessel functions (of first and third kind),<i>I(nu)</i> and <i>K(nu)</i>.
Bessel Functions of integer and fractional order, of firstand second kind, <i>J(nu)</i> and <i>Y(nu)</i>, andModified Bessel functions (of first and third kind),<i>I(nu)</i> and <i>K(nu)</i>.
Special mathematical functions related to the beta and gammafunctions.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
If Native Language Support (NLS) was enabled in this build of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> (seethe <code>bindtextdomain()</code> example), attempt totranslate character vectors or set where the translations are to be found.
Logical operations on integer vectors with elements viewed as sets of bits.
Logical operations on integer vectors with elements viewed as sets of bits.
Logical operations on integer vectors with elements viewed as sets of bits.
Logical operations on integer vectors with elements viewed as sets of bits.
Logical operations on integer vectors with elements viewed as sets of bits.
Logical operations on integer vectors with elements viewed as sets of bits.
Get or set the <em>body</em> of a function which is basically all ofthe function definition but its formal arguments (<code>formals</code>),see the ‘Details’.
Get or set the <em>body</em> of a function which is basically all ofthe function definition but its formal arguments (<code>formals</code>),see the ‘Details’.
An analogue of the LISP backquote macro.  <code>bquote</code> quotes itsargument except that terms wrapped in <code>.()</code> are evaluated in thespecified <code>where</code> environment. If <code>splice = TRUE</code> thenterms wrapped in <code>..()</code> are evaluated and spliced into a call.
These are the basic control-flow constructs of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.  Theyfunction in much the same way as control statements in any Algol-likelanguage.  They are all reserved words.
Interrupt the execution of an expression and allow the inspection ofthe environment where <code>browser</code> was called from.
A call to browser can provide context by supplying either a textargument or a condition argument.  These functions can be used toretrieve either of these arguments.
A call to browser can provide context by supplying either a textargument or a condition argument.  These functions can be used toretrieve either of these arguments.
A call to browser can provide context by supplying either a textargument or a condition argument.  These functions can be used toretrieve either of these arguments.
Return the names of all the built-in objects.  These are fetcheddirectly from the symbol table of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> interpreter.
Function <code>by</code> is an object-oriented wrapper for<code>tapply</code> applied to data frames.
Function <code>by</code> is an object-oriented wrapper for<code>tapply</code> applied to data frames.
Function <code>by</code> is an object-oriented wrapper for<code>tapply</code> applied to data frames.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
This is a generic function which combines its arguments.

The default method combines its arguments to form a vector.All arguments are coerced to a common type which is the typeof the returned value, and all attributes except names are removed.
Description of the class <code>"Date"</code> representing calendar dates.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
Print character strings without quotes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
Create or test for objects of <code>mode</code> <code>"call"</code> (or<code>"("</code>, see Details).
A downward-only version of Scheme's call with current continuation.
Report on the optional features which have been compiled into thisbuild of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
Translate characters in character vectors, in particular from upper tolower case or vice versa.
Outputs the objects, concatenating the representations.  <code>cat</code>performs much less conversion than <code>print</code>.
Take a sequence of vector, matrix or data-frame arguments and combineby <em>c</em>olumns or <em>r</em>ows, respectively.  These are genericfunctions with methods for other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> classes.
Take a sequence of vector, matrix or data-frame arguments and combineby <em>c</em>olumns or <em>r</em>ows, respectively.  These are genericfunctions with methods for other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> classes.
<code>ceiling</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the smallest integers not less than thecorresponding elements of <code>x</code>.

<code>floor</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the largest integers not greater than thecorresponding elements of <code>x</code>.

<code>trunc</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the integers formed by truncating the values in<code>x</code> toward <code>0</code>.

<code>round</code> rounds the values in its first argument to the specifiednumber of decimal places (default 0).  See ‘Details’ about“round to even” when rounding off a 5.

<code>signif</code> rounds the values in its first argument to the specifiednumber of significant digits.   Hence, for <code>numeric</code> <code>x</code>,<code>signif(x, dig)</code> is the same as <code>round(x, dig - ceiling(log10(abs(x))))</code>.For <code>complex</code> <code>x</code>, this is not the case, see the ‘Details’.
Seeks a unique match of its first argument among theelements of its second.  If successful, it returns this element;otherwise, it performs an action specified by the third argument.
Create or test for objects of type <code>"character"</code>.
<code>charmatch</code> seeks matches for the elements of its first argumentamong those of its second.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Translate characters in character vectors, in particular from upper tolower case or vice versa.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Warn about extraneous arguments in the <code>...</code> of its caller.  Autility to be used e.g., in S3 methods which need a formal <code>...</code>argument but do not make any use of it.  This helps catching usererrors in calling the function in question (which is the caller of<code>chkDots()</code>).
Compute the Choleski factorization of a real symmetricpositive-definite square matrix.
Compute the Choleski factorization of a real symmetricpositive-definite square matrix.
Invert a symmetric, positive definite square matrix from its Choleskidecomposition.  Equivalently, compute <i>(X'X)^(-1)</i>from the (<i>R</i> part) of the QR decomposition of <i>X</i>.
Special mathematical functions related to the beta and gammafunctions.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
Functions to push back text lines onto a connection, and to enquirehow many lines are currently pushed back.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
Display aspects of connections.
Returns a matrix of integers indicating their column number in amatrix-like object, or a factor of column labels.
Form row and column sums and means for numeric arrays (or data frames).
Retrieve or set the row or column names of a matrix-like object.
Retrieve or set the row or column names of a matrix-like object.
Form row and column sums and means for numeric arrays (or data frames).
Provides access to a copy of the command line arguments supplied whenthis <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session was invoked.
These functions set and query a <em>comment</em>attribute for any <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects.  This is typically useful for<code>data.frame</code>s or model fits.

Contrary to other <code>attributes</code>, the <code>comment</code> is notprinted (by <code>print</code> or <code>print.default</code>).

Assigning <code>NULL</code> or a zero-length character vector removes thecomment.
These functions set and query a <em>comment</em>attribute for any <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects.  This is typically useful for<code>data.frame</code>s or model fits.

Contrary to other <code>attributes</code>, the <code>comment</code> is notprinted (by <code>print</code> or <code>print.default</code>).

Assigning <code>NULL</code> or a zero-length character vector removes thecomment.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
<code>library</code> and <code>require</code> load and attach add-on packages.
<code>conflicts</code> reports on objects that exist with the same name intwo or more places on the <code>search</code> path, usually becausean object in the user's workspace or a package is masking a systemobject of the same name.  This helps discover unintentional masking.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
The <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> Who-is-who, describing who made significant contributions tothe development of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
These functions give the obvious hyperbolic functions.  Theyrespectively compute the hyperbolic cosine, sine, tangent, and theirinverses, arc-cosine, arc-sine, arc-tangent (or ‘<em>area cosine</em>’,etc).
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
Given matrices <code>x</code> and <code>y</code> as arguments, return a matrixcross-product.  This is formally equivalent to (but usually slightlyfaster than) the call <code>t(x) %*% y</code> (<code>crossprod</code>) or<code>x %*% t(y)</code> (<code>tcrossprod</code>).
Report information on the C stack size and usage (if available).
Returns a vector whose elements are the cumulative sums, products,minima or maxima of the elements of the argument.
Returns a vector whose elements are the cumulative sums, products,minima or maxima of the elements of the argument.
Returns a vector whose elements are the cumulative sums, products,minima or maxima of the elements of the argument.
Returns a vector whose elements are the cumulative sums, products,minima or maxima of the elements of the argument.
Retrieve the headers for a URL for a supported protocol such as<code>http://</code>, <code>ftp://</code>, <code>https://</code> and <code>ftps://</code>.An optional function not supported on all platforms.
<code>cut</code> divides the range of <code>x</code> into intervalsand codes the values in <code>x</code> according to whichinterval they fall.  The leftmost interval corresponds to level one,the next leftmost to level two and so on.
Method for <code>cut</code> applied to date-time objects.
<code>cut</code> divides the range of <code>x</code> into intervalsand codes the values in <code>x</code> according to whichinterval they fall.  The leftmost interval corresponds to level one,the next leftmost to level two and so on.
Method for <code>cut</code> applied to date-time objects.
Determine the class of an arbitrary <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
The function <code>data.frame()</code> creates data frames, tightly coupledcollections of variables which share many of the properties ofmatrices and of lists, used as the fundamental data structure by mostof <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>'s modeling software.
Return the matrix obtained by converting all the variables in a dataframe to numeric mode and then binding them together as the columns ofa matrix.  Factors and ordered factors are replaced by their internalcodes.
Returns a character string of the current system date and time.
Set, unset or query the debugging flag on a function.The <code>text</code> and <code>condition</code> arguments are the same as thosethat can be supplied via a call to <code>browser</code>.  They can be retrievedby the user once the browser has been entered, and provide a mechanism toallow users to identify which breakpoint has been activated.
Set, unset or query the debugging flag on a function.The <code>text</code> and <code>condition</code> arguments are the same as thosethat can be supplied via a call to <code>browser</code>.  They can be retrievedby the user once the browser has been entered, and provide a mechanism toallow users to identify which breakpoint has been activated.
Set, unset or query the debugging flag on a function.The <code>text</code> and <code>condition</code> arguments are the same as thosethat can be supplied via a call to <code>browser</code>.  They can be retrievedby the user once the browser has been entered, and provide a mechanism toallow users to identify which breakpoint has been activated.
The function <code>data.frame()</code> creates data frames, tightly coupledcollections of variables which share many of the properties ofmatrices and of lists, used as the fundamental data structure by mostof <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>'s modeling software.
<code>delayedAssign</code> creates a <em>promise</em> to evaluate the givenexpression if its value is requested.  This provides direct accessto the <em>lazy evaluation</em> mechanism used by <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> for the evaluationof (interpreted) functions.
Turn unevaluated expressions into character strings.
Turn unevaluated expressions into character strings.
<code>det</code> calculates the determinant of a matrix.  <code>determinant</code>is a generic function that returns separately the modulus of the determinant,optionally on the logarithm scale, and the sign of the determinant.
Detach a database, i.e., remove it from the <code>search()</code>path of available <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects.  Usually this is either a<code>data.frame</code> which has been <code>attach</code>ed or apackage which was attached by <code>library</code>.
<code>det</code> calculates the determinant of a matrix.  <code>determinant</code>is a generic function that returns separately the modulus of the determinant,optionally on the logarithm scale, and the sign of the determinant.
<code>det</code> calculates the determinant of a matrix.  <code>determinant</code>is a generic function that returns separately the modulus of the determinant,optionally on the logarithm scale, and the sign of the determinant.
Writes an ASCII text representation of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object to a file, the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>console, or a connection, or uses one to recreate the object.
Extract or replace the diagonal of a matrix,or construct a diagonal matrix.
Extract or replace the diagonal of a matrix,or construct a diagonal matrix.
Returns suitably lagged and iterated differences.
Returns suitably lagged and iterated differences.
Returns suitably lagged and iterated differences.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Returns suitably lagged and iterated differences.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Special mathematical functions related to the beta and gammafunctions.
Retrieve or set the dimension of an object.
Retrieve or set the dimension of an object.
Retrieve or set the dimension of an object.
Retrieve or set the dimnames of an object.
Retrieve or set the dimnames of an object.
Retrieve or set the dimnames of an object.
Retrieve or set the dimnames of an object.
These functions produce a character vector of the names of files ordirectories in the named directory.
These functions provide a low-level interface to the computer'sfile system.
These functions provide a low-level interface to the computer'sfile system.
<code>basename</code> removes all of the path up to and including the lastpath separator (if any).

<code>dirname</code> returns the part of the <code>path</code> up to butexcluding the last path separator, or <code>"."</code> if there is no pathseparator.
<code>do.call</code> constructs and executes a function call from a name ora function and a list of arguments to be passed to it.
The <code>dontCheck</code> function is the same as <code>identity</code>, but is interpreted by <code>R CMD check</code> code analysis as a directiveto suppress checking of <code>x</code>.  Currently this is only used by<code>checkFF(registration = TRUE)</code>when checking the <code>.NAME</code> argument of foreign function calls.
Create, coerce to or test for a double-precision vector.
Writes an ASCII text representation of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object to a file, the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>console, or a connection, or uses one to recreate the object.
Single or double quote text by combining with appropriate single ordouble left and right quotation marks.
Delete the dimensions of an array which have only one level.
The function <code>droplevels</code> is used to drop unused levels from a<code>factor</code> or, more commonly, from factors in a data frame.
The function <code>droplevels</code> is used to drop unused levels from a<code>factor</code> or, more commonly, from factors in a data frame.
The function <code>droplevels</code> is used to drop unused levels from a<code>factor</code> or, more commonly, from factors in a data frame.
This function takes a vector of names of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects and producestext representations of the objects on a file or connection.A <code>dump</code> file can usually be <code>source</code>d into another<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
<code>duplicated()</code> determines which elements of a vector or dataframe are duplicatesof elements with smaller subscripts, and returns a logical vectorindicating which elements (rows) are duplicates.

<code>anyDuplicated(.)</code> is a “generalized” more efficientversion <code>any(duplicated(.))</code>, returning positive integer indicesinstead of just <code>TRUE</code>.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
Load or unload DLLs (also known as shared objects), and test whether aC function or Fortran subroutine is available.
Load or unload DLLs (also known as shared objects), and test whether aC function or Fortran subroutine is available.
Search by name for an object (<code>get</code>) or zero or more objects(<code>mget</code>).
<code>eapply</code> applies <code>FUN</code> to the named values from an<code>environment</code> and returns the results as a list.  The usercan request that all named objects are used (normally names that beginwith a dot are not).  The output is not sorted and no enclosingenvironments are searched.
Computes eigenvalues and eigenvectors of numeric (double, integer,logical) or complex matrices.
Get, set, test for and create environments.
Read or set the declared encodings for a character vector.
Read or set the declared encodings for a character vector.
<code>encodeString</code> escapes the strings in a character vector in thesame way <code>print.default</code> does, and optionally fits the encodedstrings within a field width.
Read or set the declared encodings for a character vector.
Read or set the declared encodings for a character vector.
Determines if entries of <code>x</code> start or end with string (entries of)<code>prefix</code> or <code>suffix</code> respectively, where strings arerecycled to common lengths.
<code>substitute</code> returns the parse tree for the (unevaluated)expression <code>expr</code>, substituting any variables bound in<code>env</code>.

<code>quote</code> simply returns its argument. The argument is not evaluatedand can be any R expression.

<code>enquote</code> is a simple one-line utility which transforms a call ofthe form <code>Foo(....)</code> into the call <code>quote(Foo(....))</code>.  Thisis typically used to protect a <code>call</code> from early evaluation.
Get, set, test for and create environments.
Get, set, test for and create environments.
Get, set, test for and create environments.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
Get, set, test for and create environments.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in a specified environment.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in a specified environment.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in a specified environment.
Look for an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object of the given name and possibly return it
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
Create a data frame from all combinations of the supplied vectors orfactors.  See the description of the return value for precise details ofthe way this is done.
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
Creates or tests for objects of mode <code>"expression"</code>.
Report versions of (external) third-party software used.
Create or test for objects of type <code>"logical"</code>, and the basiclogical constants.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
Special mathematical functions related to the beta and gammafunctions.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Utility function to access information about files on the user'sfile systems.
These functions provide a low-level interface to the computer'sfile system.
Choose a file interactively.
These functions provide a low-level interface to the computer'sfile system.
These functions provide a low-level interface to the computer'sfile system.
These functions provide a low-level interface to the computer'sfile system.
Utility function to extract information about files on the user'sfile systems.
These functions provide a low-level interface to the computer'sfile system.
Utility function to extract information about files on the user'sfile systems.
Utility function to extract information about files on the user'sfile systems.
Construct the path to a file from components in a platform-independentway.
These functions provide a low-level interface to the computer'sfile system.
These functions provide a low-level interface to the computer'sfile system.
Display one or more (plain) text files, in a platformspecific way, typically via a ‘pager’.
Utility function to extract information about files on the user'sfile systems.
These functions provide a low-level interface to the computer'sfile system.
<code>Reduce</code> uses a binary function to successively combine theelements of a given vector and a possibly given initial value.<code>Filter</code> extracts the elements of a vector for which a predicate(logical) function gives true.  <code>Find</code> and <code>Position</code> givethe first or last such element and its position in the vector,respectively.  <code>Map</code> applies a function to the correspondingelements of given vectors.  <code>Negate</code> creates the negation of agiven function.
<code>Reduce</code> uses a binary function to successively combine theelements of a given vector and a possibly given initial value.<code>Filter</code> extracts the elements of a vector for which a predicate(logical) function gives true.  <code>Find</code> and <code>Position</code> givethe first or last such element and its position in the vector,respectively.  <code>Map</code> applies a function to the correspondingelements of given vectors.  <code>Negate</code> creates the negation of agiven function.
Find the paths to one or more packages.
Given a vector of non-decreasing breakpoints in <code>vec</code>, find theinterval containing each element of <code>x</code>; i.e., if<code>i &lt;- findInterval(x,v)</code>, for each index <code>j</code> in <code>x</code><i>v[i[j]] ≤ x[j] &lt; v[i[j] + 1]</i>where <i>v[0] := - Inf</i>,<i>v[N+1] := + Inf</i>, and <code>N &lt;- length(v)</code>.At the two boundaries, the returned index may differ by 1, dependingon the optional arguments <code>rightmost.closed</code> and <code>all.inside</code>.
Internal objects in the base package most of which are only user-visiblebecause of the special nature of the base namespace.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
<code>ceiling</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the smallest integers not less than thecorresponding elements of <code>x</code>.

<code>floor</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the largest integers not greater than thecorresponding elements of <code>x</code>.

<code>trunc</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the integers formed by truncating the values in<code>x</code> toward <code>0</code>.

<code>round</code> rounds the values in its first argument to the specifiednumber of decimal places (default 0).  See ‘Details’ about“round to even” when rounding off a 5.

<code>signif</code> rounds the values in its first argument to the specifiednumber of significant digits.   Hence, for <code>numeric</code> <code>x</code>,<code>signif(x, dig)</code> is the same as <code>round(x, dig - ceiling(log10(abs(x))))</code>.For <code>complex</code> <code>x</code>, this is not the case, see the ‘Details’.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
These are the basic control-flow constructs of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.  Theyfunction in much the same way as control statements in any Algol-likelanguage.  They are all reserved words.
Forces the evaluation of a function argument.
Call a function with a specified number of leading arguments forcedbefore the call if the function is a closure.
Get or set the formal arguments of a <code>function</code>.
Get or set the formal arguments of a <code>function</code>.
Format an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for pretty printing.
Format an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for pretty printing.
Format an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for pretty printing.
Functions to convert between character representations and objects ofclass <code>"Date"</code> representing calendar dates.
Format an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for pretty printing.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Format an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for pretty printing.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
Information is returned on how <code>format(x, digits, nsmall)</code>would be formatted.
<code>library</code> and <code>require</code> load and attach add-on packages.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
<code>library</code> and <code>require</code> load and attach add-on packages.
Functions to convert between character representations and objects ofclasses <code>"POSIXlt"</code> and <code>"POSIXct"</code> representing calendardates and times.
Functions to convert between character representations and objects ofclasses <code>"POSIXlt"</code> and <code>"POSIXct"</code> representing calendardates and times.
<code>format.pval</code> is intended for formatting p-values.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
<code>formatC()</code> formats numbers individually and flexibly using<code>C</code> style format specifications.

<code>prettyNum()</code> is used for “prettifying” (possiblyformatted) numbers, also in <code>format.default</code>.

<code>.format.zeros(x)</code>, an auxiliary function of <code>prettyNum()</code>,re-formats the zeros in a vector <code>x</code> of formatted numbers.
Format vectors of items and their descriptions as 2-columntables or LaTeX-style description lists.
Solves a triangular system of linear equations.
These functions provide the base mechanisms for definingnew functions in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.
Special mathematical functions related to the beta and gammafunctions.
A call of <code>gc</code> causes a garbage collection to take place.<code>gcinfo</code> sets a flag so thatautomatic collection is either silent (<code>verbose = FALSE</code>) orprints memory usage statistics (<code>verbose = TRUE</code>).
This function reports the time spent in garbage collection so far inthe <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session while GC timing was enabled.
A call of <code>gc</code> causes a garbage collection to take place.<code>gcinfo</code> sets a flag so thatautomatic collection is either silent (<code>verbose = FALSE</code>) orprints memory usage statistics (<code>verbose = TRUE</code>).
Provokes garbage collection on (nearly) every memory allocation.Intended to ferret out memory protection bugs.  Also makes <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> run<em>very</em> slowly, unfortunately.
Provokes garbage collection on (nearly) every memory allocation.Intended to ferret out memory protection bugs.  Also makes <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> run<em>very</em> slowly, unfortunately.
Search by name for an object (<code>get</code>) or zero or more objects(<code>mget</code>).
Look for an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object of the given name and possibly return it
Display aspects of connections.
This is an internal function that is called from <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>'s C code todetermine the enclosing namespace of a<code>.C</code>/<code>.Call</code>/<code>.Fortran</code>/<code>.External</code> call which hasno <code>PACKAGE</code> argument. If the call has been made from a functionwithin a namespace, then we can find the DLL associated with thatnamespace.  The purpose of this is to avoid having to use the<code>PACKAGE</code> argument in these native calls and so better supportversions of packages.

This is an internal function that may be migrated to internal Ccode in the future and so should not be used by <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> programmers.
This is an internal function that is called from <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>'s C code todetermine the enclosing namespace of a<code>.C</code>/<code>.Call</code>/<code>.Fortran</code>/<code>.External</code> call which hasno <code>PACKAGE</code> argument. If the call has been made from a functionwithin a namespace, then we can find the DLL associated with thatnamespace.  The purpose of this is to avoid having to use the<code>PACKAGE</code> argument in these native calls and so better supportversions of packages.

This is an internal function that may be migrated to internal Ccode in the future and so should not be used by <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> programmers.
Display aspects of connections.
This function allows us to query the set of routinesin a DLL that are registered with R to enhancedynamic lookup, error handling when calling native routines,and potentially security in the future.This function provides a description of each of theregistered routines in the DLL for the different interfaces,i.e. <code>.C</code>, <code>.Call</code>, <code>.Fortran</code>and <code>.External</code>.
This function allows us to query the set of routinesin a DLL that are registered with R to enhancedynamic lookup, error handling when calling native routines,and potentially security in the future.This function provides a description of each of theregistered routines in the DLL for the different interfaces,i.e. <code>.C</code>, <code>.Call</code>, <code>.Fortran</code>and <code>.External</code>.
This function allows us to query the set of routinesin a DLL that are registered with R to enhancedynamic lookup, error handling when calling native routines,and potentially security in the future.This function provides a description of each of theregistered routines in the DLL for the different interfaces,i.e. <code>.C</code>, <code>.Call</code>, <code>.Fortran</code>and <code>.External</code>.
Operators acting on vectors, matrices, arrays and lists to extract orreplace parts.
<code>stop</code> stops execution of the current expression and executesan error action.

<code>geterrmessage</code> gives the last error message.
Internal functions to support reflection on namespace objects.
These functions allow users to set actions to be taken before packagesare attached/detached and namespaces are (un)loaded.
This function provides a way to get a list of all the DLLs (see<code>dyn.load</code>) that are currently loaded in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
Internal functions to support reflection on namespace objects.
Internal functions to support reflection on namespace objects.
Internal functions to support reflection on namespace objects.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Internal functions to support reflection on namespace objects.
Internal functions to support reflection on namespace objects.
Internal functions to support reflection on namespace objects.
This finds and returns a description of one or more dynamically loadedor ‘exported’ built-in native symbols.  For each name, itreturns information about the name of the symbol, the library in whichit is located and, if available, the number of arguments it expectsand by which interface it should be called (i.e <code>.Call</code>,<code>.C</code>, <code>.Fortran</code>, or<code>.External</code>). Additionally, it returns the address of thesymbol and this can be passed to other C routines.  Specifically, thisprovides a way to explicitly share symbols between differentdynamically loaded package libraries.  Also, it provides a way toquery where symbols were resolved, and aids diagnosing strangebehavior associated with dynamic resolution.
Allow the user to set and examine a variety of global <em>options</em>which affect the way in which <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> computes and displays its results.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
This provides a way to get the names (or identifiers)for the currently registered task callbacksthat are invoked at the conclusion of each top-level task.These identifiers can be used to remove a callback.
If Native Language Support (NLS) was enabled in this build of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> (seethe <code>bindtextdomain()</code> example), attempt totranslate character vectors or set where the translations are to be found.
A wrapper for the C function <code>sprintf</code>, that returns a charactervector containing a formatted combination of text and variable values.
<code>getwd</code> returns an absolute filepath representing the currentworking directory of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process; <code>setwd(dir)</code> is used to setthe working directory to <code>dir</code>.
Generate factors by specifying the pattern of their levels.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Get, set, test for and create environments.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
<code>grepRaw</code> searches for substring <code>pattern</code> matches within araw vector <code>x</code>.
<code>grouping</code> returns a permutation which rearranges its firstargument such that identical values are adjacent to each other.  Alsoreturned as attributes are the group-wise partitioning and the maximumgroup size.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
<code>gzcon</code> provides a modified connection that wraps an existingconnection, and decompresses reads or compresses writes through thatconnection.  Standard <code>gzip</code> headers are assumed.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Change the class of an object to indicate that it should be treated‘as is’.
This uses system facilities to convert a character vector betweenencodings: the ‘i’ stands for ‘internationalization’.
This uses system facilities to convert a character vector betweenencodings: the ‘i’ stands for ‘internationalization’.
Controls the way collation is done by ICU (an optional part of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>build).
Controls the way collation is done by ICU (an optional part of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>build).
The safe and reliable way to test two objects for being<em>exactly</em> equal.  It returns <code>TRUE</code> in this case,<code>FALSE</code> in every other case.
A trivial identity function returning its argument.
These are the basic control-flow constructs of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.  Theyfunction in much the same way as control statements in any Algol-likelanguage.  They are all reserved words.
<code>ifelse</code> returns a value with the same shape as<code>test</code> which is filled with elements selectedfrom either <code>yes</code> or <code>no</code>depending on whether the element of <code>test</code>is <code>TRUE</code> or <code>FALSE</code>.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Functions to write a single <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object to a file, and to restore it.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
Creates or tests for objects of type <code>"integer"</code>.
<code>interaction</code> computes a factor which represents the interactionof the given factors.  The result of <code>interaction</code> is always unordered.
Return <code>TRUE</code> when <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> is being used interactively and<code>FALSE</code> otherwise.
Performs <b>set</b> union, intersection, (asymmetric!) difference,equality and membership on two vectors.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Conversion of UTF-8 encoded character vectors to and from integervectors representing a UTF-32 encoding.
Compute the lengths and values of runs of equal values in a vector– or the reverse operation.
Return a (temporarily) invisible copy of an object.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Creates or tests for arrays.
<code>is.atomic</code> returns <code>TRUE</code> if <code>x</code> is of an atomic type(or <code>NULL</code>) and <code>FALSE</code> otherwise.

<code>is.recursive</code> returns <code>TRUE</code> if <code>x</code> has a recursive(list-like) structure and <code>FALSE</code> otherwise.
Create or test for objects of <code>mode</code> <code>"call"</code> (or<code>"("</code>, see Details).
Create or test for objects of type <code>"character"</code>.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
Functions to check if an object is a data frame, or coerce it if possible.
Create, coerce to or test for a double-precision vector.
Performs <b>set</b> union, intersection, (asymmetric!) difference,equality and membership on two vectors.
Get, set, test for and create environments.
Creates or tests for objects of mode <code>"expression"</code>.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
<code>is.finite</code> and <code>is.infinite</code> return a vector of the samelength as <code>x</code>, indicating which elements are finite (not infiniteand not missing) or infinite.

<code>Inf</code> and <code>-Inf</code> are positive and negative infinitywhereas <code>NaN</code> means ‘Not a Number’.  (These apply to numericvalues and real and imaginary parts of complex values but not tovalues of integer vectors.)  <code>Inf</code> and <code>NaN</code> arereserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.
Checks whether its argument is a (primitive) function.
<code>is.finite</code> and <code>is.infinite</code> return a vector of the samelength as <code>x</code>, indicating which elements are finite (not infiniteand not missing) or infinite.

<code>Inf</code> and <code>-Inf</code> are positive and negative infinitywhereas <code>NaN</code> means ‘Not a Number’.  (These apply to numericvalues and real and imaginary parts of complex values but not tovalues of integer vectors.)  <code>Inf</code> and <code>NaN</code> arereserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.
Creates or tests for objects of type <code>"integer"</code>.
<code>is.language</code> returns <code>TRUE</code> if <code>x</code> is avariable <code>name</code>, a <code>call</code>, or an<code>expression</code>.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Load or unload DLLs (also known as shared objects), and test whether aC function or Fortran subroutine is available.
Create or test for objects of type <code>"logical"</code>, and the basiclogical constants.
<code>matrix</code> creates a matrix from the given set of values.

<code>as.matrix</code> attempts to turn its argument into a matrix.

<code>is.matrix</code> tests if its argument is a (strict) matrix.
<code>NA</code> is a logical constant of length 1 which contains a missingvalue indicator.  <code>NA</code> can be coerced to any other vectortype except raw.  There are also constants <code>NA_integer_</code>,<code>NA_real_</code>, <code>NA_complex_</code> and <code>NA_character_</code> of theother atomic vector types which support missing values: all of theseare reserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.

The generic function <code>is.na</code> indicates which elements are missing.

The generic function <code>is.na&lt;-</code> sets elements to <code>NA</code>.

The generic function <code>anyNA</code> implements <code>any(is.na(x))</code> in apossibly faster way (especially for atomic vectors).
<code>NA</code> is a logical constant of length 1 which contains a missingvalue indicator.  <code>NA</code> can be coerced to any other vectortype except raw.  There are also constants <code>NA_integer_</code>,<code>NA_real_</code>, <code>NA_complex_</code> and <code>NA_character_</code> of theother atomic vector types which support missing values: all of theseare reserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.

The generic function <code>is.na</code> indicates which elements are missing.

The generic function <code>is.na&lt;-</code> sets elements to <code>NA</code>.

The generic function <code>anyNA</code> implements <code>any(is.na(x))</code> in apossibly faster way (especially for atomic vectors).
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>NA</code> is a logical constant of length 1 which contains a missingvalue indicator.  <code>NA</code> can be coerced to any other vectortype except raw.  There are also constants <code>NA_integer_</code>,<code>NA_real_</code>, <code>NA_complex_</code> and <code>NA_character_</code> of theother atomic vector types which support missing values: all of theseare reserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.

The generic function <code>is.na</code> indicates which elements are missing.

The generic function <code>is.na&lt;-</code> sets elements to <code>NA</code>.

The generic function <code>anyNA</code> implements <code>any(is.na(x))</code> in apossibly faster way (especially for atomic vectors).
<code>NA</code> is a logical constant of length 1 which contains a missingvalue indicator.  <code>NA</code> can be coerced to any other vectortype except raw.  There are also constants <code>NA_integer_</code>,<code>NA_real_</code>, <code>NA_complex_</code> and <code>NA_character_</code> of theother atomic vector types which support missing values: all of theseare reserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.

The generic function <code>is.na</code> indicates which elements are missing.

The generic function <code>is.na&lt;-</code> sets elements to <code>NA</code>.

The generic function <code>anyNA</code> implements <code>any(is.na(x))</code> in apossibly faster way (especially for atomic vectors).
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
A ‘name’ (also known as a ‘symbol’) is a way to refer to<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects by name (rather than the value of the object, if any, boundto that name).

<code>as.name</code> and <code>as.symbol</code> are identical: they attempt tocoerce the argument to a name.

<code>is.symbol</code> and the identical <code>is.name</code> return <code>TRUE</code>or <code>FALSE</code> depending on whether the argument is a name or not.
<code>is.finite</code> and <code>is.infinite</code> return a vector of the samelength as <code>x</code>, indicating which elements are finite (not infiniteand not missing) or infinite.

<code>Inf</code> and <code>-Inf</code> are positive and negative infinitywhereas <code>NaN</code> means ‘Not a Number’.  (These apply to numericvalues and real and imaginary parts of complex values but not tovalues of integer vectors.)  <code>Inf</code> and <code>NaN</code> arereserved words in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.
<code>NULL</code> represents the null object in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>: it is a reservedword.  <code>NULL</code> is often returned by expressions and functionswhose value is undefined.
Creates or coerces objects of type <code>"numeric"</code>.<code>is.numeric</code> is a more general test of an object beinginterpretable as numbers.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Creates or coerces objects of type <code>"numeric"</code>.<code>is.numeric</code> is a more general test of an object beinginterpretable as numbers.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Creates or coerces objects of type <code>"numeric"</code>.<code>is.numeric</code> is a more general test of an object beinginterpretable as numbers.
A function rather for internal use.  It returns <code>TRUE</code> if theobject <code>x</code> has the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> internal <code>OBJECT</code> bit set, and<code>FALSE</code> otherwise.  The <code>OBJECT</code> bit is set when a<code>"class"</code> attribute is added and removed when that attribute isremoved, so this is a very efficient way to check if an object has aclass attribute.  (S4 objects always should.)
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Checks whether its argument is a (primitive) function.
<code>qr</code> computes the QR decomposition of a matrix.
Test if running under <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
Creates or tests for objects of type <code>"raw"</code>.
<code>is.atomic</code> returns <code>TRUE</code> if <code>x</code> is of an atomic type(or <code>NULL</code>) and <code>FALSE</code> otherwise.

<code>is.recursive</code> returns <code>TRUE</code> if <code>x</code> has a recursive(list-like) structure and <code>FALSE</code> otherwise.
<code>is.single</code> reports an error.  There are no single precisionvalues in R.
A ‘name’ (also known as a ‘symbol’) is a way to refer to<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects by name (rather than the value of the object, if any, boundto that name).

<code>as.name</code> and <code>as.symbol</code> are identical: they attempt tocoerce the argument to a name.

<code>is.symbol</code> and the identical <code>is.name</code> return <code>TRUE</code>or <code>FALSE</code> depending on whether the argument is a name or not.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
Test if an object is not sorted (in increasing order), without thecost of sorting it.
<code>vector</code> produces a vector of the given length and mode.

<code>as.vector</code>, a generic, attempts to coerce its argument into avector of mode <code>mode</code> (the default is to coerce to whichevervector mode is most convenient): if the result is atomic allattributes are removed.

<code>is.vector</code> returns <code>TRUE</code> if <code>x</code> is a vector of thespecified mode having no attributes <em>other than names</em>.  It returns<code>FALSE</code> otherwise.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
Display aspects of connections.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Set, unset or query the debugging flag on a function.The <code>text</code> and <code>condition</code> arguments are the same as thosethat can be supplied via a call to <code>browser</code>.  They can be retrievedby the user once the browser has been entered, and provide a mechanism toallow users to identify which breakpoint has been activated.
These operators act on raw, logical and number-like vectors.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Functions to load and unload name spaces.
Convenience wrappers to create date-times from numeric representations.
Convenience wrappers to create date-times from numeric representations.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Tests whether the object is an instance of an S4 class.
Functions to re-position connections.
Generic function to test if <code>object</code> is symmetric or not.Currently only a matrix method is implemented, where a<code>complex</code> matrix <code>Z</code> must be “Hermitian” for<code>isSymmetric(Z)</code> to be true.
Generic function to test if <code>object</code> is symmetric or not.Currently only a matrix method is implemented, where a<code>complex</code> matrix <code>Z</code> must be “Hermitian” for<code>isSymmetric(Z)</code> to be true.
These operators act on raw, logical and number-like vectors.
Add a small amount of noise to a numeric vector.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
The condition number of a regular (square) matrix is the product ofthe <em>norm</em> of the matrix and the norm of its inverse (orpseudo-inverse), and hence depends on the kind of matrix-norm.

<code>kappa()</code> computes by default (an estimate of) the 2-normcondition number of a matrix or of the <i>R</i> matrix of a <i>QR</i>decomposition, perhaps of a linear fit.  The 2-norm condition numbercan be shown to be the ratio of the largest to the smallest<em>non-zero</em> singular value of the matrix.

<code>rcond()</code> computes an approximation of the <b>r</b>eciprocal<b>cond</b>ition number, see the details.
The condition number of a regular (square) matrix is the product ofthe <em>norm</em> of the matrix and the norm of its inverse (orpseudo-inverse), and hence depends on the kind of matrix-norm.

<code>kappa()</code> computes by default (an estimate of) the 2-normcondition number of a matrix or of the <i>R</i> matrix of a <i>QR</i>decomposition, perhaps of a linear fit.  The 2-norm condition numbercan be shown to be the ratio of the largest to the smallest<em>non-zero</em> singular value of the matrix.

<code>rcond()</code> computes an approximation of the <b>r</b>eciprocal<b>cond</b>ition number, see the details.
The condition number of a regular (square) matrix is the product ofthe <em>norm</em> of the matrix and the norm of its inverse (orpseudo-inverse), and hence depends on the kind of matrix-norm.

<code>kappa()</code> computes by default (an estimate of) the 2-normcondition number of a matrix or of the <i>R</i> matrix of a <i>QR</i>decomposition, perhaps of a linear fit.  The 2-norm condition numbercan be shown to be the ratio of the largest to the smallest<em>non-zero</em> singular value of the matrix.

<code>rcond()</code> computes an approximation of the <b>r</b>eciprocal<b>cond</b>ition number, see the details.
The condition number of a regular (square) matrix is the product ofthe <em>norm</em> of the matrix and the norm of its inverse (orpseudo-inverse), and hence depends on the kind of matrix-norm.

<code>kappa()</code> computes by default (an estimate of) the 2-normcondition number of a matrix or of the <i>R</i> matrix of a <i>QR</i>decomposition, perhaps of a linear fit.  The 2-norm condition numbercan be shown to be the ratio of the largest to the smallest<em>non-zero</em> singular value of the matrix.

<code>rcond()</code> computes an approximation of the <b>r</b>eciprocal<b>cond</b>ition number, see the details.
Computes the generalised kronecker product of two arrays,<code>X</code> and <code>Y</code>.
Report on localization information.
Report the name of the shared object file with <code>LAPACK</code> implementationin use.
Report the version of LAPACK in use.
Compute the singular-value decomposition of a rectangular matrix.
Find a suitable set of labels from an object for use in printing orplotting, for example.  A generic function.
Find a suitable set of labels from an object for use in printing orplotting, for example.  A generic function.
<code>lapply</code> returns a list of the same length as <code>X</code>, eachelement of which is the result of applying <code>FUN</code> to thecorresponding element of <code>X</code>.

<code>sapply</code> is a user-friendly version and wrapper of <code>lapply</code>by default returning a vector, matrix or, if <code>simplify = "array"</code>, anarray if appropriate, by applying <code>simplify2array()</code>.<code>sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)</code> is the same as<code>lapply(x, f)</code>.

<code>vapply</code> is similar to <code>sapply</code>, but has a pre-specifiedtype of return value, so it can be safer (and sometimes faster) touse.

<code>replicate</code> is a wrapper for the common use of <code>sapply</code> forrepeated evaluation of an expression (which will usually involverandom number generation).

<code>simplify2array()</code> is the utility called from <code>sapply()</code>when <code>simplify</code> is not false and is similarly called from<code>mapply()</code>.
Internal functions to lazy load a database of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects.
Internal functions to lazy load a database of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects.
Internal objects in the base package most of which are only user-visiblebecause of the special nature of the base namespace.
Special mathematical functions related to the beta and gammafunctions.
Special mathematical functions related to the beta and gammafunctions.
Get or set the length of vectors (including lists) and factors, and ofany other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for which a method has been defined.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Get or set the length of vectors (including lists) and factors, and ofany other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for which a method has been defined.
Description of the class <code>"Date"</code> representing calendar dates.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Get or set the length of vectors (including lists) and factors, and ofany other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object for which a method has been defined.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Get the length of each element of a <code>list</code> or atomicvector (<code>is.atomic</code>) as an integer or numeric vector.
Constants built into <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
Constants built into <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
<code>levels</code> provides access to the levels attribute of a variable.The first form returns the value of the levels of its argumentand the second sets the attribute.
<code>levels</code> provides access to the levels attribute of a variable.The first form returns the value of the levels of its argumentand the second sets the attribute.
<code>levels</code> provides access to the levels attribute of a variable.The first form returns the value of the levels of its argumentand the second sets the attribute.
<code>levels</code> provides access to the levels attribute of a variable.The first form returns the value of the levels of its argumentand the second sets the attribute.
Special mathematical functions related to the beta and gammafunctions.
Special mathematical functions related to the beta and gammafunctions.
Report version of <code>libcurl</code> in use.
<code>library</code> and <code>require</code> load and attach add-on packages.
Load the specified file of compiled code if it has not been loadedalready, or unloads it.
Load the specified file of compiled code if it has not been loadedalready, or unloads it.
The license terms under which <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> is distributed.
The license terms under which <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> is distributed.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
These functions produce a character vector of the names of files ordirectories in the named directory.
These functions produce a character vector of the names of files ordirectories in the named directory.
Create a data frame from a list of variables.
From a <em>named</em> <code>list x</code>, create an<code>environment</code> containing all list components as objects, or“multi-assign” from <code>x</code> into a pre-existing environment.
Reload datasets written with the function <code>save</code>.
Functions to load and unload name spaces.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Functions to load and unload name spaces.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in a specified environment.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
<code>log</code> computes logarithms, by default natural logarithms,<code>log10</code> computes common (i.e., base 10) logarithms, and<code>log2</code> computes binary (i.e., base 2) logarithms.The general form <code>log(x, base)</code> computes logarithms with base<code>base</code>.

<code>log1p(x)</code> computes <i>log(1+x)</i> accurately also for<i>|x| &lt;&lt; 1</i>.

<code>exp</code> computes the exponential function.

<code>expm1(x)</code> computes <i>exp(x) - 1</i> accurately also for<i>|x| &lt;&lt; 1</i>.
Create or test for objects of type <code>"logical"</code>, and the basiclogical constants.
Returns a matrix of logicals the same size of a given matrix withentries <code>TRUE</code> in the lower or upper triangle.
<code>ls</code> and <code>objects</code> return a vector of character stringsgiving the names of the objects in the specified environment.  Wheninvoked with no argument at the top level prompt, <code>ls</code> shows whatdata sets and functions a user has defined.  When invoked with noargument inside a function, <code>ls</code> returns the names of thefunction's local variables: this is useful in conjunction with<code>browser</code>.
Make syntactically valid names out of character vectors.
Makes the elements of a character vector unique byappending sequence numbers to duplicates.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
<code>Reduce</code> uses a binary function to successively combine theelements of a given vector and a possibly given initial value.<code>Filter</code> extracts the elements of a vector for which a predicate(logical) function gives true.  <code>Find</code> and <code>Position</code> givethe first or last such element and its position in the vector,respectively.  <code>Map</code> applies a function to the correspondingelements of given vectors.  <code>Negate</code> creates the negation of agiven function.
<code>mapply</code> is a multivariate version of <code>sapply</code>.<code>mapply</code> applies <code>FUN</code> to the first elements of each ...argument, the second elements, the third elements, and so on.Arguments are recycled if necessary.

<code>.mapply()</code> is a bare-bones version of <code>mapply()</code>, e.g., to beused in other functions.
For a contingency table in array form, compute the sum of tableentries for a given margin or set of margins.
For a contingency table in array form, compute the sum of tableentries for a given margin or set of margins.
<code>mat.or.vec</code> creates an <code>nr</code> by <code>nc</code> zero matrix if<code>nc</code> is greater than 1, and a zero vector of length <code>nr</code> if<code>nc</code> equals 1.
<code>match</code> returns a vector of the positions of (first) matches ofits first argument in its second.

<code>%in%</code> is a more intuitive interface as a binary operator,which returns a logical vector indicating if there is a match or notfor its left operand.
<code>match.arg</code> matches <code>arg</code> against a table of candidatevalues as specified by <code>choices</code>, where <code>NULL</code> means to takethe first one.
<code>match.call</code> returns a call in which all of the specified arguments arespecified by their full names.
When called inside functions that take a function as argument, extractthe desired function object while avoiding undesired matching toobjects of other types.
Group generic methods can be defined for four pre-specified groups offunctions, <code>Math</code>, <code>Ops</code>, <code>Summary</code> and <code>Complex</code>.(There are no objects of these names in base <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, but there are in the<span class="pkg">methods</span> package.)

A method defined for an individual member of the group takesprecedence over a method defined for the group as a whole.
Description of the class <code>"Date"</code> representing calendar dates.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>matrix</code> creates a matrix from the given set of values.

<code>as.matrix</code> attempts to turn its argument into a matrix.

<code>is.matrix</code> tests if its argument is a (strict) matrix.
Returns the (regular or <b>p</b>arallel) maxima and minima of theinput values.

<code>pmax*()</code> and <code>pmin*()</code> take one or more vectors asarguments, recycle them to common length and return a single vectorgiving the <em>‘parallel’</em> maxima (or minima) of the argumentvectors.
Find the maximum position for each row of a matrix, breaking ties at random.
Generic function for the (trimmed) arithmetic mean.
Description of the class <code>"Date"</code> representing calendar dates.
Generic function for the (trimmed) arithmetic mean.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Query and set the maximal size of the vector heap and the maximalnumber of heap nodes for the current <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process.
Query and set the maximal size of the vector heap and the maximalnumber of heap nodes for the current <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process.
In-memory compression or decompression for raw vectors.
In-memory compression or decompression for raw vectors.
Lists the usage of the cons cells by <code>SEXPREC</code> type.
Merge two data frames by common columns or row names, or do otherversions of database <em>join</em> operations.
Merge two data frames by common columns or row names, or do otherversions of database <em>join</em> operations.
Merge two data frames by common columns or row names, or do otherversions of database <em>join</em> operations.
Generate a diagnostic message from its arguments.
Search by name for an object (<code>get</code>) or zero or more objects(<code>mget</code>).
Returns the (regular or <b>p</b>arallel) maxima and minima of theinput values.

<code>pmax*()</code> and <code>pmin*()</code> take one or more vectors asarguments, recycle them to common length and return a single vectorgiving the <em>‘parallel’</em> maxima (or minima) of the argumentvectors.
<code>missing</code> can be used to test whether a value was specifiedas an argument to a function.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
Get or set the ‘mode’ (a kind of ‘type’), or the storagemode of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
Get or set the ‘mode’ (a kind of ‘type’), or the storagemode of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
Constants built into <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
Constants built into <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
These functions access an object's attributes.The first form below returns the object's attribute list.The replacement forms uses the list on the right-handside of the assignment as the object's attributes (if appropriate).
Functions to get or set the names of an object.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Functions to get or set the names of an object.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
When used inside a function body, <code>nargs</code> returns the number ofarguments supplied to that function, <em>including</em> positionalarguments left blank.
<code>nchar</code> takes a character vector as an argument andreturns a vector whose elements contain the sizes ofthe corresponding elements of <code>x</code>. Internally, it is a generic,for which methods can be defined (see InternalMethods).

<code>nzchar</code> is a fast way to find out if elements of a charactervector are non-empty strings.
<code>nrow</code> and <code>ncol</code> return the number of rows or columnspresent in <code>x</code>.<code>NCOL</code> and <code>NROW</code> do the same treating a vector as1-column matrix, even a 0-length vector, compatibly with<code>as.matrix()</code> or <code>cbind()</code>, see the example.
<code>nrow</code> and <code>ncol</code> return the number of rows or columnspresent in <code>x</code>.<code>NCOL</code> and <code>NROW</code> do the same treating a vector as1-column matrix, even a 0-length vector, compatibly with<code>as.matrix()</code> or <code>cbind()</code>, see the example.
<code>Reduce</code> uses a binary function to successively combine theelements of a given vector and a possibly given initial value.<code>Filter</code> extracts the elements of a vector for which a predicate(logical) function gives true.  <code>Find</code> and <code>Position</code> givethe first or last such element and its position in the vector,respectively.  <code>Map</code> applies a function to the correspondingelements of given vectors.  <code>Negate</code> creates the negation of agiven function.
Get, set, test for and create environments.
These are the basic control-flow constructs of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.  Theyfunction in much the same way as control statements in any Algol-likelanguage.  They are all reserved words.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class(es) of the first argument to the generic function or ofthe object supplied as an argument to <code>UseMethod</code> or <code>NextMethod</code>.
If Native Language Support (NLS) was enabled in this build of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> (seethe <code>bindtextdomain()</code> example), attempt totranslate character vectors or set where the translations are to be found.
Return the number of levels which its argument has.
Print character strings without quotes.
Computes a matrix norm of <code>x</code> using LAPACK.  The norm can bethe one (<code>"O"</code>) norm, the infinity (<code>"I"</code>) norm, theFrobenius (<code>"F"</code>) norm, the maximum modulus (<code>"M"</code>) amongelements of a matrix, or the “spectral” or <code>"2"</code>-norm, asdetermined by the value of <code>type</code>.
Convert file paths to canonical form for the platform, to display themin a user-understandable form and so that relative and absolute paths canbe compared.
<code>nrow</code> and <code>ncol</code> return the number of rows or columnspresent in <code>x</code>.<code>NCOL</code> and <code>NROW</code> do the same treating a vector as1-column matrix, even a 0-length vector, compatibly with<code>as.matrix()</code> or <code>cbind()</code>, see the example.
<code>nrow</code> and <code>ncol</code> return the number of rows or columnspresent in <code>x</code>.<code>NCOL</code> and <code>NROW</code> do the same treating a vector as1-column matrix, even a 0-length vector, compatibly with<code>as.matrix()</code> or <code>cbind()</code>, see the example.
Display aspects of connections.
Creates or coerces objects of type <code>"numeric"</code>.<code>is.numeric</code> is a more general test of an object beinginterpretable as numbers.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
<code>nchar</code> takes a character vector as an argument andreturns a vector whose elements contain the sizes ofthe corresponding elements of <code>x</code>. Internally, it is a generic,for which methods can be defined (see InternalMethods).

<code>nzchar</code> is a fast way to find out if elements of a charactervector are non-empty strings.
<code>ls</code> and <code>objects</code> return a vector of character stringsgiving the names of the objects in the specified environment.  Wheninvoked with no argument at the top level prompt, <code>ls</code> shows whatdata sets and functions a user has defined.  When invoked with noargument inside a function, <code>ls</code> returns the names of thefunction's local variables: this is useful in conjunction with<code>browser</code>.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
Information about time zones in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.  <code>Sys.timezone</code> returnsthe name of the current time zone.
<code>on.exit</code> records the expression given as its argument as needingto be executed when the current function exits (either naturally or asthe result of an error).  This is useful for resetting graphicalparameters or performing other cleanup actions.

If no expression is provided, i.e., the call is <code>on.exit()</code>, thenthe current <code>on.exit</code> code is removed.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
Group generic methods can be defined for four pre-specified groups offunctions, <code>Math</code>, <code>Ops</code>, <code>Summary</code> and <code>Complex</code>.(There are no objects of these names in base <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, but there are in the<span class="pkg">methods</span> package.)

A method defined for an individual member of the group takesprecedence over a method defined for the group as a whole.
Operators for the <code>"Date"</code> class.

There is an <code>Ops</code> method and specificmethods for <code>+</code> and <code>-</code> for the <code>Date</code> class.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Allow the user to set and examine a variety of global <em>options</em>which affect the way in which <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> computes and displays its results.
<code>order</code> returns a permutation which rearranges its firstargument into ascending or descending order, breaking ties by furtherarguments.  <code>sort.list</code> does the same, using only one argument.<br>See the examples for how to use these functions to sort data frames,etc.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
The outer product of the arrays <code>X</code> and <code>Y</code> is the array<code>A</code> with dimension <code>c(dim(X), dim(Y))</code> where element<code>A[c(arrayindex.x, arrayindex.y)]    = FUN(X[arrayindex.x], Y[arrayindex.y], ...)</code>.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
These functions allow users to set actions to be taken before packagesare attached/detached and namespaces are (un)loaded.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Find the paths to one or more packages.
Generate a diagnostic message from its arguments.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Functions to construct, coerce and check for both kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> lists.
Get, set, test for and create environments.
Get, set, test for and create environments.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
<code>parse()</code> returns the parsed but unevaluated expressions in an<code>expression</code>, a “list” of <code>call</code>s.

<code>str2expression(s)</code> and <code>str2lang(s)</code> return special versionsof <code>parse(text=s, keep.source=FALSE)</code> and can therefore be regarded astransforming character strings <code>s</code> to expressions, calls, etc.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Concatenate vectors after converting to character.
Concatenate vectors after converting to character.
Expand a path name, for example by replacing a leading tilde by theuser's home directory (if defined on that platform).
Find the paths to one or more packages.
Report some of the configuration options of the version of PCRE in usein this <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
Constants built into <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Generic function for plotting of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects.

For simple scatter plots, <code>plot.default</code> will be used.However, there are <code>plot</code> methods for many <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects,including <code>function</code>s, <code>data.frame</code>s,<code>density</code> objects, etc.  Use <code>methods(plot)</code> andthe documentation for these. Most of these methods are implementedusing traditional graphics (the <span class="pkg">graphics</span> package), but this isnot mandatory.

For more details about graphical parameter arguments used bytraditional graphics, see <code>par</code>.
<code>pmatch</code> seeks matches for the elements of its first argumentamong those of its second.
Returns the (regular or <b>p</b>arallel) maxima and minima of theinput values.

<code>pmax*()</code> and <code>pmin*()</code> take one or more vectors asarguments, recycle them to common length and return a single vectorgiving the <em>‘parallel’</em> maxima (or minima) of the argumentvectors.
Returns the (regular or <b>p</b>arallel) maxima and minima of theinput values.

<code>pmax*()</code> and <code>pmin*()</code> take one or more vectors asarguments, recycle them to common length and return a single vectorgiving the <em>‘parallel’</em> maxima (or minima) of the argumentvectors.
Returns the (regular or <b>p</b>arallel) maxima and minima of theinput values.

<code>pmax*()</code> and <code>pmin*()</code> take one or more vectors asarguments, recycle them to common length and return a single vectorgiving the <em>‘parallel’</em> maxima (or minima) of the argumentvectors.
Returns the (regular or <b>p</b>arallel) maxima and minima of theinput values.

<code>pmax*()</code> and <code>pmin*()</code> take one or more vectors asarguments, recycle them to common length and return a single vectorgiving the <em>‘parallel’</em> maxima (or minima) of the argumentvectors.
Find zeros of a real or complex polynomial.
Returns the environment at a specified position in the search path.
<code>Reduce</code> uses a binary function to successively combine theelements of a given vector and a possibly given initial value.<code>Filter</code> extracts the elements of a vector for which a predicate(logical) function gives true.  <code>Find</code> and <code>Position</code> givethe first or last such element and its position in the vector,respectively.  <code>Map</code> applies a function to the correspondingelements of given vectors.  <code>Negate</code> creates the negation of agiven function.
Compute a  sequence of about <code>n+1</code> equally spaced ‘round’values which cover the range of the values in <code>x</code>.The values are chosen so that they are 1, 2 or 5 times a power of 10.
Compute a  sequence of about <code>n+1</code> equally spaced ‘round’values which cover the range of the values in <code>x</code>.The values are chosen so that they are 1, 2 or 5 times a power of 10.
<code>formatC()</code> formats numbers individually and flexibly using<code>C</code> style format specifications.

<code>prettyNum()</code> is used for “prettifying” (possiblyformatted) numbers, also in <code>format.default</code>.

<code>.format.zeros(x)</code>, an auxiliary function of <code>prettyNum()</code>,re-formats the zeros in a vector <code>x</code> of formatted numbers.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
Change the class of an object to indicate that it should be treated‘as is’.
Function <code>by</code> is an object-oriented wrapper for<code>tapply</code> applied to data frames.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Print a data frame.
Description of the class <code>"Date"</code> representing calendar dates.
<code>print.default</code> is the <em>default</em> method of the generic<code>print</code> function which prints its argument.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
This function provides a way to get a list of all the DLLs (see<code>dyn.load</code>) that are currently loaded in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
This function provides a way to get a list of all the DLLs (see<code>dyn.load</code>) that are currently loaded in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
This function allows us to query the set of routinesin a DLL that are registered with R to enhancedynamic lookup, error handling when calling native routines,and potentially security in the future.This function provides a description of each of theregistered routines in the DLL for the different interfaces,i.e. <code>.C</code>, <code>.Call</code>, <code>.Fortran</code>and <code>.External</code>.
Computes eigenvalues and eigenvectors of numeric (double, integer,logical) or complex matrices.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
Convert or print integers in hexadecimal format, with as many digitsas are needed to display the largest, using leading zeroes asnecessary.
<code>library</code> and <code>require</code> load and attach add-on packages.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
This function allows us to query the set of routinesin a DLL that are registered with R to enhancedynamic lookup, error handling when calling native routines,and potentially security in the future.This function provides a description of each of theregistered routines in the DLL for the different interfaces,i.e. <code>.C</code>, <code>.Call</code>, <code>.Fortran</code>and <code>.External</code>.
Print character strings without quotes.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Convert or print integers in octal format, with as many digits as areneeded to display the largest, using leading zeroes as necessary.
<code>library</code> and <code>require</code> load and attach add-on packages.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>proc.time</code> determines how much real and CPU time (in seconds)the currently running <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process has already taken.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Compute the lengths and values of runs of equal values in a vector– or the reverse operation.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
<code>print</code> prints its argument and returns it <em>invisibly</em> (via<code>invisible(x)</code>).  It is a generic function which means thatnew printing methods can be easily added for new <code>class</code>es.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
An earlier method for printing matrices, provided for S compatibility.
<code>proc.time</code> determines how much real and CPU time (in seconds)the currently running <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process has already taken.
<code>prod</code> returns the product of all the valuespresent in its arguments.
Returns conditional proportions given <code>margins</code>, i.e. entries of <code>x</code>, divided by the appropriate marginal sums. 
Returns conditional proportions given <code>margins</code>, i.e. entries of <code>x</code>, divided by the appropriate marginal sums. 
Retrieve or set the dimnames of an object.
Special mathematical functions related to the beta and gammafunctions.
Functions to push back text lines onto a connection, and to enquirehow many lines are currently pushed back.
Functions to push back text lines onto a connection, and to enquirehow many lines are currently pushed back.
The function <code>quit</code> or its alias <code>q</code> terminate the current<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
<code>qr</code> computes the QR decomposition of a matrix.
<code>qr</code> computes the QR decomposition of a matrix.
<code>qr</code> computes the QR decomposition of a matrix.
<code>qr</code> computes the QR decomposition of a matrix.
Returns the original matrix from which the object was constructed orthe components of the decomposition.
<code>qr</code> computes the QR decomposition of a matrix.
<code>qr</code> computes the QR decomposition of a matrix.
Returns the original matrix from which the object was constructed orthe components of the decomposition.
<code>qr</code> computes the QR decomposition of a matrix.
<code>qr</code> computes the QR decomposition of a matrix.
Returns the original matrix from which the object was constructed orthe components of the decomposition.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
The function <code>quit</code> or its alias <code>q</code> terminate the current<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
<code>substitute</code> returns the parse tree for the (unevaluated)expression <code>expr</code>, substituting any variables bound in<code>env</code>.

<code>quote</code> simply returns its argument. The argument is not evaluatedand can be any R expression.

<code>enquote</code> is a simple one-line utility which transforms a call ofthe form <code>Foo(....)</code> into the call <code>quote(Foo(....))</code>.  Thisis typically used to protect a <code>call</code> from early evaluation.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Return the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> home directory, or the full path to a component of the<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> installation.
<code>R.Version()</code> provides detailed information about the version of<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> running.

<code>R.version</code> is a variable (a <code>list</code>) holding thisinformation (and <code>version</code> is a copy of it for S compatibility).
<code>R.Version()</code> provides detailed information about the version of<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> running.

<code>R.version</code> is a variable (a <code>list</code>) holding thisinformation (and <code>version</code> is a copy of it for S compatibility).
<code>R.Version()</code> provides detailed information about the version of<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> running.

<code>R.version</code> is a variable (a <code>list</code>) holding thisinformation (and <code>version</code> is a copy of it for S compatibility).
<code>range</code> returns a vector containing the minimum and maximum ofall the given arguments.
<code>range</code> returns a vector containing the minimum and maximum ofall the given arguments.
Returns the sample ranks of the values in a vector.  Ties (i.e., equalvalues) and missing values can be handled in several ways.
<code>rapply</code> is a recursive version of <code>lapply</code> withflexibility in <em>how</em> the result is structured (<code>how = ".."</code>).
Creates or tests for objects of type <code>"raw"</code>.
Input and output raw connections.
Input and output raw connections.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Conversion to and from and manipulation of objects of type <code>"raw"</code>,both used as bits or “packed” 8 bits.
Take a sequence of vector, matrix or data-frame arguments and combineby <em>c</em>olumns or <em>r</em>ows, respectively.  These are genericfunctions with methods for other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> classes.
Take a sequence of vector, matrix or data-frame arguments and combineby <em>c</em>olumns or <em>r</em>ows, respectively.  These are genericfunctions with methods for other <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> classes.
The condition number of a regular (square) matrix is the product ofthe <em>norm</em> of the matrix and the norm of its inverse (orpseudo-inverse), and hence depends on the kind of matrix-norm.

<code>kappa()</code> computes by default (an estimate of) the 2-normcondition number of a matrix or of the <i>R</i> matrix of a <i>QR</i>decomposition, perhaps of a linear fit.  The 2-norm condition numbercan be shown to be the ratio of the largest to the smallest<em>non-zero</em> singular value of the matrix.

<code>rcond()</code> computes an approximation of the <b>r</b>eciprocal<b>cond</b>ition number, see the details.
Basic functions which support complex arithmetic in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, in addition tothe arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code>.
Reads or writes an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object from/to a file in Debian Control Fileformat.
Read binary data from or write binary data to a connection or raw vector.
Transfer character strings to and from connections, without assumingthey are null-terminated on the connection.
<code>readline</code> reads a line from the terminal (in interactive use).
Read some or all text lines from a connection.
Functions to write a single <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object to a file, and to restore it.
Read as file such as ‘<span class="file">.Renviron</span>’ or ‘<span class="file">Renviron.site</span>’ in theformat described in the help for Startup, and set environmentvariables as defined in the file.
<code>Recall</code> is used as a placeholder for the name of the functionin which it is called.  It allows the definition of recursivefunctions which still work after being renamed, see example below.
<code>Reduce</code> uses a binary function to successively combine theelements of a given vector and a possibly given initial value.<code>Filter</code> extracts the elements of a vector for which a predicate(logical) function gives true.  <code>Find</code> and <code>Position</code> givethe first or last such element and its position in the vector,respectively.  <code>Map</code> applies a function to the correspondingelements of given vectors.  <code>Negate</code> creates the negation of agiven function.
Registers an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function to be called upon garbage collection ofobject or (optionally) at the end of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> session.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Extract or replace matched substrings from match data obtained by<code>regexpr</code>, <code>gregexpr</code>,<code>regexec</code> or <code>gregexec</code>.
Extract or replace matched substrings from match data obtained by<code>regexpr</code>, <code>gregexpr</code>,<code>regexec</code> or <code>gregexec</code>.
<code>remove</code> and <code>rm</code> can be used to remove objects.  These canbe specified successively as character strings, or in the charactervector <code>list</code>, or through a combination of both.  All objectsthus specified will be removed.

If <code>envir</code> is NULL then the currently active environment issearched first.

If <code>inherits</code> is <code>TRUE</code> then parents of the supplieddirectory are searched until a variable with the given name isencountered.  A warning is printed for each variable that is notfound.
<code>addTaskCallback</code> registers an R functionthat is to be called each time a top-level taskis completed.

<code>removeTaskCallback</code> un-registers a functionthat was registered earlier via <code>addTaskCallback</code>.

These provide low-level access to the internal/nativemechanism for managing task-completion actions.One can use <code>taskCallbackManager</code>at the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>-language level to manage <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> functionsthat are called at the completion of each task.This is easier and more direct.
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
<code>rep</code> replicates the values in <code>x</code>.  It is a genericfunction, and the (internal) default method is described here.

<code>rep.int</code> and <code>rep_len</code> are faster simplified versions fortwo common cases.  Internally, they are generic, so methods can bedefined for them (see InternalMethods).
These are the basic control-flow constructs of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.  Theyfunction in much the same way as control statements in any Algol-likelanguage.  They are all reserved words.
<code>replace</code> replaces the values in <code>x</code>with indices given in <code>list</code> by those given in <code>values</code>.If necessary, the values in <code>values</code> are recycled.
<code>lapply</code> returns a list of the same length as <code>X</code>, eachelement of which is the result of applying <code>FUN</code> to thecorresponding element of <code>X</code>.

<code>sapply</code> is a user-friendly version and wrapper of <code>lapply</code>by default returning a vector, matrix or, if <code>simplify = "array"</code>, anarray if appropriate, by applying <code>simplify2array()</code>.<code>sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)</code> is the same as<code>lapply(x, f)</code>.

<code>vapply</code> is similar to <code>sapply</code>, but has a pre-specifiedtype of return value, so it can be safer (and sometimes faster) touse.

<code>replicate</code> is a wrapper for the common use of <code>sapply</code> forrepeated evaluation of an expression (which will usually involverandom number generation).

<code>simplify2array()</code> is the utility called from <code>sapply()</code>when <code>simplify</code> is not false and is similarly called from<code>mapply()</code>.
<code>library</code> and <code>require</code> load and attach add-on packages.
Functions to load and unload name spaces.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
This function marks an object so that a message is printed whenever theinternal code copies the object.  It is amajor cause of hard-to-predict memory use in R.
These functions provide the base mechanisms for definingnew functions in the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.
A call to <code>trace</code> allows you to insert debugging code (e.g., acall to <code>browser</code> or <code>recover</code>) at chosenplaces in any function.  A call to <code>untrace</code> cancels the tracing.Specified methods can be traced the same way, without tracing allcalls to the generic function.  Trace code (<code>tracer</code>) can be any<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression.  Tracing can be temporarily turned on or off globallyby calling <code>tracingState</code>.
<code>rev</code> provides a reversed version of its argument.  It is genericfunction with a default method for vectors and one for<code>dendrogram</code>s.

Note that this is no longer needed (nor efficient) for obtainingvectors sorted into descending order, since that is now rather moredirectly achievable by <code>sort(x, decreasing = TRUE)</code>.
<code>rev</code> provides a reversed version of its argument.  It is genericfunction with a default method for vectors and one for<code>dendrogram</code>s.

Note that this is no longer needed (nor efficient) for obtainingvectors sorted into descending order, since that is now rather moredirectly achievable by <code>sort(x, decreasing = TRUE)</code>.
Compute the lengths and values of runs of equal values in a vector– or the reverse operation.
<code>remove</code> and <code>rm</code> can be used to remove objects.  These canbe specified successively as character strings, or in the charactervector <code>list</code>, or through a combination of both.  All objectsthus specified will be removed.

If <code>envir</code> is NULL then the currently active environment issearched first.

If <code>inherits</code> is <code>TRUE</code> then parents of the supplieddirectory are searched until a variable with the given name isencountered.  A warning is printed for each variable that is notfound.
<code>.Random.seed</code> is an integer vector, containing the random numbergenerator (RNG) <b>state</b> for random number generation in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.  Itcan be saved and restored, but should not be altered by the user.

<code>RNGkind</code> is a more friendly interface to query or set the kindof RNG in use.

<code>RNGversion</code> can be used to set the random generators as theywere in an earlier <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> version (for reproducibility).

<code>set.seed</code> is the recommended way to specify seeds.
<code>.Random.seed</code> is an integer vector, containing the random numbergenerator (RNG) <b>state</b> for random number generation in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.  Itcan be saved and restored, but should not be altered by the user.

<code>RNGkind</code> is a more friendly interface to query or set the kindof RNG in use.

<code>RNGversion</code> can be used to set the random generators as theywere in an earlier <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> version (for reproducibility).

<code>set.seed</code> is the recommended way to specify seeds.
<code>ceiling</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the smallest integers not less than thecorresponding elements of <code>x</code>.

<code>floor</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the largest integers not greater than thecorresponding elements of <code>x</code>.

<code>trunc</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the integers formed by truncating the values in<code>x</code> toward <code>0</code>.

<code>round</code> rounds the values in its first argument to the specifiednumber of decimal places (default 0).  See ‘Details’ about“round to even” when rounding off a 5.

<code>signif</code> rounds the values in its first argument to the specifiednumber of significant digits.   Hence, for <code>numeric</code> <code>x</code>,<code>signif(x, dig)</code> is the same as <code>round(x, dig - ceiling(log10(abs(x))))</code>.For <code>complex</code> <code>x</code>, this is not the case, see the ‘Details’.
Round or truncate date-time objects.
Round or truncate date-time objects.
Returns a matrix of integers indicating their row number in amatrix-like object, or a factor indicating the row labels.
All data frames have row names, a character vector oflength the number of rows with no duplicates nor missing values.

There are generic functions for getting and setting row names,with default methods for arrays.The description here is for the <code>data.frame</code> method.

<code>`.rowNamesDF&lt;-`</code> is a (non-generic replacement) function to setrow names for data frames, with extra argument <code>make.names</code>.This function only exists as workaround as we cannot easily change the<code>row.names&lt;-</code> generic without breaking legacy code in existing packages.
All data frames have row names, a character vector oflength the number of rows with no duplicates nor missing values.

There are generic functions for getting and setting row names,with default methods for arrays.The description here is for the <code>data.frame</code> method.

<code>`.rowNamesDF&lt;-`</code> is a (non-generic replacement) function to setrow names for data frames, with extra argument <code>make.names</code>.This function only exists as workaround as we cannot easily change the<code>row.names&lt;-</code> generic without breaking legacy code in existing packages.
All data frames have row names, a character vector oflength the number of rows with no duplicates nor missing values.

There are generic functions for getting and setting row names,with default methods for arrays.The description here is for the <code>data.frame</code> method.

<code>`.rowNamesDF&lt;-`</code> is a (non-generic replacement) function to setrow names for data frames, with extra argument <code>make.names</code>.This function only exists as workaround as we cannot easily change the<code>row.names&lt;-</code> generic without breaking legacy code in existing packages.
All data frames have row names, a character vector oflength the number of rows with no duplicates nor missing values.

There are generic functions for getting and setting row names,with default methods for arrays.The description here is for the <code>data.frame</code> method.

<code>`.rowNamesDF&lt;-`</code> is a (non-generic replacement) function to setrow names for data frames, with extra argument <code>make.names</code>.This function only exists as workaround as we cannot easily change the<code>row.names&lt;-</code> generic without breaking legacy code in existing packages.
All data frames have row names, a character vector oflength the number of rows with no duplicates nor missing values.

There are generic functions for getting and setting row names,with default methods for arrays.The description here is for the <code>data.frame</code> method.

<code>`.rowNamesDF&lt;-`</code> is a (non-generic replacement) function to setrow names for data frames, with extra argument <code>make.names</code>.This function only exists as workaround as we cannot easily change the<code>row.names&lt;-</code> generic without breaking legacy code in existing packages.
All data frames have row names, a character vector oflength the number of rows with no duplicates nor missing values.

There are generic functions for getting and setting row names,with default methods for arrays.The description here is for the <code>data.frame</code> method.

<code>`.rowNamesDF&lt;-`</code> is a (non-generic replacement) function to setrow names for data frames, with extra argument <code>make.names</code>.This function only exists as workaround as we cannot easily change the<code>row.names&lt;-</code> generic without breaking legacy code in existing packages.
Form row and column sums and means for numeric arrays (or data frames).
Retrieve or set the row or column names of a matrix-like object.
Retrieve or set the row or column names of a matrix-like object.
Compute column sums across rows of a numeric matrix-like object foreach level of a grouping variable.  <code>rowsum</code> is generic, with amethod for data frames and a default method for vectors and matrices.
Compute column sums across rows of a numeric matrix-like object foreach level of a grouping variable.  <code>rowsum</code> is generic, with amethod for data frames and a default method for vectors and matrices.
Compute column sums across rows of a numeric matrix-like object foreach level of a grouping variable.  <code>rowsum</code> is generic, with amethod for data frames and a default method for vectors and matrices.
Form row and column sums and means for numeric arrays (or data frames).
<code>sample</code> takes a sample of the specified size from the elementsof <code>x</code> using either with or without replacement.
<code>sample</code> takes a sample of the specified size from the elementsof <code>x</code> using either with or without replacement.
<code>lapply</code> returns a list of the same length as <code>X</code>, eachelement of which is the result of applying <code>FUN</code> to thecorresponding element of <code>X</code>.

<code>sapply</code> is a user-friendly version and wrapper of <code>lapply</code>by default returning a vector, matrix or, if <code>simplify = "array"</code>, anarray if appropriate, by applying <code>simplify2array()</code>.<code>sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)</code> is the same as<code>lapply(x, f)</code>.

<code>vapply</code> is similar to <code>sapply</code>, but has a pre-specifiedtype of return value, so it can be safer (and sometimes faster) touse.

<code>replicate</code> is a wrapper for the common use of <code>sapply</code> forrepeated evaluation of an expression (which will usually involverandom number generation).

<code>simplify2array()</code> is the utility called from <code>sapply()</code>when <code>simplify</code> is not false and is similarly called from<code>mapply()</code>.
<code>save</code> writes an external representation of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects to thespecified file.  The objects can be read back from the file at a laterdate by using the function <code>load</code> or <code>attach</code>(or <code>data</code> in some cases).

<code>save.image()</code> is just a short-cut for ‘save my currentworkspace’, i.e., <code>save(list = ls(all.names = TRUE), file =    ".RData", envir = .GlobalEnv)</code>.It is also what happens with <code>q("yes")</code>.
<code>save</code> writes an external representation of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects to thespecified file.  The objects can be read back from the file at a laterdate by using the function <code>load</code> or <code>attach</code>(or <code>data</code> in some cases).

<code>save.image()</code> is just a short-cut for ‘save my currentworkspace’, i.e., <code>save(list = ls(all.names = TRUE), file =    ".RData", envir = .GlobalEnv)</code>.It is also what happens with <code>q("yes")</code>.
Functions to write a single <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object to a file, and to restore it.
<code>scale</code> is generic function whose default method centers and/orscales the columns of a numeric matrix.
<code>scale</code> is generic function whose default method centers and/orscales the columns of a numeric matrix.
Read data into a vector or list from the console or file.
Gives a list of <code>attach</code>ed <em>packages</em>(see <code>library</code>), and <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects, usually<code>data.frames</code>.
Gives a list of <code>attach</code>ed <em>packages</em>(see <code>library</code>), and <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> objects, usually<code>data.frames</code>.
Functions to re-position connections.
Functions to re-position connections.
Generate regular sequences.  <code>seq</code> is a standard generic with adefault method.  <code>seq.int</code> is a primitive which can bemuch faster but has a few restrictions.  <code>seq_along</code> and<code>seq_len</code> are very fast primitives for two common cases.
Generate regular sequences.  <code>seq</code> is a standard generic with adefault method.  <code>seq.int</code> is a primitive which can bemuch faster but has a few restrictions.  <code>seq_along</code> and<code>seq_len</code> are very fast primitives for two common cases.
Generate regular sequences.  <code>seq</code> is a standard generic with adefault method.  <code>seq.int</code> is a primitive which can bemuch faster but has a few restrictions.  <code>seq_along</code> and<code>seq_len</code> are very fast primitives for two common cases.
The method for <code>seq</code> for objects of class<code>"Date"</code> representing calendar dates.
Generate regular sequences.  <code>seq</code> is a standard generic with adefault method.  <code>seq.int</code> is a primitive which can bemuch faster but has a few restrictions.  <code>seq_along</code> and<code>seq_len</code> are very fast primitives for two common cases.
Generate regular sequences.  <code>seq</code> is a standard generic with adefault method.  <code>seq.int</code> is a primitive which can bemuch faster but has a few restrictions.  <code>seq_along</code> and<code>seq_len</code> are very fast primitives for two common cases.
The method for <code>seq</code> for date-time classes.
The default method for <code>sequence</code> generates the sequence<code>seq(from[i], by = by[i], length.out = nvec[i])</code> for eachelement <code>i</code> in the parallel (and recycled) vectors <code>from</code>,<code>by</code> and <code>nvec</code>. It then returns the result of concatenatingthose sequences.
The default method for <code>sequence</code> generates the sequence<code>seq(from[i], by = by[i], length.out = nvec[i])</code> for eachelement <code>i</code> in the parallel (and recycled) vectors <code>from</code>,<code>by</code> and <code>nvec</code>. It then returns the result of concatenatingthose sequences.
A simple low-level interface for serializing to connections.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
<code>.Random.seed</code> is an integer vector, containing the random numbergenerator (RNG) <b>state</b> for random number generation in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.  Itcan be saved and restored, but should not be altered by the user.

<code>RNGkind</code> is a more friendly interface to query or set the kindof RNG in use.

<code>RNGversion</code> can be used to set the random generators as theywere in an earlier <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> version (for reproducibility).

<code>set.seed</code> is the recommended way to specify seeds.
Performs <b>set</b> union, intersection, (asymmetric!) difference,equality and membership on two vectors.
Performs <b>set</b> union, intersection, (asymmetric!) difference,equality and membership on two vectors.
These functions allow users to set actions to be taken before packagesare attached/detached and namespaces are (un)loaded.
Internal namespace support functions.  Not intended to be calleddirectly, and only visible because of the special nature of the<span class="pkg">base</span> namespace.
Functions to set CPU and/or elapsed time limits for top-levelcomputations or the current session.
Functions to set CPU and/or elapsed time limits for top-levelcomputations or the current session.
<code>getwd</code> returns an absolute filepath representing the currentworking directory of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process; <code>setwd(dir)</code> is used to setthe working directory to <code>dir</code>.
Display aspects of connections.
Quote a string to be passed to an operating system shell.
<code>sign</code> returns a vector with the signs of the correspondingelements of <code>x</code> (the sign of a real number is 1, 0, or <i>-1</i>if the number is positive, zero, or negative, respectively).

Note that <code>sign</code> does not operate on complex vectors.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
<code>ceiling</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the smallest integers not less than thecorresponding elements of <code>x</code>.

<code>floor</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the largest integers not greater than thecorresponding elements of <code>x</code>.

<code>trunc</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the integers formed by truncating the values in<code>x</code> toward <code>0</code>.

<code>round</code> rounds the values in its first argument to the specifiednumber of decimal places (default 0).  See ‘Details’ about“round to even” when rounding off a 5.

<code>signif</code> rounds the values in its first argument to the specifiednumber of significant digits.   Hence, for <code>numeric</code> <code>x</code>,<code>signif(x, dig)</code> is the same as <code>round(x, dig - ceiling(log10(abs(x))))</code>.For <code>complex</code> <code>x</code>, this is not the case, see the ‘Details’.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
<code>lapply</code> returns a list of the same length as <code>X</code>, eachelement of which is the result of applying <code>FUN</code> to thecorresponding element of <code>X</code>.

<code>sapply</code> is a user-friendly version and wrapper of <code>lapply</code>by default returning a vector, matrix or, if <code>simplify = "array"</code>, anarray if appropriate, by applying <code>simplify2array()</code>.<code>sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)</code> is the same as<code>lapply(x, f)</code>.

<code>vapply</code> is similar to <code>sapply</code>, but has a pre-specifiedtype of return value, so it can be safer (and sometimes faster) touse.

<code>replicate</code> is a wrapper for the common use of <code>sapply</code> forrepeated evaluation of an expression (which will usually involverandom number generation).

<code>simplify2array()</code> is the utility called from <code>sapply()</code>when <code>simplify</code> is not false and is similarly called from<code>mapply()</code>.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
Create, coerce to or test for a double-precision vector.
These functions give the obvious hyperbolic functions.  Theyrespectively compute the hyperbolic cosine, sine, tangent, and theirinverses, arc-cosine, arc-sine, arc-tangent (or ‘<em>area cosine</em>’,etc).
<code>sink</code> diverts <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> output to a connection (and stops such diversions).

<code>sink.number()</code> reports how many diversions are in use.

<code>sink.number(type = "message")</code> reports the number of theconnection currently being used for error messages.
<code>sink</code> diverts <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> output to a connection (and stops such diversions).

<code>sink.number()</code> reports how many diversions are in use.

<code>sink.number(type = "message")</code> reports the number of theconnection currently being used for error messages.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
Returns a matrix of integers indicating the number of their slice in agiven array.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Waits for the first of several socket connections and server socketsto become available.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
This generic function solves the equation <code>a %*% x = b</code> for <code>x</code>,where <code>b</code> can be either a vector or a matrix.
This generic function solves the equation <code>a %*% x = b</code> for <code>x</code>,where <code>b</code> can be either a vector or a matrix.
<code>qr</code> computes the QR decomposition of a matrix.
Sort (or <em>order</em>) a vector or factor (partially) intoascending or descending order.  For ordering along more than onevariable, e.g., for sorting data frames, see <code>order</code>.
Sort (or <em>order</em>) a vector or factor (partially) intoascending or descending order.  For ordering along more than onevariable, e.g., for sorting data frames, see <code>order</code>.
Sort (or <em>order</em>) a vector or factor (partially) intoascending or descending order.  For ordering along more than onevariable, e.g., for sorting data frames, see <code>order</code>.
<code>order</code> returns a permutation which rearranges its firstargument into ascending or descending order, breaking ties by furtherarguments.  <code>sort.list</code> does the same, using only one argument.<br>See the examples for how to use these functions to sort data frames,etc.
Sort (or <em>order</em>) a vector or factor (partially) intoascending or descending order.  For ordering along more than onevariable, e.g., for sorting data frames, see <code>order</code>.
<code>source</code> causes <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> to accept its input from the named file or URLor connection or expressions directly.  Input is read and<code>parse</code>d from that fileuntil the end of the file is reached, then the parsed expressions areevaluated sequentially in the chosen environment.

<code>withAutoprint(exprs)</code> is a wrapper for <code>source(exprs =  exprs, ..)</code> with different defaults.  Its main purpose is to evaluateand auto-print expressions as if in a toplevel context, e.g, as in the<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> console.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
Description of the class <code>"Date"</code> representing calendar dates.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
A wrapper for the C function <code>sprintf</code>, that returns a charactervector containing a formatted combination of text and variable values.
<code>abs(x)</code> computes the absolute value of x, <code>sqrt(x)</code> computes the(principal) square root of x, <i>√{x}</i>.

The naming follows the standard for computer languages such as C or Fortran.
Single or double quote text by combining with appropriate single ordouble left and right quotation marks.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
The function <code>standardGeneric</code> initiates dispatch of S4methods: see the references and the documentation of the<span class="pkg">methods</span> package.  Usually, calls to this function aregenerated automatically and not explicitly by the programmer.
Determines if entries of <code>x</code> start or end with string (entries of)<code>prefix</code> or <code>suffix</code> respectively, where strings arerecycled to common lengths.
Display aspects of connections.
Display aspects of connections.
Display aspects of connections.
<code>stop</code> stops execution of the current expression and executesan error action.

<code>geterrmessage</code> gives the last error message.
If any of the expressions (in <code>...</code> or <code>exprs</code>) are not<code>all</code> <code>TRUE</code>, <code>stop</code> is called, producingan error message indicating the <em>first</em> expression which was not(<code>all</code>) true.
Get or set the ‘mode’ (a kind of ‘type’), or the storagemode of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
Get or set the ‘mode’ (a kind of ‘type’), or the storagemode of an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
<code>parse()</code> returns the parsed but unevaluated expressions in an<code>expression</code>, a “list” of <code>call</code>s.

<code>str2expression(s)</code> and <code>str2lang(s)</code> return special versionsof <code>parse(text=s, keep.source=FALSE)</code> and can therefore be regarded astransforming character strings <code>s</code> to expressions, calls, etc.
<code>parse()</code> returns the parsed but unevaluated expressions in an<code>expression</code>, a “list” of <code>call</code>s.

<code>str2expression(s)</code> and <code>str2lang(s)</code> return special versionsof <code>parse(text=s, keep.source=FALSE)</code> and can therefore be regarded astransforming character strings <code>s</code> to expressions, calls, etc.
Functions to convert between character representations and objects ofclasses <code>"POSIXlt"</code> and <code>"POSIXct"</code> representing calendardates and times.
Functions to convert between character representations and objects ofclasses <code>"POSIXlt"</code> and <code>"POSIXct"</code> representing calendardates and times.
Repeat the character strings in a character vector a given number oftimes (i.e., concatenate the respective numbers of copies of thestrings).
Split the elements of a character vector <code>x</code> into substringsaccording to the matches to substring <code>split</code> within them.
Convert strings to integers according to the given base using the Cfunction <code>strtol</code>, or choose a suitable base following the C rules.
Trim character strings to specified display widths.
<code>structure</code> returns the given object with furtherattributes set.
Each character string in the input is first split into paragraphs (orlines containing whitespace only).  The paragraphs are then formattedby breaking lines at word boundaries.  The target columns for wrappinglines and the indentation of the first and all subsequent lines of aparagraph can be controlled independently.
<code>grep</code>, <code>grepl</code>, <code>regexpr</code>, <code>gregexpr</code>, <code>regexec</code> and <code>gregexec</code> search for matches to argument<code>pattern</code> within each element of a character vector: they differ inthe format of and amount of detail in the results.

<code>sub</code> and <code>gsub</code> perform replacement of the first and allmatches respectively.
Return subsets of vectors, matrices or data frames which meet conditions.
Return subsets of vectors, matrices or data frames which meet conditions.
Return subsets of vectors, matrices or data frames which meet conditions.
Return subsets of vectors, matrices or data frames which meet conditions.
<code>substitute</code> returns the parse tree for the (unevaluated)expression <code>expr</code>, substituting any variables bound in<code>env</code>.

<code>quote</code> simply returns its argument. The argument is not evaluatedand can be any R expression.

<code>enquote</code> is a simple one-line utility which transforms a call ofthe form <code>Foo(....)</code> into the call <code>quote(Foo(....))</code>.  Thisis typically used to protect a <code>call</code> from early evaluation.
Extract or replace substrings in a character vector.
Extract or replace substrings in a character vector.
Extract or replace substrings in a character vector.
Extract or replace substrings in a character vector.
<code>sum</code> returns the sum of all the valuespresent in its arguments.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
Group generic methods can be defined for four pre-specified groups offunctions, <code>Math</code>, <code>Ops</code>, <code>Summary</code> and <code>Complex</code>.(There are no objects of these names in base <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>, but there are in the<span class="pkg">methods</span> package.)

A method defined for an individual member of the group takesprecedence over a method defined for the group as a whole.
Description of the class <code>"Date"</code> representing calendar dates.
Description of the class <code>"Date"</code> representing calendar dates.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
<code>summary</code> is a generic function used to produce result summariesof the results of various model fitting functions.  The functioninvokes particular <code>methods</code> which depend on the<code>class</code> of the first argument.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
The function <code>factor</code> is used to encode a vector as a factor (theterms ‘category’ and ‘enumerated type’ are also used forfactors).  If argument <code>ordered</code> is <code>TRUE</code>, the factorlevels are assumed to be ordered.  For compatibility with S there isalso a function <code>ordered</code>.

<code>is.factor</code>, <code>is.ordered</code>, <code>as.factor</code> and <code>as.ordered</code>are the membership and coercion functions for these classes.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>proc.time</code> determines how much real and CPU time (in seconds)the currently running <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process has already taken.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
These functions are for working with source files and more generallywith “source references” (<code>"srcref"</code>), i.e., references tosource code.  The resulting data is used for printing and source leveldebugging, and is typically available in interactive <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions,namely when <code>options(keep.source = TRUE)</code>.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
Generate a diagnostic message from its arguments.
Generate a diagnostic message from its arguments.
Generates a warning message that corresponds to its argument(s) and(optionally) the expression or function from which it was called.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Compute the singular-value decomposition of a rectangular matrix.
Return an array obtained from an input array by sweeping out a summarystatistic.
<code>switch</code> evaluates <code>EXPR</code> and accordingly chooses one of thefurther arguments (in <code>...</code>).
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide a low-level interface to the computer'sfile system.
<code>Sys.time</code> and <code>Sys.Date</code> returns the system's idea of thecurrent date with and without time.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
<code>Sys.getenv</code> obtains the values of the environment variables.
Get details of or set aspects of the locale for the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process.
Get the process ID of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> Session.  It is guaranteed by theoperating system that two <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions running simultaneously willhave different IDs, but it is possible that <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> sessions running atdifferent times will have the same ID.
Function to do wildcard expansion (also known as ‘globbing’) onfile paths.
Reports system and user information.
Internal objects in the base package most of which are only user-visiblebecause of the special nature of the base namespace.
Get details of the numerical and monetary representations in thecurrent locale.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
Find out if a file path is a symbolic link, and if so what it islinked to, <em>via</em> the system call <code>readlink</code>.

Symbolic links are a POSIX concept, not implemented on Windows but formost filesystems on Unix-alikes.
Internal objects in the base package most of which are only user-visiblebecause of the special nature of the base namespace.
<code>Sys.setenv</code> sets environment variables (for other processescalled from within <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> or future calls to <code>Sys.getenv</code> fromthis <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process).

<code>Sys.unsetenv</code> removes environment variables.
Uses system calls to set the times on a file or directory.
Get details of or set aspects of the locale for the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process.
Suspend execution of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expressions for a specified time interval.
Parses expressions in the given file, and then successively evaluatesthem in the specified environment.
These functions provide access to <code>environment</code>s(‘frames’ in S terminology) associated with functions furtherup the calling stack.
<code>Sys.time</code> and <code>Sys.Date</code> returns the system's idea of thecurrent date with and without time.
Information about time zones in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.  <code>Sys.timezone</code> returnsthe name of the current time zone.
These functions provide a low-level interface to the computer'sfile system.
<code>Sys.setenv</code> sets environment variables (for other processescalled from within <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> or future calls to <code>Sys.getenv</code> fromthis <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> process).

<code>Sys.unsetenv</code> removes environment variables.
This is an interface to the system command <code>which</code>, or to anemulation on Windows.
<code>system</code> invokes the OS command specified by <code>command</code>.
Finds the full file names of files in packages etc.
Return CPU (and other) times that <code>expr</code> used.
<code>system2</code> invokes the OS command specified by <code>command</code>.
Given a matrix or <code>data.frame</code> <code>x</code>,<code>t</code> returns the transpose of <code>x</code>.
Create or test for objects of type <code>"logical"</code>, and the basiclogical constants.
Given a matrix or <code>data.frame</code> <code>x</code>,<code>t</code> returns the transpose of <code>x</code>.
Given a matrix or <code>data.frame</code> <code>x</code>,<code>t</code> returns the transpose of <code>x</code>.
<code>table</code> uses the cross-classifying factors to build a contingencytable of the counts at each combination of factor levels.
<code>tabulate</code> takes the integer-valued vector <code>bin</code> and countsthe number of times each integer occurs in it.
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
These functions give the obvious hyperbolic functions.  Theyrespectively compute the hyperbolic cosine, sine, tangent, and theirinverses, arc-cosine, arc-sine, arc-tangent (or ‘<em>area cosine</em>’,etc).
These functions give the obvious trigonometric functions.  Theyrespectively compute the cosine, sine, tangent, arc-cosine, arc-sine,arc-tangent, and the two-argument arc-tangent.

<code>cospi(x)</code>, <code>sinpi(x)</code>, and <code>tanpi(x)</code>, compute<code>cos(pi*x)</code>, <code>sin(pi*x)</code>, and <code>tan(pi*x)</code>.
Apply a function to each cell of a ragged array, that is to each(non-empty) group of values given by a unique combination of thelevels of certain factors.
This provides an entirely <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>-language mechanismfor managing callbacks or actions  that are invoked atthe conclusion of each top-level task.  Essentially,we register a single <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function from this managerwith the underlying, nativetask-callback mechanism and this function handles invoking the otherR callbacks under the control of the manager.The manager consists of a collection of functions that access sharedvariables to manage the list of user-level callbacks.
Given matrices <code>x</code> and <code>y</code> as arguments, return a matrixcross-product.  This is formally equivalent to (but usually slightlyfaster than) the call <code>t(x) %*% y</code> (<code>crossprod</code>) or<code>x %*% t(y)</code> (<code>tcrossprod</code>).
<code>tempfile</code> returns a vector of character strings which can be used asnames for temporary files.
<code>tempfile</code> returns a vector of character strings which can be used asnames for temporary files.
Input and output text connections.
Input and output text connections.
Translate characters in character vectors, in particular from upper tolower case or vice versa.
Finding the top level <code>environment</code> from an environment<code>envir</code> and its enclosing environments.
This is a helper function for <code>format</code> to produce a singlecharacter string describing an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
This is a helper function for <code>format</code> to produce a singlecharacter string describing an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
Translate characters in character vectors, in particular from upper tolower case or vice versa.
A call to <code>trace</code> allows you to insert debugging code (e.g., acall to <code>browser</code> or <code>recover</code>) at chosenplaces in any function.  A call to <code>untrace</code> cancels the tracing.Specified methods can be traced the same way, without tracing allcalls to the generic function.  Trace code (<code>tracer</code>) can be any<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression.  Tracing can be temporarily turned on or off globallyby calling <code>tracingState</code>.
By default <code>traceback()</code> prints the call stack of the lastuncaught error, i.e., the sequence of calls that lead to the error.This is useful when an error occurs with an unidentifiable errormessage.  It can also be used to print the current stack orarbitrary lists of calls.

<code>.traceback()</code> now <em>returns</em> the above call stack (and<code>traceback(x, *)</code> can be regarded as convenience function forprinting the result of <code>.traceback(x)</code>).
This function marks an object so that a message is printed whenever theinternal code copies the object.  It is amajor cause of hard-to-predict memory use in R.
A call to <code>trace</code> allows you to insert debugging code (e.g., acall to <code>browser</code> or <code>recover</code>) at chosenplaces in any function.  A call to <code>untrace</code> cancels the tracing.Specified methods can be traced the same way, without tracing allcalls to the generic function.  Trace code (<code>tracer</code>) can be any<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression.  Tracing can be temporarily turned on or off globallyby calling <code>tracingState</code>.
<code>transform</code> is a generic function, which—at leastcurrently—only does anything useful withdata frames.  <code>transform.default</code> converts its first argument toa data frame if possible and calls <code>transform.data.frame</code>.
<code>transform</code> is a generic function, which—at leastcurrently—only does anything useful withdata frames.  <code>transform.default</code> converts its first argument toa data frame if possible and calls <code>transform.data.frame</code>.
<code>transform</code> is a generic function, which—at leastcurrently—only does anything useful withdata frames.  <code>transform.default</code> converts its first argument toa data frame if possible and calls <code>transform.data.frame</code>.
Special mathematical functions related to the beta and gammafunctions.
Remove leading and/or trailing whitespace from character strings.
<code>ceiling</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the smallest integers not less than thecorresponding elements of <code>x</code>.

<code>floor</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the largest integers not greater than thecorresponding elements of <code>x</code>.

<code>trunc</code> takes a single numeric argument <code>x</code> and returns anumeric vector containing the integers formed by truncating the values in<code>x</code> toward <code>0</code>.

<code>round</code> rounds the values in its first argument to the specifiednumber of decimal places (default 0).  See ‘Details’ about“round to even” when rounding off a 5.

<code>signif</code> rounds the values in its first argument to the specifiednumber of significant digits.   Hence, for <code>numeric</code> <code>x</code>,<code>signif(x, dig)</code> is the same as <code>round(x, dig - ceiling(log10(abs(x))))</code>.For <code>complex</code> <code>x</code>, this is not the case, see the ‘Details’.
Round or truncate date-time objects.
Round or truncate date-time objects.
Functions to re-position connections.
Functions to re-position connections.
<code>try</code> is a wrapper to run an expression that might fail and allowthe user's code to handle error-recovery.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
<code>typeof</code> determines the (<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> internal)type or storage mode of any object
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class of the first argument to the generic function.
Set, unset or query the debugging flag on a function.The <code>text</code> and <code>condition</code> arguments are the same as thosethat can be supplied via a call to <code>browser</code>.  They can be retrievedby the user once the browser has been entered, and provide a mechanism toallow users to identify which breakpoint has been activated.
Performs <b>set</b> union, intersection, (asymmetric!) difference,equality and membership on two vectors.
<code>unique</code> returns a vector, data frame or array like <code>x</code>but with duplicate elements/rows removed.
<code>unique</code> returns a vector, data frame or array like <code>x</code>but with duplicate elements/rows removed.
<code>unique</code> returns a vector, data frame or array like <code>x</code>but with duplicate elements/rows removed.
<code>unique</code> returns a vector, data frame or array like <code>x</code>but with duplicate elements/rows removed.
<code>unique</code> returns a vector, data frame or array like <code>x</code>but with duplicate elements/rows removed.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
Description of the classes <code>"POSIXlt"</code> and <code>"POSIXct"</code>representing calendar dates and times.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
Time intervals creation, printing, and some arithmetic.  The<code>print()</code> method calls these “time differences”.
These functions are provided for compatibility with older versions of<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> only, and may be defunct as soon as the next release.
<code>unlink</code> deletes the file(s) or directories specified by <code>x</code>.
Given a list structure <code>x</code>, <code>unlist</code> simplifies it toproduce a vector which contains all the atomic componentswhich occur in <code>x</code>.
Functions to load and unload name spaces.
These functions represent an interface for adjustmentsto environments and bindings within environments.  They allow forlocking environments as well as individual bindings, and for linkinga variable to a function.
Remove the <code>names</code> or <code>dimnames</code> attribute ofan <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object.
A simple low-level interface for serializing to connections.
<code>split</code> divides the data in the vector <code>x</code> into the groupsdefined by <code>f</code>.  The replacement forms replace valuescorresponding to such a division.  <code>unsplit</code> reverses the effect of<code>split</code>.
A call to <code>trace</code> allows you to insert debugging code (e.g., acall to <code>browser</code> or <code>recover</code>) at chosenplaces in any function.  A call to <code>untrace</code> cancels the tracing.Specified methods can be traced the same way, without tracing allcalls to the generic function.  Trace code (<code>tracer</code>) can be any<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression.  Tracing can be temporarily turned on or off globallyby calling <code>tracingState</code>.
This function marks an object so that a message is printed whenever theinternal code copies the object.  It is amajor cause of hard-to-predict memory use in R.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
Returns a matrix of logicals the same size of a given matrix withentries <code>TRUE</code> in the lower or upper triangle.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> possesses a simple generic function mechanism which can be used foran object-oriented style of programming.  Method dispatch takes placebased on the class(es) of the first argument to the generic function or ofthe object supplied as an argument to <code>UseMethod</code> or <code>NextMethod</code>.
Conversion of UTF-8 encoded character vectors to and from integervectors representing a UTF-32 encoding.
Check if each element of a character vector is valid in its impliedencoding.
Check if each element of a character vector is valid in its impliedencoding.
<code>lapply</code> returns a list of the same length as <code>X</code>, eachelement of which is the result of applying <code>FUN</code> to thecorresponding element of <code>X</code>.

<code>sapply</code> is a user-friendly version and wrapper of <code>lapply</code>by default returning a vector, matrix or, if <code>simplify = "array"</code>, anarray if appropriate, by applying <code>simplify2array()</code>.<code>sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)</code> is the same as<code>lapply(x, f)</code>.

<code>vapply</code> is similar to <code>sapply</code>, but has a pre-specifiedtype of return value, so it can be safer (and sometimes faster) touse.

<code>replicate</code> is a wrapper for the common use of <code>sapply</code> forrepeated evaluation of an expression (which will usually involverandom number generation).

<code>simplify2array()</code> is the utility called from <code>sapply()</code>when <code>simplify</code> is not false and is similarly called from<code>mapply()</code>.
<code>vector</code> produces a vector of the given length and mode.

<code>as.vector</code>, a generic, attempts to coerce its argument into avector of mode <code>mode</code> (the default is to coerce to whichevervector mode is most convenient): if the result is atomic allattributes are removed.

<code>is.vector</code> returns <code>TRUE</code> if <code>x</code> is a vector of thespecified mode having no attributes <em>other than names</em>.  It returns<code>FALSE</code> otherwise.
<code>Vectorize</code> creates a function wrapper that vectorizes theaction of its argument <code>FUN</code>.
<code>R.Version()</code> provides detailed information about the version of<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> running.

<code>R.version</code> is a variable (a <code>list</code>) holding thisinformation (and <code>version</code> is a copy of it for S compatibility).
Generates a warning message that corresponds to its argument(s) and(optionally) the expression or function from which it was called.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
<code>warnings</code> and its <code>print</code> method print thevariable <code>last.warning</code> in a pleasing form.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Extract the weekday, month or quarter, or the Julian time(days since some origin).  These are generic functions: the methodsfor the internal date-time classes are documented here.
Give the <code>TRUE</code> indices of a logical object, allowing for arrayindices.
Determines the location, i.e., index of the (first) minimum or maximumof a numeric (or logical) vector.
Determines the location, i.e., index of the (first) minimum or maximumof a numeric (or logical) vector.
These are the basic control-flow constructs of the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> language.  Theyfunction in much the same way as control statements in any Algol-likelanguage.  They are all reserved words.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in an environment constructed from data,possibly modifying (a copy of) the original data.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in an environment constructed from data,possibly modifying (a copy of) the original data.
<code>source</code> causes <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> to accept its input from the named file or URLor connection or expressions directly.  Input is read and<code>parse</code>d from that fileuntil the end of the file is reached, then the parsed expressions areevaluated sequentially in the chosen environment.

<code>withAutoprint(exprs)</code> is a wrapper for <code>source(exprs =  exprs, ..)</code> with different defaults.  Its main purpose is to evaluateand auto-print expressions as if in a toplevel context, e.g, as in the<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> console.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in an environment constructed from data,possibly modifying (a copy of) the original data.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in an environment constructed from data,possibly modifying (a copy of) the original data.
Evaluate an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> expression in an environment constructed from data,possibly modifying (a copy of) the original data.
These functions provide a mechanism for handling unusual conditions,including errors and warnings.
This function evaluates an expression, returning it in a two element listcontaining its value and a flag showing whether it would automatically print.
The data (usually a matrix) <code>x</code> are written to file <code>file</code>.If <code>x</code> is a two-dimensional matrix you need to transpose it to get thecolumns in <code>file</code> the same as those in the internal representation.
Reads or writes an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object from/to a file in Debian Control Fileformat.
Read binary data from or write binary data to a connection or raw vector.
Transfer character strings to and from connections, without assumingthey are null-terminated on the connection.
Write text lines to a connection.
These operators act on raw, logical and number-like vectors.
Internal auxiliary functions for use with data frames.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A simple S3 class for representing numeric versionsincluding package versions, and associated methods.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
A generic auxiliary function that produces a numeric vector whichwill sort in the same order as <code>x</code>.
Functions to create, open and close connections, i.e.,“generalized files”, such as possibly compressed files, URLs,pipes, etc.
<code>zapsmall</code> determines a <code>digits</code> argument <code>dr</code> forcalling <code>round(x, digits = dr)</code> such that values close tozero (compared with the maximal absolute value) are ‘zapped’,i.e., replaced by <code>0</code>.
This function adds one or more straight lines through the current plot.
Draw arrows between pairs of points.
Produce a Cohen-Friendly association plot indicating deviations fromindependence of rows and columns in a 2-dimensional contingencytable.
Adds an axis to the current plot, allowing thespecification of the side, position, labels, and other options.
Generic function to add a suitable axis to the current plot.
Functions to plot objects of classes <code>"POSIXlt"</code>,<code>"POSIXct"</code> and <code>"Date"</code> representing calendar dates and times.
Functions to plot objects of classes <code>"POSIXlt"</code>,<code>"POSIXct"</code> and <code>"Date"</code> representing calendar dates and times.
Compute pretty tickmark locations, the same way as <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> does internally.This is only non-trivial when <b>log</b> coordinates are active.By default, gives the <code>at</code> values which<code>axis(side)</code> would use.
Creates a bar plot with vertical or horizontal bars.
Creates a bar plot with vertical or horizontal bars.
This function draws a box around the current plot in the given colorand linetype.  The <code>bty</code> parameter determinesthe type of box drawn.  See <code>par</code> for details.
Produce box-and-whisker plot(s) of the given (grouped) values.
Produce box-and-whisker plot(s) of the given (grouped) values.
Interpreting the columns (or rows) of a matrix as different groups, draw aboxplot for each.
<code>bxp</code> draws box plots based on the given summaries in <code>z</code>.It is usually called from within <code>boxplot</code>, but can beinvoked directly.
Computes and plots conditional densities describing how theconditional distribution of a categorical variable <code>y</code> changes over anumerical variable <code>x</code>.
Set clipping region in user coordinates
<code>split.screen</code> defines a number of regions within the currentdevice which can, to some extent, be treated as separate graphicsdevices.  It is useful for generating multiple plots on a singledevice.  Screens can themselves be split, allowing for quite complexarrangements of plots.

<code>screen</code> is used to select which screen to draw in.

<code>erase.screen</code> is used to clear a single screen, which itdoes by filling with the background colour.

<code>close.screen</code> removes the specified screen definition(s).
This function produces two variants of the <b>co</b>nditioning plotsdiscussed in the reference below.
Create a contour plot, or add contour lines to an existing plot.
Create a contour plot, or add contour lines to an existing plot.
This function produces two variants of the <b>co</b>nditioning plotsdiscussed in the reference below.
Draws a curve corresponding to a function over the interval<code>[from, to]</code>. <code>curve</code> can plot also an expression in the variable<code>xname</code>, default <span class="samp">x</span>.
Draw a Cleveland dot plot.
<code>split.screen</code> defines a number of regions within the currentdevice which can, to some extent, be treated as separate graphicsdevices.  It is useful for generating multiple plots on a singledevice.  Screens can themselves be split, allowing for quite complexarrangements of plots.

<code>screen</code> is used to select which screen to draw in.

<code>erase.screen</code> is used to clear a single screen, which itdoes by filling with the background colour.

<code>close.screen</code> removes the specified screen definition(s).
This function produces a contour plot with the areas between thecontours filled in solid color (Cleveland calls this a level plot).  Akey showing how the colors map to z values is shown to the right ofthe plot.
Creates a fourfold display of a 2 by 2 by <i>k</i> contingency table onthe current graphics device, allowing for the visual inspection of theassociation between two dichotomous variables in one or severalpopulations (strata).
This function (<code>frame</code> is an alias for<code>plot.new</code>) causes the completion of plotting in the current plot(if there is one) and an advance to a new graphics frame.  This isused in all high-level plotting functions and also useful for skippingplots when a multi-figure region is in use.
Convert between graphics coordinate systems.
Convert between graphics coordinate systems.
<code>grid</code> adds an <code>nx</code> by <code>ny</code> rectangular grid to anexisting plot.
The generic function <code>hist</code> computes a histogram of the givendata values.  If <code>plot = TRUE</code>, the resulting object ofclass <code>"histogram"</code> is plotted by<code>plot.histogram</code>, before it is returned.
The generic function <code>hist</code> computes a histogram of the givendata values.  If <code>plot = TRUE</code>, the resulting object ofclass <code>"histogram"</code> is plotted by<code>plot.histogram</code>, before it is returned.
<code>identify</code> reads the position of the graphics pointer when the(first) mouse button is pressed.  It then searches the coordinatesgiven in <code>x</code> and <code>y</code> for the point closest to the pointer.If this point is close enough to the pointer, its index will be returned aspart of the value of the call.
Creates a grid of colored or gray-scale rectangles with colorscorresponding to the values in <code>z</code>.  This can be used to displaythree-dimensional or spatial data aka <em>images</em>.This is a generic function.

<em>NOTE:</em> the grid is drawn as a set of rectangles by default;see the <code>useRaster</code> argument to draw the grid as a raster image.

The function <code>hcl.colors</code> provides a broad range of sequentialcolor palettes that are suitable for displaying ordered data, with<code>n</code> giving the number of colors desired.
Creates a grid of colored or gray-scale rectangles with colorscorresponding to the values in <code>z</code>.  This can be used to displaythree-dimensional or spatial data aka <em>images</em>.This is a generic function.

<em>NOTE:</em> the grid is drawn as a set of rectangles by default;see the <code>useRaster</code> argument to draw the grid as a raster image.

The function <code>hcl.colors</code> provides a broad range of sequentialcolor palettes that are suitable for displaying ordered data, with<code>n</code> giving the number of colors desired.
<code>layout</code> divides the device up into as many rows and columns asthere are in matrix <code>mat</code>, with the column-widths and therow-heights specified in the respective arguments.
<code>layout</code> divides the device up into as many rows and columns asthere are in matrix <code>mat</code>, with the column-widths and therow-heights specified in the respective arguments.
<code>layout</code> divides the device up into as many rows and columns asthere are in matrix <code>mat</code>, with the column-widths and therow-heights specified in the respective arguments.
This function can be used to add legends to plots.  Note that a callto the function <code>locator(1)</code> can be used in place of the <code>x</code>and <code>y</code> arguments.
A generic function taking coordinates given in various ways andjoining the corresponding points with line segments.
A generic function taking coordinates given in various ways andjoining the corresponding points with line segments.
Reads the position of the graphics cursor when the (first) mousebutton is pressed.
Plot the columns of one matrix against the columns of another (whichoften is just a vector treated as 1-column matrix).
Plot the columns of one matrix against the columns of another (whichoften is just a vector treated as 1-column matrix).
Plot the columns of one matrix against the columns of another (whichoften is just a vector treated as 1-column matrix).
Plots a mosaic on the current graphics device.
Text is written in one of the four margins of the current figure regionor one of the outer margins of the device region.
A matrix of scatterplots is produced.
A matrix of scatterplots is produced.
An example of a simple useful <code>panel</code> function to be used asargument in e.g., <code>coplot</code> or <code>pairs</code>.
<code>par</code> can be used to set or query graphical parameters.Parameters can be set by specifying them as arguments to <code>par</code> in<code>tag = value</code> form, or by passing them as a list of taggedvalues.
This function draws perspective plots of a surface over thex–y plane.  <code>persp</code> is a generic function.
Draw a pie chart.
Draw a scatter plot with decorations such as axes and titlesin the active graphics window.
Draw a scatter plot with decorations such as axes and titlesin the active graphics window.
Plot univariate effects of one or more <code>factor</code>s,typically for a designed experiment as analyzed by <code>aov()</code>.
Draws a curve corresponding to a function over the interval<code>[from, to]</code>. <code>curve</code> can plot also an expression in the variable<code>xname</code>, default <span class="samp">x</span>.
This function (<code>frame</code> is an alias for<code>plot.new</code>) causes the completion of plotting in the current plot(if there is one) and an advance to a new graphics frame.  This isused in all high-level plotting functions and also useful for skippingplots when a multi-figure region is in use.
This function sets up the world coordinate system for a graphicswindow.  It is called by higher level functions such as<code>plot.default</code> (<em>after</em> <code>plot.new</code>).
This is <em>the</em> internal function that does the basic plotting ofpoints and lines.  Usually, one should rather use the higher levelfunctions instead and refer to their help pages for explanation of thearguments.
<code>points</code> is a generic function to draw a sequence of points atthe specified coordinates.  The specified character(s) are plotted,centered at the coordinates.
<code>points</code> is a generic function to draw a sequence of points atthe specified coordinates.  The specified character(s) are plotted,centered at the coordinates.
<code>polygon</code> draws the polygons whose vertices aregiven in <code>x</code> and <code>y</code>.
<code>path</code> draws a path whose vertices aregiven in <code>x</code> and <code>y</code>.
<code>rasterImage</code> draws a raster image at the given locations and sizes.
<code>rect</code> draws a rectangle (or sequence of rectangles) with thegiven coordinates, fill and border colors.
Adds a <em>rug</em> representation (1-d plot) of the data to the plot.
<code>split.screen</code> defines a number of regions within the currentdevice which can, to some extent, be treated as separate graphicsdevices.  It is useful for generating multiple plots on a singledevice.  Screens can themselves be split, allowing for quite complexarrangements of plots.

<code>screen</code> is used to select which screen to draw in.

<code>erase.screen</code> is used to clear a single screen, which itdoes by filling with the background colour.

<code>close.screen</code> removes the specified screen definition(s).
Draw line segments between pairs of points.
<code>smoothScatter</code> produces a smoothed color densityrepresentation of a scatterplot, obtained through a (2D) kerneldensity estimate.
Spine plots are a special cases of mosaic plots, and can be seen asa generalization of stacked (or highlighted) bar plots. Analogously,spinograms are an extension of histograms.
<code>split.screen</code> defines a number of regions within the currentdevice which can, to some extent, be treated as separate graphicsdevices.  It is useful for generating multiple plots on a singledevice.  Screens can themselves be split, allowing for quite complexarrangements of plots.

<code>screen</code> is used to select which screen to draw in.

<code>erase.screen</code> is used to clear a single screen, which itdoes by filling with the background colour.

<code>close.screen</code> removes the specified screen definition(s).
Draw star plots or segment diagrams of a multivariate data set.With one single location, also draws ‘spider’(or ‘radar’) plots.
<code>stem</code> produces a stem-and-leaf plot of the values in <code>x</code>.The parameter <code>scale</code> can be used to expand the scale of theplot.  A value of <code>scale = 2</code> will cause the plot to be roughlytwice as long as the default.
These functions compute the width or height, respectively, of thegiven strings or mathematical expressions <code>s[i]</code> onthe current plotting device in <em>user</em> coordinates, <em>inches</em>or as fraction of the figure width <code>par("fin")</code>.
<code>stripchart</code> produces one dimensional scatter plots (or dotplots) of the given data.  These plots are a good alternative to<code>boxplot</code>s when sample sizes are small.
These functions compute the width or height, respectively, of thegiven strings or mathematical expressions <code>s[i]</code> onthe current plotting device in <em>user</em> coordinates, <em>inches</em>or as fraction of the figure width <code>par("fin")</code>.
Multiple points are plotted as ‘sunflowers’ with multiple leaves(‘petals’) such that overplotting is visualized instead ofaccidental and invisible.
This function draws symbols on a plot.  One of six symbols;<em>circles</em>, <em>squares</em>, <em>rectangles</em>, <em>stars</em>,<em>thermometers</em>, and <em>boxplots</em>, can be plotted at aspecified set of x and y coordinates.  Specific aspects of thesymbols, such as relative size, can be customized by additionalparameters.
<code>text</code> draws the strings given in the vector <code>labels</code> at thecoordinates given by <code>x</code> and <code>y</code>.<code>y</code> may be missing since <code>xy.coords(x, y)</code> is used forconstruction of the coordinates.
<code>text</code> draws the strings given in the vector <code>labels</code> at thecoordinates given by <code>x</code> and <code>y</code>.<code>y</code> may be missing since <code>xy.coords(x, y)</code> is used forconstruction of the coordinates.
This function can be used to add labels to a plot.  Its first fourprincipal arguments can also be used as arguments in most high-levelplotting functions.  They must be of type <code>character</code> or<code>expression</code>. In the latter case, quite a bit ofmathematical notation is available such as sub- and superscripts,greek letters, fractions, etc: see plotmath
<code>xinch</code> and <code>yinch</code> convert the specified number of inchesgiven as their arguments into the correct units for plotting withgraphics functions.  Usually, this only makes sense when normalcoordinates are used, i.e., <em>no</em> <code>log</code> scale (see the<code>log</code> argument to <code>par</code>).

<code>xyinch</code> does the same for a pair of numbers <code>xy</code>,simultaneously.
Draw an X-spline, a curve drawn relative to control points.
<code>xinch</code> and <code>yinch</code> convert the specified number of inchesgiven as their arguments into the correct units for plotting withgraphics functions.  Usually, this only makes sense when normalcoordinates are used, i.e., <em>no</em> <code>log</code> scale (see the<code>log</code> argument to <code>par</code>).

<code>xyinch</code> does the same for a pair of numbers <code>xy</code>,simultaneously.
<code>xinch</code> and <code>yinch</code> convert the specified number of inchesgiven as their arguments into the correct units for plotting withgraphics functions.  Usually, this only makes sense when normalcoordinates are used, i.e., <em>no</em> <code>log</code> scale (see the<code>log</code> argument to <code>par</code>).

<code>xyinch</code> does the same for a pair of numbers <code>xy</code>,simultaneously.
It is sometimes convenient to add two vectors or matrices in an operation analogous to matrix multiplication. For matrices nXm and mYp, the matrix sum  of the i,jth element of nSp = sum(over m) of iXm + mYj. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Internal consistency measures of reliability range from <i>omega_hierchical</i> to <i>alpha</i> to <i>omega_total</i>.  This function reports two estimates: Cronbach's coefficient <i>alpha</i> and Guttman's <i>lambda_6</i>.  Also reported are item - whole correlations, <i>alpha</i> if an item is omitted, and item means and standard deviations.
Internal consistency measures of reliability range from <i>omega_hierchical</i> to <i>alpha</i> to <i>omega_total</i>.  This function reports two estimates: Cronbach's coefficient <i>alpha</i> and Guttman's <i>lambda_6</i>.  Also reported are item - whole correlations, <i>alpha</i> if an item is omitted, and item means and standard deviations.
When doing regressions from the data or from a correlation matrix using <code>setCor</code> or doing a mediation analysis using <code>link{mediate}</code>, it is useful to compare alternative models.  Since these are both regression models, the appropriate test is an Analysis of Variance.  Similar tests, using Chi Square may be done for factor analytic models. 
In many fields, decisions and outcomes are categorical even though the underlying phenomenon are probably continuous.  E.g. students are accepted to graduate school or not, they finish or not. X-Rays are diagnosed as patients having cancer or not.   Outcomes of such decisions are usually labeled as Valid Positives, Valid Negatives, False Positives and False Negatives. In hypothesis testing, False Positives are known as Type I errors, while False Negatives are Type II errors.  The relationship between these four cells depends upon the correlation between the decision rule and the outcome as well as the level of evidence needed for a decision (the criterion).  Signal Detection Theory and Decision Theory have a number of related measures of performance (accuracy = VP + VN), Sensitivity (VP/(VP + FN)), Specificity (1 - FP), d prime (d'), and the area under the Response Operating Characteristic Curve (AUC). More generally, these are examples of correlations based upon dichotomous data.  <code>AUC</code> addresses some of these questions.  
Von Neuman et al. (1941) discussed the Mean Square of Successive Differences as a measure of variability that takes into account gradual shifts in mean. This is appropriate when studying errors in ballistics or variability and stability in mood when studying affect. For random data, this will be twice the variance, but for data with a sequential order and a positive autocorrelation, this will be much smaller. Since the mssd is just twice the variance - the autocorrelation, it is thus possible to also find the autocorrelation for a particular lag. 
Goldberg (2006) described a hierarchical factor structure organization from the “top down".  The original idea was to do successive factor analyses from 1 to nf factors organized by factor score correlations from  one level to the next.  Waller (2007) discussed a simple way of doing this for components without finding the scores.  Using the factor correlations (from Gorsuch) to organize factors hierarchically results may be organized at many different levels. The algorithm may be applied to principal components (pca) or to true factor analysis.
Goldberg (2006) described a hierarchical factor structure organization from the “top down".  The original idea was to do successive factor analyses from 1 to nf factors organized by factor score correlations from  one level to the next.  Waller (2007) discussed a simple way of doing this for components without finding the scores.  Using the factor correlations (from Gorsuch) to organize factors hierarchically results may be organized at many different levels. The algorithm may be applied to principal components (pca) or to true factor analysis.
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
<code>bestScales</code> forms scales from the items/scales most correlated with a particular criterion and then cross validates on a hold out sample using unit weighted scales.  This may be repeated n.iter times using either basic bootstrap aggregation (bagging) techniques or K-fold cross validation. Thus, the technique is known as <code>BISCUIT</code> (Best Items Scales that are Cross validated, Unit weighted, Informative, and Transparent).  Given a dictionary of item content, <code>bestScales</code> will sort by criteria correlations and display the item content. Options for bagging (bootstrap aggregation) are included. An alternative to unit weighting is to weight items by their zero order correlations (cross validated) with the criteria. This weighted version is called <code>BISCWIT</code> and is an optional output. 
<code>bestScales</code> forms scales from the items/scales most correlated with a particular criterion and then cross validates on a hold out sample using unit weighted scales.  This may be repeated n.iter times using either basic bootstrap aggregation (bagging) techniques or K-fold cross validation. Thus, the technique is known as <code>BISCUIT</code> (Best Items Scales that are Cross validated, Unit weighted, Informative, and Transparent).  Given a dictionary of item content, <code>bestScales</code> will sort by criteria correlations and display the item content. Options for bagging (bootstrap aggregation) are included. An alternative to unit weighting is to weight items by their zero order correlations (cross validated) with the criteria. This weighted version is called <code>BISCWIT</code> and is an optional output. 
25 personality self report items taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis.  Three additional demographic variables (sex, education, and age) are also included. This data set is deprecated and users are encouraged to use <code>bfi</code>.
25 personality self report items taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis.  Three additional demographic variables (sex, education, and age) are also included. This data set is deprecated and users are encouraged to use <code>bfi</code>.
When showing e.g., age or education distributions for two groups, it is convenient to plot them back to back.  bi.bars will do so.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
When analyzing many subjects (ie. 100,000 or more) with many variables (i.e. 1000 or more) core R can take a long time and sometime exceed  memory limits (i.e. with 600K subjects and 6K variables).  bigCor runs (in parallel if multicores are available) by breaking the variables into subsets (of size=size), finding all subset correlations, and then stitches the resulting matrices into one large matrix.   Noticeable improvements in speed compared to cor.  
Extends the biplot function to the output of <code>fa</code>, <code>fa.poly</code>  or <code>principal</code>. Will plot factor scores and factor loadings in the same graph.  If the number of factors &gt; 2, then all pairs of factors are plotted. Factor score histograms are plotted on the diagonal. The input is the resulting object from <code>fa</code>, <code>principal</code>, or }code{linkfa.poly with the scores=TRUE option. Points may be colored according to other criteria.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
Random assignment of n subjects with an equal number in all of N conditions may  done by block randomization, where the block size is the number of experimental conditions. The number of Independent Variables and the number of levels in each IV are specified as input. The output is a the block randomized design.
An example data set used by McDonald (1999) as well as other discussions of Item Response Theory makes use of a data table on 10 items (two sets of 5) from the Law School Admissions Test (LSAT).  Included in this data set is the original table as well as the reponses for 1000 subjects on the first set (Figure Classification) and second set (Debate). 
Rindskopf and Rose (1988) use this data set to demonstrate confirmatory second order factor models.  It is a nice example data set to explore hierarchical structure and alternative factor solutions. It contains measures of fluid and crystallized intelligence.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
John Schmid and John M. Leiman (1957) discuss how to transform a hierarchical factor structure to a bifactor structure. Schmid contains the example 12 x 12 correlation matrix. schmid.leiman is a 12 x 12 correlation matrix with communalities on the diagonal. This can be used to show the effect of correcting for attenuation. Two additional data sets are taken from Chen et al. (2006).
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating these data is straightforward, and is useful for exploring alternative solutions to affect and personality structure.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating these data is straightforward, and is useful for exploring alternative solutions to affect and personality structure.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating these data is straightforward, and is useful for exploring alternative solutions to affect and personality structure.
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Given a  n x c cluster definition matrix of -1s, 0s, and 1s (the keys) , and a n x n correlation matrix, or an N x n data matrix, find the correlations of the composite clusters.  The keys matrix can be entered by hand, copied from the clipboard (<code>read.clipboard</code>), or taken as output from the <code>factor2cluster</code> or <code>make.keys</code> functions.  Similar functionality to <code>scoreItems</code> which also gives item by cluster correlations. <code>scoreBy</code> does this for individual subjects after a call to <code>statsBy</code>.
How well does the cluster model found by <code>ICLUST</code> fit the original correlation matrix?  A similar algorithm  <code>factor.fit</code> is found in <code>VSS</code>. This function is internal to ICLUST but has more general use as well.

In general, the cluster model is a Very Simple Structure model of complexity one.  That is, every item is assumed to represent only one factor/cluster. Cluster fit is an analysis of how well this model reproduces a correlation matrix.  Two measures of fit are given: cluster fit and factor fit.  Cluster fit assumes that variables that define different clusters are orthogonal.  Factor fit takes the loadings generated by a cluster model, finds the cluster loadings on all clusters, and measures the degree of fit of this somewhat more complicated model.  Because the cluster loadings are similar to, but not identical to factor loadings, the factor fits found here and by  <code>factor.fit</code> will be similar.   
Given a n x n correlation matrix and a n x c matrix of -1,0,1 cluster weights for those n items on  c clusters, find the correlation of each item with each cluster.  If the item is part of the cluster, correct for item overlap.  Part of the <code>ICLUST</code> set of functions, but useful for many item analysis problems.
Cluster analysis and factor analysis are procedures for grouping items in terms of a smaller number of (latent) factors or (observed) clusters.  Graphical presentations of clusters typically show tree structures, although they can be represented in terms of item by cluster correlations.  

Cluster.plot plots items by their cluster loadings (taken, e.g., from <code>ICLUST</code>) or factor loadings (taken, eg., from <code>fa</code>).  Cluster membership may be assigned apriori or may be determined in terms of the highest (absolute) cluster loading for each item.  

If the input is an object of class "kmeans", then the cluster centers are plotted. 
The output of the kmeans clustering function produces a vector of cluster membership.  The <code>score.items</code> and <code>cluster.cor</code> functions require a matrix of keys.  cluster2keys does this.

May also be used to take the output of an <code>ICLUST</code> analysis and find a keys matrix.  (By doing a call to the <code>factor2cluster</code> function.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Cohen's kappa (Cohen, 1960) and weighted kappa (Cohen, 1968) may be used to find the agreement of two raters when using nominal scores.  Light's kappa is just the average cohen.kappa if using more than 2 raters. 

weighted.kappa is (probability of observed matches - probability of expected matches)/(1 - probability of expected matches).  Kappa just considers the matches on the main diagonal.  Weighted kappa considers off diagonal elements as well.
The congruence coefficient two matrices is just the cross product of their respective values divided by the square root of their sums of squares. If the columns are zero centered, this is just the correlation. If the columns are centered around the scale neutral point, this is Cohen's profile correlation. A set of distances (city block, euclidean, Minkowski) may be found by the distance function. 
In medicine and clinical psychology, diagnoses tend to be categorical (someone is depressed or not, someone has an anxiety disorder or not).  Cooccurrence  of both of these symptoms is called comorbidity.   Diagnostic categories vary in their degree of comorbidity with other diagnostic categories.  From the point of view of correlation, comorbidity is just a name applied to one cell in a four fold table.  It is thus possible to analyze comorbidity rates by considering the probability of the separate diagnoses and the probability of the joint diagnosis.  This gives the two by two table needed for a phi, Yule, or tetrachoric correlation.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
Classical Test Theory (CTT) considers four or more tests to be congenerically equivalent if all tests may be expressed in terms of one factor and a residual error.  Parallel tests are the special case where (usually two) tests have equal factor loadings.  Tau equivalent tests have equal factor loadings but may have unequal errors.  Congeneric tests may differ in both factor loading and error variances.
The congruence coefficient two matrices is just the cross product of their respective values divided by the square root of their sums of squares. If the columns are zero centered, this is just the correlation. If the columns are centered around the scale neutral point, this is Cohen's profile correlation. A set of distances (city block, euclidean, Minkowski) may be found by the distance function. 
Although normal theory provides confidence intervals for correlations, this is particularly problematic with Synthetic Aperture Personality Assessment (SAPA) data where the individual items are Massively Missing at Random.  Bootstrapped confidence intervals are found for Pearson, Spearman, Kendall, tetrachoric, or polychoric correlations and for scales made from those correlations. If given a correlation matrix and sample size(s), normal theory confidence intervals are provided.
Correlation matrices may be shown graphically by using the image function to emphasize structure.  This is a particularly useful tool for showing the structure of  correlation matrices with a clear structure.  Partially meant for the pedagogical value of the graphic for teaching or discussing factor analysis and other multivariate techniques.
Correlation matrices may be shown graphically by using the image function to emphasize structure.  This is a particularly useful tool for showing the structure of  correlation matrices with a clear structure.  Partially meant for the pedagogical value of the graphic for teaching or discussing factor analysis and other multivariate techniques.
Factor analysis requires positive definite correlation matrices.  Unfortunately, with pairwise deletion of missing data or if using <code>tetrachoric</code> or <code>polychoric</code> correlations, not all correlation matrices are positive definite.  cor.smooth does a eigenvector (principal components) smoothing.  Negative eigen values are replaced with 100  * eig.tol, the matrix is reproduced and forced to a correlation matrix using cov2cor.
Factor analysis requires positive definite correlation matrices.  Unfortunately, with pairwise deletion of missing data or if using <code>tetrachoric</code> or <code>polychoric</code> correlations, not all correlation matrices are positive definite.  cor.smooth does a eigenvector (principal components) smoothing.  Negative eigen values are replaced with 100  * eig.tol, the matrix is reproduced and forced to a correlation matrix using cov2cor.
If using aggregated data, the correlation of the means does not reflect the sample size used for each mean. cov.wt in RCore does this and returns a covariance matrix or the correlation matrix.  The cor.wt function weights by sample size or by standard errors and by default return correlations. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
A minor helper function to convert correlations (ranging from -1 to 1) to distances (ranging from 0 to 2). <i>d = √{(2(1-r))}</i>. 
Although normal theory provides confidence intervals for correlations, this is particularly problematic with Synthetic Aperture Personality Assessment (SAPA) data where the individual items are Massively Missing at Random.  Bootstrapped confidence intervals are found for Pearson, Spearman, Kendall, tetrachoric, or polychoric correlations and for scales made from those correlations. If given a correlation matrix and sample size(s), normal theory confidence intervals are provided.
Makes use of functions adapted from the lavaan package to find FIML covariance/correlation matrices.  FIML can be much slower than the normal pairwise deletion option of cor, but provides slightly more precise estimates.
Correlation matrices may be shown graphically by using the image function to emphasize structure.  This is a particularly useful tool for showing the structure of  correlation matrices with a clear structure.  Partially meant for the pedagogical value of the graphic for teaching or discussing factor analysis and other multivariate techniques.
Correlation matrices may be shown graphically by using the image function to emphasize structure.  This is a particularly useful tool for showing the structure of  correlation matrices with a clear structure.  Partially meant for the pedagogical value of the graphic for teaching or discussing factor analysis and other multivariate techniques.
Although the cor function finds the correlations for a matrix,  it does not report probability values.  cor.test does, but for only one pair of variables at a time.  corr.test uses cor to find the correlations for either complete or pairwise data and reports the sample sizes and probability values as well. For symmetric matrices, raw probabilites are reported below the diagonal and correlations adjusted for multiple comparisons above the diagonal. In the case of different x and ys, the default is to adjust the probabilities for multiple tests. Both corr.test and corr.p return raw and adjusted confidence intervals for each correlation. 
Although the cor function finds the correlations for a matrix,  it does not report probability values.  cor.test does, but for only one pair of variables at a time.  corr.test uses cor to find the correlations for either complete or pairwise data and reports the sample sizes and probability values as well. For symmetric matrices, raw probabilites are reported below the diagonal and correlations adjusted for multiple comparisons above the diagonal. In the case of different x and ys, the default is to adjust the probabilities for multiple tests. Both corr.test and corr.p return raw and adjusted confidence intervals for each correlation. 
Given a raw correlation matrix and a vector of reliabilities, report the disattenuated correlations above the diagonal.
Steiger (1980) pointed out that the sum of the squared elements of a correlation matrix, or the Fisher z score equivalents, is distributed as chi square under the null hypothesis that the values are zero (i.e., elements of the identity matrix).  This is particularly useful for examining whether correlations in a single matrix differ from zero or for comparing two matrices. Jennrich (1970) also examined tests of differences between matrices.
Bartlett (1951) proposed that -ln(det(R)*(N-1 - (2p+5)/6) was distributed as chi square if R were an identity matrix.  A useful test that residuals correlations are all zero. Contrast to the Kaiser-Meyer-Olkin test.
Steiger (1980) pointed out that the sum of the squared elements of a correlation matrix, or the Fisher z score equivalents, is distributed as chi square under the null hypothesis that the values are zero (i.e., elements of the identity matrix).  This is particularly useful for examining whether correlations in a single matrix differ from zero or for comparing two matrices. Jennrich (1970) also examined tests of differences between matrices.
Steiger (1980) pointed out that the sum of the squared elements of a correlation matrix, or the Fisher z score equivalents, is distributed as chi square under the null hypothesis that the values are zero (i.e., elements of the identity matrix).  This is particularly useful for examining whether correlations in a single matrix differ from zero or for comparing two matrices. Jennrich (1970) also examined tests of differences between matrices.
Steiger (1980) pointed out that the sum of the squared elements of a correlation matrix, or the Fisher z score equivalents, is distributed as chi square under the null hypothesis that the values are zero (i.e., elements of the identity matrix).  This is particularly useful for examining whether correlations in a single matrix differ from zero or for comparing two matrices. Jennrich (1970) also examined tests of differences between matrices.
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
Circadian data are periodic with a phase of 24 hours. These functions  find the best fitting phase angle (cosinor), the circular mean,  circular correlation with circadian data, and the linear by circular correlation
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Dynamic motivational models such as the Dynamics of Action (Atkinson and Birch, 1970, Revelle, 1986) may be reparameterized as a simple pair of differential (matrix) equations (Revelle, 1986, 2008). This function simulates the dynamic aspects of the CTA.  The CTA model is discussed in detail in Revelle and Condon (2015).
Dynamic motivational models such as the Dynamics of Action (Atkinson and Birch, 1970, Revelle, 1986) may be reparameterized as a simple pair of differential (matrix) equations (Revelle, 1986, 2008). This function simulates the dynamic aspects of the CTA.  The CTA model is discussed in detail in Revelle and Condon (2015).
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Among the many ways to describe a data set, one is a density plot for each value of a grouping variable and another is violin plot of multiple variables.  A density plot shows the density for different groups to show effect sizes. A violin plot is similar to a box plot but shows the actual distribution.Median and 25th and 75th percentile lines are added to the display. If a grouping variable is specified, violinBy will draw violin plots for each variable and for each group. Data points may be drawn as well.
There are many summary statistics available in R; this functionprovides the ones most useful for scale construction and item analysis in classic psychometrics. Range is most useful for the first pass in a data set, to check for coding errors.  
Report basic summary statistics by a grouping variable.  Useful if the grouping variable is some experimental variable and data are to be aggregated for plotting.  Partly a wrapper for by and <code>describe</code>
Report basic summary statistics by a grouping variable.  Useful if the grouping variable is some experimental variable and data are to be aggregated for plotting.  Partly a wrapper for by and <code>describe</code>
There are many summary statistics available in R; this functionprovides the ones most useful for scale construction and item analysis in classic psychometrics. Range is most useful for the first pass in a data set, to check for coding errors.  
There are many summary statistics available in R; this functionprovides the ones most useful for scale construction and item analysis in classic psychometrics. Range is most useful for the first pass in a data set, to check for coding errors.  
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
The congruence coefficient two matrices is just the cross product of their respective values divided by the square root of their sums of squares. If the columns are zero centered, this is just the correlation. If the columns are centered around the scale neutral point, this is Cohen's profile correlation. A set of distances (city block, euclidean, Minkowski) may be found by the distance function. 
A graphic of a correlation ellipse divided into 4 regions based upon x and y cutpoints on two normal distributions.  This is also an example of using the layout function. Draw a bivariate density plot to show how tetrachorics work.
A graphic of a correlation ellipse divided into 4 regions based upon x and y cutpoints on two normal distributions.  This is also an example of using the layout function. Draw a bivariate density plot to show how tetrachorics work.
Given a variable x with n distinct values, create n new dummy coded variables coded 0/1 for presence (1) or absence (0) of each variable.  A typical application would be to create dummy coded college majors from a vector of college majors. Can also combine categories by group.  By default, NA values of x are returned as NA (added 10/20/17)
Dwyer (1937) introduced a technique for factor extension and used 8 cognitive variables from Thurstone.  This is the example data set used in his paper.
 The default procedures for principal component returns values not immediately equivalent to the loadings from a factor analysis.  eigen.loadings translates them into the more typical metric of eigen vectors multiplied by the squareroot of the eigenvalues.   This lets us find pseudo factor loadings if we have used princomp  or eigen. <br>If we use <code>principal</code> to do our principal components analysis, then we do not need this routine.
For teaching correlation, it is useful to draw ellipses around the mean to reflect the correlation.  This variation of the ellipse function from John Fox's car package does so.  Input may be either two vectors or a matrix or data.frame.  In the latter cases, if the number of variables &gt;2, then the ellipses are done in the <code>pairs.panels</code> function. Ellipses may be added to existing plots. The minkowski function is included as a generalized ellipse.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
One of the many functions in R to plot means and confidence intervals. Can be done using barplots if desired.  Can also be combined with such functions as boxplot to summarize distributions.  Means and standard errors are calculated from the raw data using <code>describe</code>. Alternatively, plots of means +/- one standard deviation may be drawn.
One of the many functions in R to plot means and confidence intervals.  Meant mainly for demonstration purposes for showing the probabilty of replication from multiple samples.  Can also be combined with such functions as boxplot to summarize distributions.  Means and standard errors for each group are calculated using <code>describeBy</code>.
One of the many functions in R to plot means and confidence intervals. Can be done using barplots if desired.  Can also be combined with such functions as boxplot to summarize distributions.  Means and standard errors are calculated from the raw data using <code>describe</code>. Alternatively, plots of means +/- one standard deviation may be drawn.
Given two vectors of data (X and Y), plot the means and show standard errors in both X and Y directions. 
Yet one more of the graphical ways of showing data with error bars for different groups.A dot.chart with error bars for different groups or variables is found using from <code>describe</code>,  <code>describeBy</code>,  <code>statsBy</code> or data from <code>bestScales</code>.
Given a matrix or data frame, data, find statistics based upon a grouping variable and then plot x and y means with error bars for each value of the grouping variable.  If the data are paired (e.g. by gender), then plot means and error bars for the two groups on all variables. 
Structural Equation Modeling (SEM) is a powerful tool for confirming multivariate structures and is well done by the lavaan, sem, or OpenMx packages. Because they are confirmatory, SEM  models test specific models.  Exploratory Structural Equation Modeling (ESEM), on the other hand, takes a more exploratory approach.  By using factor extension, it is possible to extend the factors of one set of variables (X) into the variable space of another set (Y). Using this technique, it is then possible to estimate the correlations between the two sets of latent variables, much the way normal SEM would do.  Based upon exploratory factor analysis (EFA) this approach provides a quick and easy approach to do exploratory structural equation modeling.  
Structural Equation Modeling (SEM) is a powerful tool for confirming multivariate structures and is well done by the lavaan, sem, or OpenMx packages. Because they are confirmatory, SEM  models test specific models.  Exploratory Structural Equation Modeling (ESEM), on the other hand, takes a more exploratory approach.  By using factor extension, it is possible to extend the factors of one set of variables (X) into the variable space of another set (Y). Using this technique, it is then possible to estimate the correlations between the two sets of latent variables, much the way normal SEM would do.  Based upon exploratory factor analysis (EFA) this approach provides a quick and easy approach to do exploratory structural equation modeling.  
Factor analysis or principal components analysis results are typically interpreted in terms of the major loadings on each factor.  These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value &gt; some cut point are represented as an edge (path). <code>fa.diagram</code> uses the various <code>diagram</code> functions to draw the diagram. <code>fa.graph</code> generates dot code for external plotting.  <code>fa.rgraph</code> uses the Rgraphviz package (if available) to draw the graph. <code>het.diagram</code> will draw "heterarchy" diagrams of factor/scale solutions at different levels.
Among the many ways to do latent variable exploratory factor analysis (EFA), one of the better is to use Ordinary Least Squares (OLS) to find the minimum residual (minres) solution. This produces solutions very similar to maximum likelihood even for badly behaved matrices. A variation on minres is to do weighted least squares (WLS). Perhaps the most conventional technique is principal axes (PAF).  An eigen value decomposition of a correlation matrix is done and then the communalities for each variable are estimated by the first n factors. These communalities are entered onto the diagonal and the procedure is repeated until the sum(diag(r)) does not vary.   Yet another estimate procedure is maximum likelihood. For well behaved matrices, maximum likelihood factor analysis (either in the fa or in the factanal function) is probably preferred.  Bootstrapped confidence intervals of the loadings and interfactor correlations are found by fa  with n.iter &gt; 1. 
Given two sets of factor loadings, report their degree of congruence (vector cosine). Although first reported by Burt (1937,1  1948), this is frequently known as the Tucker index of factor congruence. Cohen's Profile similarity may be found as well. 
Factor analysis or principal components analysis results are typically interpreted in terms of the major loadings on each factor.  These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value &gt; some cut point are represented as an edge (path). <code>fa.diagram</code> uses the various <code>diagram</code> functions to draw the diagram. <code>fa.graph</code> generates dot code for external plotting.  <code>fa.rgraph</code> uses the Rgraphviz package (if available) to draw the graph. <code>het.diagram</code> will draw "heterarchy" diagrams of factor/scale solutions at different levels.
Dwyer (1937) introduced a method for finding factor loadings for variables not included in the original analysis.  This is basically finding the unattenuated correlation of the extension variables with the factor scores.  An alternative, which does not correct for factor reliability was proposed by Gorsuch (1997). Both options are an application of exploratory factor analysis with extensions to new variables. Also useful for finding the validities of variables in the factor space.  
Dwyer (1937) introduced a method for finding factor loadings for variables not included in the original analysis.  This is basically finding the unattenuated correlation of the extension variables with the factor scores.  An alternative, which does not correct for factor reliability was proposed by Gorsuch (1997). Both options are an application of exploratory factor analysis with extensions to new variables. Also useful for finding the validities of variables in the factor space.  
Factor analysis or principal components analysis results are typically interpreted in terms of the major loadings on each factor.  These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value &gt; some cut point are represented as an edge (path). <code>fa.diagram</code> uses the various <code>diagram</code> functions to draw the diagram. <code>fa.graph</code> generates dot code for external plotting.  <code>fa.rgraph</code> uses the Rgraphviz package (if available) to draw the graph. <code>het.diagram</code> will draw "heterarchy" diagrams of factor/scale solutions at different levels.
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
Some factor analytic solutions produce correlated factors which may in turn be factored.  If the solution has one higher order, the omega function is most appropriate.  But, in the case of multi higher order factors, then the faMulti function will do a lower level factoring and then factor the resulting correlation matrix.  Multi level factor diagrams are also shown.
Some factor analytic solutions produce correlated factors which may in turn be factored.  If the solution has one higher order, the omega function is most appropriate.  But, in the case of multi higher order factors, then the faMulti function will do a lower level factoring and then factor the resulting correlation matrix.  Multi level factor diagrams are also shown.
Although the print.psych function will sort factor analysis loadings, sometimes it is useful to do this outside of the print function. fa.sort takes the output from the fa or principal functions and sorts the loadings for each factor.  Items are located in terms of their greatest loading.  The new order is returned as an element in the fa list. fa.organize allows for the columns or rows to be reorganized.
One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the “scree" plot of the successive eigenvalues.  Sharp breaks in the plot suggest the appropriate number of components or factors to extract.  “Parallel" analyis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original. This may be done for continuous , dichotomous, or polytomous data using Pearson, tetrachoric or polychoric correlations.
One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the “scree" plot of the successive eigenvalues.  Sharp breaks in the plot suggest the appropriate number of components or factors to extract.  “Parallel" analyis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original. This may be done for continuous , dichotomous, or polytomous data using Pearson, tetrachoric or polychoric correlations.
Cluster analysis and factor analysis are procedures for grouping items in terms of a smaller number of (latent) factors or (observed) clusters.  Graphical presentations of clusters typically show tree structures, although they can be represented in terms of item by cluster correlations.  

Cluster.plot plots items by their cluster loadings (taken, e.g., from <code>ICLUST</code>) or factor loadings (taken, eg., from <code>fa</code>).  Cluster membership may be assigned apriori or may be determined in terms of the highest (absolute) cluster loading for each item.  

If the input is an object of class "kmeans", then the cluster centers are plotted. 
After 6  years, it is time to stop using these deprecated functions!  Please see <code>fa</code> which includes all of the functionality of these older functions.  
Among the many ways to do latent variable exploratory factor analysis (EFA), one of the better is to use Ordinary Least Squares (OLS) to find the minimum residual (minres) solution. This produces solutions very similar to maximum likelihood even for badly behaved matrices. A variation on minres is to do weighted least squares (WLS). Perhaps the most conventional technique is principal axes (PAF).  An eigen value decomposition of a correlation matrix is done and then the communalities for each variable are estimated by the first n factors. These communalities are entered onto the diagonal and the procedure is repeated until the sum(diag(r)) does not vary.   Yet another estimate procedure is maximum likelihood. For well behaved matrices, maximum likelihood factor analysis (either in the fa or in the factanal function) is probably preferred.  Bootstrapped confidence intervals of the loadings and interfactor correlations are found by fa  with n.iter &gt; 1. 
Inspired, in part, by the wprifm function in the profileR package, fa.random removes between subject differences in mean level and then does a normal exploratory factor analysis of the ipsatized data.  Functionally, this removes a general factor of the data before factoring. To prevent non-positive definiteness of the residual data matrix, a very small amount of random noise is added to each variable. This is just a call to fa after removing the between subjects effect. Read the help file for <code>fa</code> for a detailed explanation of all of the input parameters and the output objects. 
Factor analysis or principal components analysis results are typically interpreted in terms of the major loadings on each factor.  These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value &gt; some cut point are represented as an edge (path). <code>fa.diagram</code> uses the various <code>diagram</code> functions to draw the diagram. <code>fa.graph</code> generates dot code for external plotting.  <code>fa.rgraph</code> uses the Rgraphviz package (if available) to draw the graph. <code>het.diagram</code> will draw "heterarchy" diagrams of factor/scale solutions at different levels.
Among the many ways to do latent variable exploratory factor analysis (EFA), one of the better is to use Ordinary Least Squares (OLS) to find the minimum residual (minres) solution. This produces solutions very similar to maximum likelihood even for badly behaved matrices. A variation on minres is to do weighted least squares (WLS). Perhaps the most conventional technique is principal axes (PAF).  An eigen value decomposition of a correlation matrix is done and then the communalities for each variable are estimated by the first n factors. These communalities are entered onto the diagonal and the procedure is repeated until the sum(diag(r)) does not vary.   Yet another estimate procedure is maximum likelihood. For well behaved matrices, maximum likelihood factor analysis (either in the fa or in the factanal function) is probably preferred.  Bootstrapped confidence intervals of the loadings and interfactor correlations are found by fa  with n.iter &gt; 1. 
Although the print.psych function will sort factor analysis loadings, sometimes it is useful to do this outside of the print function. fa.sort takes the output from the fa or principal functions and sorts the loadings for each factor.  Items are located in terms of their greatest loading.  The new order is returned as an element in the fa list. fa.organize allows for the columns or rows to be reorganized.
Chi square and other goodness of fit statistics are found based upon the fit of a factor or components model to a correlation matrix.  Although these statistics are normally associated with a maximum likelihood solution, they can be found for minimal residual (OLS), principal axis, or principal component solutions as well.  Primarily called from within these functions, factor.stats can be used by itself. Measures of factorial adequacy and validity follow the paper by Grice, 2001.
Although exploratory factor analysis and Item Response Theory seem to be very different models of binary data, they can provide equivalent parameter estimates of item difficulty and item discrimination.  Tetrachoric or polychoric correlations of a data set of dichotomous or polytomous items may be factor analysed using a minimum residual or maximum likelihood factor analysis and the result loadings transformed to item discrimination parameters.  The tau parameter from the tetrachoric/polychoric correlations combined with the item factor loading may be used to estimate item difficulties. 
When examining data at two levels (e.g., the individual and by some set of grouping variables), it is useful to find basic descriptive statistics (means, sds, ns per group, within group correlations) as well as between group statistics (over all descriptive statistics, and overall between group correlations). Of particular use is the ability to decompose a matrix of correlations at the individual level into correlations within group and correlations between groups. 
Among the many ways to do latent variable exploratory factor analysis (EFA), one of the better is to use Ordinary Least Squares (OLS) to find the minimum residual (minres) solution. This produces solutions very similar to maximum likelihood even for badly behaved matrices. A variation on minres is to do weighted least squares (WLS). Perhaps the most conventional technique is principal axes (PAF).  An eigen value decomposition of a correlation matrix is done and then the communalities for each variable are estimated by the first n factors. These communalities are entered onto the diagonal and the procedure is repeated until the sum(diag(r)) does not vary.   Yet another estimate procedure is maximum likelihood. For well behaved matrices, maximum likelihood factor analysis (either in the fa or in the factanal function) is probably preferred.  Bootstrapped confidence intervals of the loadings and interfactor correlations are found by fa  with n.iter &gt; 1. 
Given two factor analysis or pca solutions to a data matrix or correlation, what are the similarities between the two solutions. This may be found by factor correlations as well as factor congruences.  Factor correlations are found by the matrix product of the factor weights and the correlation matrix and are estimates of what the factor score correlations would be.  Factor congruence (aka Tucker or Burt coefficient) is the cosine of the vectors of factor loadings.
Given two sets of factor loadings, report their degree of congruence (vector cosine). Although first reported by Burt (1937,1  1948), this is frequently known as the Tucker index of factor congruence. Cohen's Profile similarity may be found as well. 
The basic factor or principal components model is that a correlation or covariance matrix may be reproduced by the product of a factor loading matrix times its transpose: F'F or P'P.  One simple index of fit is the 1 - sum squared residuals/sum squared original correlations. This fit index is used by <code>VSS</code>, <code>ICLUST</code>, etc. 
After 6  years, it is time to stop using these deprecated functions!  Please see <code>fa</code> which includes all of the functionality of these older functions.  
The basic factor or principal components model is that a correlation or covariance matrix may be reproduced by the product of a factor loading matrix times its transpose.  Find this reproduced matrix.  Used by <code>factor.fit</code>, <code>VSS</code>, <code>ICLUST</code>, etc.
After 6  years, it is time to stop using these deprecated functions!  Please see <code>fa</code> which includes all of the functionality of these older functions.  
Cluster analysis and factor analysis are procedures for grouping items in terms of a smaller number of (latent) factors or (observed) clusters.  Graphical presentations of clusters typically show tree structures, although they can be represented in terms of item by cluster correlations.  

Cluster.plot plots items by their cluster loadings (taken, e.g., from <code>ICLUST</code>) or factor loadings (taken, eg., from <code>fa</code>).  Cluster membership may be assigned apriori or may be determined in terms of the highest (absolute) cluster loading for each item.  

If the input is an object of class "kmeans", then the cluster centers are plotted. 
The basic factor or principal components model is that a correlation or covariance matrix may be reproduced by the product of a factor loading matrix times its transpose.  Find the residuals of the original minus the  reproduced matrix.  Used by <code>factor.fit</code>, <code>VSS</code>, <code>ICLUST</code>, etc.
Given a factor or components matrix, it is sometimes useful to do arbitrary rotations of particular pairs of variables.  This supplements the much more powerful rotation package GPArotation and is meant for specific requirements to do unusual rotations.
A fundamental problem with factor analysis is that although the model is defined at the structural level, it is indeterminate at the data level. This problem of factor indeterminancy leads to alternative ways of estimating factor scores, none of which is ideal.  Following Grice (2001) four different methods are available here.
Chi square and other goodness of fit statistics are found based upon the fit of a factor or components model to a correlation matrix.  Although these statistics are normally associated with a maximum likelihood solution, they can be found for minimal residual (OLS), principal axis, or principal component solutions as well.  Primarily called from within these functions, factor.stats can be used by itself. Measures of factorial adequacy and validity follow the paper by Grice, 2001.
After 6  years, it is time to stop using these deprecated functions!  Please see <code>fa</code> which includes all of the functionality of these older functions.  
Given a factor or principal components loading matrix, assign each item to a cluster corresponding to the largest (signed) factor loading for that item.  Essentially, this is a Very Simple Structure approach to cluster definition that corresponds to what most people actually do: highlight the largest loading for each item and ignore the rest.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
A dirty little secret of factor rotation algorithms is the problem of local minima (Nguyen and Waller,2022).  Following ideas in that article, we allow for multiple random restarts and then return the global optimal solution.  Used as part of the <code>fa</code> function or available as a stand alone function. 
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Formula input from e.g., lm,  may be extended to include mediators,  quadratic and partial terms using a standard syntax. This is use by <code>setCor</code> and <code>mediate</code>.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Garcia, Schmitt, Branscombe, and Ellemers (2010) report data for 129 subjects on the effects of perceived sexism on anger and liking of women's reactions to ingroup members who protest discrimination. This data set is also used as the ‘protest’ data set by Hayes (2013 and 2018).  It is a useful example of mediation and moderation in regression. It may also be used as an example of plotting interactions.
The geometric mean is the nth root of n products or e to the mean log of x.Useful for describing non-normal, i.e., geometric distributions.
Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<i>μ_0 … μ_3)</i> as well as <i>β</i> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <i>ω_h</i> and <i>ω_t</i> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
The greatest lower bound solves the “educational testing problem". That is, what is the reliability of a test? (See <code>guttman</code> for a discussion of the problem). Although there are many estimates of a test reliability (Guttman, 1945) most underestimate the true reliability of a test.

For a given covariance matrix of items, C, the function finds the greatest lower bound to reliability of the total score using the csdp function from the Rcsdp package.
Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<i>μ_0 … μ_3)</i> as well as <i>β</i> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <i>ω_h</i> and <i>ω_t</i> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
Gleser, Cronbach and Rajaratnam (1965) discuss the estimation of variance components and their ratios as part of their introduction to generalizability theory.  This is a adaptation of their "illustrative data for a completely matched G study" (Table 3).  12 patients are rated on 6 symptoms by two judges.  Components of variance are derived from the ANOVA. 
Gorsuch (1997) suggests an alternative to the classic Dwyer (1937) factor extension technique.  This data set is taken from that article.  Useful for comparing <code>link{fa.extension}</code> with and without the correct=TRUE option.  
Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<i>μ_0 … μ_3)</i> as well as <i>β</i> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <i>ω_h</i> and <i>ω_t</i> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
Five classic data sets reported by Harman (1967) are 9 psychological (cognitive) variables taken from Holzinger  and 8 emotional variables taken from Burt. Two others are socioeconomic and political data sets. Additionally, 8 physical variables.  All five of these are used for tests and demonstrations of various factoring algortithms.
Five classic data sets reported by Harman (1967) are 9 psychological (cognitive) variables taken from Holzinger  and 8 emotional variables taken from Burt. Two others are socioeconomic and political data sets. Additionally, 8 physical variables.  All five of these are used for tests and demonstrations of various factoring algortithms.
Five classic data sets reported by Harman (1967) are 9 psychological (cognitive) variables taken from Holzinger  and 8 emotional variables taken from Burt. Two others are socioeconomic and political data sets. Additionally, 8 physical variables.  All five of these are used for tests and demonstrations of various factoring algortithms.
Five classic data sets reported by Harman (1967) are 9 psychological (cognitive) variables taken from Holzinger  and 8 emotional variables taken from Burt. Two others are socioeconomic and political data sets. Additionally, 8 physical variables.  All five of these are used for tests and demonstrations of various factoring algortithms.
Five classic data sets reported by Harman (1967) are 9 psychological (cognitive) variables taken from Holzinger  and 8 emotional variables taken from Burt. Two others are socioeconomic and political data sets. Additionally, 8 physical variables.  All five of these are used for tests and demonstrations of various factoring algortithms.
The harmonic mean is merely the reciprocal of the arithmetic mean of the reciprocals.
A quick way to show the first and last n lines of a data.frame, matrix, or a text object.  Just a pretty call to <code>head</code> and <code>tail</code> or <code>View</code>
A quick way to show the first and last n lines of a data.frame, matrix, or a text object.  Just a pretty call to <code>head</code> and <code>tail</code> or <code>View</code>
Factor analysis or principal components analysis results are typically interpreted in terms of the major loadings on each factor.  These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value &gt; some cut point are represented as an edge (path). <code>fa.diagram</code> uses the various <code>diagram</code> functions to draw the diagram. <code>fa.graph</code> generates dot code for external plotting.  <code>fa.rgraph</code> uses the Rgraphviz package (if available) to draw the graph. <code>het.diagram</code> will draw "heterarchy" diagrams of factor/scale solutions at different levels.
Given a matrix or data.frame, produce histograms for each variable in a "matrix" form. Include normal fits and density distributions for each plot.

The number of rows and columns may be specified, or calculated.May be used for single variables.
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
The Intraclass correlation is used as a measure of association when studying the reliability of raters.  Shrout and Fleiss (1979) outline 6 different estimates, that depend upon the particular experimental design. All are implemented and given confidence limits.  Uses either aov or lmer depending upon options.  lmer allows for missing values.
A common data reduction technique is to cluster cases (subjects). Less common, but particularly useful in psychological research, is to cluster items (variables). This may be thought of as an alternative to factor analysis, based upon a much simpler model. The cluster model is that the correlations between variables reflect that each item loads on at most one cluster, and that items that load on those clusters correlate as a function of their respective loadings on that cluster and items that define different clusters correlate as a function of their respective cluster loadings and the intercluster correlations. Essentially, the cluster model is a Very Simple Structure factor model of complexity one (see <code>VSS</code>).

This function applies the iclust algorithm to hierarchically cluster items to form composite scales. Clusters are combined if coefficients alpha and beta will increase in the new cluster.

Alpha, the mean split half correlation, and beta, the worst split half correlation, are estimates of the reliability and general factor saturation of the test.  (See also the <code>omega</code> function to estimate McDonald's coeffients   <i>omega hierarchical</i> and   <i>omega total</i>)
A common data reduction technique is to cluster cases (subjects). Less common, but particularly useful in psychological research, is to cluster items (variables). This may be thought of as an alternative to factor analysis, based upon a much simpler model. The cluster model is that the correlations between variables reflect that each item loads on at most one cluster, and that items that load on those clusters correlate as a function of their respective loadings on that cluster and items that define different clusters correlate as a function of their respective cluster loadings and the intercluster correlations. Essentially, the cluster model is a Very Simple Structure factor model of complexity one (see <code>VSS</code>).

This function applies the iclust algorithm to hierarchically cluster items to form composite scales. Clusters are combined if coefficients alpha and beta will increase in the new cluster.

Alpha, the mean split half correlation, and beta, the worst split half correlation, are estimates of the reliability and general factor saturation of the test.  (See also the <code>omega</code> function to estimate McDonald's coeffients   <i>omega hierarchical</i> and   <i>omega total</i>)
The guts of the  <code>ICLUST</code> algorithm.  Called by <code>ICLUST</code> See ICLUST for description.
Given a cluster structure determined by <code>ICLUST</code>, create a graphic structural diagram using graphic functions in the psych package To create dot code to describe the <code>ICLUST</code> output with more precision, use <code>ICLUST.graph</code>. If Rgraphviz has been successfully installed, the alternative is to use <code>ICLUST.rgraph</code>.
Given a cluster structure determined by <code>ICLUST</code>, create dot code to describe the <code>ICLUST</code> output.  To use the dot code, use either https://www.graphviz.org/ Graphviz or a commercial viewer (e.g., OmniGraffle).  This function parallels <code>ICLUST.rgraph</code> which uses Rgraphviz.  
Given a cluster structure determined by <code>ICLUST</code>, create a rgraphic directly using Rgraphviz.  To create dot code to describe the <code>ICLUST</code> output with more precision, use <code>ICLUST.graph</code>.  As an option, dot code is also generated and saved in a file. To  use the dot code, use either https://www.graphviz.org/ Graphviz or a commercial viewer (e.g., OmniGraffle).
Given a cluster analysis or factor analysis loadings matrix, sort the items by the (absolute) size of each column of loadings.  Used as part of ICLUST and SAPA analyses. The columns are rearranged by the 
Given a cluster analysis or factor analysis loadings matrix, sort the items by the (absolute) size of each column of loadings.  Used as part of ICLUST and SAPA analyses. The columns are rearranged by the 
Structural Equation Modeling (SEM) is a powerful tool for confirming multivariate structures and is well done by the lavaan, sem, or OpenMx packages. Because they are confirmatory, SEM  models test specific models.  Exploratory Structural Equation Modeling (ESEM), on the other hand, takes a more exploratory approach.  By using factor extension, it is possible to extend the factors of one set of variables (X) into the variable space of another set (Y). Using this technique, it is then possible to estimate the correlations between the two sets of latent variables, much the way normal SEM would do.  Based upon exploratory factor analysis (EFA) this approach provides a quick and easy approach to do exploratory structural equation modeling.  
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
For data with a limited number of response categories (e.g., attitude items), it is useful treat each response category as range with width, w and linearly interpolate the median, quartiles, or any quantile value within the median response.   
Item Response Theory models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.  This is an early and crude attempt to capture this modeling procedure. A better procedure is to use  <code>irt.fa</code>. 
Item Response Theory models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.  This is an early and crude attempt to capture this modeling procedure. A better procedure is to use  <code>irt.fa</code>. 
Item Response Theory models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.  This is an early and crude attempt to capture this modeling procedure. A better procedure is to use  <code>irt.fa</code>. 
Steps toward a  very crude and preliminary IRT program. These two functions  estimate item difficulty and discrimination parameters.  A better procedure is to use <code>irt.fa</code> or the ltm package.
Although exploratory factor analysis and Item Response Theory seem to be very different models of binary data, they can provide equivalent parameter estimates of item difficulty and item discrimination.  Tetrachoric or polychoric correlations of a data set of dichotomous or polytomous items may be factor analysed using a minimum residual or maximum likelihood factor analysis and the result loadings transformed to item discrimination parameters.  The tau parameter from the tetrachoric/polychoric correlations combined with the item factor loading may be used to estimate item difficulties. 
Steps toward a  very crude and preliminary IRT program. These two functions  estimate item difficulty and discrimination parameters.  A better procedure is to use <code>irt.fa</code> or the ltm package.
Item Response Theory models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.  This is an early and crude attempt to capture this modeling procedure. A better procedure is to use  <code>irt.fa</code>. 
When analyzing ability tests, it is important to consider how the distractor alternatives vary as a function of the latent trait.  The simple graphical solution is to plot response endorsement frequencies against the values of the latent trait found from multiple items. A good item is one in which the probability of the distractors decrease and the keyed answer increases as the latent trait increases. 
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
Although exploratory factor analysis and Item Response Theory seem to be very different models of binary data, they can provide equivalent parameter estimates of item difficulty and item discrimination.  Tetrachoric or polychoric correlations of a data set of dichotomous or polytomous items may be factor analysed using a minimum residual or maximum likelihood factor analysis and the result loadings transformed to item discrimination parameters.  The tau parameter from the tetrachoric/polychoric correlations combined with the item factor loading may be used to estimate item difficulties. 
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
The validity of a scale varies as a function of the number of items in the scale, their average intercorrelation, and their average validity. The asymptotic limit of a scales validity for any particular criterion is just the average validity divided by the square root of the average within scale item correlation.  <code>predicted.validity</code> will find the predicted validity for a set of scales (defined by a keys.list) and the average item validity for various criteria.  

The function will find (and report) scale reliabilities (using <code>reliability</code>) and average item validities (using <code>item.validity</code>)
Kaiser (1958) suggested normalizing factor loadings before rotating them, and then denormalizing them after rotation.  The GPArotation package does not (by default) normalize, nor does the <code>fa</code> function. Then, to make it more confusing, varimax in stats does,Varimax in GPArotation does not. <code>kaiser</code> will take the output of a non-normalized solution and report the normalized solution. 
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
 When scoring items by forming composite scales either from the raw data using <code>scoreItems</code> or from the correlation matrix using <code>cluster.cor</code>, it used to be  necessary to create a keys matrix. This is no longer necessary as most of the scoring functions will directly use a keys list. <code>make.keys</code>  is just a short cut for creating a keys matrix.  The keys matrix is a nvar x nscales matrix of -1,0, 1 and defines the membership for each scale. Items can be specified by location or by name.
Given a set of n items, form n/2 or n/3 mini scales or parcels of the most similar pairs or triplets of items.  These may be used as the basis for subsequent scale construction or multivariate (e.g., factor) analysis.
Henry Kaiser (1970) introduced an Measure of Sampling Adequacy (MSA) of factor analytic data matrices. Kaiser and Rice (1974) then modified it. This is just a function of the squared elements of the ‘image’ matrix compared to the squares of the original correlations.  The overall MSA as well as estimates for each item are found. The index is known as the Kaiser-Meyer-Olkin (KMO) index.
Find the skew and kurtosis for each variable in a data.frame or matrix.  Unlike skew and kurtosis in e1071, this calculates a different skew for each variable or column of a data.frame/matrix. mardia applies Mardia's tests for multivariate skew and kurtosis
Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.

Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 

Input can be specified as matrices or the output from  <code>fa</code>, factanal,  or a rotation package such as <em>GPArotation</em>.

For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.

As an option, for those without Rgraphviz installed, <code>structure.sem</code> will just create the sem model and skip the graph. (This functionality is now included in <code>structure.diagram</code>.)

structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.

<code>lavaan.diagram</code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code>structure.diagram</code>.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
The logistic function (1/(1+exp(-x)) and logit function (log(p/(1-p)) are fundamental to Item Response Theory.  Although just one line functions, they are included here for ease of demonstrations and in drawing IRT models. Also included is the logistic.grm for a graded response model.
The logistic function (1/(1+exp(-x)) and logit function (log(p/(1-p)) are fundamental to Item Response Theory.  Although just one line functions, they are included here for ease of demonstrations and in drawing IRT models. Also included is the logistic.grm for a graded response model.
The logistic function (1/(1+exp(-x)) and logit function (log(p/(1-p)) are fundamental to Item Response Theory.  Although just one line functions, they are included here for ease of demonstrations and in drawing IRT models. Also included is the logistic.grm for a graded response model.
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
When reporting correlation matrices for two samples (e.g., males and females), it is convenient to show them as one matrix, with entries below the diagonal representing one matrix, and entries above the diagonal the other matrix.  It is also useful to compare a correlation matrix with the residuals from a fitted (e.g., factor) model.  
An example data set used by McDonald (1999) as well as other discussions of Item Response Theory makes use of a data table on 10 items (two sets of 5) from the Law School Admissions Test (LSAT).  Included in this data set is the original table as well as the reponses for 1000 subjects on the first set (Figure Classification) and second set (Debate). 
An example data set used by McDonald (1999) as well as other discussions of Item Response Theory makes use of a data table on 10 items (two sets of 5) from the Law School Admissions Test (LSAT).  Included in this data set is the original table as well as the reponses for 1000 subjects on the first set (Figure Classification) and second set (Debate). 
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Classical Test Theory (CTT) considers four or more tests to be congenerically equivalent if all tests may be expressed in terms of one factor and a residual error.  Parallel tests are the special case where (usually two) tests have equal factor loadings.  Tau equivalent tests have equal factor loadings but may have unequal errors.  Congeneric tests may differ in both factor loading and error variances.
Create a population orthogonal or hierarchical correlation matrix from a set of factor loadings and factor intercorrelations. Samples of size n may be then be drawn from this population.  Return either the sample data, sample correlations, or population correlations.  This is used to create sample data sets for instruction and demonstration.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
 When scoring items by forming composite scales either from the raw data using <code>scoreItems</code> or from the correlation matrix using <code>cluster.cor</code>, it used to be  necessary to create a keys matrix. This is no longer necessary as most of the scoring functions will directly use a keys list. <code>make.keys</code>  is just a short cut for creating a keys matrix.  The keys matrix is a nvar x nscales matrix of -1,0, 1 and defines the membership for each scale. Items can be specified by location or by name.
 When scoring items by forming composite scales either from the raw data using <code>scoreItems</code> or from the correlation matrix using <code>cluster.cor</code>, it used to be  necessary to create a keys matrix. This is no longer necessary as most of the scoring functions will directly use a keys list. <code>make.keys</code>  is just a short cut for creating a keys matrix.  The keys matrix is a nvar x nscales matrix of -1,0, 1 and defines the membership for each scale. Items can be specified by location or by name.
A useful way of showing the strength of many correlations with a particular criterion is the Manhattan plot.  This is just a plot of correlations ordered by some keying variable.  Useful to understand the basis of items used in <code>bestScales</code>.
Find the skew and kurtosis for each variable in a data.frame or matrix.  Unlike skew and kurtosis in e1071, this calculates a different skew for each variable or column of a data.frame/matrix. mardia applies Mardia's tests for multivariate skew and kurtosis
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
To see the structure of a correlation matrix, it is helpful to organize the items so that the similar items are grouped together. One such grouping technique is factor analysis.  mat.sort will sort the items by a factor model (if specified), or any other order, or by the loadings on the first factor (if unspecified)
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
To see the structure of a correlation matrix, it is helpful to organize the items so that the similar items are grouped together. One such grouping technique is factor analysis.  mat.sort will sort the items by a factor model (if specified), or any other order, or by the loadings on the first factor (if unspecified)
Find the direct and indirect effects of a predictor in path models of mediation and moderation. Bootstrap confidence intervals for the indirect effects.  Mediation models are just extended regression models making explicit the effect of particular covariates in the model. Moderation is done by multiplication of the predictor variables.   This function supplies basic mediation/moderation analyses for some of the classic problem types. 
Find the direct and indirect effects of a predictor in path models of mediation and moderation. Bootstrap confidence intervals for the indirect effects.  Mediation models are just extended regression models making explicit the effect of particular covariates in the model. Moderation is done by multiplication of the predictor variables.   This function supplies basic mediation/moderation analyses for some of the classic problem types. 
For teaching correlation, it is useful to draw ellipses around the mean to reflect the correlation.  This variation of the ellipse function from John Fox's car package does so.  Input may be either two vectors or a matrix or data.frame.  In the latter cases, if the number of variables &gt;2, then the ellipses are done in the <code>pairs.panels</code> function. Ellipses may be added to existing plots. The minkowski function is included as a generalized ellipse.
For data sets with continuous, polytomous and dichotmous variables, the absolute Pearson correlation is downward biased from the underlying latent correlation.  mixedCor finds Pearson correlations for the continous variables, <code>polychoric</code>s for the polytomous items, <code>tetrachoric</code>s for the dichotomous items, and the <code>polyserial</code> or <code>biserial</code> correlations for the various mixed variables. Results include the complete correlation matrix, as well as the separate correlation matrices and difficulties for the polychoric and tetrachoric correlations.
For data sets with continuous, polytomous and dichotmous variables, the absolute Pearson correlation is downward biased from the underlying latent correlation.  mixedCor finds Pearson correlations for the continous variables, <code>polychoric</code>s for the polytomous items, <code>tetrachoric</code>s for the dichotomous items, and the <code>polyserial</code> or <code>biserial</code> correlations for the various mixed variables. Results include the complete correlation matrix, as well as the separate correlation matrices and difficulties for the polychoric and tetrachoric correlations.
Various indicators of reliability of multilevel data (e.g., items over time nested within subjects) may be found using generalizability theory.  A basic three way anova is applied to the data from which variance components are extracted. Random effects for a nested design are found by lme.    These are, in turn, converted to several reliability/generalizability coefficients.  An optional call to lme4 to use lmer may be used for unbalanced designs with missing data. mlArrange is a  helper function to convert wide to long format.  Data can be rearranged from wide to long format, and multiple lattice plots of observations overtime for multiple variables and multiple subjects are created.
Various indicators of reliability of multilevel data (e.g., items over time nested within subjects) may be found using generalizability theory.  A basic three way anova is applied to the data from which variance components are extracted. Random effects for a nested design are found by lme.    These are, in turn, converted to several reliability/generalizability coefficients.  An optional call to lme4 to use lmer may be used for unbalanced designs with missing data. mlArrange is a  helper function to convert wide to long format.  Data can be rearranged from wide to long format, and multiple lattice plots of observations overtime for multiple variables and multiple subjects are created.
Various indicators of reliability of multilevel data (e.g., items over time nested within subjects) may be found using generalizability theory.  A basic three way anova is applied to the data from which variance components are extracted. Random effects for a nested design are found by lme.    These are, in turn, converted to several reliability/generalizability coefficients.  An optional call to lme4 to use lmer may be used for unbalanced designs with missing data. mlArrange is a  helper function to convert wide to long format.  Data can be rearranged from wide to long format, and multiple lattice plots of observations overtime for multiple variables and multiple subjects are created.
Find the direct and indirect effects of a predictor in path models of mediation and moderation. Bootstrap confidence intervals for the indirect effects.  Mediation models are just extended regression models making explicit the effect of particular covariates in the model. Moderation is done by multiplication of the predictor variables.   This function supplies basic mediation/moderation analyses for some of the classic problem types. 
Von Neuman et al. (1941) discussed the Mean Square of Successive Differences as a measure of variability that takes into account gradual shifts in mean. This is appropriate when studying errors in ballistics or variability and stability in mood when studying affect. For random data, this will be twice the variance, but for data with a sequential order and a positive autocorrelation, this will be much smaller. Since the mssd is just twice the variance - the autocorrelation, it is thus possible to also find the autocorrelation for a particular lag. 
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Given a matrix or data.frame, produce histograms for each variable in a "matrix" form. Include normal fits and density distributions for each plot.

The number of rows and columns may be specified, or calculated.May be used for single variables.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Path models are used to describe structural equation models or cluster analytic output.  These functions provide the primitives for drawing path models.  Used as a substitute for some of the functionality of Rgraphviz.
Various indicators of reliability of multilevel data (e.g., items over time nested within subjects) may be found using generalizability theory.  A basic three way anova is applied to the data from which variance components are extracted. Random effects for a nested design are found by lme.    These are, in turn, converted to several reliability/generalizability coefficients.  An optional call to lme4 to use lmer may be used for unbalanced designs with missing data. mlArrange is a  helper function to convert wide to long format.  Data can be rearranged from wide to long format, and multiple lattice plots of observations overtime for multiple variables and multiple subjects are created.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
There are multiple ways to determine the appropriate number of factors in exploratory factor analysis. Routines for the Very Simple Structure (VSS) criterion allow one to compare solutions of varying complexity and for different number of factors. Graphic output indicates the "optimal" number of factors for different levels of complexity.  The Velicer MAP criterion is another good choice. <code>nfactors</code> finds and plots several of these alternative estimates.
McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
Hierarchical factor structures represent the correlations between variables in terms of a smaller set of correlated factors which themselves can be represented by a higher order factor.

Two alternative solutions to such structures are found by the <code>omega</code> function.  The correlated factors solutions represents the effect of the higher level, general factor, through its effect on the correlated factors.  The other representation makes use of the Schmid Leiman transformation to find the direct effect of the general factor upon the original variables as well as the effect of orthogonal residual group factors upon the items. 

Graphic presentations of these two alternatives are helpful in understanding the structure.  omega.graph  and omega.diagram draw both such structures.  Graphs are drawn directly onto the graphics window or expressed in “dot" commands for conversion to graphics using implementations of Graphviz (if using omega.graph).

Using Graphviz allows the user to clean up the Rgraphviz output. However, if Graphviz and Rgraphviz are not available, use omega.diagram.  

See the other structural diagramming functions, <code>fa.diagram</code> and <code>structure.diagram</code>. 

In addition 
Hierarchical factor structures represent the correlations between variables in terms of a smaller set of correlated factors which themselves can be represented by a higher order factor.

Two alternative solutions to such structures are found by the <code>omega</code> function.  The correlated factors solutions represents the effect of the higher level, general factor, through its effect on the correlated factors.  The other representation makes use of the Schmid Leiman transformation to find the direct effect of the general factor upon the original variables as well as the effect of orthogonal residual group factors upon the items. 

Graphic presentations of these two alternatives are helpful in understanding the structure.  omega.graph  and omega.diagram draw both such structures.  Graphs are drawn directly onto the graphics window or expressed in “dot" commands for conversion to graphics using implementations of Graphviz (if using omega.graph).

Using Graphviz allows the user to clean up the Rgraphviz output. However, if Graphviz and Rgraphviz are not available, use omega.diagram.  

See the other structural diagramming functions, <code>fa.diagram</code> and <code>structure.diagram</code>. 

In addition 
McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
McDonald has proposed coefficient omega as an estimate of the general factor saturation of a test.  One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. This function estimates omega as suggested by McDonald by using hierarchical factor analysis (following Jensen). A related option is to define the model using omega and then perform a confirmatory (bi-factor) analysis using the sem or lavaan packages.  This is done by omegaSem and omegaFromSem. omegaFromSem will convert appropriate sem/lavaan objects to find omega.  Yet another option is to do the direct Schmid-Leiman of Waller.
The Mahalanobis distance is <i>D^2 = (x-μ)' Σ^-1 (x-μ)</i> where <i>Σ</i> is the covariance of the x matrix.  D2 may be used as a way of detecting outliers in distribution.  Large D2 values, compared to the expected Chi Square values indicate an unusual response pattern.  The mahalanobis function in stats does not handle missing data.
The probability of replication of an experimental or correlational finding as discussed by Peter Killeen (2005) is the probability of finding an effect in the same direction upon an exact replication.  For articles submitted to Psychological Science, p.rep needs to be reported. 

F, t, p and r are all estimates of the size of an effect.  But F, t, and p also are also a function of the sample size.  Effect size, d prime, may be expressed as differences between means compared to within cell standard deviations, or as a correlation coefficient.  These functions convert p, F, and t to d prime and the r equivalent.
The probability of replication of an experimental or correlational finding as discussed by Peter Killeen (2005) is the probability of finding an effect in the same direction upon an exact replication.  For articles submitted to Psychological Science, p.rep needs to be reported. 

F, t, p and r are all estimates of the size of an effect.  But F, t, and p also are also a function of the sample size.  Effect size, d prime, may be expressed as differences between means compared to within cell standard deviations, or as a correlation coefficient.  These functions convert p, F, and t to d prime and the r equivalent.
The probability of replication of an experimental or correlational finding as discussed by Peter Killeen (2005) is the probability of finding an effect in the same direction upon an exact replication.  For articles submitted to Psychological Science, p.rep needs to be reported. 

F, t, p and r are all estimates of the size of an effect.  But F, t, and p also are also a function of the sample size.  Effect size, d prime, may be expressed as differences between means compared to within cell standard deviations, or as a correlation coefficient.  These functions convert p, F, and t to d prime and the r equivalent.
The probability of replication of an experimental or correlational finding as discussed by Peter Killeen (2005) is the probability of finding an effect in the same direction upon an exact replication.  For articles submitted to Psychological Science, p.rep needs to be reported. 

F, t, p and r are all estimates of the size of an effect.  But F, t, and p also are also a function of the sample size.  Effect size, d prime, may be expressed as differences between means compared to within cell standard deviations, or as a correlation coefficient.  These functions convert p, F, and t to d prime and the r equivalent.
   Test the difference between two (paired or unpaired) correlations. Given 3 variables, x, y, z,  is the correlation between xy different than that between xz?  If y and z are independent, this is a simple t-test of the z transformed rs.  But, if they are dependent, it is a bit more complicated. 
Adapted from the  help page for pairs, pairs.panels shows a scatter plot of matrices (SPLOM), with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal. Useful for descriptive statistics of small data sets.  If lm=TRUE, linear regression fits are shown for both y by x and x by y.  Correlation ellipses are also shown. Points may be given different colors depending upon some grouping variable.  Robust fitting is done using lowess or loess regression. Confidence intervals of either the lm or loess are drawn if requested.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
When doing cor(x, use= "pairwise"), it is nice to know the number of cases for each pairwise correlation.  This is particularly useful when doing SAPA type analyses. More importantly, when there are some missing pairs, it is useful to supply imputed values so that further analyses may be done.  This is useful if using the Massively Missing Completely at Random (MMCAR) designs used by the SAPA project.  The specific pairs missing may be identified by pairwiseZero.  Summaries of the counts are given by pairwiseDescribe.
Given a set of n items, form n/2 or n/3 mini scales or parcels of the most similar pairs or triplets of items.  These may be used as the basis for subsequent scale construction or multivariate (e.g., factor) analysis.
A straightforward application of matrix algebra to remove the effect of the variables in the y set from the x set. Input may be either a data matrix or a correlation matrix.  Variables in x and y are specified by location.  If x and y are not specified, then the effect of all variables are partialled from all the other correlations.  May also be done using formula input which is more convenient when comparing results to regression models
Does an eigen value decomposition and returns eigen values, loadings, and degree of fit for a specified number of components.  Basically it is just  doing a principal components analysis (PCA) for n principal components of either a correlation or covariance matrix.  Can show the residual correlations as well. The quality of reduction in the squared correlations is reported by comparing residual correlations to original correlations. Unlike princomp, this returns a subset of just the best nfactors. The eigen vectors are rescaled by the sqrt of the eigen values to produce the component loadings more typical in factor analysis.
Given a 1 x 4 vector or a 2 x 2 matrix of frequencies, find the phi coefficient of correlation.  Typical use is in the case of predicting a dichotomous criterion from a dichotomous predictor.
A not very interesting demo of what happens if bivariate continuous data are dichotomized.  Bascially a demo of r, phi, and polychor.  
When creating a structural diagram or a structural model, it is convenient to not have to specify all of the zero loadings in a structural matrix.  structure.list converts list input into a design matrix.  phi.list does the same for a correlation matrix. Factors with NULL values are filled with 0s.
Given a phi coefficient (a Pearson r calculated on two dichotomous variables), and the marginal frequencies (in percentages), what is the corresponding estimate of the tetrachoric correlation?

Given a two x two table of counts <br>

The phi coefficient is (a - (a+b)*(a+c))/sqrt((a+b)(a+c)(b+d)(c+c)).

This function reproduces the cell entries for specified marginals and then calls the tetrachoric function. (Which was originally based upon John Fox's polychor function.)The phi2poly name will become deprecated in the future. 
A set of deprecated functions that have replaced by <code>Yule2tetra</code> and <code>Yule2phi</code>. 

Some older correlation matrices were reported as matrices of Phi or of Yule correlations.  That is, correlations were found from the two by two table of counts:<br>

Yule Q is (ad - bc)/(ad+bc). <br>

With marginal frequencies of a+b, c+d, a+c, b+d.

Given a square matrix of such correlations, and the proportions for each variable that are in the a + b cells, it is possible to reconvert each correlation into a two by two table and then estimate the corresponding polychoric correlation (using John Fox's polychor function. 
Given a phi coefficient (a Pearson r calculated on two dichotomous variables), and the marginal frequencies (in percentages), what is the corresponding estimate of the tetrachoric correlation?

Given a two x two table of counts <br>

The phi coefficient is (a - (a+b)*(a+c))/sqrt((a+b)(a+c)(b+d)(c+c)).

This function reproduces the cell entries for specified marginals and then calls the tetrachoric function. (Which was originally based upon John Fox's polychor function.)The phi2poly name will become deprecated in the future. 
Given a matrix of less than full rank, the conventional inverse function will fail.  The pseudoinverse or generalized inverse resolves this problem by using just the postive  values of the singular value decomposition d matrix. An adaptation of the ginv function from MASS and the pinv function from pracma. 
Combines several plotting functions into one for objects of class “psych".  This can be used to plot the results of <code>fa</code>, <code>irt.fa</code>, <code>VSS</code>, <code>ICLUST</code>, <code>omega</code>, <code>factor.pa</code>, or <code>principal</code>. 
Combines several plotting functions into one for objects of class “psych".  This can be used to plot the results of <code>fa</code>, <code>irt.fa</code>, <code>VSS</code>, <code>ICLUST</code>, <code>omega</code>, <code>factor.pa</code>, or <code>principal</code>. 
One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the “scree" plot of the successive eigenvalues.  Sharp breaks in the plot suggest the appropriate number of components or factors to extract.  “Parallel" analyis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original. This may be done for continuous , dichotomous, or polytomous data using Pearson, tetrachoric or polychoric correlations.
Combines several plotting functions into one for objects of class “psych".  This can be used to plot the results of <code>fa</code>, <code>irt.fa</code>, <code>VSS</code>, <code>ICLUST</code>, <code>omega</code>, <code>factor.pa</code>, or <code>principal</code>. 
Revelle and Condon, (2019) reviewed the problem of reliability in a tutorial meant to useful to the theoretician as well as the practitioner.  Although there are a number of functions in psych for estimating reliability of single scales, (e.g. <code>alpha</code> and <code>omega</code>), for split half reliability <code>splitHalf</code> or for finding test-retest reliability <code>testRetest</code> or multilevel reliability <code>mlr</code>, the <code>reliability</code> function  combines several of these functions to report these recommended measures for multiple scales. 

To quote from Revelle and Condon (2019) “Reliability is a fundamental problem for measurement in all of science for ‘(a)ll measurement is befuddled by error’ (p 294 McNemar, 1946). Perhaps because psychological measures are more befuddled than those of the other natural sciences, psychologists have long studied the problem of reliability.   

“Issues of reliability are fundamental to understanding how correlations between observed variables are (attenuated) underestimates of the relationships between the underlying constructs, how observed estimates of a person's score are biased estimates of their latent score, and how to estimate the confidence intervals around any particular measurement. Understanding the many ways to estimate reliability as well as the ways to use these estimates allows one to better assess individuals and to evaluate selection and prediction techniques. This is not just a problem for measurement specialists but for all who want to make theoretical inferences from observed data.

“ It is no longer  acceptable to report one  coefficient that is only correct if all items are exactly equally good measures of a construct.  Researchers are encouraged to report at least two coefficients (e.g., omega_h and omega_t) and then discuss why each is appropriate for the inference that is being made.  They are discouraged from reporting just alpha unless they can justify the assumptions implicit in using it (i.e., tau equivalence and unidimensionality)."  Here we make it easy to do so.

Although the <code>alpha</code> and <code>omega</code> functions will find reliability estimates for a single scale, and <code>scoreItems</code> and <code>scoreOverlap</code> will find alpha for multiple scales, it sometimes is convenient to call <code>omega</code> and <code>splitHalf</code> for multiple scales.  <code>reliability</code> takes a keys list (suitable for <code>scoreItems</code> ) and then finds hierarchical and total omega as well as split half reliabilities for each separate scale. 

<code>plot.reliability</code> takes the output of  <code>reliability</code> and displays it as a dot chart showing the values of both omegas as well as alpha and the distributions of split half reliabilities.
Combines several plotting functions into one for objects of class “psych".  This can be used to plot the results of <code>fa</code>, <code>irt.fa</code>, <code>VSS</code>, <code>ICLUST</code>, <code>omega</code>, <code>factor.pa</code>, or <code>principal</code>. 
Factor and cluster analysis output typically presents item by factor correlations (loadings).  Tables of factor loadings are frequently sorted by the size of loadings.  This style of presentation tends to make it difficult to notice the pattern of loadings on other, secondary, dimensions.  By converting to polar coordinates, it is easier to see the pattern of the secondary loadings. 
The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
Finds predicted factor/component scores from a factor analysis or principal components analysis (pca) of data set A predicted to data set B.  Predicted factor scores use the weights matrix used to find estimated factor scores, predicted components use the loadings matrix. Scores are either standardized with respect to the prediction sample or based upon the original data. Predicted scores from a bestScales model are based upon the statistics from the original sample.
The validity of a scale varies as a function of the number of items in the scale, their average intercorrelation, and their average validity. The asymptotic limit of a scales validity for any particular criterion is just the average validity divided by the square root of the average within scale item correlation.  <code>predicted.validity</code> will find the predicted validity for a set of scales (defined by a keys.list) and the average item validity for various criteria.  

The function will find (and report) scale reliabilities (using <code>reliability</code>) and average item validities (using <code>item.validity</code>)
Does an eigen value decomposition and returns eigen values, loadings, and degree of fit for a specified number of components.  Basically it is just  doing a principal components analysis (PCA) for n principal components of either a correlation or covariance matrix.  Can show the residual correlations as well. The quality of reduction in the squared correlations is reported by comparing residual correlations to original correlations. Unlike princomp, this returns a subset of just the best nfactors. The eigen vectors are rescaled by the sqrt of the eigen values to produce the component loadings more typical in factor analysis.
Give limited output (print) or somewhat more detailed (summary) for most of the functions in psych. 
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
Overview of the psych package.

The psych package has been developed at Northwestern University to include functions most useful for personality and psychological research.  Some of the functions (e.g., <code>read.file</code>, <code>read.clipboard</code>, <code>describe</code>,  <code>pairs.panels</code>, <code>error.bars</code> and <code>error.dots</code>) are useful for basic data entry and descriptive analyses. Use help(package="psych") or objects("package:psych") for a list of all functions  Three vignettes are included as part of the package.  The intro vignette tells how to install psych and overview vignette provides examples of using psych in many applications.  In addition, there are a growing set of tutorials available on the <a href="https://personality-project.org/r/">https://personality-project.org/r/</a> webpages. 

A companion package <code>psychTools</code> includes larger data set examples and one more vignette. 

Psychometric applications include routines (<code>fa</code> for  maximum likelihood (fm="mle"), minimum residual (fm="minres"), minimum rank (fm=minrank)  principal axes (fm="pa") and weighted least squares (fm="wls")  factor analysis  as well as functions to do Schmid Leiman transformations (<code>schmid</code>) to transform a hierarchical factor structure into a bifactor solution. Principal Components Analysis (<code>pca</code>) is also available.  Rotations  may be done using factor or components transformations to a target matrix include the standard Promax transformation (<code>Promax</code>), a transformation to a cluster target, or to any simple target matrix (<code>target.rot</code>) as well as the ability to call many of the GPArotation functions (e.g., oblimin, quartimin, varimax, geomin, ...). Functions for determining the number of factors in a data matrix include Very Simple Structure (<code>VSS</code>) and Minimum Average Partial correlation (<code>MAP</code>). 

An alternative approach to factor analysis is Item Cluster Analysis (<code>ICLUST</code>). This function is particularly appropriate for exploratory scale construction.

There are a number of functions for finding various reliability coefficients (see Revelle and Condon, 2019). These include the traditional  <code>alpha</code>  (found for multiple scales and with more useful output by <code>scoreItems</code>, <code>score.multiple.choice</code>), beta (<code>ICLUST</code>) and  both of McDonald's omega coefficients (<code>omega</code>, <code>omegaSem</code> and  <code>omega.diagram</code>) as well as Guttman's six estimates of internal consistency reliability (<code>guttman</code>) and the six measures of Intraclass correlation coefficients (<code>ICC</code>) discussed by Shrout and Fleiss are also available.  

Multilevel analyses may be done by <code>statsBy</code> and  <code>multilevel.reliability</code>.

The <code>scoreItems</code>, and <code>score.multiple.choice</code> functions may be used to form single or multiple scales from sets of dichotomous, multilevel, or multiple choice items by specifying scoring keys.  <code>scoreOverlap</code> correct interscale correlations for overlapping items, so that it is possible to examine hierarchical or nested structures.

Scales can be formed that best predict (after cross validation) particular criteria using <code>bestScales</code> using unit weighted or correlation weights. This procedure, also called  the <code>BISCUIT</code> algorithm (Best Items Scales that are Cross validated, Unit weighted, Informative, and Transparent) is a simple alternative to more complicated machine learning algorithms.

Additional functions make for more convenient descriptions of item characteristics  include 1 and 2 parameter Item Response measures.  The <code>tetrachoric</code>, <code>polychoric</code> and <code>irt.fa</code> functions are used to find 2 parameter descriptions of item functioning. <code>scoreIrt</code>, <code>scoreIrt.1pl</code> and <code>scoreIrt.2pl</code> do basic IRT based scoring.

A number of procedures have been developed as part of the Synthetic Aperture Personality Assessment (SAPA <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) project.  These routines facilitate forming and analyzing composite scales equivalent to using the raw data but doing so by adding within and between cluster/scale item correlations. These functions include extracting clusters from factor loading matrices (<code>factor2cluster</code>), synthetically forming clusters from correlation matrices (<code>cluster.cor</code>), and finding multiple  ((<code>setCor</code>) and partial ((<code>partial.r</code>) correlations from correlation matrices.

<code>setCor</code> and <code>mediate</code> meet the desire to do regressions and mediation analysis from either raw data or from correlation matrices.  If raw data are provided, these functions can also do  moderation analyses.

Functions to generate simulated data with particular structures include <code>sim.circ</code> (for circumplex structures), <code>sim.item</code> (for general structures) and <code>sim.congeneric</code> (for a specific demonstration of congeneric measurement).  The functions <code>sim.congeneric</code>  and <code>sim.hierarchical</code> can be used to create data sets with particular structural properties. A more general form for all of these is <code>sim.structural</code> for generating general structural models.  These are discussed in more detail in the vignette (psych_for_sem).

Functions to apply various standard statistical tests include <code>p.rep</code> and its variants for testing the probability of replication, <code>r.con</code> for the confidence intervals of a correlation, and <code>r.test</code> to test single, paired, or sets of correlations. 

In order to study diurnal or circadian variations in mood, it is helpful to use circular statistics.  Functions to find the circular mean (<code>circadian.mean</code>), circular (phasic) correlations (<code>circadian.cor</code>) and the correlation between linear variables and circular variables (<code>circadian.linear.cor</code>) supplement a function to find the best fitting phase angle (<code>cosinor</code>) for measures taken with a fixed period (e.g., 24 hours).

A dynamic model of personality and motivation (the Cues-Tendency-Actions model) is include as (<code>cta</code>.

A number of useful helper functions allow for data input (<code>read.file</code>), and data manipulation <code>cs</code> and <code>dfOrder</code>,

The most recent development version of the package is always available for download as a <em>source</em> file from the repository at the PMC lab:

install.packages("psych", repos = "https://personality-project.org/r/", type="source").

This will provide the most recent version for PCs and Macs. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
A quick way to show the first and last n lines of a data.frame, matrix, or a text object.  Just a pretty call to <code>head</code> and <code>tail</code> or <code>View</code>
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Tests the significance of a single correlation, the difference between two independent correlations, the difference between two dependent correlations sharing one variable (Williams's Test), or the difference between two dependent correlations with different variables (Steiger Tests).
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Radar plots and spider plots are just two of the many ways to show multivariate data.  <code>radar</code>  plots correlations as vectors ranging in length from 0 (corresponding to r=-1) to 1 (corresponding to an r=1).  The vectors are arranged radially around a circle. Spider plots connect the end points of each vector. The plots are most appropriate if the variables are organized in some meaningful manner. 
In applied settings, it is typical to find a correlation between a predictor and some criterion.  Unfortunately, if the predictor is used to choose the subjects, the range of the predictor is seriously reduced.  This restricts the observed correlation to be less than would be observed in the full range of the predictor.  A correction for this problem is well known as Thorndike Case 2:

Let R the unrestricted correlaton, r the restricted correlation, S the unrestricted standard deviation, s the restricted standard deviation, then

R = (rS/s)/  sqrt(1-r^2 + r^2(S^2/s^2)).   

Several other cases of restriction were also considered by Thorndike and are implemented in <code>rangeCorrection</code>.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
Revelle and Condon, (2019) reviewed the problem of reliability in a tutorial meant to useful to the theoretician as well as the practitioner.  Although there are a number of functions in psych for estimating reliability of single scales, (e.g. <code>alpha</code> and <code>omega</code>), for split half reliability <code>splitHalf</code> or for finding test-retest reliability <code>testRetest</code> or multilevel reliability <code>mlr</code>, the <code>reliability</code> function  combines several of these functions to report these recommended measures for multiple scales. 

To quote from Revelle and Condon (2019) “Reliability is a fundamental problem for measurement in all of science for ‘(a)ll measurement is befuddled by error’ (p 294 McNemar, 1946). Perhaps because psychological measures are more befuddled than those of the other natural sciences, psychologists have long studied the problem of reliability.   

“Issues of reliability are fundamental to understanding how correlations between observed variables are (attenuated) underestimates of the relationships between the underlying constructs, how observed estimates of a person's score are biased estimates of their latent score, and how to estimate the confidence intervals around any particular measurement. Understanding the many ways to estimate reliability as well as the ways to use these estimates allows one to better assess individuals and to evaluate selection and prediction techniques. This is not just a problem for measurement specialists but for all who want to make theoretical inferences from observed data.

“ It is no longer  acceptable to report one  coefficient that is only correct if all items are exactly equally good measures of a construct.  Researchers are encouraged to report at least two coefficients (e.g., omega_h and omega_t) and then discuss why each is appropriate for the inference that is being made.  They are discouraged from reporting just alpha unless they can justify the assumptions implicit in using it (i.e., tau equivalence and unidimensionality)."  Here we make it easy to do so.

Although the <code>alpha</code> and <code>omega</code> functions will find reliability estimates for a single scale, and <code>scoreItems</code> and <code>scoreOverlap</code> will find alpha for multiple scales, it sometimes is convenient to call <code>omega</code> and <code>splitHalf</code> for multiple scales.  <code>reliability</code> takes a keys list (suitable for <code>scoreItems</code> ) and then finds hierarchical and total omega as well as split half reliabilities for each separate scale. 

<code>plot.reliability</code> takes the output of  <code>reliability</code> and displays it as a dot chart showing the values of both omegas as well as alpha and the distributions of split half reliabilities.
Psychologists frequently report data in terms of transformed scales such as “IQ" (mean=100, sd=15, “SAT/GRE" (mean=500, sd=100), “ACT" (mean=18, sd=6), “T-scores" (mean=50, sd=10), or “Stanines" (mean=5, sd=2). The <code>rescale</code> function converts the data to standard scores and then rescales to the specified mean(s) and standard deviation(s). 
Residuals in the various psych functions are extracted and then may be "pretty" printed.
Residuals in the various psych functions are extracted and then may be "pretty" printed.
Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code>make.keys</code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code>cluster.cor</code>). <code>response.frequencies</code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code>scoreFast</code> and   <code>scoreVeryFast</code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code>make.keys</code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code>cluster.cor</code>). <code>response.frequencies</code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code>scoreFast</code> and   <code>scoreVeryFast</code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
Some IRT functions require all items to be coded in the same direction.  Some data sets have items that need to be reverse coded (e.g., 6 -&gt; 1, 1 -&gt; 6).  reverse.code will flip items based upon a keys vector of 1s and -1s.  Reversed items are subtracted from the item max + item min.  These may be specified or may be calculated.
Von Neuman et al. (1941) discussed the Mean Square of Successive Differences as a measure of variability that takes into account gradual shifts in mean. This is appropriate when studying errors in ballistics or variability and stability in mood when studying affect. For random data, this will be twice the variance, but for data with a sequential order and a positive autocorrelation, this will be much smaller. Since the mssd is just twice the variance - the autocorrelation, it is thus possible to also find the autocorrelation for a particular lag. 
Self reported scores on the SAT Verbal, SAT Quantitative and ACT   were collected as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  Age, gender, and education are also reported. The data from 700 subjects are included here as a demonstration set for correlation and analysis.
Given a matrix of choices and a vector of scale values, how well do the scale values capture the choices?  That is, what is size of the squared residuals given the model versus the size of the squared choice values?
Draw a X Y scatter plot with associated X and Y histograms with estimated densities.  Will also draw density plots by groups, as well as distribution ellipses by group. Partly a demonstration of the use of layout. Also includes lowess smooth or linear model slope, as well as correlation. 
Draw a X Y scatter plot with associated X and Y histograms with estimated densities.  Will also draw density plots by groups, as well as distribution ellipses by group. Partly a demonstration of the use of layout. Also includes lowess smooth or linear model slope, as well as correlation. 
One way to find omega is to do a factor analysis of the original data set, rotate the factors obliquely, do a Schmid Leiman transformation, and then find omega. Here is the code for Schmid Leiman.  The S-L transform takes a factor or PC solution, transforms it to an oblique solution, factors the oblique solution to find a higher order (g ) factor, and then residualizes g out of the the group factors.
John Schmid and John M. Leiman (1957) discuss how to transform a hierarchical factor structure to a bifactor structure. Schmid contains the example 12 x 12 correlation matrix. schmid.leiman is a 12 x 12 correlation matrix with communalities on the diagonal. This can be used to show the effect of correcting for attenuation. Two additional data sets are taken from Chen et al. (2006).
John Schmid and John M. Leiman (1957) discuss how to transform a hierarchical factor structure to a bifactor structure. Schmid contains the example 12 x 12 correlation matrix. schmid.leiman is a 12 x 12 correlation matrix with communalities on the diagonal. This can be used to show the effect of correcting for attenuation. Two additional data sets are taken from Chen et al. (2006).
Given a matrix or data.frame of k keys for m items (-1, 0, 1), and a matrix or data.frame of items scores for m items and n people, find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, the average r, the scale intercorrelations, and the item by scale correlations.  (Superseded by  <code>score.items</code>). 
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code>make.keys</code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code>cluster.cor</code>). <code>response.frequencies</code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code>scoreFast</code> and   <code>scoreVeryFast</code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
Ability tests are typically multiple choice with one right answer.  score.multiple.choice takes a scoring key and a data matrix (or data.frame) and finds total or average number right for each participant.  Basic test statistics (alpha, average r, item means, item-whole correlations) are also reported. 
Given a  n x c cluster definition matrix of -1s, 0s, and 1s (the keys) , and a n x n correlation matrix, or an N x n data matrix, find the correlations of the composite clusters.  The keys matrix can be entered by hand, copied from the clipboard (<code>read.clipboard</code>), or taken as output from the <code>factor2cluster</code> or <code>make.keys</code> functions.  Similar functionality to <code>scoreItems</code> which also gives item by cluster correlations. <code>scoreBy</code> does this for individual subjects after a call to <code>statsBy</code>.
Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code>make.keys</code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code>cluster.cor</code>). <code>response.frequencies</code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code>scoreFast</code> and   <code>scoreVeryFast</code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
<code>irt.fa</code> finds Item Response Theory (IRT) parameters through factor analysis of the tetrachoric or polychoric correlations of dichtomous or polytomous items. <code>scoreIrt</code> uses these parameter estimates of discrimination and location to find IRT based scores for the responses. As many factors as found for the correlation matrix will be scored. <code>scoreIrt.2pl</code> will score lists of scales.
Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code>make.keys</code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code>cluster.cor</code>). <code>response.frequencies</code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code>scoreFast</code> and   <code>scoreVeryFast</code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
Given a  n x c cluster definition matrix of -1s, 0s, and 1s (the keys) , and a n x n correlation matrix, or an N x n data matrix, find the correlations of the composite clusters.  The keys matrix can be entered by hand, copied from the clipboard (<code>read.clipboard</code>), or taken as output from the <code>factor2cluster</code> or <code>make.keys</code> functions.  Similar functionality to <code>scoreItems</code> which also gives item by cluster correlations. <code>scoreBy</code> does this for individual subjects after a call to <code>statsBy</code>.
Given a data.frame or matrix of n items and N observations and a list of the direction to score them (a keys.list with k keys)  find the sum scores or average scores for each person and each scale.  In addition, report Cronbach's alpha, Guttman's Lambda 6,  the average r, the scale intercorrelations, and the item by scale correlations (raw and corrected for item overlap).  Replace missing values with the item median or mean if desired. Items may be keyed 1 (score it), -1 ) (reverse score it), or 0 (do not score it). Negatively keyed items will be reverse scored.  Although prior versions used a keys matrix, it is now recommended to just use a list of scoring keys. See <code>make.keys</code> for a convenient way to make the keys file.  If the input is a square matrix, then it is assumed that the input is a covariance or correlation matix and scores are not found, but the item statistics are reported. (Similar functionality to <code>cluster.cor</code>). <code>response.frequencies</code> reports the frequency of item endorsements fore each response category for polytomous or multiple choice items. <code>scoreFast</code> and   <code>scoreVeryFast</code> just find sums/mean scores and do not report reliabilities.  Much faster for large data sets.
Item weights from <code>bestScales</code> or <code>setCor</code> are used to find weighted scale scores. In contrast to the unit weights used in <code>scoreItems</code>, <code>scoreWtd</code> will multiply the data by a set of weights to find scale scores.  These weight may come from a regression (e.g., <code>lm</code> or <code>setCor</code>) or may be the zero order correlation weights from <code>bestScales</code>.
Cattell's scree test is one of most simple ways of testing the number of components or factors in  a correlation matrix. Here we plot the  eigen values of a correlation matrix as well as the eigen values of a factor analysis.
A tedious part of data analysis is addressing the problem of miscoded data that need to be converted to NA or some other value.  For a given data.frame or matrix, scrub will set all values of columns from=from to to=to that are less than a set (vector) of min values or more than a vector of max values to NA. Can also be used to do basic recoding of data for all values=isvalue to newvalue. Will also recode continuus variables into fewer categories.  Will convert Nan, -Inf and Inf to NA

The length of the where, isvalue, and newvalues must either match, or be 1.
Find the standard deviation of a vector, matrix, or data.frame.  In the latter two cases, return the sd of each column.  Unlike the sd function, return NA if there are no observations rather than throw an error.
 When scoring items by forming composite scales either from the raw data using <code>scoreItems</code> or from the correlation matrix using <code>cluster.cor</code>, it used to be  necessary to create a keys matrix. This is no longer necessary as most of the scoring functions will directly use a keys list. <code>make.keys</code>  is just a short cut for creating a keys matrix.  The keys matrix is a nvar x nscales matrix of -1,0, 1 and defines the membership for each scale. Items can be specified by location or by name.
Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.

Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 

Input can be specified as matrices or the output from  <code>fa</code>, factanal,  or a rotation package such as <em>GPArotation</em>.

For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.

As an option, for those without Rgraphviz installed, <code>structure.sem</code> will just create the sem model and skip the graph. (This functionality is now included in <code>structure.diagram</code>.)

structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.

<code>lavaan.diagram</code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code>structure.diagram</code>.
Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.

Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 

Input can be specified as matrices or the output from  <code>fa</code>, factanal,  or a rotation package such as <em>GPArotation</em>.

For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.

As an option, for those without Rgraphviz installed, <code>structure.sem</code> will just create the sem model and skip the graph. (This functionality is now included in <code>structure.diagram</code>.)

structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.

<code>lavaan.diagram</code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code>structure.diagram</code>.
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
Given a correlation matrix or a  matrix or dataframe of raw data, find the multiple regressions and draw a path diagram relating a set of y variables as a function of a set of x variables.  A set of covariates (z) can be partialled from the x and y sets. Regression diagrams are automatically included.   Model can be specified in conventional formula form, or in terms of x variables and y variables.  Multiplicative models (interactions) and quadratic terms may be specified in the formula mode if using raw data. By default, the data may be  zero centered before finding the interactions.  Will also find Cohen's Set Correlation between a predictor set of variables (x) and a criterion set (y). Also finds the canonical correlations between the x and y sets.
 When constructing scales through rational, factorial, or empirical means, it is useful to examine the content of the items that relate most highly to each other (e.g., the factor loadings of <code>fa.lookup</code> of a set of items) , or to some specific set of criteria  (e.g., <code>bestScales</code>). Given a dictionary of item content, these routines will sort by factor loading or criteria correlations and display the item content. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: <code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
For teaching basic statistics, it is useful to be able to generate examples suitable for analysis of variance or simple linear models.  sim.anova will generate the design matrix of three independent variables (IV1, IV2, IV3) with an arbitrary number of levels and effect sizes for each main effect and interaction.  IVs can be either continuous or categorical and can have linear or quadratic effects. Either a single dependent variable or multiple (within subject) dependent variables are generated according to the specified model. The repeated measures are assumed to be tau equivalent with a specified reliability.
Create a population orthogonal or hierarchical correlation matrix from a set of factor loadings and factor intercorrelations. Samples of size n may be then be drawn from this population.  Return either the sample data, sample correlations, or population correlations.  This is used to create sample data sets for instruction and demonstration.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
Classical Test Theory (CTT) considers four or more tests to be congenerically equivalent if all tests may be expressed in terms of one factor and a residual error.  Parallel tests are the special case where (usually two) tests have equal factor loadings.  Tau equivalent tests have equal factor loadings but may have unequal errors.  Congeneric tests may differ in both factor loading and error variances.
Structural Equation Models decompose correlation or correlation matrices into a measurement (factor) model and a structural (regression) model.  sim.structural creates data sets with known measurement and structural properties. Population or sample correlation matrices with known properties are generated. Optionally raw data are produced. 

It is also possible to specify a measurement model for a set of x variables separately from a set of y variables.  They are then combined into one model with the correlation structure between the two sets.

Finally, the general case is given a population correlation matrix, generate data that will reproduce (with sampling variability) that correlation matrix.  <code>sim.correlation</code>.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: <code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
Create a population orthogonal or hierarchical correlation matrix from a set of factor loadings and factor intercorrelations. Samples of size n may be then be drawn from this population.  Return either the sample data, sample correlations, or population correlations.  This is used to create sample data sets for instruction and demonstration.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: <code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
Multilevel data occur when observations are nested within groups. This can produce correlational structures that are sometimes difficult to understand. These two simulations allow for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. The correlations of aggregated data is sometimes called an 'ecological correlation'. That group level and individual level correlations are independent makes such inferences problematic.  Within individual data are simulated in sim.multi with a variety of possible within person structures.  
Multilevel data occur when observations are nested within groups. This can produce correlational structures that are sometimes difficult to understand. These two simulations allow for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. The correlations of aggregated data is sometimes called an 'ecological correlation'. That group level and individual level correlations are independent makes such inferences problematic.  Within individual data are simulated in sim.multi with a variety of possible within person structures.  
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: <code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: <code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors,<code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
A number of functions in the psych package will generate simulated data with particular structures.  These functions include<code>sim</code> for a factor simplex, and <code>sim.simplex</code> for a data simplex, <code>sim.circ</code> for a circumplex structure, <code>sim.congeneric</code> for a one factor factor congeneric model, <code>sim.dichot</code> to simulate dichotomous items, <code>sim.hierarchical</code> to create a hierarchical factor model, <code>sim.item</code> a more general item simulation,<code>sim.minor</code> to simulate major and minor factors,<code>sim.omega</code> to test various examples of omega,<code>sim.parallel</code> to compare the efficiency of various ways of deterimining the number of factors.   See the help pages for some more simulation functions here: <code>sim.rasch</code> to create simulated rasch data, <code>sim.irt</code> to create general 1 to 4 parameter IRT data by calling <code>sim.npl</code> 1 to 4 parameter logistic IRT or <code>sim.npn</code> 1 to 4 paramater normal IRT,<code>sim.poly</code> to create polytomous ideas by calling<code>sim.poly.npn</code> 1-4 parameter polytomous normal theory items or<code>sim.poly.npl</code> 1-4 parameter polytomous logistic items, and <code>sim.poly.ideal</code> which creates data following an ideal point or unfolding model by calling <code>sim.poly.ideal.npn</code> 1-4 parameter polytomous normal theory ideal point model or <code>sim.poly.ideal.npl</code> 1-4 parameter polytomous logistic ideal point model.

<code>sim.structural</code> a general simulation of structural models,  and <code>sim.anova</code> for ANOVA and lm simulations, and <code>sim.VSS</code>. Some of these functions are separately documented and are listed here for ease of the help function.  See each function for more detailed help.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating simple structure and circumplex data is straightforward, and is useful for exploring alternative solutions to affect and personality structure. A generalization to 3 dimensional (spherical) data is straightforward.
Structural Equation Models decompose correlation or correlation matrices into a measurement (factor) model and a structural (regression) model.  sim.structural creates data sets with known measurement and structural properties. Population or sample correlation matrices with known properties are generated. Optionally raw data are produced. 

It is also possible to specify a measurement model for a set of x variables separately from a set of y variables.  They are then combined into one model with the correlation structure between the two sets.

Finally, the general case is given a population correlation matrix, generate data that will reproduce (with sampling variability) that correlation matrix.  <code>sim.correlation</code>.
Structural Equation Models decompose correlation or correlation matrices into a measurement (factor) model and a structural (regression) model.  sim.structural creates data sets with known measurement and structural properties. Population or sample correlation matrices with known properties are generated. Optionally raw data are produced. 

It is also possible to specify a measurement model for a set of x variables separately from a set of y variables.  They are then combined into one model with the correlation structure between the two sets.

Finally, the general case is given a population correlation matrix, generate data that will reproduce (with sampling variability) that correlation matrix.  <code>sim.correlation</code>.
Simulation is one of most useful techniques in statistics and psychometrics.  Here we simulate a correlation matrix with a simple structure composed of a specified number of factors.  Each item is assumed to have complexity one.  See <code>circ.sim</code> and <code>item.sim</code> for alternative simulations.
Structural Equation Models decompose correlation or correlation matrices into a measurement (factor) model and a structural (regression) model.  sim.structural creates data sets with known measurement and structural properties. Population or sample correlation matrices with known properties are generated. Optionally raw data are produced. 

It is also possible to specify a measurement model for a set of x variables separately from a set of y variables.  They are then combined into one model with the correlation structure between the two sets.

Finally, the general case is given a population correlation matrix, generate data that will reproduce (with sampling variability) that correlation matrix.  <code>sim.correlation</code>.
Rotations of factor analysis and principal components analysis solutions typically try to represent correlation matrices as simple structured.  An alternative structure, appealing to some, is a circumplex structure where the variables are uniformly spaced on the perimeter of a circle in a two dimensional space.  Generating these data is straightforward, and is useful for exploring alternative solutions to affect and personality structure.
Find the skew and kurtosis for each variable in a data.frame or matrix.  Unlike skew and kurtosis in e1071, this calculates a different skew for each variable or column of a data.frame/matrix. mardia applies Mardia's tests for multivariate skew and kurtosis
The squared multiple correlation of a variable with the remaining variables in a matrix is sometimes used as initial estimates of the communality of a variable.

SMCs are also used when estimating reliability using Guttman's lambda 6  <code>guttman</code> coefficient. 

The SMC is just  1 - 1/diag(R.inv) where R.inv is the inverse of R.
Radar plots and spider plots are just two of the many ways to show multivariate data.  <code>radar</code>  plots correlations as vectors ranging in length from 0 (corresponding to r=-1) to 1 (corresponding to an r=1).  The vectors are arranged radially around a circle. Spider plots connect the end points of each vector. The plots are most appropriate if the variables are organized in some meaningful manner. 
Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<i>μ_0 … μ_3)</i> as well as <i>β</i> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <i>ω_h</i> and <i>ω_t</i> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
When examining data at two levels (e.g., the individual and by some set of grouping variables), it is useful to find basic descriptive statistics (means, sds, ns per group, within group correlations) as well as between group statistics (over all descriptive statistics, and overall between group correlations). Of particular use is the ability to decompose a matrix of correlations at the individual level into correlations within group and correlations between groups. 
When examining data at two levels (e.g., the individual and by some set of grouping variables), it is useful to find basic descriptive statistics (means, sds, ns per group, within group correlations) as well as between group statistics (over all descriptive statistics, and overall between group correlations). Of particular use is the ability to decompose a matrix of correlations at the individual level into correlations within group and correlations between groups. 
When examining data at two levels (e.g., the individual and by some set of grouping variables), it is useful to find basic descriptive statistics (means, sds, ns per group, within group correlations) as well as between group statistics (over all descriptive statistics, and overall between group correlations). Of particular use is the ability to decompose a matrix of correlations at the individual level into correlations within group and correlations between groups. 
Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.

Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 

Input can be specified as matrices or the output from  <code>fa</code>, factanal,  or a rotation package such as <em>GPArotation</em>.

For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.

As an option, for those without Rgraphviz installed, <code>structure.sem</code> will just create the sem model and skip the graph. (This functionality is now included in <code>structure.diagram</code>.)

structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.

<code>lavaan.diagram</code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code>structure.diagram</code>.
Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.

Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 

Input can be specified as matrices or the output from  <code>fa</code>, factanal,  or a rotation package such as <em>GPArotation</em>.

For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.

As an option, for those without Rgraphviz installed, <code>structure.sem</code> will just create the sem model and skip the graph. (This functionality is now included in <code>structure.diagram</code>.)

structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.

<code>lavaan.diagram</code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code>structure.diagram</code>.
When creating a structural diagram or a structural model, it is convenient to not have to specify all of the zero loadings in a structural matrix.  structure.list converts list input into a design matrix.  phi.list does the same for a correlation matrix. Factors with NULL values are filled with 0s.
Graphic presentations of structural equation models are a very useful way to conceptualize sem and confirmatory factor models. Given a measurement model on x (xmodel) and on y (ymodel) as well as a path model connecting x and y (phi), draw the graph.  If the ymodel is not specified, just draw the measurement model (xmodel + phi). If the Rx or Ry matrices are specified, show the correlations between the x variables, or y variables.

Perhaps even more usefully, the function returns a model appropriate for running directly in the <em>sem package</em> written by John Fox or the <em>lavaan</em> package by Yves Rosseel.    For this option to work directly, it is necessary to specfy that errrors=TRUE. 

Input can be specified as matrices or the output from  <code>fa</code>, factanal,  or a rotation package such as <em>GPArotation</em>.

For symbolic graphs, the input matrices can be character strings or mixtures of character strings and numeric vectors.

As an option, for those without Rgraphviz installed, <code>structure.sem</code> will just create the sem model and skip the graph. (This functionality is now included in <code>structure.diagram</code>.)

structure.diagram will draw the diagram without using Rgraphviz and is probably the preferred option. structure.graph will be removed eventually.

<code>lavaan.diagram</code> will draw either cfa or sem results from the lavaan package. It has been tested for  cfa, sem  and mimic type output.  It takes  the output object from  <em>lavaan</em> and then calls <code>structure.diagram</code>.
Give limited output (print) or somewhat more detailed (summary) for most of the functions in psych. 
Given the matrices nXm, and jYk, form the super matrix of dimensions (n+j) and (m+k) with  with elements x and y along the super diagonal. Useful when considering structural equations.  The measurement models x and y can be combined into a larger measurement model of all of the variables.  If either x or y is a list of matrices, then recursively form a super matrix of all of those elements.  superCor will form a matrix from two matrices and the intercorrelation of the elements of the two.
Given the matrices nXm, and jYk, form the super matrix of dimensions (n+j) and (m+k) with  with elements x and y along the super diagonal. Useful when considering structural equations.  The measurement models x and y can be combined into a larger measurement model of all of the variables.  If either x or y is a list of matrices, then recursively form a super matrix of all of those elements.  superCor will form a matrix from two matrices and the intercorrelation of the elements of the two.
Given the matrices nXm, and jYk, form the super matrix of dimensions (n+j) and (m+k) with  with elements x and y along the super diagonal. Useful when considering structural equations.  The measurement models x and y can be combined into a larger measurement model of all of the variables.  If either x or y is a list of matrices, then recursively form a super matrix of all of those elements.  superCor will form a matrix from two matrices and the intercorrelation of the elements of the two.
Given a data.frame or matrix, find the standardized mean difference (Cohen's d) and confidence intervals for each variable depending upon a grouping variable.  Convert the d statistic to the r equivalent, report the student's t statistic and associated p values, and return statistics for both values of the grouping variable.  The Mahalanobis distance between the centroids of the two groups in the space defined by all the variables ia also found.  Confidence intervals for Cohen d for one group (difference from 0) may also be found. Several measures of the distributional overlap (e.g. OVL, OVL2, etc.) are available.
Convert a correlation to a z or t, or d, or chi or covariance matrixor z to r using the Fisher transformation or find the confidence intervals for a specified correlation.  r2d converts a correlation to an effect size (Cohen's d) and d2r converts a d into an r. g2r converts Hedge's g to a correlation.   t2r converts a t test to r, r2t converts a correlation to a t-test value. chi2r converts a chi square to r, r2chi converts it back.  r2c and cor2cov convert a correlation matrix to a covariance matrix. d2t and t2d convert cohen's d into a t and a t into a cohen d.  See <code>cohen.d</code> for other conversions.
Some historical sets are reported as summary tables of counts in a limited number of bins.  Transforming these tables to data.frames representing the original values is useful for pedagogical purposes.  (E.g., transforming the original Galton table of height x cubits in order to demonstrate regression.) The column and row names must be able to be converted to numeric values.
Some historical sets are reported as summary tables of counts in a limited number of bins.  Transforming these tables to data.frames representing the original values is useful for pedagogical purposes.  (E.g., transforming the original Galton table of height x cubits in order to demonstrate regression.) The column and row names must be able to be converted to numeric values.
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
Nurit Tal-Or, Jonanathan Cohen, Yariv Tasfati, and Albert Gunther (2010) examined the presumed effect of media on other people and change in attitudes.  This data set is from Study 2, and examined the effect of presumed influence of the media upon subsequent actions.  It is used as an example of mediation by Hayes (2013) and for the mediate function. 
Nurit Tal-Or, Jonanathan Cohen, Yariv Tasfati, and Albert Gunther (2010) examined the presumed effect of media on other people and change in attitudes.  This data set is from Study 2, and examined the effect of presumed influence of the media upon subsequent actions.  It is used as an example of mediation by Hayes (2013) and for the mediate function. 
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<i>μ_0 … μ_3)</i> as well as <i>β</i> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <i>ω_h</i> and <i>ω_t</i> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
This is a set of minor, if not trivial, helper functions.lowerCor finds the correlation of x variables and then prints them using lowerMat which is a trivial, but useful, function to round off and print the lower triangle of a matrix.reflect reflects the output of a factor analysis or principal components analysis so that one or more factors is reflected. (Requested by Alexander Weiss.)progressBar prints out ...  as a calling routine (e.g., <code>tetrachoric</code>) works through a tedious calculation.  shannon finds the Shannon index (H) of diversity or of information. test.all tests all the examples in a package.  best.items sorts a factor matrix for absolute values and displays the expanded items names. fa.lookup returns sorted factor analysis output with item labels. <code>cor2</code> correlates two data.frames (of equal length). levels2numeric and char2numeric convert dataframe columns that are categorical/levels to numeric values. 
 Item Response Theory provides a number of alternative ways of estimating latent scores.  Here we compare 6 different ways to estimate the latent variable associated with a pattern of responses. Originally developed as a test for scoreIrt, but perhaps useful for demonstration purposes.  Items are simulated using <code>sim.irt</code> and then scored using factor scores from <code>factor.scores</code> using statistics found using <code>irt.fa</code>, simple weighted models for 1 and 2 PL and 2 PN. Results show almost perfect agreement with estimates from MIRT and ltm for the dichotomous case and with MIRT for the polytomous case.  (Results from ltm are unstable for the polytomous case, sometimes agreeing with <code>scoreIrt</code> and MIRT, sometimes being much worse.)  
Test to make sure the psych functions run on basic test data sets
Given two presentations of a test, it is straightforward to find the test-retest reliablity, as well as the item reliability and person stability across items.  Using the multi-level structure of the data, it is also possible to do a variance deomposition to find variance components for people, items, time, people x time, people x items, and items x time as well as the residual variance. This leads to various generalizability cofficients.
The tetrachoric correlation is the inferred Pearson Correlation from a two x two table with the assumption of bivariate normality. The polychoric correlation generalizes this to the n x m table. Particularly important when doing Item Response Theory or converting comorbidity statistics using normal theory to correlations. Input may be a 2 x 2 table of cell frequencies, a vector of cell frequencies, or a data.frame or matrix of dichotomous data (for tetrachoric) or of numeric data (for polychoric).The biserial correlation is between a continuous y variable and a dichotmous x variable, which is assumed to have resulted from a dichotomized normal variable. Biserial is a special case of the polyserial correlation, which is the inferred latent correlation between a continuous variable (X) and a ordered categorical variable (e.g., an item response). Input for these later two are data frames or matrices.  Requires the mnormt package.
Thurstone Case V scaling allows for a scaling of objects compared to other objects. As one of the cases considered by Thurstone, Case V makes the assumption of equal variances and uncorrelated distributions. 
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
Holzinger-Swineford (1937) introduced the bifactor model of a general factor and uncorrelated group factors. The Holzinger data sets are   original 14 * 14 matrix from their paper as well as a 9 *9 matrix used as an example by Joreskog. The Thurstone correlation matrix is a 9 * 9 matrix of correlations of ability items.  The Reise data set is 16 * 16 correlation matrix of mental health items. The Bechtholdt data sets are both 17 x 17 correlation matrices of ability tests.  
A quick way to show the first and last n lines of a data.frame, matrix, or a text object.  Just a pretty call to <code>head</code> and <code>tail</code> or <code>View</code>
Hardly worth coding, if it didn't appear in so many formulae in psychometrics, the trace of a (square) matrix is just the sum of the diagonal elements. 
Tucker and Lewis (1973) introduced a reliability coefficient for ML factor analysis.  Their example data set was previously reported by Tucker (1958) and taken from Thurstone and Thurstone (1941).  The correlation matrix is a 9 x 9 for 710 subjects and has two correlated factors of ability: Word Fluency and Verbal.
There are a variety of ways of assessing whether a set of items measures one latent trait.  <code>unidim</code> is just one more way.  If a one factor model holds in the data, then the factor analytic decomposition F implies that FF' should reproduce the correlations with communalities along the diagonal. In this case, the fit FF' should be identical to the correlation matrix minus the uniquenesses.  unidim is just the ratio of these two estimates.  The higher it is, the more the evidence for unidimensionality. A number of alternative statistics are estimated.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
The bifactor rotation implements the rotation introduced by Jennrich and Bentler (2011) by calling GPForth in the GPArotation package.  promax is an oblique rotation function introduced by Hendrickson and White (1964) and implemented in the promax function in the stats package.  Unfortunately, promax does not report the inter factor correlations.  Promax does.  TargetQ does a target rotation with elements that can be missing (NA), or numeric (e.g., 0, 1).  It uses the GPArotation package. target.rot does general target rotations to an arbitrary target matrix. The default target rotation is for an independent cluster solution. equamax facilitates the call to GPArotation to do an equamax rotation.  Equamax, although available as a specific option within GPArotation is easier to call by name if using equamax.  The varimin rotation suggested by Ertl (2013) is implemented by appropriate calls to GPArotation.
Among the many ways to describe a data set, one is a density plot for each value of a grouping variable and another is violin plot of multiple variables.  A density plot shows the density for different groups to show effect sizes. A violin plot is similar to a box plot but shows the actual distribution.Median and 25th and 75th percentile lines are added to the display. If a grouping variable is specified, violinBy will draw violin plots for each variable and for each group. Data points may be drawn as well.
Among the many ways to describe a data set, one is a density plot for each value of a grouping variable and another is violin plot of multiple variables.  A density plot shows the density for different groups to show effect sizes. A violin plot is similar to a box plot but shows the actual distribution.Median and 25th and 75th percentile lines are added to the display. If a grouping variable is specified, violinBy will draw violin plots for each variable and for each group. Data points may be drawn as well.
There are multiple ways to determine the appropriate number of factors in exploratory factor analysis. Routines for the Very Simple Structure (VSS) criterion allow one to compare solutions of varying complexity and for different number of factors. Graphic output indicates the "optimal" number of factors for different levels of complexity.  The Velicer MAP criterion is another good choice. <code>nfactors</code> finds and plots several of these alternative estimates.
There are multiple ways to determine the appropriate number of factors in exploratory factor analysis. Routines for the Very Simple Structure (VSS) criterion allow one to compare solutions of varying complexity and for different number of factors. Graphic output indicates the "optimal" number of factors for different levels of complexity.  The Velicer MAP criterion is another good choice. <code>nfactors</code> finds and plots several of these alternative estimates.
Another useful test for the number of factors is when the eigen values of a random matrix are greater than the eigen values of a a real matrix. Here we show VSS solutions to random data. A better test is probably <code>fa.parallel</code>.
The Very Simple Structure criterion ( <code>VSS</code>) for estimating the optimal number of factors is plotted as a function of the increasing complexity and increasing number of factors.  
Cattell's scree test is one of most simple ways of testing the number of components or factors in  a correlation matrix. Here we plot the  eigen values of a correlation matrix as well as the eigen values of a factor analysis.
Simulation is one of most useful techniques in statistics and psychometrics.  Here we simulate a correlation matrix with a simple structure composed of a specified number of factors.  Each item is assumed to have complexity one.  See <code>circ.sim</code> and <code>item.sim</code> for alternative simulations.
Simulation is one of most useful techniques in statistics and psychometrics.  Here we simulate a correlation matrix with a simple structure composed of a specified number of factors.  Each item is assumed to have complexity one.  See <code>circ.sim</code> and <code>item.sim</code> for alternative simulations.
John Schmid and John M. Leiman (1957) discuss how to transform a hierarchical factor structure to a bifactor structure. Schmid contains the example 12 x 12 correlation matrix. schmid.leiman is a 12 x 12 correlation matrix with communalities on the diagonal. This can be used to show the effect of correcting for attenuation. Two additional data sets are taken from Chen et al. (2006).
Among the  robust estimates of central tendency are trimmed means and Winsorized means.  This function finds the Winsorized scores.   The top and bottom trim values are given values of the trimmed and 1- trimmed quantiles.  Then means, sds, and variances are found.    
Among the  robust estimates of central tendency are trimmed means and Winsorized means.  This function finds the Winsorized scores.   The top and bottom trim values are given values of the trimmed and 1- trimmed quantiles.  Then means, sds, and variances are found.    
Among the  robust estimates of central tendency are trimmed means and Winsorized means.  This function finds the Winsorized scores.   The top and bottom trim values are given values of the trimmed and 1- trimmed quantiles.  Then means, sds, and variances are found.    
Among the  robust estimates of central tendency are trimmed means and Winsorized means.  This function finds the Winsorized scores.   The top and bottom trim values are given values of the trimmed and 1- trimmed quantiles.  Then means, sds, and variances are found.    
Among the  robust estimates of central tendency are trimmed means and Winsorized means.  This function finds the Winsorized scores.   The top and bottom trim values are given values of the trimmed and 1- trimmed quantiles.  Then means, sds, and variances are found.    
A demonstration that a correlation may be decomposed to a within group correlation  and a between group correlations and these two correlations are independent.  Between group correlations are sometimes called ecological correlations, the decomposition into  within and between group correlations is a basic concept in multilevel modeling.  This data set shows the composite correlations between 9 variables, representing 16 cases with four groups.    
Cohen's kappa (Cohen, 1960) and weighted kappa (Cohen, 1968) may be used to find the agreement of two raters when using nominal scores.  Light's kappa is just the average cohen.kappa if using more than 2 raters. 

weighted.kappa is (probability of observed matches - probability of expected matches)/(1 - probability of expected matches).  Kappa just considers the matches on the main diagonal.  Weighted kappa considers off diagonal elements as well.
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
A set of deprecated functions that have replaced by <code>Yule2tetra</code> and <code>Yule2phi</code>. 

Some older correlation matrices were reported as matrices of Phi or of Yule correlations.  That is, correlations were found from the two by two table of counts:<br>

Yule Q is (ad - bc)/(ad+bc). <br>

With marginal frequencies of a+b, c+d, a+c, b+d.

Given a square matrix of such correlations, and the proportions for each variable that are in the a + b cells, it is possible to reconvert each correlation into a two by two table and then estimate the corresponding polychoric correlation (using John Fox's polychor function. 
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
A set of deprecated functions that have replaced by <code>Yule2tetra</code> and <code>Yule2phi</code>. 

Some older correlation matrices were reported as matrices of Phi or of Yule correlations.  That is, correlations were found from the two by two table of counts:<br>

Yule Q is (ad - bc)/(ad+bc). <br>

With marginal frequencies of a+b, c+d, a+c, b+d.

Given a square matrix of such correlations, and the proportions for each variable that are in the a + b cells, it is possible to reconvert each correlation into a two by two table and then estimate the corresponding polychoric correlation (using John Fox's polychor function. 
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
One of the many measures of association is the Yule coefficient.  Given a two x two table of counts <br>

Yule Q is (ad - bc)/(ad+bc). <br>Conceptually, this is the number of pairs in agreement (ad) - the number in disagreement (bc) over the total number of paired observations.  Warren (2008) has shown that  Yule's Q is one of the “coefficients that have zero value under statistical independence, maximum value unity, and minimum value minus unity independent of the marginal distributions" (p 787). <br>ad/bc is the odds ratio and Q = (OR-1)/(OR+1) <br>Yule's coefficient of colligation is Y = (sqrt(OR) - 1)/(sqrt(OR)+1)Yule.inv finds the cell entries for a particular Q and the marginals (a+b,c+d,a+c, b+d).  This is useful for converting old tables of correlations into more conventional <code>phi</code> or tetrachoric correlations <code>tetrachoric</code><br>Yule2phi and Yule2tetra convert the Yule Q with set marginals to the correponding phi or tetrachoric correlation.

Bonett and Price show that the Q and Y coefficients are both part of a general family of coefficients raising the OR to a power (c).  If c=1, then this is Yule's Q.  If .5, then Yule's Y, if c = .75, then this is Digby's H.  They propose that c = .5 - (.5 * min(cell probabilty)^2  is a more general coefficient.  YuleBonett implements this for the 2 x 2 case, YuleCor for the data matrix case.
The function <code>acf</code> computes (and by default plots) estimates ofthe autocovariance or autocorrelation function.  Function <code>pacf</code>is the function used for the partial autocorrelations.  Function<code>ccf</code> computes the cross-correlation or cross-covariance of twounivariate series.
Compute an AR process exactly fitting an autocorrelation function.
<code>add.scope</code> and <code>drop.scope</code> compute those terms that can beindividually added to or dropped from a model while respecting thehierarchy of terms.
Compute all the single terms in the <code>scope</code> argument that can beadded to or dropped from the model, fit those models and compute atable of the changes in fit.
For a given table one can specify which of the classifying factors toexpand by one or more levels to hold margins to be calculated.  One may forexample form sums and means over the first dimension and medians over thesecond.  The resulting table will then have two extra levels for the firstdimension and one extra level for the second.  The default is to sum overall margins in the table.  Other possibilities may give results thatdepend on the order in which the margins are computed.  This is flaggedin the printed output from the function.
Splits the data into subsets, computes summary statistics for each,and returns the result in a convenient form.
Splits the data into subsets, computes summary statistics for each,and returns the result in a convenient form.
Splits the data into subsets, computes summary statistics for each,and returns the result in a convenient form.
Generic function calculating Akaike's ‘An Information Criterion’ forone or several fitted model objects for which a log-likelihood valuecan be obtained, according to the formula<i>-2*log-likelihood + k*npar</i>,where <i>npar</i> represents the number of parameters in thefitted model, and <i>k = 2</i> for the usual AIC, or<i>k = log(n)</i>(<i>n</i> being the number of observations) for the so-called BIC or SBC(Schwarz's Bayesian criterion).
Find aliases (linearly dependent terms) in a linear model specified bya formula.
Compute analysis of variance (or deviance) tables for one or morefitted model objects.
Performs the Ansari-Bradley two-sample test for a difference in scaleparameters.
Fit an analysis of variance model by a call to <code>lm</code> for each stratum.
Return a list of points which linearly interpolate given data points,or a function performing the linear (or constant) interpolation.
Return a list of points which linearly interpolate given data points,or a function performing the linear (or constant) interpolation.
Fit an autoregressive time series model to the data, by defaultselecting the complexity by AIC.
Fit an autoregressive time series model to the data, by defaultselecting the complexity by AIC.
Fit an autoregressive time series model to the data, by defaultselecting the complexity by AIC.
Fit an autoregressive time series model to the data by ordinaryleast squares, by default selecting the complexity by AIC.
Fit an autoregressive time series model to the data, by defaultselecting the complexity by AIC.
Fit an ARIMA model to a univariate time series.
Simulate from an ARIMA model.
Fit an ARIMA model to a univariate time series, and forecast fromthe fitted model.
The functions or variables listed here are no longer part of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> asthey are not needed (any more).
Compute the theoretical autocorrelation function or partialautocorrelation function for an ARMA process.
Convert ARMA process to infinite MA process.
Class <code>"dendrogram"</code> provides general functions for handlingtree-like structures.  It is intended as a replacement for similarfunctions in hierarchical clustering and classification/regressiontrees, such that all of these can use the same engine for plotting orcutting trees.
This function computes and returns the distance matrix computed byusing the specified distance measure to compute the distances betweenthe rows of a data matrix.
The generic function <code>formula</code> and its specific methods provide away of extracting formulae which have been included in other objects.

<code>as.formula</code> is almost identical, additionally preservingattributes when <code>object</code> already inherits from<code>"formula"</code>.
Converts objects from other hierarchical clustering functions toclass <code>"hclust"</code>.
Given the vectors <i>(x[1], …, x[n])</i> and<i>(y[0], y[1], …, y[n])</i> (one valuemore!), <code>stepfun(x, y, ...)</code> returns an interpolating‘step’ function, say <code>fn</code>. I.e., <i>fn(t) =    c</i><i>[i]</i> (constant) for <i>t in (    x[i], x[i+1])</i> and at the abscissa values, if (by default)<code>right = FALSE</code>, <i>fn(x[i]) = y[i]</i> and for<code>right = TRUE</code>, <i>fn(x[i]) = y[i-1]</i>, for<i>i=1, …, n</i>.

The value of the constant <i>c[i]</i> above depends on the‘continuity’ parameter <code>f</code>.For the default, <code>right = FALSE, f = 0</code>,<code>fn</code> is a <em>cadlag</em> function, i.e., continuous from the right,limits from the left, so that the function is piecewise constant onintervals that include their <em>left</em> endpoint.In general, <i>c[i]</i> is interpolated in between theneighbouring <i>y</i> values,<i>c[i] = (1-f)*y[i] + f*y[i+1]</i>.Therefore, for non-0 values of <code>f</code>, <code>fn</code> may no longer be a properstep function, since it can be discontinuous from both sides, unless<code>right = TRUE, f = 1</code> which is left-continuous (i.e., constantpieces contain their right endpoint).
The function <code>ts</code> is used to create time-series objects.

<code>as.ts</code> and <code>is.ts</code> coerce an object to a time-series andtest whether an object is a time series.
Names, expressions, numeric values, and character strings are converted toone-sided formulae. If <code>object</code> is a formula, it must beone-sided, in which case it is returned unaltered.
Subsets of <code>x[]</code> are averaged, where each subset consist of thoseobservations with the same factor levels.
The <code>"tskernel"</code> class is designed to represent discretesymmetric normalized smoothing kernels.  These kernels can be used tosmooth vectors, matrices, or time series objects.

There are <code>print</code>, <code>plot</code> and <code>[</code>methods for these kernel objects.
Performs Bartlett's test of the null that the variances in each of thegroups (samples) are the same.
Generic function calculating Akaike's ‘An Information Criterion’ forone or several fitted model objects for which a log-likelihood valuecan be obtained, according to the formula<i>-2*log-likelihood + k*npar</i>,where <i>npar</i> represents the number of parameters in thefitted model, and <i>k = 2</i> for the usual AIC, or<i>k = log(n)</i>(<i>n</i> being the number of observations) for the so-called BIC or SBC(Schwarz's Bayesian criterion).
Performs an exact test of a simple null hypothesis about theprobability of success in a Bernoulli experiment.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
Plot a biplot on the current graphics device.
Compute the Box–Pierce or Ljung–Box test statistic for examining thenull hypothesis of independence in a given time series.  These aresometimes known as ‘portmanteau’ tests.
Bandwidth selectors for Gaussian kernels in <code>density</code>.
Bandwidth selectors for Gaussian kernels in <code>density</code>.
Bandwidth selectors for Gaussian kernels in <code>density</code>.
Bandwidth selectors for Gaussian kernels in <code>density</code>.
Bandwidth selectors for Gaussian kernels in <code>density</code>.
Sets the <code>"contrasts"</code> attribute for the factor.
Compute the canonical correlations between two data matrices.
Simple utilities returning (non-missing) case names, and(non-eliminated) variable names.
The function <code>acf</code> computes (and by default plots) estimates ofthe autocovariance or autocorrelation function.  Function <code>pacf</code>is the function used for the partial autocorrelations.  Function<code>ccf</code> computes the cross-correlation or cross-covariance of twounivariate series.
<code>chisq.test</code> performs chi-squared contingency table testsand goodness-of-fit tests.
Classical multidimensional scaling (MDS) of a data matrix.Also known as <em>principal coordinates analysis</em> (Gower, 1966).
<code>coef</code> is a generic function which extracts model coefficientsfrom objects returned by modeling functions.  <code>coefficients</code> isan <em>alias</em> for it.
<code>coef</code> is a generic function which extracts model coefficientsfrom objects returned by modeling functions.  <code>coefficients</code> isan <em>alias</em> for it.
Return a logical vector indicating which cases are complete, i.e.,have no missing values.
Computes confidence intervals for one or more parameters in a fittedmodel.  There is a default and a method for objects inheriting from class<code>"lm"</code>.
Computes confidence intervals for one or more parameters in a fittedmodel.  There is a default and a method for objects inheriting from class<code>"lm"</code>.
Computes confidence intervals for one or more parameters in a fittedmodel.  There is a default and a method for objects inheriting from class<code>"lm"</code>.
Minimise a function subject to linear inequality constraints using anadaptive barrier algorithm.
Return a matrix of contrasts.
Return a matrix of contrasts.
Return a matrix of contrasts.
Return a matrix of contrasts.
Return a matrix of contrasts.
Set and view the contrasts associated with a factor.
Set and view the contrasts associated with a factor.
Use the Fast Fourier Transform to compute the several kinds ofconvolutions of two sequences.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
Computes the cophenetic distances for a hierarchical clustering.
<code>var</code>, <code>cov</code> and <code>cor</code> compute the variance of <code>x</code>and the covariance or correlation of <code>x</code> and <code>y</code> if theseare vectors.   If <code>x</code> and <code>y</code> are matrices then thecovariances (or correlations) between the columns of <code>x</code> and thecolumns of <code>y</code> are computed.

<code>cov2cor</code> scales a covariance matrix into the correspondingcorrelation matrix <em>efficiently</em>.
Test for association between paired samples, using one ofPearson's product moment correlation coefficient,Kendall's <i>tau</i> or Spearman's <i>rho</i>.
<code>var</code>, <code>cov</code> and <code>cor</code> compute the variance of <code>x</code>and the covariance or correlation of <code>x</code> and <code>y</code> if theseare vectors.   If <code>x</code> and <code>y</code> are matrices then thecovariances (or correlations) between the columns of <code>x</code> and thecolumns of <code>y</code> are computed.

<code>cov2cor</code> scales a covariance matrix into the correspondingcorrelation matrix <em>efficiently</em>.
Returns a list containing estimates of the weighted covariance matrixand the mean of the data, and optionally of the (weighted) correlationmatrix.
<code>var</code>, <code>cov</code> and <code>cor</code> compute the variance of <code>x</code>and the covariance or correlation of <code>x</code> and <code>y</code> if theseare vectors.   If <code>x</code> and <code>y</code> are matrices then thecovariances (or correlations) between the columns of <code>x</code> and thecolumns of <code>y</code> are computed.

<code>cov2cor</code> scales a covariance matrix into the correspondingcorrelation matrix <em>efficiently</em>.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
Plots a cumulative periodogram.
Cuts a tree, e.g., as resulting from <code>hclust</code>, into severalgroups either by specifying the desired number(s) of groups or the cutheight(s).
<code>time</code> creates the vector of times at which a time series was sampled.

<code>cycle</code> gives the positions in the cycle of each observation.

<code>frequency</code> returns the number of samples per unit time and<code>deltat</code> the time interval between observations (see<code>ts</code>).
Compute derivatives of simple expressions, symbolically and algorithmically.
Density, distribution function, quantile function and randomgeneration for the Beta distribution with parameters <code>shape1</code> and<code>shape2</code> (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the binomial distribution with parameters <code>size</code>and <code>prob</code>.

This is conventionally interpreted as the number of ‘successes’in <code>size</code> trials.
Density, distribution function, quantile function and randomgeneration for the Cauchy distribution with location parameter<code>location</code> and scale parameter <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the chi-squared (<i>chi^2</i>) distribution with<code>df</code> degrees of freedom and optional non-centrality parameter<code>ncp</code>.
Decompose a time series into seasonal, trend and irregular componentsusing moving averages.  Deals with additive or multiplicativeseasonal component.
<code>delete.response</code> returns a <code>terms</code> object for the samemodel but with no response variable.

<code>drop.terms</code> removes variables from the right-hand side of themodel. There is also a <code>"[.terms"</code> method to perform the samefunction (with <code>keep.response = TRUE</code>).

<code>reformulate</code> creates a formula from a character vector.  If<code>length(termlabels) &gt; 1</code>, its elements are concatenated with <code>+</code>.Non-syntactic names (e.g. containing spaces or special characters; see<code>make.names</code>) must be protected with backticks  (see examples).A non-<code>parse</code>able <code>response</code> still works for now,back compatibly, with a deprecation warning.
<code>time</code> creates the vector of times at which a time series was sampled.

<code>cycle</code> gives the positions in the cycle of each observation.

<code>frequency</code> returns the number of samples per unit time and<code>deltat</code> the time interval between observations (see<code>ts</code>).
Apply function <code>FUN</code> to each node of a <code>dendrogram</code>recursively.  When  <code>y &lt;- dendrapply(x, fn)</code>, then <code>y</code> is adendrogram of the same graph structure as <code>x</code> and for each node,<code>y.node[j] &lt;- FUN( x.node[j], ...)</code> (where <code>y.node[j]</code> is an(invalid!) notation for the j-th node of y).
The (S3) generic function <code>density</code> computes kernel densityestimates.  Its default method does so with the given kernel andbandwidth for univariate observations.
The (S3) generic function <code>density</code> computes kernel densityestimates.  Its default method does so with the given kernel andbandwidth for univariate observations.
Compute derivatives of simple expressions, symbolically and algorithmically.
Compute derivatives of simple expressions, symbolically and algorithmically.
Returns the deviance of a fitted model object.
Density, distribution function, quantile function and randomgeneration for the exponential distribution with rate <code>rate</code>(i.e., mean <code>1/rate</code>).
Density, distribution function, quantile function and randomgeneration for the F distribution with <code>df1</code> and <code>df2</code>degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
The <code>"tskernel"</code> class is designed to represent discretesymmetric normalized smoothing kernels.  These kernels can be used tosmooth vectors, matrices, or time series objects.

There are <code>print</code>, <code>plot</code> and <code>[</code>methods for these kernel objects.
Returns the residual degrees-of-freedom extracted from a fitted modelobject.
The generic function <code>formula</code> and its specific methods provide away of extracting formulae which have been included in other objects.

<code>as.formula</code> is almost identical, additionally preservingattributes when <code>object</code> already inherits from<code>"formula"</code>.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
Density, distribution function, quantile function and randomgeneration for the Gamma distribution with parameters <code>shape</code> and<code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the geometric distribution with parameter <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the hypergeometric distribution.
Computes the inverse function of the lagged differences function<code>diff</code>.
This function computes and returns the distance matrix computed byusing the specified distance measure to compute the distances betweenthe rows of a data matrix.
Density, distribution function, quantile function and randomgeneration for the log normal distribution whose logarithm has meanequal to <code>meanlog</code> and standard deviation equal to <code>sdlog</code>.
Density, distribution function, quantile function and randomgeneration for the logistic distribution with parameters<code>location</code> and <code>scale</code>.
Generate multinomially distributed random number vectors andcompute multinomial probabilities.
Density, distribution function, quantile function and randomgeneration for the negative binomial distribution with parameters<code>size</code> and <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the normal distribution with mean equal to <code>mean</code>and standard deviation equal to <code>sd</code>.
Density, distribution function, quantile function and randomgeneration for the Poisson distribution with parameter <code>lambda</code>.
<code>add.scope</code> and <code>drop.scope</code> compute those terms that can beindividually added to or dropped from a model while respecting thehierarchy of terms.
<code>delete.response</code> returns a <code>terms</code> object for the samemodel but with no response variable.

<code>drop.terms</code> removes variables from the right-hand side of themodel. There is also a <code>"[.terms"</code> method to perform the samefunction (with <code>keep.response = TRUE</code>).

<code>reformulate</code> creates a formula from a character vector.  If<code>length(termlabels) &gt; 1</code>, its elements are concatenated with <code>+</code>.Non-syntactic names (e.g. containing spaces or special characters; see<code>make.names</code>) must be protected with backticks  (see examples).A non-<code>parse</code>able <code>response</code> still works for now,back compatibly, with a deprecation warning.
Compute all the single terms in the <code>scope</code> argument that can beadded to or dropped from the model, fit those models and compute atable of the changes in fit.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon Signed Rank statisticobtained from a sample with size <code>n</code>.
Density, distribution function, quantile function and randomgeneration for the t distribution with <code>df</code> degrees of freedom(and optional non-centrality parameter <code>ncp</code>).
This extracts coefficients in terms of the original levels of thecoefficients rather than the coded variables.
This extracts coefficients in terms of the original levels of thecoefficients rather than the coded variables.
These functions provide information about the uniform distributionon the interval from <code>min</code> to <code>max</code>.  <code>dunif</code> gives thedensity, <code>punif</code> gives the distribution function <code>qunif</code>gives the quantile function and <code>runif</code> generates randomdeviates.
Density, distribution function, quantile function and randomgeneration for the Weibull distribution with parameters <code>shape</code>and <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon rank sum statisticobtained from samples with size <code>m</code> and <code>n</code>, respectively.
Compute an empirical cumulative distribution function, with severalmethods for plotting, printing and computing with such an“ecdf” object.
Computes the efficiencies of fixed-effect terms in an analysis ofvariance model with multiple strata.
Returns (orthogonal) effects from a fitted model, usually a linearmodel. This is a generic function, but currently only has a methods forobjects inheriting from classes <code>"lm"</code> and <code>"glm"</code>.
Embeds the time series <code>x</code> into a low-dimensionalEuclidean space.
Extract and encode the times the first and last observations weretaken. Provided only for compatibility with S version 2.
Functions to compute matrix of residual sums of squares and products,or the estimated variance matrix for multivariate linear models.
Evaluates new variables as if they had been part of the formula of thespecified model.  This ensures that the same <code>na.action</code> and<code>subset</code> arguments are applied and allows, for example, <code>x</code>to be recovered for a model using <code>sin(x)</code> as a predictor.
Computes the (generalized) Akaike <b>A</b>n <b>I</b>nformation<b>C</b>riterion for a fitted parametric model.
Perform maximum-likelihood factor analysis on a covariance matrix ordata matrix.
<code>add.scope</code> and <code>drop.scope</code> compute those terms that can beindividually added to or dropped from a model while respecting thehierarchy of terms.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
Computes the Discrete Fourier Transform (DFT) of an array with a fastalgorithm, the “Fast Fourier Transform” (FFT).
Applies linear filtering to a univariate time series or to each seriesseparately of a multivariate time series.
Performs Fisher's exact test for testing the null of independence ofrows and columns in a contingency table with fixed marginals.
<code>fitted</code> is a generic function which extracts fitted values fromobjects returned by modeling functions.  <code>fitted.values</code> is analias for it.

All object classes which are returned by model fitting functionsshould provide a <code>fitted</code> method.  (Note that the generic is<code>fitted</code> and not <code>fitted.values</code>.)

Methods can make use of <code>napredict</code> methods to compensatefor the omission of missing values.  The default and <code>nls</code>methods do.
<code>fitted</code> is a generic function which extracts fitted values fromobjects returned by modeling functions.  <code>fitted.values</code> is analias for it.

All object classes which are returned by model fitting functionsshould provide a <code>fitted</code> method.  (Note that the generic is<code>fitted</code> and not <code>fitted.values</code>.)

Methods can make use of <code>napredict</code> methods to compensatefor the omission of missing values.  The default and <code>nls</code>methods do.
Returns Tukey's five number summary (minimum, lower-hinge, median,upper-hinge, maximum) for the input data.
Performs a Fligner-Killeen (median) test of the null that thevariances in each of the groups (samples) are the same.
The generic function <code>formula</code> and its specific methods provide away of extracting formulae which have been included in other objects.

<code>as.formula</code> is almost identical, additionally preservingattributes when <code>object</code> already inherits from<code>"formula"</code>.
<code>time</code> creates the vector of times at which a time series was sampled.

<code>cycle</code> gives the positions in the cycle of each observation.

<code>frequency</code> returns the number of samples per unit time and<code>deltat</code> the time interval between observations (see<code>ts</code>).
Performs a Friedman rank sum test with unreplicated blocked data.
Create ‘flat’ contingency tables.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
<code>model.frame</code> (a generic function) and its methods return a<code>data.frame</code> with the variables needed to use<code>formula</code> and any <code>...</code> arguments.
<code>update</code> will update and (by default) re-fit a model.  It does thisby extracting the call stored in the object, updating the call and (bydefault) evaluating that call.  Sometimes it is useful to call<code>update</code> with only one argument, for example if the data frame hasbeen corrected.

“Extracting the call” in <code>update()</code> and similar functionsuses <code>getCall()</code> which itself is a (S3) generic function with adefault method that simply gets <code>x$call</code>.

Because of this, <code>update()</code> will often work (via its defaultmethod) on new model classes, either automatically, or by providing asimple <code>getCall()</code> method for that class.
This function evaluates initial parameter estimates for a nonlinearregression model.  If <code>data</code> is a parameterized data frame or<code>pframe</code> object, its <code>parameters</code> attribute is returned.Otherwise the object is examined to see if it contains a call to a<code>selfStart</code> object whose <code>initial</code> attribute can beevaluated.
<code>glm</code> is used to fit generalized linear models, specified bygiving a symbolic description of the linear predictor and adescription of the error distribution.
Auxiliary function for <code>glm</code> fitting.Typically only used internally by <code>glm.fit</code>, but may beused to construct a <code>control</code> argument to either function.
<code>glm</code> is used to fit generalized linear models, specified bygiving a symbolic description of the linear predictor and adescription of the error distribution.
<code>tsp</code> returns the <code>tsp</code> attribute (or <code>NULL</code>).It is included for compatibility with S version 2. <code>tsp&lt;-</code>sets the <code>tsp</code> attribute. <code>hasTsp</code> ensures <code>x</code> has a<code>tsp</code> attribute, by adding one if needed.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
Hierarchical cluster analysis on a set of dissimilarities andmethods for analyzing it.
A heat map is a false color image (basically<code>image(t(x))</code>) with a dendrogram added to the left sideand to the top.  Typically, reordering of the rows and columnsaccording to some set of values (row or column means) within therestrictions imposed by the dendrogram is carried out.
Computes Holt-Winters Filtering of a given time series.Unknown parameters are determined by minimizing the squaredprediction error.
This function provides the basic quantities which areused in forming a wide variety of diagnostics forchecking the quality of regression fits.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
Adaptive quadrature of functions of one variable over a finite orinfinite interval.
Plots the mean (or other summary) of the response for two-waycombinations of factors, thereby illustrating possible interactions.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
computes interquartile range of the <code>x</code> values.
<span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>'s formula notation allows models with no intercept and nopredictors. These require special handling internally.<code>is.empty.model()</code> checks whether an object describes an emptymodel.
Class <code>"dendrogram"</code> provides general functions for handlingtree-like structures.  It is intended as a replacement for similarfunctions in hierarchical clustering and classification/regressiontrees, such that all of these can use the same engine for plotting orcutting trees.
The function <code>ts</code> is used to create time-series objects.

<code>as.ts</code> and <code>is.ts</code> coerce an object to a time-series andtest whether an object is a time series.
Given the vectors <i>(x[1], …, x[n])</i> and<i>(y[0], y[1], …, y[n])</i> (one valuemore!), <code>stepfun(x, y, ...)</code> returns an interpolating‘step’ function, say <code>fn</code>. I.e., <i>fn(t) =    c</i><i>[i]</i> (constant) for <i>t in (    x[i], x[i+1])</i> and at the abscissa values, if (by default)<code>right = FALSE</code>, <i>fn(x[i]) = y[i]</i> and for<code>right = TRUE</code>, <i>fn(x[i]) = y[i-1]</i>, for<i>i=1, …, n</i>.

The value of the constant <i>c[i]</i> above depends on the‘continuity’ parameter <code>f</code>.For the default, <code>right = FALSE, f = 0</code>,<code>fn</code> is a <em>cadlag</em> function, i.e., continuous from the right,limits from the left, so that the function is piecewise constant onintervals that include their <em>left</em> endpoint.In general, <i>c[i]</i> is interpolated in between theneighbouring <i>y</i> values,<i>c[i] = (1-f)*y[i] + f*y[i+1]</i>.Therefore, for non-0 values of <code>f</code>, <code>fn</code> may no longer be a properstep function, since it can be discontinuous from both sides, unless<code>right = TRUE, f = 1</code> which is left-continuous (i.e., constantpieces contain their right endpoint).
The function <code>ts</code> is used to create time-series objects.

<code>as.ts</code> and <code>is.ts</code> coerce an object to a time-series andtest whether an object is a time series.
The <code>"tskernel"</code> class is designed to represent discretesymmetric normalized smoothing kernels.  These kernels can be used tosmooth vectors, matrices, or time series objects.

There are <code>print</code>, <code>plot</code> and <code>[</code>methods for these kernel objects.
Compute the isotonic (monotonely increasing nonparametric) leastsquares regression which is piecewise constant.
Use Kalman Filtering to find the (Gaussian) log-likelihood, or forforecasting or smoothing.
Use Kalman Filtering to find the (Gaussian) log-likelihood, or forforecasting or smoothing.
Use Kalman Filtering to find the (Gaussian) log-likelihood, or forforecasting or smoothing.
Use Kalman Filtering to find the (Gaussian) log-likelihood, or forforecasting or smoothing.
<code>kernapply</code> computes the convolution between an input sequenceand a specific kernel.
The <code>"tskernel"</code> class is designed to represent discretesymmetric normalized smoothing kernels.  These kernels can be used tosmooth vectors, matrices, or time series objects.

There are <code>print</code>, <code>plot</code> and <code>[</code>methods for these kernel objects.
Perform k-means clustering on a data matrix.
Given the vectors <i>(x[1], …, x[n])</i> and<i>(y[0], y[1], …, y[n])</i> (one valuemore!), <code>stepfun(x, y, ...)</code> returns an interpolating‘step’ function, say <code>fn</code>. I.e., <i>fn(t) =    c</i><i>[i]</i> (constant) for <i>t in (    x[i], x[i+1])</i> and at the abscissa values, if (by default)<code>right = FALSE</code>, <i>fn(x[i]) = y[i]</i> and for<code>right = TRUE</code>, <i>fn(x[i]) = y[i-1]</i>, for<i>i=1, …, n</i>.

The value of the constant <i>c[i]</i> above depends on the‘continuity’ parameter <code>f</code>.For the default, <code>right = FALSE, f = 0</code>,<code>fn</code> is a <em>cadlag</em> function, i.e., continuous from the right,limits from the left, so that the function is piecewise constant onintervals that include their <em>left</em> endpoint.In general, <i>c[i]</i> is interpolated in between theneighbouring <i>y</i> values,<i>c[i] = (1-f)*y[i] + f*y[i+1]</i>.Therefore, for non-0 values of <code>f</code>, <code>fn</code> may no longer be a properstep function, since it can be discontinuous from both sides, unless<code>right = TRUE, f = 1</code> which is left-continuous (i.e., constantpieces contain their right endpoint).
Performs a Kruskal-Wallis rank sum test.
Perform a one- or two-sample Kolmogorov-Smirnov test.
The Nadaraya–Watson kernel regression estimate.
Compute a lagged version of a time series, shifting the time baseback by a given number of observations.

<code>lag</code> is a generic function; this page documents its defaultmethod.
Plot time series against lagged versions of themselves.Helps visualizing ‘auto-dependence’ even when auto-correlationsvanish.
Fit a line robustly as recommended in <em>Exploratory Data Analysis</em>.

Currently by default (<code>iter = 1</code>) the initial median-median line is <em>not</em> iterated (asopposed to Tukey's “resistant line” in the references).
<code>lm</code> is used to fit linear models.It can be used to carry out regression,single stratum analysis of variance andanalysis of covariance (although <code>aov</code> may provide a moreconvenient interface for these).
These are the basic computing engines called by <code>lm</code> usedto fit linear models.  These should usually <em>not</em> be useddirectly unless by experienced users.  <code>.lm.fit()</code> is bare bonewrapper to the innermost QR-based C code, on which<code>glm.fit</code> and <code>lsfit</code> are based as well, foreven more experienced users.
This function provides the basic quantities which areused in forming a wide variety of diagnostics forchecking the quality of regression fits.
These are the basic computing engines called by <code>lm</code> usedto fit linear models.  These should usually <em>not</em> be useddirectly unless by experienced users.  <code>.lm.fit()</code> is bare bonewrapper to the innermost QR-based C code, on which<code>glm.fit</code> and <code>lsfit</code> are based as well, foreven more experienced users.
Extract or print loadings in factor analysis (or principalcomponents analysis).
Fit a polynomial surface determined by one or more numericalpredictors, using local fitting.
Set control parameters for <code>loess</code> fits.
Plot and add a smooth curve computed by <code>loess</code> to a scatter plot.
This function is generic; method functions can be written to handlespecific classes of objects.  Classes which have methods for thisfunction include: <code>"glm"</code>, <code>"lm"</code>, <code>"nls"</code> and<code>"Arima"</code>.  Packages contain methods for other classes, such as<code>"fitdistr"</code>, <code>"negbin"</code> and <code>"polr"</code> in package<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>, <code>"multinom"</code> in package <a href="https://CRAN.R-project.org/package=nnet"><span class="pkg">nnet</span></a> and<code>"gls"</code>, <code>"gnls"</code> <code>"lme"</code> and others in package<a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>.
<code>loglin</code> is used to fit log-linear models to multidimensionalcontingency tables by Iterative Proportional Fitting.
This function performs the computations for the<em>LOWESS</em> smoother which uses locally-weighted polynomialregression (see the references).
Computes basic statistics, including standard errors, t- and p-valuesfor the regression coefficients.
Computes basic statistics, including standard errors, t- and p-valuesfor the regression coefficients and prints them if <code>print.it</code> is<code>TRUE</code>.
The least squares estimate of <b><i>b</i></b> in the model

<p style="text-align: center;"><i>y = X b + e</i>

is found.
Compute the median absolute deviation, i.e., the (lo-/hi-) median ofthe absolute deviations from the median, and (by default) adjust by afactor for asymptotically normal consistency.
Returns the squared Mahalanobis distance of all rows in <code>x</code> and thevector <i>mu</i> = <code>center</code> with respect to<i>Sigma</i> = <code>cov</code>.This is (for vector <code>x</code>) defined as

<p style="text-align: center;"><i>D^2 = (x - μ)' Σ^-1 (x - μ)</i>
This function is used with the <code>family</code> functions in<code>glm()</code>.Given the name of a link, it returns a link function, an inverse linkfunction, the derivative <i>dmu/deta</i> and a functionfor domain checking.
Use Kalman Filtering to find the (Gaussian) log-likelihood, or forforecasting or smoothing.
A utility to help <code>model.frame.default</code> create the rightmatrices when predicting from models with terms like (univariate)<code>poly</code> or <code>ns</code>.
A class for the multivariate analysis of variance.
Performs a Cochran-Mantel-Haenszel chi-squared test of the null thattwo nominal variables are conditionally independent in each stratum,assuming that there is no three-way interaction.
Tests whether a Wishart-distributed covariance matrix (ortransformation thereof) is proportional to a given matrix.
Performs McNemar's chi-squared test for symmetry of rows and columnsin a two-dimensional contingency table.
Compute the sample median.
Compute the sample median.
Fits an additive model (twoway decomposition) using Tukey's<em>median polish</em> procedure.
Returns the response, offset, subset, weights or otherspecial components of a model frame passed as optional arguments to<code>model.frame</code>.
<code>model.frame</code> (a generic function) and its methods return a<code>data.frame</code> with the variables needed to use<code>formula</code> and any <code>...</code> arguments.
<code>model.frame</code> (a generic function) and its methods return a<code>data.frame</code> with the variables needed to use<code>formula</code> and any <code>...</code> arguments.
<code>model.matrix</code> creates a design (or model) matrix, e.g., byexpanding factors to a set of dummy variables (depending on thecontrasts) and expanding interactions similarly.
<code>model.matrix</code> creates a design (or model) matrix, e.g., byexpanding factors to a set of dummy variables (depending on thecontrasts) and expanding interactions similarly.
<code>model.matrix</code> creates a design (or model) matrix, e.g., byexpanding factors to a set of dummy variables (depending on thecontrasts) and expanding interactions similarly.
Returns the response, offset, subset, weights or otherspecial components of a model frame passed as optional arguments to<code>model.frame</code>.
Returns the response, offset, subset, weights or otherspecial components of a model frame passed as optional arguments to<code>model.frame</code>.
Computes summary tables for model fits, especially complex <code>aov</code>fits.
Returns the response, offset, subset, weights or otherspecial components of a model frame passed as optional arguments to<code>model.frame</code>.
These functions plot seasonal (or other) subseries of a time series.For each season (or other category), a time series is plotted.
Performs Mood's two-sample test for a difference in scale parameters.
Computes the Discrete Fourier Transform (DFT) of an array with a fastalgorithm, the “Fast Fourier Transform” (FFT).
Extract information on the NA action used to create an object.
Find the longest consecutive stretch of non-missing values in a timeseries object.  (In the event of a tie, the first such stretch.)
These generic functions are useful for dealing with <code>NA</code>sin e.g., data frames.<code>na.fail</code> returns the object if it does not contain anymissing values, and signals an error otherwise.<code>na.omit</code> returns the object with incomplete cases removed.<code>na.pass</code> returns the object unchanged.
These generic functions are useful for dealing with <code>NA</code>sin e.g., data frames.<code>na.fail</code> returns the object if it does not contain anymissing values, and signals an error otherwise.<code>na.omit</code> returns the object with incomplete cases removed.<code>na.pass</code> returns the object unchanged.
These generic functions are useful for dealing with <code>NA</code>sin e.g., data frames.<code>na.fail</code> returns the object if it does not contain anymissing values, and signals an error otherwise.<code>na.omit</code> returns the object with incomplete cases removed.<code>na.pass</code> returns the object unchanged.
These generic functions are useful for dealing with <code>NA</code>sin e.g., data frames.<code>na.fail</code> returns the object if it does not contain anymissing values, and signals an error otherwise.<code>na.omit</code> returns the object with incomplete cases removed.<code>na.pass</code> returns the object unchanged.
Use missing value information to adjust residuals and predictions.
Use missing value information to report the effects of an <code>na.action</code>.
Use missing value information to adjust residuals and predictions.
<code>nextn</code> returns the smallest integer,greater than or equal to <code>n</code>, which can be obtainedas a product of powers of the values contained in <code>factors</code>.

<code>nextn()</code> is intended to be used to find a suitable lengthto zero-pad the argument of <code>fft</code>so that the transform is computed quickly.The default value for <code>factors</code> ensures this.
This function carries out a minimization of the function <code>f</code>using a Newton-type algorithm.  See the references for details.
Unconstrained and box-constrained optimization using PORT routines.

For historical compatibility.
Determine the nonlinear (weighted) least-squares estimates of theparameters of a nonlinear model.
Allow the user to set some characteristics of the <code>nls</code>nonlinear least squares algorithm.
Fits the asymptotic regression model, in the form <code>b0 +      b1*(1-exp(-exp(lrc) * x))</code> to the <code>xy</code> data.This can be used as a building block in determining starting estimatesfor more complicated models.
Use inverse linear interpolation to approximate the <code>x</code> value atwhich the function represented by <code>xy</code> is equal to <code>yval</code>.
Provide an initial guess at the horizontal asymptote on the left side(i.e., small values of <code>x</code>) of the graph of <code>y</code> versus<code>x</code> from the <code>xy</code> object.  Primarily used within<code>initial</code> functions for self-starting nonlinear regressionmodels.
Provide an initial guess at the horizontal asymptote on the right side(i.e., large values of <code>x</code>) of the graph of <code>y</code> versus<code>x</code> from the <code>xy</code> object.  Primarily used within<code>initial</code> functions for self-starting nonlinear regressionmodels.
Extract the number of ‘observations’ from a model fit.  This isprincipally intended to be used in computing BIC (see <code>AIC</code>).
<code>numericDeriv</code> numerically evaluates the gradient of an expression.
An offset is a term to be added to a linear predictor, such as in ageneralised linear model, with known coefficient 1 rather than anestimated coefficient.
Test whether two or more samples from normal distributions have thesame means.  The variances are not necessarily assumed to be equal.
General-purpose optimization based on Nelder–Mead, quasi-Newton andconjugate-gradient algorithms. It includes an option forbox-constrained optimization and simulated annealing.
General-purpose optimization based on Nelder–Mead, quasi-Newton andconjugate-gradient algorithms. It includes an option forbox-constrained optimization and simulated annealing.
The function <code>optimize</code> searches the interval from<code>lower</code> to <code>upper</code> for a minimum or maximum ofthe function <code>f</code> with respect to its first argument.

<code>optimise</code> is an alias for <code>optimize</code>.
The function <code>optimize</code> searches the interval from<code>lower</code> to <code>upper</code> for a minimum or maximum ofthe function <code>f</code> with respect to its first argument.

<code>optimise</code> is an alias for <code>optimize</code>.
Theses functions return the order (index) or the <code>"label"</code>attribute for the leaves in adendrogram.  These indices can then be used to access the appropriatecomponents of any additional data.
Given a set of p-values, returns p-values adjusted usingone of several methods.
Given a set of p-values, returns p-values adjusted usingone of several methods.
The function <code>acf</code> computes (and by default plots) estimates ofthe autocovariance or autocorrelation function.  Function <code>pacf</code>is the function used for the partial autocorrelations.  Function<code>ccf</code> computes the cross-correlation or cross-covariance of twounivariate series.
Combines two vectors into an object of class <code>"Pair"</code>
Calculate pairwise comparisons between pairs of proportions withcorrection for multiple testing
Calculate pairwise comparisons between group levels with correctionsfor multiple testing
Creates  table of p values for pairwise comparisonswith corrections for multiple testing.
Calculate pairwise comparisons between group levels with correctionsfor multiple testing.
Density, distribution function, quantile function and randomgeneration for the Beta distribution with parameters <code>shape1</code> and<code>shape2</code> (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the binomial distribution with parameters <code>size</code>and <code>prob</code>.

This is conventionally interpreted as the number of ‘successes’in <code>size</code> trials.
Computes answers to a generalised <em>birthday paradox</em> problem.<code>pbirthday</code> computes the probability of a coincidence and<code>qbirthday</code> computes the smallest number of observations neededto have at least a specified probability of coincidence.
Density, distribution function, quantile function and randomgeneration for the Cauchy distribution with location parameter<code>location</code> and scale parameter <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the chi-squared (<i>chi^2</i>) distribution with<code>df</code> degrees of freedom and optional non-centrality parameter<code>ncp</code>.
Density, distribution function, quantile function and randomgeneration for the exponential distribution with rate <code>rate</code>(i.e., mean <code>1/rate</code>).
Density, distribution function, quantile function and randomgeneration for the F distribution with <code>df1</code> and <code>df2</code>degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the Gamma distribution with parameters <code>shape</code> and<code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the geometric distribution with parameter <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the hypergeometric distribution.
The functions or variables listed here are no longer part of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> asthey are not needed (any more).
Density, distribution function, quantile function and randomgeneration for the log normal distribution whose logarithm has meanequal to <code>meanlog</code> and standard deviation equal to <code>sdlog</code>.
Density, distribution function, quantile function and randomgeneration for the logistic distribution with parameters<code>location</code> and <code>scale</code>.
Compute an empirical cumulative distribution function, with severalmethods for plotting, printing and computing with such an“ecdf” object.
Plotting method for objects of class <code>"spec"</code>.  For multivariatetime series it plots the marginal spectra of the series or pairs plotsof the coherency and phase of the cross-spectra.
Plotting method for objects of class <code>"spec"</code>.  For multivariatetime series it plots the marginal spectra of the series or pairs plotsof the coherency and phase of the cross-spectra.
Method of the generic <code>plot</code> for <code>stepfun</code>objects and utility for plotting piecewise constant functions.
Plotting method for objects inheriting from class <code>"ts"</code>.
Density, distribution function, quantile function and randomgeneration for the negative binomial distribution with parameters<code>size</code> and <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the normal distribution with mean equal to <code>mean</code>and standard deviation equal to <code>sd</code>.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
Performs an exact test of a simple null hypothesis about therate parameter in Poisson distribution, or for theratio between two rate parameters.
Returns or evaluates orthogonal polynomials of degree 1 to<code>degree</code> over the specified set of points <code>x</code>: these are allorthogonal to the constant polynomial of degree 0.  Alternatively,evaluate raw polynomials.
Returns or evaluates orthogonal polynomials of degree 1 to<code>degree</code> over the specified set of points <code>x</code>: these are allorthogonal to the constant polynomial of degree 0.  Alternatively,evaluate raw polynomials.
Creates a link object based on the link function<i>η = μ ^ λ</i>.
Compute power of test or determine parameters to obtain target power.
Compute the power of the two-sample test for proportions, or determineparameters to obtain a target power.
Compute the power of the one- or two- sample t test,or determine parameters to obtain a target power.
Computes the Phillips-Perron test for the null hypothesis that<code>x</code> has a unit root against a stationary alternative.
Generates the sequence of probability points<code>(1:m - a)/(m + (1-a)-a)</code>where <code>m</code> is either <code>n</code>, if <code>length(n)==1</code>, or<code>length(n)</code>.
Density, distribution function, quantile function and randomgeneration for the Poisson distribution with parameter <code>lambda</code>.
Fit a projection pursuit regression model.
Performs a principal components analysis on the given data matrixand returns the results as an object of class <code>prcomp</code>.
<code>predict</code> is a generic function for predictions from the results ofvarious model fitting functions.  The function invokes particular<em>methods</em> which depend on the <code>class</code> ofthe first argument.
Obtains predictions and optionally estimates standard errors of thosepredictions from a fitted generalized linear model object.
Predicted values based on linear model object.
Compute an object to be used for plots relating to the given model object.
<code>princomp</code> performs a principal components analysis on the givennumeric data matrix and returns the results as an object of class<code>princomp</code>.
Utility function to be used in higher-level <code>print</code>methods, such as those for <code>summary.lm</code>,<code>summary.glm</code> and <code>anova</code>.  Thegoal is to provide a flexible interface with smart defaults suchthat often, only <code>x</code> needs to be specified.
Investigates behavior of objective function near the solutionrepresented by <code>fitted</code>.

See documentation on method functions for further details.
<code>proj</code> returns a matrix or list of matrices giving the projectionsof the data onto the terms of a linear model.  It is most frequentlyused for <code>aov</code> models.
These functions ‘rotate’ loading matrices in factor analysis.
<code>prop.test</code> can be used for testing the null that theproportions (probabilities of success) in several groups are thesame, or that they equal certain given values.
Performs chi-squared test for trend in proportions, i.e., a testasymptotically optimal for local alternatives where the log odds varyin proportion with <code>score</code>.  By default, <code>score</code> is chosenas the group numbers.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon Signed Rank statisticobtained from a sample with size <code>n</code>.
Density, distribution function, quantile function and randomgeneration for the t distribution with <code>df</code> degrees of freedom(and optional non-centrality parameter <code>ncp</code>).
Functions of the distribution of the studentized range, <i>R/s</i>,where <i>R</i> is the range of a standard normal sample and<i>df*s^2</i> is independently distributed aschi-squared with <i>df</i> degrees of freedom, see <code>pchisq</code>.
These functions provide information about the uniform distributionon the interval from <code>min</code> to <code>max</code>.  <code>dunif</code> gives thedensity, <code>punif</code> gives the distribution function <code>qunif</code>gives the quantile function and <code>runif</code> generates randomdeviates.
Density, distribution function, quantile function and randomgeneration for the Weibull distribution with parameters <code>shape</code>and <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon rank sum statisticobtained from samples with size <code>m</code> and <code>n</code>, respectively.
Density, distribution function, quantile function and randomgeneration for the Beta distribution with parameters <code>shape1</code> and<code>shape2</code> (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the binomial distribution with parameters <code>size</code>and <code>prob</code>.

This is conventionally interpreted as the number of ‘successes’in <code>size</code> trials.
Computes answers to a generalised <em>birthday paradox</em> problem.<code>pbirthday</code> computes the probability of a coincidence and<code>qbirthday</code> computes the smallest number of observations neededto have at least a specified probability of coincidence.
Density, distribution function, quantile function and randomgeneration for the Cauchy distribution with location parameter<code>location</code> and scale parameter <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the chi-squared (<i>chi^2</i>) distribution with<code>df</code> degrees of freedom and optional non-centrality parameter<code>ncp</code>.
Density, distribution function, quantile function and randomgeneration for the exponential distribution with rate <code>rate</code>(i.e., mean <code>1/rate</code>).
Density, distribution function, quantile function and randomgeneration for the F distribution with <code>df1</code> and <code>df2</code>degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the Gamma distribution with parameters <code>shape</code> and<code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the geometric distribution with parameter <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the hypergeometric distribution.
Density, distribution function, quantile function and randomgeneration for the log normal distribution whose logarithm has meanequal to <code>meanlog</code> and standard deviation equal to <code>sdlog</code>.
Density, distribution function, quantile function and randomgeneration for the logistic distribution with parameters<code>location</code> and <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the negative binomial distribution with parameters<code>size</code> and <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the normal distribution with mean equal to <code>mean</code>and standard deviation equal to <code>sd</code>.
Density, distribution function, quantile function and randomgeneration for the Poisson distribution with parameter <code>lambda</code>.
<code>qqnorm</code> is a generic function the default method of whichproduces a normal QQ plot of the values in <code>y</code>.<code>qqline</code> adds a line to a “theoretical”, by defaultnormal, quantile-quantile plot which passes through the <code>probs</code>quantiles, by default the first and third quartiles.

<code>qqplot</code> produces a QQ plot of two datasets.

Graphical parameters may be given as arguments to <code>qqnorm</code>,<code>qqplot</code> and <code>qqline</code>.
<code>qqnorm</code> is a generic function the default method of whichproduces a normal QQ plot of the values in <code>y</code>.<code>qqline</code> adds a line to a “theoretical”, by defaultnormal, quantile-quantile plot which passes through the <code>probs</code>quantiles, by default the first and third quartiles.

<code>qqplot</code> produces a QQ plot of two datasets.

Graphical parameters may be given as arguments to <code>qqnorm</code>,<code>qqplot</code> and <code>qqline</code>.
<code>qqnorm</code> is a generic function the default method of whichproduces a normal QQ plot of the values in <code>y</code>.<code>qqline</code> adds a line to a “theoretical”, by defaultnormal, quantile-quantile plot which passes through the <code>probs</code>quantiles, by default the first and third quartiles.

<code>qqplot</code> produces a QQ plot of two datasets.

Graphical parameters may be given as arguments to <code>qqnorm</code>,<code>qqplot</code> and <code>qqline</code>.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon Signed Rank statisticobtained from a sample with size <code>n</code>.
Density, distribution function, quantile function and randomgeneration for the t distribution with <code>df</code> degrees of freedom(and optional non-centrality parameter <code>ncp</code>).
Functions of the distribution of the studentized range, <i>R/s</i>,where <i>R</i> is the range of a standard normal sample and<i>df*s^2</i> is independently distributed aschi-squared with <i>df</i> degrees of freedom, see <code>pchisq</code>.
Performs a Quade test with unreplicated blocked data.
The generic function <code>quantile</code> produces sample quantilescorresponding to the given probabilities.The smallest observation corresponds to a probability of 0 and thelargest to a probability of 1.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
Family objects provide a convenient way to specify the details of themodels used by functions such as <code>glm</code>.  See thedocumentation for <code>glm</code> for the details on how such modelfitting takes place.
These functions provide information about the uniform distributionon the interval from <code>min</code> to <code>max</code>.  <code>dunif</code> gives thedensity, <code>punif</code> gives the distribution function <code>qunif</code>gives the quantile function and <code>runif</code> generates randomdeviates.
Density, distribution function, quantile function and randomgeneration for the Weibull distribution with parameters <code>shape</code>and <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon rank sum statisticobtained from samples with size <code>m</code> and <code>n</code>, respectively.
Generate random 2-way tables with given marginals using Patefield'salgorithm.
Density, distribution function, quantile function and randomgeneration for the Beta distribution with parameters <code>shape1</code> and<code>shape2</code> (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the binomial distribution with parameters <code>size</code>and <code>prob</code>.

This is conventionally interpreted as the number of ‘successes’in <code>size</code> trials.
Density, distribution function, quantile function and randomgeneration for the Cauchy distribution with location parameter<code>location</code> and scale parameter <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the chi-squared (<i>chi^2</i>) distribution with<code>df</code> degrees of freedom and optional non-centrality parameter<code>ncp</code>.
Read, write and coerce ‘flat’ (contingency) tables, aka<code>ftable</code>s.
Draws rectangles around the branches of a dendrogram highlighting thecorresponding clusters. First the dendrogram is cut at a certainlevel, then a rectangle is drawn around selected branches.
<code>delete.response</code> returns a <code>terms</code> object for the samemodel but with no response variable.

<code>drop.terms</code> removes variables from the right-hand side of themodel. There is also a <code>"[.terms"</code> method to perform the samefunction (with <code>keep.response = TRUE</code>).

<code>reformulate</code> creates a formula from a character vector.  If<code>length(termlabels) &gt; 1</code>, its elements are concatenated with <code>+</code>.Non-syntactic names (e.g. containing spaces or special characters; see<code>make.names</code>) must be protected with backticks  (see examples).A non-<code>parse</code>able <code>response</code> still works for now,back compatibly, with a deprecation warning.
The levels of a factor are re-ordered so that the level specified by<code>ref</code> is first and the others are moved down. This is usefulfor <code>contr.treatment</code> contrasts which take the first level asthe reference.
<code>reorder</code> is a generic function.  The <code>"default"</code> methodtreats its first argument as a categorical variable, and reorders itslevels based on the values of a second variable, usually numeric.
Returns a vector or a list of the number of replicates foreach term in the formula.
This function reshapes a data frame between ‘wide’ format (withrepeated measurements in separate columns of the same row) and‘long’ format (with the repeated measurements in separaterows).
<code>residuals</code> is a generic function which extracts model residualsfrom objects returned by modeling functions.

The abbreviated form <code>resid</code> is an alias for <code>residuals</code>.It is intended to encourage users to access object components throughan accessor function rather than by directly referencing an objectslot.

All object classes which are returned by model fitting functionsshould provide a <code>residuals</code> method.  (Note that the method isfor <span class="samp">residuals</span> and not <span class="samp">resid</span>.)

Methods can make use of <code>naresid</code> methods to compensatefor the omission of missing values.  The default, <code>nls</code> and<code>smooth.spline</code> methods do.
<code>residuals</code> is a generic function which extracts model residualsfrom objects returned by modeling functions.

The abbreviated form <code>resid</code> is an alias for <code>residuals</code>.It is intended to encourage users to access object components throughan accessor function rather than by directly referencing an objectslot.

All object classes which are returned by model fitting functionsshould provide a <code>residuals</code> method.  (Note that the method isfor <span class="samp">residuals</span> and not <span class="samp">resid</span>.)

Methods can make use of <code>naresid</code> methods to compensatefor the omission of missing values.  The default, <code>nls</code> and<code>smooth.spline</code> methods do.
These functions are all <code>methods</code> for class <code>glm</code> or<code>summary.glm</code> objects.
All these functions are <code>methods</code> for class <code>"lm"</code>  objects.
Density, distribution function, quantile function and randomgeneration for the exponential distribution with rate <code>rate</code>(i.e., mean <code>1/rate</code>).
Density, distribution function, quantile function and randomgeneration for the F distribution with <code>df1</code> and <code>df2</code>degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
Density, distribution function, quantile function and randomgeneration for the Gamma distribution with parameters <code>shape</code> and<code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the geometric distribution with parameter <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the hypergeometric distribution.
Density, distribution function, quantile function and randomgeneration for the log normal distribution whose logarithm has meanequal to <code>meanlog</code> and standard deviation equal to <code>sdlog</code>.
Density, distribution function, quantile function and randomgeneration for the logistic distribution with parameters<code>location</code> and <code>scale</code>.
Generate multinomially distributed random number vectors andcompute multinomial probabilities.
Density, distribution function, quantile function and randomgeneration for the negative binomial distribution with parameters<code>size</code> and <code>prob</code>.
Density, distribution function, quantile function and randomgeneration for the normal distribution with mean equal to <code>mean</code>and standard deviation equal to <code>sd</code>.
Density, distribution function, quantile function and randomgeneration for the Poisson distribution with parameter <code>lambda</code>.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon Signed Rank statisticobtained from a sample with size <code>n</code>.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
This suite of functions can be used to compute some of the regression(leave-one-out deletion) diagnostics for linear and generalized linearmodels discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),etc.
Density, distribution function, quantile function and randomgeneration for the t distribution with <code>df</code> degrees of freedom(and optional non-centrality parameter <code>ncp</code>).
These functions provide information about the uniform distributionon the interval from <code>min</code> to <code>max</code>.  <code>dunif</code> gives thedensity, <code>punif</code> gives the distribution function <code>qunif</code>gives the quantile function and <code>runif</code> generates randomdeviates.
Compute running medians of odd span.  This is the ‘most robust’scatter plot smoothing possible.  For efficiency (and historicalreason), you can use one of two different algorithms giving identicalresults.
Density, distribution function, quantile function and randomgeneration for the Weibull distribution with parameters <code>shape</code>and <code>scale</code>.
Density, distribution function, quantile function and randomgeneration for the distribution of the Wilcoxon rank sum statisticobtained from samples with size <code>m</code> and <code>n</code>, respectively.
Generate <code>n</code> random matrices, distributed according to theWishart distribution with parameters <code>Sigma</code> and <code>df</code>,<i>W_p(Sigma, df)</i>.
Plot and add a smooth curve computed by <code>loess</code> to a scatter plot.
<code>screeplot.default</code> plots the variances against the number of theprincipal component. This is also the <code>plot</code> method for classes<code>"princomp"</code> and <code>"prcomp"</code>.
This function computes the standard deviation of the values in<code>x</code>.If <code>na.rm</code> is <code>TRUE</code> then missing values are removed beforecomputation proceeds.
Returns the standard errors for one or more contrasts in an <code>aov</code>object.
Construct self-starting nonlinear models to be used in<code>nls</code>, etc.  Via function <code>initial</code> to computeapproximate parameter values from data, such models are“self-starting”, i.e., do not need a <code>start</code> argument in,e.g., <code>nls()</code>.
This is a convenience function that sets the names on an object andreturns the object.  It is most useful at the end of a functiondefinition where one is creating the object to be returned and wouldprefer not to store it under a name just so the names can be assigned.
Performs the Shapiro-Wilk test of normality.
Extract the estimated standard deviation of the errors, the“residual standard deviation” (misnamed also“residual standard error”, e.g., in<code>summary.lm()</code>'s output, from a fitted model).

Many classical statistical models have a <em>scale parameter</em>,typically the standard deviation of a zero-mean normal (or Gaussian)random variable which is denoted as <i>σ</i>.<code>sigma(.)</code> extracts the <em>estimated</em> parameter from a fittedmodel, i.e., <i>sigma^</i>.
Simulate one or more responses from the distributioncorresponding to a fitted model object.
Tukey's smoothers, <em>3RS3R</em>, <em>3RSS</em>, <em>3R</em>, etc.
Fits a cubic smoothing spline to the supplied data.
Smooth end points of a vector <code>y</code> using subsequently smallermedians and Tukey's end point rule at the very end. (of odd span),
This is a constructor function for the class of <code>sortedXyData</code>objects.  These objects are mostly used in the <code>initial</code>function for a self-starting nonlinear regression model, which will beof the <code>selfStart</code> class.
Fits an AR model to <code>x</code> (or uses the existing fit) and computes(and by default plots) the spectral density of the fitted model.
<code>spec.pgram</code> calculates the periodogram using a fast Fouriertransform, and optionally smooths the result with a series ofmodified Daniell smoothers (moving averages giving half weight tothe end values).
Apply a cosine-bell taper to a time series.
The <code>spectrum</code> function estimates the spectral density of atime series.
Perform cubic (or Hermite) spline interpolation of given data points,returning either a list of points obtained by the interpolation or a<em>function</em> performing the interpolation.
Perform cubic (or Hermite) spline interpolation of given data points,returning either a list of points obtained by the interpolation or a<em>function</em> performing the interpolation.
Perform cubic (or Hermite) spline interpolation of given data points,returning either a list of points obtained by the interpolation or a<em>function</em> performing the interpolation.
This <code>selfStart</code> model evaluates the asymptotic regressionfunction and its gradient.  It has an <code>initial</code> attribute thatwill evaluate initial estimates of the parameters <code>Asym</code>, <code>R0</code>,and <code>lrc</code> for a given set of data.

Note that <code>SSweibull()</code> generalizes this asymptotic modelwith an extra parameter.
This <code>selfStart</code> model evaluates an alternative parametrizationof the asymptoticregression function and the gradient with respect to those parameters.It has an <code>initial</code>attribute that creates initial estimates of the parameters<code>Asym</code>, <code>lrc</code>, and <code>c0</code>.
This <code>selfStart</code> model evaluates the asymptotic regressionfunction through the origin and its gradient.  It has an<code>initial</code> attribute that will evaluate initial estimates of theparameters <code>Asym</code> and <code>lrc</code> for a given set of data.
This <code>selfStart</code> model evaluates the biexponential model functionand its gradient.  It has an <code>initial</code> attribute thatcreates initial estimates of the parameters <code>A1</code>, <code>lrc1</code>,<code>A2</code>, and <code>lrc2</code>.
Functions to compute matrix of residual sums of squares and products,or the estimated variance matrix for multivariate linear models.
This <code>selfStart</code> model evaluates the first-order compartmentfunction and its gradient.  It has an <code>initial</code> attribute thatcreates initial estimates of the parameters <code>lKe</code>, <code>lKa</code>,and <code>lCl</code>.
This <code>selfStart</code> model evaluates the four-parameter logisticfunction and its gradient.  It has an <code>initial</code> attribute computinginitial estimates of the parameters <code>A</code>, <code>B</code>,<code>xmid</code>, and <code>scal</code> for a given set of data.
This <code>selfStart</code> model evaluates the Gompertz growth modeland its gradient.  It has an <code>initial</code> attribute thatcreates initial estimates of the parameters <code>Asym</code>,<code>b2</code>, and <code>b3</code>.
This <code>selfStart</code> model evaluates the logisticfunction and its gradient.  It has an <code>initial</code> attribute thatcreates initial estimates of the parameters <code>Asym</code>,<code>xmid</code>, and <code>scal</code>.   In <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> 3.4.2 and earlier, thatinit function failed when <code>min(input)</code> was exactly zero.
This <code>selfStart</code> model evaluates the Michaelis-Menten model andits gradient.  It has an <code>initial</code> attribute thatwill evaluate initial estimates of the parameters <code>Vm</code> and <code>K</code>
This <code>selfStart</code> model evaluates the Weibull model for growthcurve data and its gradient.  It has an <code>initial</code> attribute thatwill evaluate initial estimates of the parameters <code>Asym</code>, <code>Drop</code>,<code>lrc</code>, and <code>pwr</code> for a given set of data.
Extract and encode the times the first and last observations weretaken. Provided only for compatibility with S version 2.
This is a utility function, used in <code>lm</code> and<code>glm</code> methods for <code>anova(..., test != NULL)</code>and should not be used by the average user.
Select a formula-based model by AIC.
Given the vectors <i>(x[1], …, x[n])</i> and<i>(y[0], y[1], …, y[n])</i> (one valuemore!), <code>stepfun(x, y, ...)</code> returns an interpolating‘step’ function, say <code>fn</code>. I.e., <i>fn(t) =    c</i><i>[i]</i> (constant) for <i>t in (    x[i], x[i+1])</i> and at the abscissa values, if (by default)<code>right = FALSE</code>, <i>fn(x[i]) = y[i]</i> and for<code>right = TRUE</code>, <i>fn(x[i]) = y[i-1]</i>, for<i>i=1, …, n</i>.

The value of the constant <i>c[i]</i> above depends on the‘continuity’ parameter <code>f</code>.For the default, <code>right = FALSE, f = 0</code>,<code>fn</code> is a <em>cadlag</em> function, i.e., continuous from the right,limits from the left, so that the function is piecewise constant onintervals that include their <em>left</em> endpoint.In general, <i>c[i]</i> is interpolated in between theneighbouring <i>y</i> values,<i>c[i] = (1-f)*y[i] + f*y[i+1]</i>.Therefore, for non-0 values of <code>f</code>, <code>fn</code> may no longer be a properstep function, since it can be discontinuous from both sides, unless<code>right = TRUE, f = 1</code> which is left-continuous (i.e., constantpieces contain their right endpoint).
Decompose a time series into seasonal, trend and irregular componentsusing <code>loess</code>, acronym STL.
Fit a structural model for a time series by maximum likelihood.
Summarize an analysis of variance model.
These functions are all <code>methods</code> for class <code>glm</code> or<code>summary.glm</code> objects.
<code>summary</code> method for class <code>"lm"</code>.
A <code>summary</code> method for class <code>"manova"</code>.
Given the vectors <i>(x[1], …, x[n])</i> and<i>(y[0], y[1], …, y[n])</i> (one valuemore!), <code>stepfun(x, y, ...)</code> returns an interpolating‘step’ function, say <code>fn</code>. I.e., <i>fn(t) =    c</i><i>[i]</i> (constant) for <i>t in (    x[i], x[i+1])</i> and at the abscissa values, if (by default)<code>right = FALSE</code>, <i>fn(x[i]) = y[i]</i> and for<code>right = TRUE</code>, <i>fn(x[i]) = y[i-1]</i>, for<i>i=1, …, n</i>.

The value of the constant <i>c[i]</i> above depends on the‘continuity’ parameter <code>f</code>.For the default, <code>right = FALSE, f = 0</code>,<code>fn</code> is a <em>cadlag</em> function, i.e., continuous from the right,limits from the left, so that the function is piecewise constant onintervals that include their <em>left</em> endpoint.In general, <i>c[i]</i> is interpolated in between theneighbouring <i>y</i> values,<i>c[i] = (1-f)*y[i] + f*y[i+1]</i>.Therefore, for non-0 values of <code>f</code>, <code>fn</code> may no longer be a properstep function, since it can be discontinuous from both sides, unless<code>right = TRUE, f = 1</code> which is left-continuous (i.e., constantpieces contain their right endpoint).
Smooth the (x, y) values by Friedman's ‘super smoother’.
Symbolically encode a given numeric or logical vector or array.Particularly useful for visualization of structured matrices,e.g., correlation, sparse, or logical ones.
Performs one and two sample t-tests on vectors of data.
Plots regression terms against their predictors, optionally withstandard errors and partial residuals added.
The function <code>terms</code> is a generic functionwhich can be used to extract <em>terms</em> objectsfrom various kinds of <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> data objects.
This function takes a formula and some optional arguments andconstructs a terms object. The terms object can then be used toconstruct a <code>model.matrix</code>.
<code>time</code> creates the vector of times at which a time series was sampled.

<code>cycle</code> gives the positions in the cycle of each observation.

<code>frequency</code> returns the number of samples per unit time and<code>deltat</code> the time interval between observations (see<code>ts</code>).
Forms a symmetric Toeplitz matrix given its first row.
The function <code>ts</code> is used to create time-series objects.

<code>as.ts</code> and <code>is.ts</code> coerce an object to a time-series andtest whether an object is a time series.
Bind time series which have a common frequency. <code>ts.union</code> padswith <code>NA</code>s to the total time coverage, <code>ts.intersect</code>restricts to the time covered by all the series.
Plot several time series on a common plot. Unlike<code>plot.ts</code> the series can have a different time bases,but they should have the same frequency.
Bind time series which have a common frequency. <code>ts.union</code> padswith <code>NA</code>s to the total time coverage, <code>ts.intersect</code>restricts to the time covered by all the series.
A generic function to plot time-series diagnostics.
<code>tsp</code> returns the <code>tsp</code> attribute (or <code>NULL</code>).It is included for compatibility with S version 2. <code>tsp&lt;-</code>sets the <code>tsp</code> attribute. <code>hasTsp</code> ensures <code>x</code> has a<code>tsp</code> attribute, by adding one if needed.
<code>tsp</code> returns the <code>tsp</code> attribute (or <code>NULL</code>).It is included for compatibility with S version 2. <code>tsp&lt;-</code>sets the <code>tsp</code> attribute. <code>hasTsp</code> ensures <code>x</code> has a<code>tsp</code> attribute, by adding one if needed.
Performs fixed-interval smoothing on a univariate time series via astate-space model.  Fixed-interval smoothing gives the best estimateof the state at each time point based on the whole observed series.
Create a set of confidence intervals on the differences between themeans of the levels of a factor with the specified family-wiseprobability of coverage.  The intervals are based on the Studentizedrange statistic, Tukey's ‘Honest Significant Difference’method.
The function <code>uniroot</code> searches the interval from <code>lower</code>to <code>upper</code> for a root (i.e., zero) of the function <code>f</code> withrespect to its first argument.

Setting <code>extendInt</code> to a non-<code>"no"</code> string, means searchingfor the correct <code>interval = c(lower,upper)</code> if <code>sign(f(x))</code>does not satisfy the requirements at the interval end points; see the‘Details’ section.
<code>update</code> will update and (by default) re-fit a model.  It does thisby extracting the call stored in the object, updating the call and (bydefault) evaluating that call.  Sometimes it is useful to call<code>update</code> with only one argument, for example if the data frame hasbeen corrected.

“Extracting the call” in <code>update()</code> and similar functionsuses <code>getCall()</code> which itself is a (S3) generic function with adefault method that simply gets <code>x$call</code>.

Because of this, <code>update()</code> will often work (via its defaultmethod) on new model classes, either automatically, or by providing asimple <code>getCall()</code> method for that class.
<code>update</code> will update and (by default) re-fit a model.  It does thisby extracting the call stored in the object, updating the call and (bydefault) evaluating that call.  Sometimes it is useful to call<code>update</code> with only one argument, for example if the data frame hasbeen corrected.

“Extracting the call” in <code>update()</code> and similar functionsuses <code>getCall()</code> which itself is a (S3) generic function with adefault method that simply gets <code>x$call</code>.

Because of this, <code>update()</code> will often work (via its defaultmethod) on new model classes, either automatically, or by providing asimple <code>getCall()</code> method for that class.
<code>update.formula</code> is used to update model formulae.This typically involves adding or dropping terms,but updates can be more general.
<code>var</code>, <code>cov</code> and <code>cor</code> compute the variance of <code>x</code>and the covariance or correlation of <code>x</code> and <code>y</code> if theseare vectors.   If <code>x</code> and <code>y</code> are matrices then thecovariances (or correlations) between the columns of <code>x</code> and thecolumns of <code>y</code> are computed.

<code>cov2cor</code> scales a covariance matrix into the correspondingcorrelation matrix <em>efficiently</em>.
Performs an F test to compare the variances of two samples from normalpopulations.
Simple utilities returning (non-missing) case names, and(non-eliminated) variable names.
These functions ‘rotate’ loading matrices in factor analysis.
Returns the variance-covariance matrix of the main parameters ofa fitted model object.  The “main” parameters of modelcorrespond to those returned by <code>coef</code>, and typically donot contain a nuisance scale parameter (<code>sigma</code>).
Compute a weighted mean.
Computed weighted residuals from a linear model fit.
<code>weights</code> is a generic function which extracts fitting weights fromobjects returned by modeling functions.

Methods can make use of <code>napredict</code> methods to compensatefor the omission of missing values.  The default methods does so.
Performs one- and two-sample Wilcoxon tests on vectors of data; thelatter is also known as ‘Mann-Whitney’ test.
<code>window</code> is a generic function whichextracts the subset of the object <code>x</code>observed between the times <code>start</code> and <code>end</code>. If afrequency is specified, the series is then re-sampled at the newfrequency.
<code>window</code> is a generic function whichextracts the subset of the object <code>x</code>observed between the times <code>start</code> and <code>end</code>. If afrequency is specified, the series is then re-sampled at the newfrequency.
Read, write and coerce ‘flat’ (contingency) tables, aka<code>ftable</code>s.
Create a contingency table (optionally a sparse matrix) fromcross-classifying factors, usually contained in a data frame,using a formula interface.

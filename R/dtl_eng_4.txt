x
Combines the goodness of fit tests used in fa and principal into one function.  If the matrix is singular, will smooth the correlation matrix before finding the fit functions. Now will find the RMSEA (root mean square error of approximation) and the alpha confidence intervals similar to a SEM function.  Also reports the root mean square residual.Chi square is found two ways.  The first (STATISTIC) applies the goodness of fit test from Maximum Likelihood objective function (see below).  This assumes multivariate normality.  The second is the empirical chi square based upon the observed residual correlation matrix and the observed sample size for each correlation.  This is found by summing the squared residual correlations time the sample size.  
Please see the writeup for fa for all of the functionality in these older functions.
A factor/principal components analysis loading matrix is converted to a cluster (-1,0,1) definition matrix where each item is assigned to one and only one cluster.  This is a fast way to extract items that will be unit weighted to form cluster composites.  Use this function in combination with cluster.cor to find the corrleations of these composite scores. A typical use in the SAPA project is to form item composites by clustering or factoring (see ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using mat.regress.The input may be a matrix of item loadings, or the output from a factor analysis which includes a loadings matrix.
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
Nguyen and Waller review the problem of local minima in factor analysis.  This is a problem for all rotation algorithms, but is more so for some.  faRotate generates n.rotations different starting values and then applies the specified rotation to the original loadings using multiple start values.  Hyperplane counts and complexity indices are reported for each starting matrix, and the one with the highest hyoerplane count and the lowest complexity is returned.
NA
NA
The basic formula input given as DV1 + DV2 ~ IV1 + IV2 + (IV3) + I(IV4^2) - IV5 will be parsed to return 2 DVs (1 and 2), two normal IVs (1 and 2), a mediator (IV3) a quadratic (IV4) and a variable to be partialed (IV5). See the various examples in setCor and mediate.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
NA
The reaction of women to women who protest discriminatory treatment was examined in an experiment reported by Garcia et al. (2010). 129 women  were given a description of sex discrimination in the workplace (a male lawyer was promoted over a clearly more qualified female lawyer).  Subjects then read that the target lawyer felt that the decision was unfair.  Subjects were then randomly assigned to three conditions: Control (no protest), Individual Protest (“They are treating me unfairly") , or Collective Protest (“The firm is is treating women unfairly"). Participants were then asked how much they liked the target (liking), how angry they were to the target (anger) and to evaluate the appropriateness of the target's response (respappr).  Garcia et al (2010) report a number of interactions (moderation effects) as well as moderated-mediation effects.This data set is used as an example in Hayes (2013) for moderated mediation.  It is used here to show how to do moderation (interaction terms) in regression (see setCor) , how to do moderated mediation (see mediate) and how draw interaction graphs (see help).
Useful for teaching how to write functions, also useful for showing the different ways of estimating central tendency. 
Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using splitHalf for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's β (1979) using splitHalf. The companion function, omega calculates omega hierarchical (ω_h)  and omega total (ω_t). Guttman's first estimate λ_1 assumes that all the variance of an item is error:lambda 1= 1-tr(Vx)/VxThis is a clear underestimate.The second bound, λ_2, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' , then λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x)Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  Guttman's 3rd lower bound, λ_3, also  modifies λ_1 and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's α. λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx)  = ((n)/(n-1))(Vx-tr(Vx)/Vx  = αThis is just replacing the diagonal elements with the average off diagonal elements.  λ_2 ≥ λ_3 with  λ_2 > λ_3 if the covariances are not identical.λ_3 and λ_2 are both corrections to λ_1 and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)where p_h = sum(σ^2h,  h = 0, 1, 2, ... r-1  andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978).  Clearly μ_0 = λ_3 = α and  μ_1 = λ_2.  μ_r ≥ μ_{r-1} ≥ … μ_1 ≥ μ_0, although the series does not improve much after the first two steps.Guttman's fourth lower bound, λ_4 was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If \vec{X} is split into  two parts, \vec{X}_a and \vec{X}_b, with correlation r_{ab} thenλ 4 = 4rab/(Va + Vb + 2rabVaVb)which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. λ_5, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariancesλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.) Although superior to λ_1, λ_5 underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in λ_3:λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and isλ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The smc is found from all the items.  A modification to Guttman λ_6, λ_6* reported by the score.items function is to find the smc from the entire pool of items given, not just the items on the selected scale.  Guttman's λ_4 is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using splitHalf to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.The algorithms that had been tried before included:a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.The brute force and the sampling procedures seem to provide more stable and larger estimates. Yet another procedure, implemented in splitHalf is actually form all possible (for n items <= 16) or sample 10,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples.Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, λ_4. This considers the test as set of items and examines how best to partition the items into splits. The other two, glb.fa and glb.algebraic, are alternative ways of weighting the diagonal of the matrix. glb.fa estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/VxThis estimate will differ slightly from that found by  glb.algebraic, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. Compared to glb.algebraic, glb.fa seems to have less (positive) bias for smallish sample sizes (n < 500) but larger for large (> 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  glb.algebraic seems to converge on the population value while glb.fa has a positive bias. 
If C is a p * p-covariance matrix, v = diag(C) its diagonal (i. e. the vector of variances v_i = c_{ii}), C0 = C - Diag(v) is the covariance matrix with 0s substituted in the diagonal and x = the vector (x1, . . . , xp) the educational testing problem is (see e. g., Al-Homidan 2008)(Sum i = 1 to p  xi) ->  min s.t. C0 + Diag(x) >= 0(i.e. positive semidefinite) and xi ≤ vi, i = 1 ..., p. This is the same as minimizing the trace of the symmetric matrixC0 + Diag(x)s. t. C0 + Diag(x) is positive semidefinite and xi ≤ vi. The greatest lower bound to reliability is (sum cij (i \ne j) + sum xi )/ sum cijAdditionally, function glb.algebraic allows the user to  change the upper bounds xi ≤ vi toxi ≤ ui and add lower bounds li ≤ xi.The greatest lower bound to reliability is applicable for tests with non-homogeneous items. It gives a sharp lower bound to the reliability of the total test score.Caution: Though glb.algebraic gives exact lower bounds for exact covariance matrices, the estimates from empirical matrices may be strongly biased upwards for small and medium sample sizes.glb.algebraic is wrapper for a call to function csdp of package Rcsdp (see its documentation).If Cov is the covariance matrix of subtests/items with known lower bounds, rel, to their reliabilities (e. g. Cronbachs α), LoBounds can be used to improve the lower bound to reliability by setting LoBounds <- rel*diag(Cov).Changing UpBounds can be used to relax constraints xi ≤ vi or to fix xi-values by setting LoBounds[i] < -z; UpBounds[i] <- z.
Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using splitHalf for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's β (1979) using splitHalf. The companion function, omega calculates omega hierarchical (ω_h)  and omega total (ω_t). Guttman's first estimate λ_1 assumes that all the variance of an item is error:lambda 1= 1-tr(Vx)/VxThis is a clear underestimate.The second bound, λ_2, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' , then λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x)Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  Guttman's 3rd lower bound, λ_3, also  modifies λ_1 and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's α. λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx)  = ((n)/(n-1))(Vx-tr(Vx)/Vx  = αThis is just replacing the diagonal elements with the average off diagonal elements.  λ_2 ≥ λ_3 with  λ_2 > λ_3 if the covariances are not identical.λ_3 and λ_2 are both corrections to λ_1 and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)where p_h = sum(σ^2h,  h = 0, 1, 2, ... r-1  andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978).  Clearly μ_0 = λ_3 = α and  μ_1 = λ_2.  μ_r ≥ μ_{r-1} ≥ … μ_1 ≥ μ_0, although the series does not improve much after the first two steps.Guttman's fourth lower bound, λ_4 was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If \vec{X} is split into  two parts, \vec{X}_a and \vec{X}_b, with correlation r_{ab} thenλ 4 = 4rab/(Va + Vb + 2rabVaVb)which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. λ_5, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariancesλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.) Although superior to λ_1, λ_5 underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in λ_3:λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and isλ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The smc is found from all the items.  A modification to Guttman λ_6, λ_6* reported by the score.items function is to find the smc from the entire pool of items given, not just the items on the selected scale.  Guttman's λ_4 is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using splitHalf to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.The algorithms that had been tried before included:a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.The brute force and the sampling procedures seem to provide more stable and larger estimates. Yet another procedure, implemented in splitHalf is actually form all possible (for n items <= 16) or sample 10,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples.Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, λ_4. This considers the test as set of items and examines how best to partition the items into splits. The other two, glb.fa and glb.algebraic, are alternative ways of weighting the diagonal of the matrix. glb.fa estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/VxThis estimate will differ slightly from that found by  glb.algebraic, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. Compared to glb.algebraic, glb.fa seems to have less (positive) bias for smallish sample sizes (n < 500) but larger for large (> 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  glb.algebraic seems to converge on the population value while glb.fa has a positive bias. 
Generalizability theory is the application of a components of variance approach to the analysis of reliability.  Given a G study (generalizability) the components are estimated and then may be used in a D study (Decision).  Different ratios are formed as appropriate for the particular D study.
Gorsuc (1997) suggested an alternative model for factor extension.  His method is appropriate for the case of repeated variables.  This is handled in link{fa.extension} with correct=FALSE
Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using splitHalf for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's β (1979) using splitHalf. The companion function, omega calculates omega hierarchical (ω_h)  and omega total (ω_t). Guttman's first estimate λ_1 assumes that all the variance of an item is error:lambda 1= 1-tr(Vx)/VxThis is a clear underestimate.The second bound, λ_2, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' , then λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x)Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  Guttman's 3rd lower bound, λ_3, also  modifies λ_1 and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's α. λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx)  = ((n)/(n-1))(Vx-tr(Vx)/Vx  = αThis is just replacing the diagonal elements with the average off diagonal elements.  λ_2 ≥ λ_3 with  λ_2 > λ_3 if the covariances are not identical.λ_3 and λ_2 are both corrections to λ_1 and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)where p_h = sum(σ^2h,  h = 0, 1, 2, ... r-1  andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978).  Clearly μ_0 = λ_3 = α and  μ_1 = λ_2.  μ_r ≥ μ_{r-1} ≥ … μ_1 ≥ μ_0, although the series does not improve much after the first two steps.Guttman's fourth lower bound, λ_4 was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If \vec{X} is split into  two parts, \vec{X}_a and \vec{X}_b, with correlation r_{ab} thenλ 4 = 4rab/(Va + Vb + 2rabVaVb)which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. λ_5, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariancesλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.) Although superior to λ_1, λ_5 underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in λ_3:λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and isλ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The smc is found from all the items.  A modification to Guttman λ_6, λ_6* reported by the score.items function is to find the smc from the entire pool of items given, not just the items on the selected scale.  Guttman's λ_4 is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using splitHalf to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.The algorithms that had been tried before included:a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.The brute force and the sampling procedures seem to provide more stable and larger estimates. Yet another procedure, implemented in splitHalf is actually form all possible (for n items <= 16) or sample 10,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples.Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, λ_4. This considers the test as set of items and examines how best to partition the items into splits. The other two, glb.fa and glb.algebraic, are alternative ways of weighting the diagonal of the matrix. glb.fa estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/VxThis estimate will differ slightly from that found by  glb.algebraic, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. Compared to glb.algebraic, glb.fa seems to have less (positive) bias for smallish sample sizes (n < 500) but larger for large (> 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  glb.algebraic seems to converge on the population value while glb.fa has a positive bias. 
 Harman.Holzinger: 9 x 9 correlation matrix of ability tests, N = 696. Harman.Burt: a 8 x 8 correlation matrix of “emotional" items. N = 172 Harman.5:  12 census tracts for 5 socioeconomic data (Harman p 14) Harman.political:  p 166. Harman.8   8 physical measuresHarman.Holzinger. The nine psychological variables from Harman (1967, p 244) are taken from unpublished class notes of K.J. Holzinger with 696 participants.  This is a subset of 12 tests with 4 factors. It is yet another nice example of a bifactor solution.  Bentler (2007) uses this data set to discuss reliablity analysis. The data show a clear bifactor structure and are a nice example of the various estimates of reliability included in the omega function. Should not be confused with the Holzinger or  Holzinger.9 data sets in bifactor.See also the Holzinger-Swineford data set of 301 subjects with 26 variables in holzinger.swineford. These data were provided by Keith Widaman. "The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the "24 Psychological Variables" example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.Rosseel, who developed the "lavaan" package for R , included 9 of the manifest variables from Holzinger and Swineford (1939) as a "resident" data set when one downloads the lavaan package. Several background variables are included in this "resident" data set in addition to 9 of the psychological tests (which are named x1 - x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the "resident" data set in lavaan, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 - with obvious attendant differences in sample means and SDs."Harman.Burt. Eight “emotional" variables are taken from Harman (1967, p 164) who in turn adapted them from Burt (1939).  They are said be from 172 normal children aged nine to twelve.  As pointed out by Harman, this correlation matrix is singular and has squared multiple correlations > 1.  Because of this problem, it is a nice test case for various factoring algorithms.  (For instance, omega will issue warning messages for fm="minres" or fm="pa" but will fail for fm="ml".)The Eight Physical Variables problem is taken from Harman (1976) and represents the correlations between eight physical variables for 305 girls.  The two correlated clusters represent four measures of "lankiness" and then four measures of "stockiness".  The original data were selected from 17 variables reported in an unpublished dissertation by Mullen (1939). Variable 6 ("Bitrochanteric diamter") is the distance between the outer points of the hips.  The row names match the original Harman paper, the column names have been abbreviated.See also the usaf data set for other physical measurements.The fa solution for principal axes (fm="pa") matches the reported minres solution, as does the fm="minres". For those interested in teaching examples using various body measurements, see the body data set in the gclus package.  The Burt data set probably has a typo in the original correlation matrix.  Changing the Sorrow- Tenderness correlation from .87 to .81 makes the correlation positive definite. As pointed out by Jan DeLeeuw, the Burt data set is a subset of 8 variables from the original 11 reported by Burt in 1915.  That matrix has the same problem. See burt.Other example data sets that are useful demonstrations of factor analysis are the seven bifactor examples  in Bechtoldt and the 24 ability measures in Harman74.corThere are several other Harman  examples in the psych package (i.e., Harman.8) as well as in the dataseta and  GPArotation packages.  The Harman 24 mental tests problem is in the basic datasets package at Harman74.cor. Other Harman data sets are 5 socioeconomic variables for 12 census tracts Harman.5 used by John Loehlin as an example for EFA.  Another one of the many Harman (1967) data sets is Harman.political.  This contains 8 political variables taken over 147 election areas.  The principal factor method with SMCs as communalities match those of table 8.18.  The data are used by Dziubian and Shirkey as an example of the Kaiser-Meyer-Olkin test of factor adequacy.
 Harman.Holzinger: 9 x 9 correlation matrix of ability tests, N = 696. Harman.Burt: a 8 x 8 correlation matrix of “emotional" items. N = 172 Harman.5:  12 census tracts for 5 socioeconomic data (Harman p 14) Harman.political:  p 166. Harman.8   8 physical measuresHarman.Holzinger. The nine psychological variables from Harman (1967, p 244) are taken from unpublished class notes of K.J. Holzinger with 696 participants.  This is a subset of 12 tests with 4 factors. It is yet another nice example of a bifactor solution.  Bentler (2007) uses this data set to discuss reliablity analysis. The data show a clear bifactor structure and are a nice example of the various estimates of reliability included in the omega function. Should not be confused with the Holzinger or  Holzinger.9 data sets in bifactor.See also the Holzinger-Swineford data set of 301 subjects with 26 variables in holzinger.swineford. These data were provided by Keith Widaman. "The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the "24 Psychological Variables" example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.Rosseel, who developed the "lavaan" package for R , included 9 of the manifest variables from Holzinger and Swineford (1939) as a "resident" data set when one downloads the lavaan package. Several background variables are included in this "resident" data set in addition to 9 of the psychological tests (which are named x1 - x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the "resident" data set in lavaan, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 - with obvious attendant differences in sample means and SDs."Harman.Burt. Eight “emotional" variables are taken from Harman (1967, p 164) who in turn adapted them from Burt (1939).  They are said be from 172 normal children aged nine to twelve.  As pointed out by Harman, this correlation matrix is singular and has squared multiple correlations > 1.  Because of this problem, it is a nice test case for various factoring algorithms.  (For instance, omega will issue warning messages for fm="minres" or fm="pa" but will fail for fm="ml".)The Eight Physical Variables problem is taken from Harman (1976) and represents the correlations between eight physical variables for 305 girls.  The two correlated clusters represent four measures of "lankiness" and then four measures of "stockiness".  The original data were selected from 17 variables reported in an unpublished dissertation by Mullen (1939). Variable 6 ("Bitrochanteric diamter") is the distance between the outer points of the hips.  The row names match the original Harman paper, the column names have been abbreviated.See also the usaf data set for other physical measurements.The fa solution for principal axes (fm="pa") matches the reported minres solution, as does the fm="minres". For those interested in teaching examples using various body measurements, see the body data set in the gclus package.  The Burt data set probably has a typo in the original correlation matrix.  Changing the Sorrow- Tenderness correlation from .87 to .81 makes the correlation positive definite. As pointed out by Jan DeLeeuw, the Burt data set is a subset of 8 variables from the original 11 reported by Burt in 1915.  That matrix has the same problem. See burt.Other example data sets that are useful demonstrations of factor analysis are the seven bifactor examples  in Bechtoldt and the 24 ability measures in Harman74.corThere are several other Harman  examples in the psych package (i.e., Harman.8) as well as in the dataseta and  GPArotation packages.  The Harman 24 mental tests problem is in the basic datasets package at Harman74.cor. Other Harman data sets are 5 socioeconomic variables for 12 census tracts Harman.5 used by John Loehlin as an example for EFA.  Another one of the many Harman (1967) data sets is Harman.political.  This contains 8 political variables taken over 147 election areas.  The principal factor method with SMCs as communalities match those of table 8.18.  The data are used by Dziubian and Shirkey as an example of the Kaiser-Meyer-Olkin test of factor adequacy.
 Harman.Holzinger: 9 x 9 correlation matrix of ability tests, N = 696. Harman.Burt: a 8 x 8 correlation matrix of “emotional" items. N = 172 Harman.5:  12 census tracts for 5 socioeconomic data (Harman p 14) Harman.political:  p 166. Harman.8   8 physical measuresHarman.Holzinger. The nine psychological variables from Harman (1967, p 244) are taken from unpublished class notes of K.J. Holzinger with 696 participants.  This is a subset of 12 tests with 4 factors. It is yet another nice example of a bifactor solution.  Bentler (2007) uses this data set to discuss reliablity analysis. The data show a clear bifactor structure and are a nice example of the various estimates of reliability included in the omega function. Should not be confused with the Holzinger or  Holzinger.9 data sets in bifactor.See also the Holzinger-Swineford data set of 301 subjects with 26 variables in holzinger.swineford. These data were provided by Keith Widaman. "The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the "24 Psychological Variables" example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.Rosseel, who developed the "lavaan" package for R , included 9 of the manifest variables from Holzinger and Swineford (1939) as a "resident" data set when one downloads the lavaan package. Several background variables are included in this "resident" data set in addition to 9 of the psychological tests (which are named x1 - x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the "resident" data set in lavaan, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 - with obvious attendant differences in sample means and SDs."Harman.Burt. Eight “emotional" variables are taken from Harman (1967, p 164) who in turn adapted them from Burt (1939).  They are said be from 172 normal children aged nine to twelve.  As pointed out by Harman, this correlation matrix is singular and has squared multiple correlations > 1.  Because of this problem, it is a nice test case for various factoring algorithms.  (For instance, omega will issue warning messages for fm="minres" or fm="pa" but will fail for fm="ml".)The Eight Physical Variables problem is taken from Harman (1976) and represents the correlations between eight physical variables for 305 girls.  The two correlated clusters represent four measures of "lankiness" and then four measures of "stockiness".  The original data were selected from 17 variables reported in an unpublished dissertation by Mullen (1939). Variable 6 ("Bitrochanteric diamter") is the distance between the outer points of the hips.  The row names match the original Harman paper, the column names have been abbreviated.See also the usaf data set for other physical measurements.The fa solution for principal axes (fm="pa") matches the reported minres solution, as does the fm="minres". For those interested in teaching examples using various body measurements, see the body data set in the gclus package.  The Burt data set probably has a typo in the original correlation matrix.  Changing the Sorrow- Tenderness correlation from .87 to .81 makes the correlation positive definite. As pointed out by Jan DeLeeuw, the Burt data set is a subset of 8 variables from the original 11 reported by Burt in 1915.  That matrix has the same problem. See burt.Other example data sets that are useful demonstrations of factor analysis are the seven bifactor examples  in Bechtoldt and the 24 ability measures in Harman74.corThere are several other Harman  examples in the psych package (i.e., Harman.8) as well as in the dataseta and  GPArotation packages.  The Harman 24 mental tests problem is in the basic datasets package at Harman74.cor. Other Harman data sets are 5 socioeconomic variables for 12 census tracts Harman.5 used by John Loehlin as an example for EFA.  Another one of the many Harman (1967) data sets is Harman.political.  This contains 8 political variables taken over 147 election areas.  The principal factor method with SMCs as communalities match those of table 8.18.  The data are used by Dziubian and Shirkey as an example of the Kaiser-Meyer-Olkin test of factor adequacy.
 Harman.Holzinger: 9 x 9 correlation matrix of ability tests, N = 696. Harman.Burt: a 8 x 8 correlation matrix of “emotional" items. N = 172 Harman.5:  12 census tracts for 5 socioeconomic data (Harman p 14) Harman.political:  p 166. Harman.8   8 physical measuresHarman.Holzinger. The nine psychological variables from Harman (1967, p 244) are taken from unpublished class notes of K.J. Holzinger with 696 participants.  This is a subset of 12 tests with 4 factors. It is yet another nice example of a bifactor solution.  Bentler (2007) uses this data set to discuss reliablity analysis. The data show a clear bifactor structure and are a nice example of the various estimates of reliability included in the omega function. Should not be confused with the Holzinger or  Holzinger.9 data sets in bifactor.See also the Holzinger-Swineford data set of 301 subjects with 26 variables in holzinger.swineford. These data were provided by Keith Widaman. "The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the "24 Psychological Variables" example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.Rosseel, who developed the "lavaan" package for R , included 9 of the manifest variables from Holzinger and Swineford (1939) as a "resident" data set when one downloads the lavaan package. Several background variables are included in this "resident" data set in addition to 9 of the psychological tests (which are named x1 - x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the "resident" data set in lavaan, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 - with obvious attendant differences in sample means and SDs."Harman.Burt. Eight “emotional" variables are taken from Harman (1967, p 164) who in turn adapted them from Burt (1939).  They are said be from 172 normal children aged nine to twelve.  As pointed out by Harman, this correlation matrix is singular and has squared multiple correlations > 1.  Because of this problem, it is a nice test case for various factoring algorithms.  (For instance, omega will issue warning messages for fm="minres" or fm="pa" but will fail for fm="ml".)The Eight Physical Variables problem is taken from Harman (1976) and represents the correlations between eight physical variables for 305 girls.  The two correlated clusters represent four measures of "lankiness" and then four measures of "stockiness".  The original data were selected from 17 variables reported in an unpublished dissertation by Mullen (1939). Variable 6 ("Bitrochanteric diamter") is the distance between the outer points of the hips.  The row names match the original Harman paper, the column names have been abbreviated.See also the usaf data set for other physical measurements.The fa solution for principal axes (fm="pa") matches the reported minres solution, as does the fm="minres". For those interested in teaching examples using various body measurements, see the body data set in the gclus package.  The Burt data set probably has a typo in the original correlation matrix.  Changing the Sorrow- Tenderness correlation from .87 to .81 makes the correlation positive definite. As pointed out by Jan DeLeeuw, the Burt data set is a subset of 8 variables from the original 11 reported by Burt in 1915.  That matrix has the same problem. See burt.Other example data sets that are useful demonstrations of factor analysis are the seven bifactor examples  in Bechtoldt and the 24 ability measures in Harman74.corThere are several other Harman  examples in the psych package (i.e., Harman.8) as well as in the dataseta and  GPArotation packages.  The Harman 24 mental tests problem is in the basic datasets package at Harman74.cor. Other Harman data sets are 5 socioeconomic variables for 12 census tracts Harman.5 used by John Loehlin as an example for EFA.  Another one of the many Harman (1967) data sets is Harman.political.  This contains 8 political variables taken over 147 election areas.  The principal factor method with SMCs as communalities match those of table 8.18.  The data are used by Dziubian and Shirkey as an example of the Kaiser-Meyer-Olkin test of factor adequacy.
 Harman.Holzinger: 9 x 9 correlation matrix of ability tests, N = 696. Harman.Burt: a 8 x 8 correlation matrix of “emotional" items. N = 172 Harman.5:  12 census tracts for 5 socioeconomic data (Harman p 14) Harman.political:  p 166. Harman.8   8 physical measuresHarman.Holzinger. The nine psychological variables from Harman (1967, p 244) are taken from unpublished class notes of K.J. Holzinger with 696 participants.  This is a subset of 12 tests with 4 factors. It is yet another nice example of a bifactor solution.  Bentler (2007) uses this data set to discuss reliablity analysis. The data show a clear bifactor structure and are a nice example of the various estimates of reliability included in the omega function. Should not be confused with the Holzinger or  Holzinger.9 data sets in bifactor.See also the Holzinger-Swineford data set of 301 subjects with 26 variables in holzinger.swineford. These data were provided by Keith Widaman. "The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the "24 Psychological Variables" example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.Rosseel, who developed the "lavaan" package for R , included 9 of the manifest variables from Holzinger and Swineford (1939) as a "resident" data set when one downloads the lavaan package. Several background variables are included in this "resident" data set in addition to 9 of the psychological tests (which are named x1 - x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the "resident" data set in lavaan, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 - with obvious attendant differences in sample means and SDs."Harman.Burt. Eight “emotional" variables are taken from Harman (1967, p 164) who in turn adapted them from Burt (1939).  They are said be from 172 normal children aged nine to twelve.  As pointed out by Harman, this correlation matrix is singular and has squared multiple correlations > 1.  Because of this problem, it is a nice test case for various factoring algorithms.  (For instance, omega will issue warning messages for fm="minres" or fm="pa" but will fail for fm="ml".)The Eight Physical Variables problem is taken from Harman (1976) and represents the correlations between eight physical variables for 305 girls.  The two correlated clusters represent four measures of "lankiness" and then four measures of "stockiness".  The original data were selected from 17 variables reported in an unpublished dissertation by Mullen (1939). Variable 6 ("Bitrochanteric diamter") is the distance between the outer points of the hips.  The row names match the original Harman paper, the column names have been abbreviated.See also the usaf data set for other physical measurements.The fa solution for principal axes (fm="pa") matches the reported minres solution, as does the fm="minres". For those interested in teaching examples using various body measurements, see the body data set in the gclus package.  The Burt data set probably has a typo in the original correlation matrix.  Changing the Sorrow- Tenderness correlation from .87 to .81 makes the correlation positive definite. As pointed out by Jan DeLeeuw, the Burt data set is a subset of 8 variables from the original 11 reported by Burt in 1915.  That matrix has the same problem. See burt.Other example data sets that are useful demonstrations of factor analysis are the seven bifactor examples  in Bechtoldt and the 24 ability measures in Harman74.corThere are several other Harman  examples in the psych package (i.e., Harman.8) as well as in the dataseta and  GPArotation packages.  The Harman 24 mental tests problem is in the basic datasets package at Harman74.cor. Other Harman data sets are 5 socioeconomic variables for 12 census tracts Harman.5 used by John Loehlin as an example for EFA.  Another one of the many Harman (1967) data sets is Harman.political.  This contains 8 political variables taken over 147 election areas.  The principal factor method with SMCs as communalities match those of table 8.18.  The data are used by Dziubian and Shirkey as an example of the Kaiser-Meyer-Olkin test of factor adequacy.
Included as an example for teaching about functions. As well as for a discussion of how to estimate central tendencies.Also used in statsBy to weight by the harmonic mean.Values of 0 can be included (in which case the harmonic.mean = 0) or converted to NA according to the zero option.  Added the zero option, March, 2017.
NA
NA
Path diagram representations have become standard in confirmatory factor analysis, but are not yet common in exploratory factor analysis.  Representing factor structures graphically helps some people understand the structure. By default the arrows come from the latent variables to the observed variables.  This is, of course, the factor model.  However, if the class of the object to be drawn is 'principal', then  reverse the direction of the arrows, and the 'latent' variables are no longer latent, and are shown as boxes. For cluster models, the default is to treat them as factors, but if ic =TRUE, then we treat it as a components model.fa.diagram does not use Rgraphviz and is the preferred function.  fa.graph generates dot code to be used by an external graphics program. It does not have all the bells and whistles of fa.diagram, but these may be done in the external editor. Hierarchical (bifactor) models may be drawn by specifying the g parameter as TRUE.  This allows for an graphical displays of various factor transformations with a bifactor structure (e.g., bifactor and biquartimin.  See omega for an alternative way to find these structures.  The het.diagram function will show the case of a hetarchical structure at multiple levels.  It can also be used to show the patterns of correlations between sets of scales (e.g., EPI, NEO, BFI).  The example is for showing the relationship between 3 sets of 4 variables from the Thurstone data set. The parameters l.cex and gap.size are used to adjust the font size of the labels and the gap in the lines.  extension.diagram will draw a fa.extend result with slightly more control than using fa.diagram or the more generic diagram function.In fa.rgraph although a nice graph is drawn for the orthogonal factor case, the oblique factor drawing is acceptable, but is better if cleaned up outside of R or done using fa.diagram. The normal input is taken from the output of either fa or ICLUST. This latter case displays the ICLUST results in terms of the cluster loadings, not in terms of the cluster structure.  Actually an interesting option.It is also possible to just give a factor loading matrix as input.  In this case, supplying a Phi matrix of factor correlations is also possible. It is possible, using fa.graph, to export dot code for an omega solution.  fa.graph should be applied to the schmid$sl object with labels specified as the rownames of schmid$sl.  The results will need editing to make fully compatible with dot language plotting.  To specify the model for a structural equation confirmatory analysis of the results, use structure.diagram instead. 
This allows for quick summaries of multiple distributions.  Particularly useful when examining the results of multiple-split halves that come from the reliability function.  By default, will try to make a square plot with equal number of rows and columns.  However, the number of columns and rows may be specified for a particular plot.
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
Shrout and Fleiss (1979) consider six cases of reliability of ratings done by k raters on n targets. McGraw and Wong (1996) consider 10, 6 of which are identical to Shrout and Fleiss and  4 are conceptually different but use the same equations as the 6 in Shrout and Fleiss. The intraclass correlation is used if raters are all of the same “class".  That is, there is no logical way of distinguishing them.  Examples include correlations between  pairs of twins, correlations between raters.  If the variables are logically distinguishable (e.g., different items on a test), then the more typical coefficient is based upon the inter-class correlation (e.g., a Pearson r) and a statistic such as alpha or omega might be used.  alpha and ICC3k are identical.Where the data are laid out in terms of Rows (subjects) and Columns (rater or tests), the various ICCs are found by the ratio of various estimates of variance components.  In all cases, subjects are taken as varying at random, and the residual variance is also random.  The distinction between models 2 and 3 is whether the judges (items/tests) are seen as random or fixed.  A further distinction is whether the emphasis is upon absolute agreement of the judges, or merely consistency.  As discussed by Liljequist et al. (2019), McGraw and Wong lay out 5 models which use just three forms of the ICC equations.Model 1  is a one way model with ICC1: Each  target is rated by a different  judge and the judges are selected at random.  ICC(1,1) = ρ_{1,1} = \frac{σ^2_r}{σ^2_r + σ^2_w} (This is a one-way ANOVA fixed effects model and is found by  (MSB- MSW)/(MSB+ (nr-1)*MSW))   ICC2: A random sample of k judges rate each target.  The measure is one of absolute agreement in the ratings.  ICC(2,1) = ρ_{2,1} = \frac{σ^2_r}{σ^2_r + σ^2_c +σ^2_{rc}  + σ^2_e} Found as (MSB- MSE)/(MSB + (nr-1)*MSE + nr*(MSJ-MSE)/nc) ICC3: A fixed set of k judges rate each target. There is no generalization to a larger population of judges. ICC(3,1) = ρ_{3,1} = \frac{σ^2_r}{σ^2_r + σ^2_c + σ^2_e} (MSB - MSE)/(MSB+ (nr-1)*MSE)Then, for each of these cases, is reliability to be estimated for a single rating or for the average of k ratings?  (The 1 rating case is equivalent to the average intercorrelation, the k rating case to the Spearman Brown adjusted reliability.)ICC1 is sensitive to differences in means between raters and is a measure of absolute agreement.ICC2 and ICC3 remove mean differences between judges, but are sensitive to interactions of raters by judges.  The difference between ICC2 and ICC3 is whether raters are seen as fixed or random effects.ICC1k, ICC2k, ICC3K reflect the means of k raters.   If using the lmer option, then missing data are allowed.  In addition the lme object returns the variance decomposition.  (This is simliar to  testRetest which works on the items from two occasions.The check.keys option by default reverses items that are negatively correlated with total score.  A message is issued.
 Extensive documentation and justification of the algorithm is available in the original MBR 1979 https://personality-project.org/revelle/publications/iclust.pdf paper.  Further discussion of the algorithm and sample output is available on the personality-project.org web page: https://personality-project.org/r/r.ICLUST.html The results are best visualized using  ICLUST.graph, the results of which can be saved as a dot file for the Graphviz program.  https://www.graphviz.org/. The iclust.diagram is called automatically to produce cluster diagrams.  The resulting diagram is not quite as pretty as what can be achieved in dot code but is quite adequate if you don't want to use an external graphics program. With the installation of Rgraphviz, ICLUST can also provide cluster graphs.A common problem in the social sciences is to construct scales or composites of items to measure constructs of theoretical interest and practical importance. This process frequently involves administering a battery of items from which those that meet certain criteria are selected. These criteria might be rational, empirical,or factorial. A similar problem is to analyze the adequacy of scales that already have been formed and to decide whether the putative constructs are measured properly. Both of these problems have been discussed in numerous texts,  as well as in myriad articles. Proponents of various methods have argued for the importance of face validity, discriminant validity, construct validity, factorial homogeneity, and theoretical importance. Revelle (1979) proposed that hierachical cluster analysis could be used to estimate a new coefficient (beta) that was an estimate of the  general factor saturation of a test.  More recently, Zinbarg, Revelle, Yovel and Li (2005) compared McDonald's Omega to Chronbach's alpha and Revelle's beta. They conclude that   omega hierarchical is the best estimate.  An algorithm for estimating omega  is available as part of this package. Revelle and Zinbarg (2009) discuss alpha, beta, and omega, as well as other estimates of reliability. The original ICLUST program was written in FORTRAN to run on CDC and IBM mainframes and was then modified to run in PC-DOS.  The R version of iclust is a completely new version  written for the psych package.  Please email me if you want help with this version of iclust or if you desire more features.A requested feature (not yet available) is to specify certain items as forming a cluster.  That is, to do confirmatory cluster analysis.  The program currently has three primary functions: cluster, loadings, and graphics.  In June, 2009, the option of weighted versus unweighted beta was introduced.  Unweighted beta calculates beta based upon the correlation between  two clusters, corrected for test length using the Spearman-Brown prophecy formala, while weighted beta finds the average interitem correlation between the items  within two clusters and then finds beta from this.  That is, for two clusters A and B of size N and M with between average correlation rb, weighted beta is (N+M)^2 rb/(Va +Vb + 2Cab).  Raw (unweighted) beta is 2rab/(1+rab) where rab = Cab/sqrt(VaVb).   Weighted beta seems a more appropriate estimate and is now the default.  Unweighted beta is still available for consistency with prior versions.Also modified in June, 2009 was the way of correcting for item overlap when calculating the cluster-subcluster correlations for the graphic output.  This does not affect the final cluster solution, but does produce slightly different path values.  In addition, there are two ways to solve for the cluster - subcluster correlation.Given the covariance between two clusters, Cab with average rab = Cab/(N*M), and cluster variances Va and Vb with Va = N  + N*(N-1)*ra  then the  correlation of cluster A with the combined cluster AB  is eithera) ((N^2)ra + Cab)/sqrt(Vab*Va)   (option cor.gen=TRUE) orb) (Va - N + Nra + Cab)/sqrt(Vab*Va)    (option cor.gen=FALSE)The default is to use cor.gen=TRUE.Although iclust will give what it thinks is the best solution in terms of the number of clusters to extract, the user will sometimes disagree.  To get more clusters than the default solution, just set the nclusters parameter to the number desired.  However, to get fewer than meet the alpha and beta criteria, it is sometimes necessary to set alpha=0 and beta=0 and then set the nclusters to the desired number.   Clustering 24 tests of mental abilityA sample output using the 24 variable problem by Harman can be represented both graphically and in terms of the cluster order. The default is to produce graphics using the diagram functions.  An alternative is to use the Rgraphviz package (from BioConductor).  Because this package is sometimes hard to install, there is an alternative option (ICLUST.graph to write  dot language instructions for subsequent processing.  This will create a  graphic instructions suitable for any viewing program that uses the dot language.  ICLUST.rgraph produces the dot code for Graphviz.  Somewhat lower resolution graphs with fewer options are available in the ICLUST.rgraph function which requires Rgraphviz.  Dot code can be viewed directly in Graphviz or can be tweaked using commercial software packages (e.g., OmniGraffle)Note that for the Harman 24 variable problem, with the default parameters, the data form one large cluster. (This is consistent with the Very Simple Structure (VSS) output as well, which shows a clear one factor solution for complexity 1 data.)  An alternative solution is to ask for a somewhat more stringent set of criteria and require an increase in the size of beta for all clusters greater than 3 variables.  This produces a 4 cluster solution.It is also possible to use the original parameter settings, but ask for a 4 cluster solution.At least for the Harman 24 mental ability measures, it is interesting to compare the cluster pattern matrix with the oblique rotation solution from a factor analysis.  The factor congruence of a four factor oblique pattern solution with the four cluster solution is > .99 for three of the four clusters and > .97 for the fourth cluster.  The cluster pattern matrix (returned as an invisible object in the output)In September, 2012, the fit statistics (pattern fit and cluster fit) were slightly modified to (by default) not consider the diagonal (diagonal=FALSE).  Until then, the diagonal was included in the cluster fit statistics.  The pattern fit is analogous to factor analysis and is based upon the model = P x Structure where Structure is Pattern * Phi.  Then R* = R - model and fit is the ratio of sum(r*^2)/sum(r^2) for the off diagonal elements.  
 Extensive documentation and justification of the algorithm is available in the original MBR 1979 https://personality-project.org/revelle/publications/iclust.pdf paper.  Further discussion of the algorithm and sample output is available on the personality-project.org web page: https://personality-project.org/r/r.ICLUST.html The results are best visualized using  ICLUST.graph, the results of which can be saved as a dot file for the Graphviz program.  https://www.graphviz.org/. The iclust.diagram is called automatically to produce cluster diagrams.  The resulting diagram is not quite as pretty as what can be achieved in dot code but is quite adequate if you don't want to use an external graphics program. With the installation of Rgraphviz, ICLUST can also provide cluster graphs.A common problem in the social sciences is to construct scales or composites of items to measure constructs of theoretical interest and practical importance. This process frequently involves administering a battery of items from which those that meet certain criteria are selected. These criteria might be rational, empirical,or factorial. A similar problem is to analyze the adequacy of scales that already have been formed and to decide whether the putative constructs are measured properly. Both of these problems have been discussed in numerous texts,  as well as in myriad articles. Proponents of various methods have argued for the importance of face validity, discriminant validity, construct validity, factorial homogeneity, and theoretical importance. Revelle (1979) proposed that hierachical cluster analysis could be used to estimate a new coefficient (beta) that was an estimate of the  general factor saturation of a test.  More recently, Zinbarg, Revelle, Yovel and Li (2005) compared McDonald's Omega to Chronbach's alpha and Revelle's beta. They conclude that   omega hierarchical is the best estimate.  An algorithm for estimating omega  is available as part of this package. Revelle and Zinbarg (2009) discuss alpha, beta, and omega, as well as other estimates of reliability. The original ICLUST program was written in FORTRAN to run on CDC and IBM mainframes and was then modified to run in PC-DOS.  The R version of iclust is a completely new version  written for the psych package.  Please email me if you want help with this version of iclust or if you desire more features.A requested feature (not yet available) is to specify certain items as forming a cluster.  That is, to do confirmatory cluster analysis.  The program currently has three primary functions: cluster, loadings, and graphics.  In June, 2009, the option of weighted versus unweighted beta was introduced.  Unweighted beta calculates beta based upon the correlation between  two clusters, corrected for test length using the Spearman-Brown prophecy formala, while weighted beta finds the average interitem correlation between the items  within two clusters and then finds beta from this.  That is, for two clusters A and B of size N and M with between average correlation rb, weighted beta is (N+M)^2 rb/(Va +Vb + 2Cab).  Raw (unweighted) beta is 2rab/(1+rab) where rab = Cab/sqrt(VaVb).   Weighted beta seems a more appropriate estimate and is now the default.  Unweighted beta is still available for consistency with prior versions.Also modified in June, 2009 was the way of correcting for item overlap when calculating the cluster-subcluster correlations for the graphic output.  This does not affect the final cluster solution, but does produce slightly different path values.  In addition, there are two ways to solve for the cluster - subcluster correlation.Given the covariance between two clusters, Cab with average rab = Cab/(N*M), and cluster variances Va and Vb with Va = N  + N*(N-1)*ra  then the  correlation of cluster A with the combined cluster AB  is eithera) ((N^2)ra + Cab)/sqrt(Vab*Va)   (option cor.gen=TRUE) orb) (Va - N + Nra + Cab)/sqrt(Vab*Va)    (option cor.gen=FALSE)The default is to use cor.gen=TRUE.Although iclust will give what it thinks is the best solution in terms of the number of clusters to extract, the user will sometimes disagree.  To get more clusters than the default solution, just set the nclusters parameter to the number desired.  However, to get fewer than meet the alpha and beta criteria, it is sometimes necessary to set alpha=0 and beta=0 and then set the nclusters to the desired number.   Clustering 24 tests of mental abilityA sample output using the 24 variable problem by Harman can be represented both graphically and in terms of the cluster order. The default is to produce graphics using the diagram functions.  An alternative is to use the Rgraphviz package (from BioConductor).  Because this package is sometimes hard to install, there is an alternative option (ICLUST.graph to write  dot language instructions for subsequent processing.  This will create a  graphic instructions suitable for any viewing program that uses the dot language.  ICLUST.rgraph produces the dot code for Graphviz.  Somewhat lower resolution graphs with fewer options are available in the ICLUST.rgraph function which requires Rgraphviz.  Dot code can be viewed directly in Graphviz or can be tweaked using commercial software packages (e.g., OmniGraffle)Note that for the Harman 24 variable problem, with the default parameters, the data form one large cluster. (This is consistent with the Very Simple Structure (VSS) output as well, which shows a clear one factor solution for complexity 1 data.)  An alternative solution is to ask for a somewhat more stringent set of criteria and require an increase in the size of beta for all clusters greater than 3 variables.  This produces a 4 cluster solution.It is also possible to use the original parameter settings, but ask for a 4 cluster solution.At least for the Harman 24 mental ability measures, it is interesting to compare the cluster pattern matrix with the oblique rotation solution from a factor analysis.  The factor congruence of a four factor oblique pattern solution with the four cluster solution is > .99 for three of the four clusters and > .97 for the fourth cluster.  The cluster pattern matrix (returned as an invisible object in the output)In September, 2012, the fit statistics (pattern fit and cluster fit) were slightly modified to (by default) not consider the diagonal (diagonal=FALSE).  Until then, the diagonal was included in the cluster fit statistics.  The pattern fit is analogous to factor analysis and is based upon the model = P x Structure where Structure is Pattern * Phi.  Then R* = R - model and fit is the ratio of sum(r*^2)/sum(r^2) for the off diagonal elements.  
See ICLUST
iclust.diagram provides most of the power of ICLUST.rgraph without the difficulties involved in installing Rgraphviz.  It is called automatically from ICLUST. Following a request by Michael Kubovy, cluster.names may be specified to replace the normal C1 ... Cn names.  If access to a dot language graphics program is available, it is probably better to use the iclust.graph function to get dot output for offline editing.
Will create (or overwrite) an output file and print out the dot code to show a cluster structure. This dot file may be imported directly into a dot viewer (e.g.,  https://www.graphviz.org/).  The "dot" language is a powerful graphic description language that is particulary appropriate for viewing cluster output.  Commercial graphics programs (e.g., OmniGraffle) can also read (and clean up) dot files.  ICLUST.graph takes the output from ICLUST results and processes it to provide a pretty picture of the results.  Original variables shown as rectangles and ordered on the left hand side (if rank direction is RL) of the graph.  Clusters are drawn as ellipses and include the alpha, beta, and size of the cluster.  Edges show the cluster intercorrelations.It is possible to trim the output to not show all cluster information. Clusters < min.size are shown as small ovals without alpha, beta, and size information.Although it would be nice to process the dot code directly in R, the Rgraphviz package is difficult to use on all platforms and thus the dot code is written directly.
Will create (or overwrite) an output file and print out the dot code to show a cluster structure. This dot file may be imported directly into a dot viewer (e.g.,  https://www.graphviz.org/).  The "dot" language is a powerful graphic description language that is particulary appropriate for viewing cluster output.  Commercial graphics programs (e.g., OmniGraffle) can also read (and clean up) dot files.  ICLUST.rgraph takes the output from ICLUST results and processes it to provide a pretty picture of the results.  Original variables shown as rectangles and ordered on the left hand side (if rank direction is RL) of the graph.  Clusters are drawn as ellipses and include the alpha, beta, and size of the cluster.  Edges show the cluster intercorrelations.It is possible to trim the output to not show all cluster information. Clusters < min.size are shown as small ovals without alpha, beta, and size information.
When interpreting cluster or factor analysis outputs, is is useful to group the items in terms of which items have their biggest loading on each factor/cluster and then to sort the items by size of the absolute factor loading.A stable cluster solution will be one in which the output of these cluster definitions does not vary when clusters are formed from the clusters so defined.With the keys=TRUE option, the resulting cluster keys may be used to score the original data or the correlation matrix to form clusters from the factors.
When interpreting cluster or factor analysis outputs, is is useful to group the items in terms of which items have their biggest loading on each factor/cluster and then to sort the items by size of the absolute factor loading.A stable cluster solution will be one in which the output of these cluster definitions does not vary when clusters are formed from the clusters so defined.With the keys=TRUE option, the resulting cluster keys may be used to score the original data or the correlation matrix to form clusters from the factors.
Factor analysis as implemented in fa attempts to  summarize the covariance (correlational) structure of a set of variables with a small set of latent variables or “factors".  This solution may be ‘extended’ into a larger space with more variables without changing the original solution (see fa.extension.  Similarly,  the factors of a second set of variables  (the Y set) may be extended into the original (X ) set.  Doing so allows two independent measurement models, a measurement model for X and a measurement model for Y.  These two sets of latent variables may then be correlated  for an Exploratory Structural Equation Model.  (This is exploratory because it is based upon exploratory factor analysis (EFA) rather than a confirmatory factor model (CFA) using more traditional Structural Equation Modeling packages such as sem, lavaan, or Mx.)Although the output seems very similar to that of a normal EFA using  fa, it is actually two independent factor analyses (of the X and the Y sets) that are then mutually extended into each other.  That is, the loadings and structure matrices from sets X and Y are merely combined, and the correlations between the two sets of factors are found.Interbattery factor analysis was developed by Tucker (1958) as a way of comparing the factors in common to two batteries of tests.   (Currently under development and not yet complete). Using some straight forward linear algebra It is easy to find the factors of the intercorrelations between the two sets of variables.  This does not require estimating communalities and is highly related to the procedures of canonical correlation.  The difference between the esem and the interbattery approach is that the first factors the X set and then relates those factors to factors of the Y set.  Interbattery factor analysis, on the other hand, tries to find one set of factors that links both sets but is still distinct from factoring both sets together.  
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
If the total number of responses is N, with median, M, and the number of responses at the median value, Nm >1, and Nb= the number of responses less than the median,  then with the assumption that the responses are distributed uniformly within the category,  the interpolated median is M - .5w + w*(N/2 - Nb)/Nm. The generalization to 1st, 2nd and 3rd quartiles as well as the general quantiles is straightforward.A somewhat different generalization allows for graphic presentation of the difference between interpolated and non-interpolated points.  This uses the interp.values function.If the input is a matrix or data frame, quantiles are reported for each variable.
A very preliminary IRT estimation procedure.Given scores xij for ith individual on jth item Classical Test Theory ignores item difficulty and defines ability as expected score : abilityi = theta(i) = x(i.)A zero parameter model rescales these mean scores from 0 to 1 to a quasi logistic scale ranging from - 4 to 4This is merely a non-linear transform of the raw data to reflect a logistic mapping.Basic 1 parameter (Rasch) model considers item difficulties (delta j):  p(correct on item j for the ith subject |theta i, deltaj) = 1/(1+exp(deltaj - thetai))If we have estimates of item difficulty (delta), then we can find theta i by optimization Two parameter model adds item sensitivity (beta j):p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj- theta i)))Estimate delta, beta, and theta  to maximize fit of model to data.The procedure used here is to first find the item difficulties assuming theta = 0Then find theta given those deltasThen find beta given delta and theta.This is not an "official" way to do IRT, but is useful for basic item development. See irt.fa and score.irt  for far better options. 
A very preliminary IRT estimation procedure.Given scores xij for ith individual on jth item Classical Test Theory ignores item difficulty and defines ability as expected score : abilityi = theta(i) = x(i.)A zero parameter model rescales these mean scores from 0 to 1 to a quasi logistic scale ranging from - 4 to 4This is merely a non-linear transform of the raw data to reflect a logistic mapping.Basic 1 parameter (Rasch) model considers item difficulties (delta j):  p(correct on item j for the ith subject |theta i, deltaj) = 1/(1+exp(deltaj - thetai))If we have estimates of item difficulty (delta), then we can find theta i by optimization Two parameter model adds item sensitivity (beta j):p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj- theta i)))Estimate delta, beta, and theta  to maximize fit of model to data.The procedure used here is to first find the item difficulties assuming theta = 0Then find theta given those deltasThen find beta given delta and theta.This is not an "official" way to do IRT, but is useful for basic item development. See irt.fa and score.irt  for far better options. 
A very preliminary IRT estimation procedure.Given scores xij for ith individual on jth item Classical Test Theory ignores item difficulty and defines ability as expected score : abilityi = theta(i) = x(i.)A zero parameter model rescales these mean scores from 0 to 1 to a quasi logistic scale ranging from - 4 to 4This is merely a non-linear transform of the raw data to reflect a logistic mapping.Basic 1 parameter (Rasch) model considers item difficulties (delta j):  p(correct on item j for the ith subject |theta i, deltaj) = 1/(1+exp(deltaj - thetai))If we have estimates of item difficulty (delta), then we can find theta i by optimization Two parameter model adds item sensitivity (beta j):p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj- theta i)))Estimate delta, beta, and theta  to maximize fit of model to data.The procedure used here is to first find the item difficulties assuming theta = 0Then find theta given those deltasThen find beta given delta and theta.This is not an "official" way to do IRT, but is useful for basic item development. See irt.fa and score.irt  for far better options. 
Item Response Theory (aka "The new psychometrics") models individual responses to items with a logistic function and an individual (theta) and item difficulty (diff) parameter.irt.item.diff.rasch finds item difficulties with the assumption of theta=0 for all subjects and that all items are equally discriminating.irt.discrim takes those  difficulties and theta estimates from irt.person.rasch to find item discrimination (beta) parameters.A far better package with these features is the ltm package.  The IRT functions in the psych-package are for pedagogical rather than production purposes.  They are believed to be accurate, but are not guaranteed. They do seem to be slightly more robust to missing data structures associated with SAPA data sets than the ltm package. The irt.fa function is also an alternative. This will find tetrachoric or polychoric correlations and then convert to IRT parameters using factor analysis (fa).
irt.fa combines several functions into one to make the process of item response analysis easier.  Correlations are found using either tetrachoric or polychoric.  Exploratory factor analyeses with all the normal options are then done using fa.  The results are then organized to be reported in terms of IRT parameters (difficulties and discriminations) as well as the more conventional factor analysis output. In addition, because the correlation step is somewhat slow, reanalyses may be done using the correlation matrix found in the first step.  In this case, if it is desired to use the fm="minchi" factoring method, the number of observations needs to be specified as the matrix resulting from pairwiseCount.The tetrachoric correlation matrix of dichotomous items may be factored using a (e.g.) minimum residual factor analysis function fa and the resulting loadings, λ_i are transformed to discriminations bya = λ / (sqrt(1-λ^2).The difficulty parameter, δ is found from the τ parameter of the tetrachoric or polychoric function.δ = τ / (sqrt(1-λ^2)Similar analyses may be done with discrete item responses using polychoric correlations and distinct estimates of item difficulty (location)  for each item response.The results may be shown graphically using link{plot.irt} for dichotomous items or  link{plot.poly} for polytomous items.  These called by plotting the irt.fa output, see the examples).   For plotting there are three options: type = "ICC" will plot the item characteristic response function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.  Invisible output from the plot function will return tables of item information as a function of several levels of the trait, as well as the standard error of measurement and the reliability at each of those levels.The normal input is just the raw data.  If, however, the correlation matrix has already been found using tetrachoric, polychoric, or a previous analysis using irt.fa then that result can be processed directly.  Because  irt.fa saves the rho and tau matrices from the analysis, subsequent analyses of the same data set are much faster if the input is the object returned on the first run.  A similar feature is available in omega. The output is best seen in terms of graphic displays.  Plot the output from irt.fa to see item and test information functions.  The print function will print the item location and discriminations.  The additional factor analysis output is available as an object in the output and may be printed directly by specifying the $fa object.The irt.select function is a helper function to allow for selecting a subset of a prior analysis for further analysis. First run irt.fa, then select a subset of variables to be analyzed in a subsequent irt.fa analysis.  Perhaps a better approach is to just plot and find the information for selected items.  The plot function for an irt.fa object will plot ICC (item characteristic curves), IIC (item information curves), or test information curves. In addition, by using the "keys" option,  these three kinds of plots can be done for selected items. This is particularly useful when trying to see the information characteristics of short forms of tests based upon the longer form factor analysis.The plot function will also return (invisibly) the informaton at multiple levels of the trait, the average information (area under the curve) as well as the location of the peak information for each item.  These may be then printed or printed in sorted order using the sort option in print.
Item Response Theory (aka "The new psychometrics") models individual responses to items with a logistic function and an individual (theta) and item difficulty (diff) parameter.irt.item.diff.rasch finds item difficulties with the assumption of theta=0 for all subjects and that all items are equally discriminating.irt.discrim takes those  difficulties and theta estimates from irt.person.rasch to find item discrimination (beta) parameters.A far better package with these features is the ltm package.  The IRT functions in the psych-package are for pedagogical rather than production purposes.  They are believed to be accurate, but are not guaranteed. They do seem to be slightly more robust to missing data structures associated with SAPA data sets than the ltm package. The irt.fa function is also an alternative. This will find tetrachoric or polychoric correlations and then convert to IRT parameters using factor analysis (fa).
A very preliminary IRT estimation procedure.Given scores xij for ith individual on jth item Classical Test Theory ignores item difficulty and defines ability as expected score : abilityi = theta(i) = x(i.)A zero parameter model rescales these mean scores from 0 to 1 to a quasi logistic scale ranging from - 4 to 4This is merely a non-linear transform of the raw data to reflect a logistic mapping.Basic 1 parameter (Rasch) model considers item difficulties (delta j):  p(correct on item j for the ith subject |theta i, deltaj) = 1/(1+exp(deltaj - thetai))If we have estimates of item difficulty (delta), then we can find theta i by optimization Two parameter model adds item sensitivity (beta j):p(correct on item j for subject i |thetai, deltaj, betaj) = 1/(1+exp(betaj *(deltaj- theta i)))Estimate delta, beta, and theta  to maximize fit of model to data.The procedure used here is to first find the item difficulties assuming theta = 0Then find theta given those deltasThen find beta given delta and theta.This is not an "official" way to do IRT, but is useful for basic item development. See irt.fa and score.irt  for far better options. 
This function is a convenient way to analyze the quality of item alternatives in a multiple choice ability test.  The typical use is to first score the test (using, e.g., score.multiple.choice according to some scoring key and to then find the score.irt based scores.  Response frequencies for each alternative are then plotted against total score.  An ideal item is one in which just one alternative (the correct one) has a monotonically increasing response probability.Because of the similar pattern of results for IRT based or simple sum based item scoring, the function can be run on scores calculated either by score.irt or by  score.multiple.choice. In the latter case, the number of breaks should not exceed the number of possible score alternatives.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
irt.fa combines several functions into one to make the process of item response analysis easier.  Correlations are found using either tetrachoric or polychoric.  Exploratory factor analyeses with all the normal options are then done using fa.  The results are then organized to be reported in terms of IRT parameters (difficulties and discriminations) as well as the more conventional factor analysis output. In addition, because the correlation step is somewhat slow, reanalyses may be done using the correlation matrix found in the first step.  In this case, if it is desired to use the fm="minchi" factoring method, the number of observations needs to be specified as the matrix resulting from pairwiseCount.The tetrachoric correlation matrix of dichotomous items may be factored using a (e.g.) minimum residual factor analysis function fa and the resulting loadings, λ_i are transformed to discriminations bya = λ / (sqrt(1-λ^2).The difficulty parameter, δ is found from the τ parameter of the tetrachoric or polychoric function.δ = τ / (sqrt(1-λ^2)Similar analyses may be done with discrete item responses using polychoric correlations and distinct estimates of item difficulty (location)  for each item response.The results may be shown graphically using link{plot.irt} for dichotomous items or  link{plot.poly} for polytomous items.  These called by plotting the irt.fa output, see the examples).   For plotting there are three options: type = "ICC" will plot the item characteristic response function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.  Invisible output from the plot function will return tables of item information as a function of several levels of the trait, as well as the standard error of measurement and the reliability at each of those levels.The normal input is just the raw data.  If, however, the correlation matrix has already been found using tetrachoric, polychoric, or a previous analysis using irt.fa then that result can be processed directly.  Because  irt.fa saves the rho and tau matrices from the analysis, subsequent analyses of the same data set are much faster if the input is the object returned on the first run.  A similar feature is available in omega. The output is best seen in terms of graphic displays.  Plot the output from irt.fa to see item and test information functions.  The print function will print the item location and discriminations.  The additional factor analysis output is available as an object in the output and may be printed directly by specifying the $fa object.The irt.select function is a helper function to allow for selecting a subset of a prior analysis for further analysis. First run irt.fa, then select a subset of variables to be analyzed in a subsequent irt.fa analysis.  Perhaps a better approach is to just plot and find the information for selected items.  The plot function for an irt.fa object will plot ICC (item characteristic curves), IIC (item information curves), or test information curves. In addition, by using the "keys" option,  these three kinds of plots can be done for selected items. This is particularly useful when trying to see the information characteristics of short forms of tests based upon the longer form factor analysis.The plot function will also return (invisibly) the informaton at multiple levels of the trait, the average information (area under the curve) as well as the location of the peak information for each item.  These may be then printed or printed in sorted order using the sort option in print.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
When predicting criteria from a set of items formed into scales, the validity of the scale (that is, the correlations of the scale with each criteria) is a function of the average item validity (r_y), the average intercorrelation of the items in the scale (r_x), and the number of items in the scale (n).  The limit of validity is r_y/sqrt(r_x).  Criteria will differ in their predictability from a set of scales. These asymptotic values may be used to help the decision on which scales to develop further.  
Best results if called from an unrotated solution.  Repeated calls using a rotated solution will produce incorrect estimates of the correlations between the factors.
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
The easiest way to prepare keys for scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl  is to specify a keys.list.  This is just a list specifying the name of the scales to be scores and the direction of the items to be used.In earlier versions (prior to 1.6.9) keys were formed as a matrix of -1, 0, and 1s for all the items using make.keys.  This is no longer necessary, but make.keys is kept for compatibility with earlier versions.There are three ways to create keys for the scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl functions. One is to laboriously do it in a spreadsheet and then copy them into R.  The other is to just specify them by item number in a list. make.keys allows one to specify items by name or by location or a mixture of both.keys2list reverses the make.keys process and returns a list of scoring keys with the item names for each  item to be keyed.  If sign=FALSE, this is just a list of the items to be scored. (Useful for scoreIrt.2plselectFromKeys will strip the signs from a keys.list and create a vector of item names (deleting duplicates) associated with those keys.  This is useful if using a keys.list to define scales and then just selecting those items that are in subset of the keys.list.  This is now done in the scoring functions in the interest of speed. Since these scoring functions scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl can now (> version 1.6.9)  just take a keys.list as input, make.keys is not as important, but is kept for documentation purposes.To address items by name it is necessary to specify item names, either by using the item.labels value, or by putting the name of the data file or the colnames of the data file to be scored into the first (nvars) position.If specifying by number (location), then nvars is the total number of items in the object to be scored, not just the number of items used.See the examples for the various options.Note that make.keys was revised in Sept, 2013 to allow for keying by name.It is also possible to do several make.keys operations and then combine them using superMatrix.  The alternative, if using the keys.list features is just to concatenate them. makePositiveKeys is useful for taking subsets of keys (e.g. from bestScales )and create separate keys for the positively and negatively keyed items.
Items used in measuring ability or other aspects of personality are typically not very reliable.  One suggestion has been to form items into homogeneous item composites (HICs), Factorially Homogeneous Item Dimensions (FHIDs) or mini scales (parcels).  Parcelling may be done rationally, factorially, or empirically based upon the structure of the correlation/covariance matrix.  link{parcels} facilitates the finding of parcels by forming a keys matrix suitable for using in score.items.  These keys represent the n/2 most similar pairs or the n/3 most similar triplets.The algorithm is straightforward:  For size = 2, the correlation matrix is searched for the highest correlation.  These two items form the first parcel and are dropped from the matrix.  The procedure is repeated until there are no more pairs to form.For size=3, the three items with the greatest sum of variances and covariances with each other is found.  This triplet is the first parcel.  All three items are removed and the procedure then identifies the next most similar triplet.  The procedure repeats until n/3 parcels are identified.  
Let  S^2 = diag(R^{-1})^{-1}  and Q = SR^{-1}S.  Then Q is said to  be the anti-image intercorrelation matrix.  Let sumr2 = ∑{R^2} and sumq2 = ∑{Q^2} for all off diagonal elements of R and Q, then  SMA=sumr2/(sumr2 + sumq2).  Although originally MSA was 1 - sumq2/sumr2  (Kaiser, 1970), this was modified in Kaiser and Rice, (1974) to be   SMA=sumr2/(sumr2 + sumq2).  This is the formula used by Dziuban and Shirkey (1974) and by SPSS.In his delightfully flamboyant style, Kaiser (1975)suggested that KMO > .9 were marvelous, in the .80s, mertitourious, in the .70s, middling, in the .60s, medicore, in the 50s, miserable, and  less than .5, unacceptable.An alternative measure of whether the matrix is factorable is the Bartlett test  cortest.bartlett which tests the degree that the matrix deviates from an identity matrix.
given a matrix or data.frame x, find the skew or kurtosis for each column (for skew and kurtosis) or the multivariate skew and kurtosis in the case of mardia.As of version 1.2.3,when finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). If we define m_r = [sum(X- mx)^r]/n then Type 1 finds skewness and kurtosis by g_1 = m_3/(m_2)^{3/2}  and g_2 = m_4/(m_2)^2 -3.  Type 2 is G1 = g1 * √{n *(n-1)}/(n-2) and G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)).  Type 3 is b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2} and b2 =  [(n-1)/n]^{3/2} m_4/m_2^2). For consistency with e1071 and with the Joanes and Gill, the types are now defined as above.However, from revision 1.0.93 to 1.2.3, kurtosi by default gives an unbiased estimate of the kurtosis (DeCarlo, 1997). Prior versions used a different equation which produced a biased estimate.  (See the kurtosis function in the e1071 package for the distinction between these two formulae.  The default, type 1 gave what is called type 2 in e1071.  The other is their type 3.)  For comparison with previous releases, specifying type = 2 will give the old estimate.  These type numbers are now changed.  
The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.The figure is organized to show the appropriate paths between:The correlations between the X variables (if Rx is specified) The X variables and their latent factors  (if fx is specified) The latent X and the latent Y (if Phi is specified)  The latent Y and the observed Y (if fy is specified) The correlations between the Y variables (if Ry is specified)A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.lavaan.diagram may be called from the diagram function which also will call  fa.diagram, omega.diagram or  iclust.diagram, depending upon the class of the fit.Other diagram functions include fa.diagram, omega.diagram.  All of these functions use the various dia functions such as dia.rect, dia.ellipse, dia.arrow, dia.curve, dia.curved.arrow, and dia.shape.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
These three functions are provided as simple helper functions for demonstrations of Item Response Theory. The one parameter logistic (1PL) model is also known as the Rasch model.  It assumes items differ only in difficulty.1PL, 2PL, 3PL and 4PL curves may be drawn by choosing the appropriate d (delta or item difficulty), a (discrimination or slope), c (gamma or  guessing) and z (zeta or upper asymptote).logit is just the inverse of logistic.logistic.grm will create the responses for a graded response model for the rth category where cutpoints are in s.
These three functions are provided as simple helper functions for demonstrations of Item Response Theory. The one parameter logistic (1PL) model is also known as the Rasch model.  It assumes items differ only in difficulty.1PL, 2PL, 3PL and 4PL curves may be drawn by choosing the appropriate d (delta or item difficulty), a (discrimination or slope), c (gamma or  guessing) and z (zeta or upper asymptote).logit is just the inverse of logistic.logistic.grm will create the responses for a graded response model for the rth category where cutpoints are in s.
These three functions are provided as simple helper functions for demonstrations of Item Response Theory. The one parameter logistic (1PL) model is also known as the Rasch model.  It assumes items differ only in difficulty.1PL, 2PL, 3PL and 4PL curves may be drawn by choosing the appropriate d (delta or item difficulty), a (discrimination or slope), c (gamma or  guessing) and z (zeta or upper asymptote).logit is just the inverse of logistic.logistic.grm will create the responses for a graded response model for the rth category where cutpoints are in s.
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
If just one matrix is provided (i.e., upper is missing), it is decomposed into two square matrices, one equal to the lower off diagonal entries, the other to the upper off diagonal entries. In the normal case two symmetric matrices are provided and combined into one non-symmetric matrix with the lower off diagonals representing the lower matrix and the upper off diagonals representing the upper matrix.If diff is true, the upper off diagonal matrix reflects the differences between the two matrices.
The lsat6 data set is analyzed in the ltm package as well as by McDonald (1999). lsat7 is another 1000 subjects on part 7 of the LSAT. Both sets are described by Bock and Lieberman (1970). Both sets are useful examples of testing out IRT procedures and showing the use of tetrachoric correlations and item factor analysis using the irt.fa function.
The lsat6 data set is analyzed in the ltm package as well as by McDonald (1999). lsat7 is another 1000 subjects on part 7 of the LSAT. Both sets are described by Bock and Lieberman (1970). Both sets are useful examples of testing out IRT procedures and showing the use of tetrachoric correlations and item factor analysis using the irt.fa function.
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
When constructing examples for reliability analysis, it is convenient to simulate congeneric data structures.  These are the most simple of item structures, having just one factor. Mainly used for a discussion of reliability theory as well as factor score estimates. The implied covariance matrix is just pattern %*% t(pattern). 
Many personality and cognitive tests have a hierarchical factor structure.  For demonstration purposes, it is useful to be able to create such matrices, either with population values, or sample values. Given a matrix of item factor loadings (fload) and of loadings of these factors on a general factor (gload), we create a population correlation matrix by using the general factor law (R = F' theta F where theta = g'g).  The default is to return population correlation matrices. Sample correlation matrices are generated if n > 0.  Raw data are returned if raw = TRUE.The default values for gload and fload create a data matrix discussed by Jensen and Weng, 1994.Although written to create hierarchical structures, if the gload matrix is all 0, then a non-hierarchical structure will be generated.Yet another model is that of Godfrey H. Thomson (1916) who suggested that independent bonds could produce the same factor structure as a g factor model. This is simulated in sim.bonds.  Compare the omega solutions for a sim.hierarchical with a sim.bonds model. Both produce reasonable values of omega, although the one was generated without a general factor.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
The easiest way to prepare keys for scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl  is to specify a keys.list.  This is just a list specifying the name of the scales to be scores and the direction of the items to be used.In earlier versions (prior to 1.6.9) keys were formed as a matrix of -1, 0, and 1s for all the items using make.keys.  This is no longer necessary, but make.keys is kept for compatibility with earlier versions.There are three ways to create keys for the scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl functions. One is to laboriously do it in a spreadsheet and then copy them into R.  The other is to just specify them by item number in a list. make.keys allows one to specify items by name or by location or a mixture of both.keys2list reverses the make.keys process and returns a list of scoring keys with the item names for each  item to be keyed.  If sign=FALSE, this is just a list of the items to be scored. (Useful for scoreIrt.2plselectFromKeys will strip the signs from a keys.list and create a vector of item names (deleting duplicates) associated with those keys.  This is useful if using a keys.list to define scales and then just selecting those items that are in subset of the keys.list.  This is now done in the scoring functions in the interest of speed. Since these scoring functions scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl can now (> version 1.6.9)  just take a keys.list as input, make.keys is not as important, but is kept for documentation purposes.To address items by name it is necessary to specify item names, either by using the item.labels value, or by putting the name of the data file or the colnames of the data file to be scored into the first (nvars) position.If specifying by number (location), then nvars is the total number of items in the object to be scored, not just the number of items used.See the examples for the various options.Note that make.keys was revised in Sept, 2013 to allow for keying by name.It is also possible to do several make.keys operations and then combine them using superMatrix.  The alternative, if using the keys.list features is just to concatenate them. makePositiveKeys is useful for taking subsets of keys (e.g. from bestScales )and create separate keys for the positively and negatively keyed items.
The easiest way to prepare keys for scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl  is to specify a keys.list.  This is just a list specifying the name of the scales to be scores and the direction of the items to be used.In earlier versions (prior to 1.6.9) keys were formed as a matrix of -1, 0, and 1s for all the items using make.keys.  This is no longer necessary, but make.keys is kept for compatibility with earlier versions.There are three ways to create keys for the scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl functions. One is to laboriously do it in a spreadsheet and then copy them into R.  The other is to just specify them by item number in a list. make.keys allows one to specify items by name or by location or a mixture of both.keys2list reverses the make.keys process and returns a list of scoring keys with the item names for each  item to be keyed.  If sign=FALSE, this is just a list of the items to be scored. (Useful for scoreIrt.2plselectFromKeys will strip the signs from a keys.list and create a vector of item names (deleting duplicates) associated with those keys.  This is useful if using a keys.list to define scales and then just selecting those items that are in subset of the keys.list.  This is now done in the scoring functions in the interest of speed. Since these scoring functions scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl can now (> version 1.6.9)  just take a keys.list as input, make.keys is not as important, but is kept for documentation purposes.To address items by name it is necessary to specify item names, either by using the item.labels value, or by putting the name of the data file or the colnames of the data file to be scored into the first (nvars) position.If specifying by number (location), then nvars is the total number of items in the object to be scored, not just the number of items used.See the examples for the various options.Note that make.keys was revised in Sept, 2013 to allow for keying by name.It is also possible to do several make.keys operations and then combine them using superMatrix.  The alternative, if using the keys.list features is just to concatenate them. makePositiveKeys is useful for taking subsets of keys (e.g. from bestScales )and create separate keys for the positively and negatively keyed items.
When exploring the correlations of many items with a few criteria, it is useful to form scales from the most correlated items (see bestScales.  To get a feeling of the distribution of items across various measures, we can display their correlations (or the log of the probabilities) grouped by some set of scale keys. May also be used to display and order correlations (rows) with a criteria (columns) if given a correlation as input (raw=FALSE).
given a matrix or data.frame x, find the skew or kurtosis for each column (for skew and kurtosis) or the multivariate skew and kurtosis in the case of mardia.As of version 1.2.3,when finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). If we define m_r = [sum(X- mx)^r]/n then Type 1 finds skewness and kurtosis by g_1 = m_3/(m_2)^{3/2}  and g_2 = m_4/(m_2)^2 -3.  Type 2 is G1 = g1 * √{n *(n-1)}/(n-2) and G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)).  Type 3 is b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2} and b2 =  [(n-1)/n]^{3/2} m_4/m_2^2). For consistency with e1071 and with the Joanes and Gill, the types are now defined as above.However, from revision 1.0.93 to 1.2.3, kurtosi by default gives an unbiased estimate of the kurtosis (DeCarlo, 1997). Prior versions used a different equation which produced a biased estimate.  (See the kurtosis function in the e1071 package for the distinction between these two formulae.  The default, type 1 gave what is called type 2 in e1071.  The other is their type 3.)  For comparison with previous releases, specifying type = 2 will give the old estimate.  These type numbers are now changed.  
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
The factor analysis output is sorted by size of the largest  factor loading for each variable  and then the matrix items are organized by those loadings.  The default is to sort by the loadings on the first factor.  Alternatives allow for ordering based upon any vector or matrix. 
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
The factor analysis output is sorted by size of the largest  factor loading for each variable  and then the matrix items are organized by those loadings.  The default is to sort by the loadings on the first factor.  Alternatives allow for ordering based upon any vector or matrix. 
When doing linear modeling, it is frequently convenient to estimate the direct effect of a predictor controlling for the indirect effect of a mediator.  See Preacher and Hayes (2004) for a very thorough discussion of mediation.  The mediate function will do some basic mediation and moderation models, with bootstrapped confidence intervals for the mediation/moderation effects. Functionally, this is just regular linear regression and partial correlation with some different output.In the case of two predictor variables, X and M, and a criterion variable Y, then the direct effect of X on Y, labeled with the path c, is said to be mediated by the effect of x on M (path a) and the effect of M on Y (path b).  This partial effect (a b) is said to mediate the direct effect of X –c–> Y:     X –a ->  M  –b–> Y with X –c'–> Y where c' = c - ab.Testing the significance of the ab mediation effect is done through bootstrapping many random resamples (with replacement) of the data.  For moderation, the moderation effect of Z on the relationship between X -> Y is found by taking the (centered) product of X and Z and then adding this XZ term into the regression. By default, the data are zero centered before doing moderation (product terms).  This is following the advice of Cohen, Cohen, West and Aiken (2003).  However, to agree with the analyses reported in Hayes (2013) we can set the zero=FALSE option to not zero center the data.   To partial out variables, either define them in the z term, or express as negative entries in the formula mode:y1 ~ x1 + x2 + (m1)+ (m2) -z    will look for the effect of x1 and x2 on y, mediated through m1 and m2 after z is partialled out.  Moderated mediation is done by specifying a product term.y1 ~ x1 + x2*x3 + (m1)+ (m2) -z    will look for the effect of x1, x2, x3 and the product of x2 and x3 on y, mediated through m1 and m2 after z is partialled out.  In the case of being provided just a correlation matrix, the bootstrapped values are based upon bootstrapping from data matching the original covariance/correlation matrix with the addition of normal errors.  This allows us to test the mediation/moderation effect even if not given raw data.  Moderation can not be done with just correlation matrix.The function has been tested against some of the basic cases and examples in Hayes (2013) and the associated data sets.Unless there is a temporal component that allows one to directly distinguish causal paths (time does not reverse direction), interpreting mediation models is problematic. Some people find it useful to compare the differences between mediation models where the causal paths (arrows) are reversed.  This is a mistake  and should not be done (Thoemmes, 2015). For fine tuning the size of the graphic output, xlim and ylim can be specified in the mediate.diagram function. Otherwise, the graphics produced by mediate and moderate use the default xlim and ylim values.Interaction terms (moderation) or mediated moderation can be specified as product terms.
When doing linear modeling, it is frequently convenient to estimate the direct effect of a predictor controlling for the indirect effect of a mediator.  See Preacher and Hayes (2004) for a very thorough discussion of mediation.  The mediate function will do some basic mediation and moderation models, with bootstrapped confidence intervals for the mediation/moderation effects. Functionally, this is just regular linear regression and partial correlation with some different output.In the case of two predictor variables, X and M, and a criterion variable Y, then the direct effect of X on Y, labeled with the path c, is said to be mediated by the effect of x on M (path a) and the effect of M on Y (path b).  This partial effect (a b) is said to mediate the direct effect of X –c–> Y:     X –a ->  M  –b–> Y with X –c'–> Y where c' = c - ab.Testing the significance of the ab mediation effect is done through bootstrapping many random resamples (with replacement) of the data.  For moderation, the moderation effect of Z on the relationship between X -> Y is found by taking the (centered) product of X and Z and then adding this XZ term into the regression. By default, the data are zero centered before doing moderation (product terms).  This is following the advice of Cohen, Cohen, West and Aiken (2003).  However, to agree with the analyses reported in Hayes (2013) we can set the zero=FALSE option to not zero center the data.   To partial out variables, either define them in the z term, or express as negative entries in the formula mode:y1 ~ x1 + x2 + (m1)+ (m2) -z    will look for the effect of x1 and x2 on y, mediated through m1 and m2 after z is partialled out.  Moderated mediation is done by specifying a product term.y1 ~ x1 + x2*x3 + (m1)+ (m2) -z    will look for the effect of x1, x2, x3 and the product of x2 and x3 on y, mediated through m1 and m2 after z is partialled out.  In the case of being provided just a correlation matrix, the bootstrapped values are based upon bootstrapping from data matching the original covariance/correlation matrix with the addition of normal errors.  This allows us to test the mediation/moderation effect even if not given raw data.  Moderation can not be done with just correlation matrix.The function has been tested against some of the basic cases and examples in Hayes (2013) and the associated data sets.Unless there is a temporal component that allows one to directly distinguish causal paths (time does not reverse direction), interpreting mediation models is problematic. Some people find it useful to compare the differences between mediation models where the causal paths (arrows) are reversed.  This is a mistake  and should not be done (Thoemmes, 2015). For fine tuning the size of the graphic output, xlim and ylim can be specified in the mediate.diagram function. Otherwise, the graphics produced by mediate and moderate use the default xlim and ylim values.Interaction terms (moderation) or mediated moderation can be specified as product terms.
Ellipse dimensions are calculated from the correlation between the x and y variables and are scaled as sqrt(1+r) and sqrt(1-r). They are then scaled as size[1] and size[2] standard deviation units.   To scale  for 95 and 99 percent confidence use c(1.64,2.32)
This function is particularly useful as part of the Synthetic Apeture Personality Assessment (SAPA) (https://www.sapa-project.org/) data sets where continuous variables (age, SAT V, SAT Q, etc) and mixed with polytomous personality items taken from the International Personality Item Pool (IPIP) and the dichotomous experimental IQ items that have been developed as part of SAPA (see, e.g., Revelle, Wilt and Rosenthal, 2010 or Revelle, Dworak and Condon, 2020.).  This is a very computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package. (See the note for timing comparisons.) This adjusts the  number of cores to use when doing polychoric or tetrachoric. The greatest step in speed is going from 1 core to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.Item response analyses using irt.fa may be done separately on the polytomous and dichotomous items  in order to develop internally consistent scales. These scale may, in turn, be correlated with each other using the complete correlation matrix found by mixed.cor and using the score.items function.This function is not quite as flexible as the hetcor function in John Fox's polychor package. Note that the variables may be organized by type of data:  continuous, polytomous, and dichotomous. This is done by simply specifying c, p, and d. This is advantageous in the case of some continuous variables having a limited number of categories because of subsetting.  mixedCor is essentially a wrapper for cor, polychoric, tetrachoric, polydi and polyserial. It first identifies the types of variables, organizes them by type (continuous, polytomous, dichotomous), calls the appropriate correlation function, and then binds the resulting matrices together.   
This function is particularly useful as part of the Synthetic Apeture Personality Assessment (SAPA) (https://www.sapa-project.org/) data sets where continuous variables (age, SAT V, SAT Q, etc) and mixed with polytomous personality items taken from the International Personality Item Pool (IPIP) and the dichotomous experimental IQ items that have been developed as part of SAPA (see, e.g., Revelle, Wilt and Rosenthal, 2010 or Revelle, Dworak and Condon, 2020.).  This is a very computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package. (See the note for timing comparisons.) This adjusts the  number of cores to use when doing polychoric or tetrachoric. The greatest step in speed is going from 1 core to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.Item response analyses using irt.fa may be done separately on the polytomous and dichotomous items  in order to develop internally consistent scales. These scale may, in turn, be correlated with each other using the complete correlation matrix found by mixed.cor and using the score.items function.This function is not quite as flexible as the hetcor function in John Fox's polychor package. Note that the variables may be organized by type of data:  continuous, polytomous, and dichotomous. This is done by simply specifying c, p, and d. This is advantageous in the case of some continuous variables having a limited number of categories because of subsetting.  mixedCor is essentially a wrapper for cor, polychoric, tetrachoric, polydi and polyserial. It first identifies the types of variables, organizes them by type (continuous, polytomous, dichotomous), calls the appropriate correlation function, and then binds the resulting matrices together.   
Classical reliabiiity theory estimates the amount of variance in a set of observations due to a true score that varies over subjects.  Generalizability theory extends this model to include other sources of variance, specifically, time.  The classic studies using this approach are people measured over multiple time points with multiple items.  Then the question is, how stable are various individual differences. Intraclass correlations (ICC) are found for each subject over items, and for each subject over time. Alpha reliabilities  are found for each subject for the items across time.   More importantly, components of variance for people, items, time, and their interactions are found either by classical analysis of variance (aov) or by multilevel mixed effect modeling (lme).  These are then used to form several different estimates of generalizability.   Very thoughtful discussions of these procedure may be found in chapters by Shrout and Lane.  The variance components are the Between Person Variance σ^2_P, the variance between items σ^2_I, over time σ^2_T,  and their interactions. Then, RKF is the  reliability of average of all ratings across all items and  times (Fixed time effects). (Shrout and Lane, Equation 6): Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))The generalizability of a single time point across all items (Random time effects) is justR1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane equation 7 with a correction per Sean Lane.)Generalizability of average time points across all items (Random effects). (Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n.I n.T))Generalizability of change scores (Shrout and Lane, equation  9)RC = (σ^2_PT)/(σ^2_PT +  σ^2_e/(n.I)).If the design may be thought of as fully crossed, then either aov or lmer can be used to estimate the components of variance.  With no missing data and a balanced design, these will give identical answers. However aov breaks down with missing data and seems to be very slow and very memory intensive for large problems ( 5,919  seconds for 209 cases with with 88 time points and three items on a Mac Powerbook with a 2.8 GHZ Intel Core I7). The slowdown probably is memory related, as the memory demands increased to 22.62 GB of compressed memory.   lmer will handle this design but is not nearly as slow  (242 seconds for the 209 cases with 88 time points and three items) as the aov approach.   If the design is thought of as nested, rather than crossed, the components of variance are found using the lme function from nlme. This is very fast (114 cases with 88 time points and three items took 3.5 seconds). The nested design leads to the generalizability of K random effects Nested (Shrout and Lane, equation 10):RKkRN = (σ^2_P)/(σ^2_P + σ^2_{T(P)}/n.p + σ^2_e/(n.I n.T))And, finally, to the reliability of between person differences, averaged over items.  (Shrout and Lane, equation 11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))Unfortunately, when doing the nested analysis, lme will sometimes issue an obnoxious error about failing to converge.  To fix this, turning off lme  and just using lmer seems to solve the problem (i.e., set lme=FALSE and lmer=TRUE).  (lme is part of core R and its namespace is automatically attached when loading psych). For many problems, lmer is not necessary and is thus not loaded.  However sometimes it is useful.  To use lmer it is necessary to have the lme4 package installed.  It will be automatically loaded if it is installed and requested. In the interests of making a 'thin' package, lmer is suggested,not required.The input can either be in 'wide' or 'long' form.  If in wide form, then specify the grouping variable, the 'time' variable, and the the column numbers or names of the items. (See the first example).  If in  long format, then what is the column (name or number) of the dependent variable.  (See the second example.)mlArrange takes a wide data.frame and organizes it into a ‘long’ data.frame suitable for a lattice xyplot.  This is a convenient alternative to stack, particularly for unbalanced designs.  The wide data frame is reorganized into a long data frame organized by grp (typically a subject id), by Time (typically a time varying variable, but can be anything, and then stacks the items within each person and time.  Extra variables are carried over and matched to the appropriate grp  and Time. Thus, if we have N subjects over t time points for k items, in wide format for N * t rows where each row has k items and e extra pieces of information, we get a N x t * k row by 4 + e column dataframe.  The first four columns in the long output are id, time, values, and item names, the remaining columns are the extra values.  These  could be something such as a trait measure for each subject, or the situation in which the items are given.mlArrange plots k items over the  t time dimensions for each subject. 
Classical reliabiiity theory estimates the amount of variance in a set of observations due to a true score that varies over subjects.  Generalizability theory extends this model to include other sources of variance, specifically, time.  The classic studies using this approach are people measured over multiple time points with multiple items.  Then the question is, how stable are various individual differences. Intraclass correlations (ICC) are found for each subject over items, and for each subject over time. Alpha reliabilities  are found for each subject for the items across time.   More importantly, components of variance for people, items, time, and their interactions are found either by classical analysis of variance (aov) or by multilevel mixed effect modeling (lme).  These are then used to form several different estimates of generalizability.   Very thoughtful discussions of these procedure may be found in chapters by Shrout and Lane.  The variance components are the Between Person Variance σ^2_P, the variance between items σ^2_I, over time σ^2_T,  and their interactions. Then, RKF is the  reliability of average of all ratings across all items and  times (Fixed time effects). (Shrout and Lane, Equation 6): Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))The generalizability of a single time point across all items (Random time effects) is justR1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane equation 7 with a correction per Sean Lane.)Generalizability of average time points across all items (Random effects). (Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n.I n.T))Generalizability of change scores (Shrout and Lane, equation  9)RC = (σ^2_PT)/(σ^2_PT +  σ^2_e/(n.I)).If the design may be thought of as fully crossed, then either aov or lmer can be used to estimate the components of variance.  With no missing data and a balanced design, these will give identical answers. However aov breaks down with missing data and seems to be very slow and very memory intensive for large problems ( 5,919  seconds for 209 cases with with 88 time points and three items on a Mac Powerbook with a 2.8 GHZ Intel Core I7). The slowdown probably is memory related, as the memory demands increased to 22.62 GB of compressed memory.   lmer will handle this design but is not nearly as slow  (242 seconds for the 209 cases with 88 time points and three items) as the aov approach.   If the design is thought of as nested, rather than crossed, the components of variance are found using the lme function from nlme. This is very fast (114 cases with 88 time points and three items took 3.5 seconds). The nested design leads to the generalizability of K random effects Nested (Shrout and Lane, equation 10):RKkRN = (σ^2_P)/(σ^2_P + σ^2_{T(P)}/n.p + σ^2_e/(n.I n.T))And, finally, to the reliability of between person differences, averaged over items.  (Shrout and Lane, equation 11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))Unfortunately, when doing the nested analysis, lme will sometimes issue an obnoxious error about failing to converge.  To fix this, turning off lme  and just using lmer seems to solve the problem (i.e., set lme=FALSE and lmer=TRUE).  (lme is part of core R and its namespace is automatically attached when loading psych). For many problems, lmer is not necessary and is thus not loaded.  However sometimes it is useful.  To use lmer it is necessary to have the lme4 package installed.  It will be automatically loaded if it is installed and requested. In the interests of making a 'thin' package, lmer is suggested,not required.The input can either be in 'wide' or 'long' form.  If in wide form, then specify the grouping variable, the 'time' variable, and the the column numbers or names of the items. (See the first example).  If in  long format, then what is the column (name or number) of the dependent variable.  (See the second example.)mlArrange takes a wide data.frame and organizes it into a ‘long’ data.frame suitable for a lattice xyplot.  This is a convenient alternative to stack, particularly for unbalanced designs.  The wide data frame is reorganized into a long data frame organized by grp (typically a subject id), by Time (typically a time varying variable, but can be anything, and then stacks the items within each person and time.  Extra variables are carried over and matched to the appropriate grp  and Time. Thus, if we have N subjects over t time points for k items, in wide format for N * t rows where each row has k items and e extra pieces of information, we get a N x t * k row by 4 + e column dataframe.  The first four columns in the long output are id, time, values, and item names, the remaining columns are the extra values.  These  could be something such as a trait measure for each subject, or the situation in which the items are given.mlArrange plots k items over the  t time dimensions for each subject. 
Classical reliabiiity theory estimates the amount of variance in a set of observations due to a true score that varies over subjects.  Generalizability theory extends this model to include other sources of variance, specifically, time.  The classic studies using this approach are people measured over multiple time points with multiple items.  Then the question is, how stable are various individual differences. Intraclass correlations (ICC) are found for each subject over items, and for each subject over time. Alpha reliabilities  are found for each subject for the items across time.   More importantly, components of variance for people, items, time, and their interactions are found either by classical analysis of variance (aov) or by multilevel mixed effect modeling (lme).  These are then used to form several different estimates of generalizability.   Very thoughtful discussions of these procedure may be found in chapters by Shrout and Lane.  The variance components are the Between Person Variance σ^2_P, the variance between items σ^2_I, over time σ^2_T,  and their interactions. Then, RKF is the  reliability of average of all ratings across all items and  times (Fixed time effects). (Shrout and Lane, Equation 6): Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))The generalizability of a single time point across all items (Random time effects) is justR1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane equation 7 with a correction per Sean Lane.)Generalizability of average time points across all items (Random effects). (Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n.I n.T))Generalizability of change scores (Shrout and Lane, equation  9)RC = (σ^2_PT)/(σ^2_PT +  σ^2_e/(n.I)).If the design may be thought of as fully crossed, then either aov or lmer can be used to estimate the components of variance.  With no missing data and a balanced design, these will give identical answers. However aov breaks down with missing data and seems to be very slow and very memory intensive for large problems ( 5,919  seconds for 209 cases with with 88 time points and three items on a Mac Powerbook with a 2.8 GHZ Intel Core I7). The slowdown probably is memory related, as the memory demands increased to 22.62 GB of compressed memory.   lmer will handle this design but is not nearly as slow  (242 seconds for the 209 cases with 88 time points and three items) as the aov approach.   If the design is thought of as nested, rather than crossed, the components of variance are found using the lme function from nlme. This is very fast (114 cases with 88 time points and three items took 3.5 seconds). The nested design leads to the generalizability of K random effects Nested (Shrout and Lane, equation 10):RKkRN = (σ^2_P)/(σ^2_P + σ^2_{T(P)}/n.p + σ^2_e/(n.I n.T))And, finally, to the reliability of between person differences, averaged over items.  (Shrout and Lane, equation 11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))Unfortunately, when doing the nested analysis, lme will sometimes issue an obnoxious error about failing to converge.  To fix this, turning off lme  and just using lmer seems to solve the problem (i.e., set lme=FALSE and lmer=TRUE).  (lme is part of core R and its namespace is automatically attached when loading psych). For many problems, lmer is not necessary and is thus not loaded.  However sometimes it is useful.  To use lmer it is necessary to have the lme4 package installed.  It will be automatically loaded if it is installed and requested. In the interests of making a 'thin' package, lmer is suggested,not required.The input can either be in 'wide' or 'long' form.  If in wide form, then specify the grouping variable, the 'time' variable, and the the column numbers or names of the items. (See the first example).  If in  long format, then what is the column (name or number) of the dependent variable.  (See the second example.)mlArrange takes a wide data.frame and organizes it into a ‘long’ data.frame suitable for a lattice xyplot.  This is a convenient alternative to stack, particularly for unbalanced designs.  The wide data frame is reorganized into a long data frame organized by grp (typically a subject id), by Time (typically a time varying variable, but can be anything, and then stacks the items within each person and time.  Extra variables are carried over and matched to the appropriate grp  and Time. Thus, if we have N subjects over t time points for k items, in wide format for N * t rows where each row has k items and e extra pieces of information, we get a N x t * k row by 4 + e column dataframe.  The first four columns in the long output are id, time, values, and item names, the remaining columns are the extra values.  These  could be something such as a trait measure for each subject, or the situation in which the items are given.mlArrange plots k items over the  t time dimensions for each subject. 
When doing linear modeling, it is frequently convenient to estimate the direct effect of a predictor controlling for the indirect effect of a mediator.  See Preacher and Hayes (2004) for a very thorough discussion of mediation.  The mediate function will do some basic mediation and moderation models, with bootstrapped confidence intervals for the mediation/moderation effects. Functionally, this is just regular linear regression and partial correlation with some different output.In the case of two predictor variables, X and M, and a criterion variable Y, then the direct effect of X on Y, labeled with the path c, is said to be mediated by the effect of x on M (path a) and the effect of M on Y (path b).  This partial effect (a b) is said to mediate the direct effect of X –c–> Y:     X –a ->  M  –b–> Y with X –c'–> Y where c' = c - ab.Testing the significance of the ab mediation effect is done through bootstrapping many random resamples (with replacement) of the data.  For moderation, the moderation effect of Z on the relationship between X -> Y is found by taking the (centered) product of X and Z and then adding this XZ term into the regression. By default, the data are zero centered before doing moderation (product terms).  This is following the advice of Cohen, Cohen, West and Aiken (2003).  However, to agree with the analyses reported in Hayes (2013) we can set the zero=FALSE option to not zero center the data.   To partial out variables, either define them in the z term, or express as negative entries in the formula mode:y1 ~ x1 + x2 + (m1)+ (m2) -z    will look for the effect of x1 and x2 on y, mediated through m1 and m2 after z is partialled out.  Moderated mediation is done by specifying a product term.y1 ~ x1 + x2*x3 + (m1)+ (m2) -z    will look for the effect of x1, x2, x3 and the product of x2 and x3 on y, mediated through m1 and m2 after z is partialled out.  In the case of being provided just a correlation matrix, the bootstrapped values are based upon bootstrapping from data matching the original covariance/correlation matrix with the addition of normal errors.  This allows us to test the mediation/moderation effect even if not given raw data.  Moderation can not be done with just correlation matrix.The function has been tested against some of the basic cases and examples in Hayes (2013) and the associated data sets.Unless there is a temporal component that allows one to directly distinguish causal paths (time does not reverse direction), interpreting mediation models is problematic. Some people find it useful to compare the differences between mediation models where the causal paths (arrows) are reversed.  This is a mistake  and should not be done (Thoemmes, 2015). For fine tuning the size of the graphic output, xlim and ylim can be specified in the mediate.diagram function. Otherwise, the graphics produced by mediate and moderate use the default xlim and ylim values.Interaction terms (moderation) or mediated moderation can be specified as product terms.
When examining multiple measures within subjects, it is sometimes useful to consider the variability of trial by trial observations in addition to the over all between trial variation.  The Mean Square of Successive Differences (mssd) and root mean square of successive differences (rmssd) is just σ^2 = Σ(x_i - x_{i+1})^2 /(n-lag)  Where n-lag is used because there are only n-lag cases.In the case of multiple subjects  (groups) with multiple observations per subject (group), specify the grouping variable will produce output for each group.   Similar functions are available in the matrixStats package. However, the varDiff function in that package is variance of the difference rather than the MeanSquare. This is just the variance and standard deviation applied to the result from the diff function.Perhaps useful when studying mood, the autoR function finds the autocorrelation for each item for the specified lag.  It also returns the rmssd (root means square successive difference). This is done by finding the correlation of the lag data. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
This allows for quick summaries of multiple distributions.  Particularly useful when examining the results of multiple-split halves that come from the reliability function.  By default, will try to make a square plot with equal number of rows and columns.  However, the number of columns and rows may be specified for a particular plot.
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
The diagram function calls  fa.diagram, omega.diagram,  ICLUST.diagram, lavaan.diagram or bassAckward.diagram depending upon the class of the fit input.  See those functions for particular parameter values.The remaining functions are the graphic primitives used by fa.diagram, structure.diagram, omega.diagram, ICLUST.diagram and het.diagramThey create rectangles, ellipses or triangles surrounding text, connect them to straight or curved arrows, and can draw an arrow from and to the same rectangle. To speed up the plotting, dia.rect and dia.arrow can suppress the actual drawing and return the locations and values to plot.  These values  can then be directly called by text or rect with matrix input. This leads to an impressive increase in speed when doing many variables. The functions multi.rect, multi.self, multi.arrow and multi.curved.arrow  will take the saved output from the appropriate primitives and then draw them all at once. Each shape (ellipse, rectangle or triangle) has a left, right, top and bottom and center coordinate that may be used to connect the arrows. Curves are double-headed arrows.   By default they go from one location to another and curve either left or right (if going up or down) or up or down (going left to right).  The direction of the curve may be set by dir="up" for left right curvature.The helper functions were developed to get around the infelicities associated with trying to install Rgraphviz and graphviz. These functions form the core of fa.diagram,het.diagram. Better documentation will be added as these functions get improved.  Currently the helper functions are just a work around for Rgraphviz.dia.cone draws a cone with (optionally) arrows as sides and centers to show the problem of factor indeterminacy. 
Classical reliabiiity theory estimates the amount of variance in a set of observations due to a true score that varies over subjects.  Generalizability theory extends this model to include other sources of variance, specifically, time.  The classic studies using this approach are people measured over multiple time points with multiple items.  Then the question is, how stable are various individual differences. Intraclass correlations (ICC) are found for each subject over items, and for each subject over time. Alpha reliabilities  are found for each subject for the items across time.   More importantly, components of variance for people, items, time, and their interactions are found either by classical analysis of variance (aov) or by multilevel mixed effect modeling (lme).  These are then used to form several different estimates of generalizability.   Very thoughtful discussions of these procedure may be found in chapters by Shrout and Lane.  The variance components are the Between Person Variance σ^2_P, the variance between items σ^2_I, over time σ^2_T,  and their interactions. Then, RKF is the  reliability of average of all ratings across all items and  times (Fixed time effects). (Shrout and Lane, Equation 6): Rkf = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P + σ^2_{PI}/n.I + σ^2_e/(n.I n.P))The generalizability of a single time point across all items (Random time effects) is justR1R = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T + σ^2_{PT} σ^2_e/n.I)(Shrout and Lane equation 7 with a correction per Sean Lane.)Generalizability of average time points across all items (Random effects). (Shrout and Lane, equation 8)RkR = (σ^2_P + σ^2_{PI}/n.I)/(σ^2_P +  σ^2_{PI}/n.I + σ^2_T/n.T + σ^2_{PT} σ^2_e/(n.I n.T))Generalizability of change scores (Shrout and Lane, equation  9)RC = (σ^2_PT)/(σ^2_PT +  σ^2_e/(n.I)).If the design may be thought of as fully crossed, then either aov or lmer can be used to estimate the components of variance.  With no missing data and a balanced design, these will give identical answers. However aov breaks down with missing data and seems to be very slow and very memory intensive for large problems ( 5,919  seconds for 209 cases with with 88 time points and three items on a Mac Powerbook with a 2.8 GHZ Intel Core I7). The slowdown probably is memory related, as the memory demands increased to 22.62 GB of compressed memory.   lmer will handle this design but is not nearly as slow  (242 seconds for the 209 cases with 88 time points and three items) as the aov approach.   If the design is thought of as nested, rather than crossed, the components of variance are found using the lme function from nlme. This is very fast (114 cases with 88 time points and three items took 3.5 seconds). The nested design leads to the generalizability of K random effects Nested (Shrout and Lane, equation 10):RKkRN = (σ^2_P)/(σ^2_P + σ^2_{T(P)}/n.p + σ^2_e/(n.I n.T))And, finally, to the reliability of between person differences, averaged over items.  (Shrout and Lane, equation 11).RCN = (σ^2_T(P)/(σ^2_T(P) + σ^2_e/(n.I))Unfortunately, when doing the nested analysis, lme will sometimes issue an obnoxious error about failing to converge.  To fix this, turning off lme  and just using lmer seems to solve the problem (i.e., set lme=FALSE and lmer=TRUE).  (lme is part of core R and its namespace is automatically attached when loading psych). For many problems, lmer is not necessary and is thus not loaded.  However sometimes it is useful.  To use lmer it is necessary to have the lme4 package installed.  It will be automatically loaded if it is installed and requested. In the interests of making a 'thin' package, lmer is suggested,not required.The input can either be in 'wide' or 'long' form.  If in wide form, then specify the grouping variable, the 'time' variable, and the the column numbers or names of the items. (See the first example).  If in  long format, then what is the column (name or number) of the dependent variable.  (See the second example.)mlArrange takes a wide data.frame and organizes it into a ‘long’ data.frame suitable for a lattice xyplot.  This is a convenient alternative to stack, particularly for unbalanced designs.  The wide data frame is reorganized into a long data frame organized by grp (typically a subject id), by Time (typically a time varying variable, but can be anything, and then stacks the items within each person and time.  Extra variables are carried over and matched to the appropriate grp  and Time. Thus, if we have N subjects over t time points for k items, in wide format for N * t rows where each row has k items and e extra pieces of information, we get a N x t * k row by 4 + e column dataframe.  The first four columns in the long output are id, time, values, and item names, the remaining columns are the extra values.  These  could be something such as a trait measure for each subject, or the situation in which the items are given.mlArrange plots k items over the  t time dimensions for each subject. 
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
Determining the most interpretable number of factors from a factor analysis is perhaps one of the greatest challenges in factor analysis.  There are many solutions to this problem, none of which is uniformly the best.  "Solving the number of factors problem is easy, I do it everyday before breakfast."  But knowing the right solution is harder. (Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  Techniques most commonly used include1)  Extracting factors until the chi square of the residual matrix is not significant.2) Extracting factors until the change in chi square from factor n to factor n+1 is not significant.3) Extracting factors until the eigen values of the real data are less than the corresponding eigen values of a random data set of the same size (parallel analysis) fa.parallel.4) Plotting the magnitude of the successive eigen values and applying the scree test (a sudden drop in eigen values analogous to the change in slope seen when scrambling up the talus slope of a mountain and approaching the rock face.5) Extracting principal components until the eigen value < 1. 6) Extracting factors as long as they are interpetable.7) Using the Very Simple Structure Criterion (VSS).8) Using Wayne Velicer's Minimum Average Partial (MAP) criterion. Each of the procedures has its advantages and disadvantages.  Using either the chi square test or the change in square test is, of course, sensitive to the number of subjects and leads to the nonsensical condition that if one wants to find many factors, one simply runs more subjects. Parallel analysis is partially sensitive to sample size in that for large samples the eigen values of random factors will be very small.  The scree test is quite appealling but can lead to differences of interpretation as to when the scree "breaks". The eigen value of 1 rule, although the default for many programs, seems to be a rough way of dividing the number of variables by 3.  Extracting interpretable factors means that the number of factors reflects the investigators creativity more than the data.  VSS, while very simple to understand, will not work very well if the data are very factorially complex. (Simulations suggests it will work fine if the complexities of some of the items are no more than 2).Most users of factor analysis tend to interpret factor output by focusing their attention on the largest loadings for every variable and ignoring the smaller ones.  Very Simple Structure operationalizes this tendency by  comparing the original correlation matrix to that reproduced by a simplified version (S) of the original factor matrix (F).  R = SS' + U2.   S is composed of just the c greatest (in absolute value) loadings for each variable.  C (or complexity) is a parameter of the model and may vary from 1 to the number of factors.  The VSS criterion compares the fit of the simplified model to the original correlations: VSS = 1 -sumsquares(r*)/sumsquares(r)  where R* is the residual matrix R* = R - SS' and r* and r are the elements of R* and R respectively. VSS for a given complexity will tend to peak at the optimal (most interpretable) number of factors (Revelle and Rocklin, 1979). Although originally written in Fortran for main frame computers, VSS has been adapted to micro computers (e.g., Macintosh OS 6-9) using Pascal. We now release R code for calculating VSS. Note that if using a correlation matrix (e.g., my.matrix) and doing a factor analysis, the parameters n.obs should be specified for the factor analysis:e.g., the call is VSS(my.matrix,n.obs=500).  Otherwise it defaults to 1000. Wayne Velicer's MAP criterion has been added as an additional test for the optimal number of components to extract.  Note that VSS and MAP will not always agree as to the optimal number.The nfactors function will do a VSS, find MAP, and report a number of other criteria (e.g., BIC, complexity, chi square, ...)A variety of rotation options are available. These include varimax, promax, and oblimin. Others can be added.  Suggestions are welcome.
“Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale." (Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonald has proposed coefficient omega_hierarchical (ω_h) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf compare McDonald's ω_h to Cronbach's α and Revelle's β.  They conclude that ω_h is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   One way to find omega_h is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (schmid) transformation to find general factor loadings, and then find omega_h.  Here we present code to do that.  omega_h differs as a function of how the factors are estimated.  Four options are available, three use the fa function but with different factoring methods: the default does a minres factor solution, fm="pa"  does a principle axes factor analysis  fm="mle" does a maximum likelihood solution; fm="pc" does a principal components analysis using (principal).  For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the scoreItems function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).beta, an alternative to ω_h, is defined as the worst split half reliability (Revelle, 1979).  It can be estimated by using ICLUST (a hierarchical clustering algorithm originally developed for main frames and written in Fortran and that is now part of the psych package.  (For a very complimentary review of why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). The omega function uses exploratory factor analysis to estimate the ω_h coefficient.  It is important to remember that  “A recommendation that should be heeded, regardless of the method chosen to estimate ω_h, is to always examine the pattern of the estimated general factor loadings prior to estimating ω_h. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading ω_h estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137).A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (describe) the $schmid$sl results.Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option="first" or option="second" to the call.Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.In addition to ω_h, another of McDonald's coefficients is ω_t.  This is an estimate of the total reliability of a test. McDonald's ω_t, which is similar to Guttman's λ_6, guttman but uses the estimates of uniqueness (u^2) from factor analysis to find e_j^2. This is based on a decomposition of the variance of a test score, V_x  into four parts: that due to a general factor, \vec{g}, that due to a set of group factors, \vec{f},  (factors common to some but not all of the items), specific factors, \vec{s} unique to each item, and \vec{e}, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). Letting x = cg + Af + Ds + ethen the communality of item_j, based upon general as well as group factors,h_j^2 = c_j^2 + sum(f_ij^2)and the unique variance for the itemu_j^2 = σ_j^2 (1-h_j^2)may be used to estimate the test reliability.That is, if h_j^2 is the communality of item_j, based upon general as well as group factors,  then for standardized items,  e_j^2 = 1 - h_j^2 andω_t = (1 cc' 1 + 1 AA' 1')/(V_x)Because h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.It is important to distinguish here between the two ω coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, ω_t and ω_h.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. ω_h = (1 cc' 1')/VxAnother estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1'). Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  omega is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  omegaSem first calls omega and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  ω_h is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. It is also possible to give omega a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  omegaFromSem takes the output from a sem model and uses it to find ω_h.  The estimate of factor indeterminacy, found by the multiple R^2 of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.Finally, and still being tested, is omegaDirect adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (directSl).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures omega and omegaSem.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.Moral: Finding omega_h is tricky and one should probably compare omega, omegaSem,  omegaDirect and even iclust solutions to understand the differences.The summary of the omega object is a reduced set of the most useful output. The various objects returned from omega include:
While omega.graph requires the Rgraphviz package, omega.diagram does not.  codeomega requires the GPArotation package.
While omega.graph requires the Rgraphviz package, omega.diagram does not.  codeomega requires the GPArotation package.
“Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale." (Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonald has proposed coefficient omega_hierarchical (ω_h) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf compare McDonald's ω_h to Cronbach's α and Revelle's β.  They conclude that ω_h is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   One way to find omega_h is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (schmid) transformation to find general factor loadings, and then find omega_h.  Here we present code to do that.  omega_h differs as a function of how the factors are estimated.  Four options are available, three use the fa function but with different factoring methods: the default does a minres factor solution, fm="pa"  does a principle axes factor analysis  fm="mle" does a maximum likelihood solution; fm="pc" does a principal components analysis using (principal).  For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the scoreItems function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).beta, an alternative to ω_h, is defined as the worst split half reliability (Revelle, 1979).  It can be estimated by using ICLUST (a hierarchical clustering algorithm originally developed for main frames and written in Fortran and that is now part of the psych package.  (For a very complimentary review of why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). The omega function uses exploratory factor analysis to estimate the ω_h coefficient.  It is important to remember that  “A recommendation that should be heeded, regardless of the method chosen to estimate ω_h, is to always examine the pattern of the estimated general factor loadings prior to estimating ω_h. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading ω_h estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137).A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (describe) the $schmid$sl results.Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option="first" or option="second" to the call.Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.In addition to ω_h, another of McDonald's coefficients is ω_t.  This is an estimate of the total reliability of a test. McDonald's ω_t, which is similar to Guttman's λ_6, guttman but uses the estimates of uniqueness (u^2) from factor analysis to find e_j^2. This is based on a decomposition of the variance of a test score, V_x  into four parts: that due to a general factor, \vec{g}, that due to a set of group factors, \vec{f},  (factors common to some but not all of the items), specific factors, \vec{s} unique to each item, and \vec{e}, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). Letting x = cg + Af + Ds + ethen the communality of item_j, based upon general as well as group factors,h_j^2 = c_j^2 + sum(f_ij^2)and the unique variance for the itemu_j^2 = σ_j^2 (1-h_j^2)may be used to estimate the test reliability.That is, if h_j^2 is the communality of item_j, based upon general as well as group factors,  then for standardized items,  e_j^2 = 1 - h_j^2 andω_t = (1 cc' 1 + 1 AA' 1')/(V_x)Because h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.It is important to distinguish here between the two ω coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, ω_t and ω_h.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. ω_h = (1 cc' 1')/VxAnother estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1'). Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  omega is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  omegaSem first calls omega and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  ω_h is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. It is also possible to give omega a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  omegaFromSem takes the output from a sem model and uses it to find ω_h.  The estimate of factor indeterminacy, found by the multiple R^2 of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.Finally, and still being tested, is omegaDirect adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (directSl).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures omega and omegaSem.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.Moral: Finding omega_h is tricky and one should probably compare omega, omegaSem,  omegaDirect and even iclust solutions to understand the differences.The summary of the omega object is a reduced set of the most useful output. The various objects returned from omega include:
“Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale." (Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonald has proposed coefficient omega_hierarchical (ω_h) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf compare McDonald's ω_h to Cronbach's α and Revelle's β.  They conclude that ω_h is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   One way to find omega_h is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (schmid) transformation to find general factor loadings, and then find omega_h.  Here we present code to do that.  omega_h differs as a function of how the factors are estimated.  Four options are available, three use the fa function but with different factoring methods: the default does a minres factor solution, fm="pa"  does a principle axes factor analysis  fm="mle" does a maximum likelihood solution; fm="pc" does a principal components analysis using (principal).  For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the scoreItems function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).beta, an alternative to ω_h, is defined as the worst split half reliability (Revelle, 1979).  It can be estimated by using ICLUST (a hierarchical clustering algorithm originally developed for main frames and written in Fortran and that is now part of the psych package.  (For a very complimentary review of why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). The omega function uses exploratory factor analysis to estimate the ω_h coefficient.  It is important to remember that  “A recommendation that should be heeded, regardless of the method chosen to estimate ω_h, is to always examine the pattern of the estimated general factor loadings prior to estimating ω_h. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading ω_h estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137).A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (describe) the $schmid$sl results.Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option="first" or option="second" to the call.Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.In addition to ω_h, another of McDonald's coefficients is ω_t.  This is an estimate of the total reliability of a test. McDonald's ω_t, which is similar to Guttman's λ_6, guttman but uses the estimates of uniqueness (u^2) from factor analysis to find e_j^2. This is based on a decomposition of the variance of a test score, V_x  into four parts: that due to a general factor, \vec{g}, that due to a set of group factors, \vec{f},  (factors common to some but not all of the items), specific factors, \vec{s} unique to each item, and \vec{e}, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). Letting x = cg + Af + Ds + ethen the communality of item_j, based upon general as well as group factors,h_j^2 = c_j^2 + sum(f_ij^2)and the unique variance for the itemu_j^2 = σ_j^2 (1-h_j^2)may be used to estimate the test reliability.That is, if h_j^2 is the communality of item_j, based upon general as well as group factors,  then for standardized items,  e_j^2 = 1 - h_j^2 andω_t = (1 cc' 1 + 1 AA' 1')/(V_x)Because h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.It is important to distinguish here between the two ω coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, ω_t and ω_h.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. ω_h = (1 cc' 1')/VxAnother estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1'). Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  omega is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  omegaSem first calls omega and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  ω_h is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. It is also possible to give omega a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  omegaFromSem takes the output from a sem model and uses it to find ω_h.  The estimate of factor indeterminacy, found by the multiple R^2 of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.Finally, and still being tested, is omegaDirect adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (directSl).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures omega and omegaSem.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.Moral: Finding omega_h is tricky and one should probably compare omega, omegaSem,  omegaDirect and even iclust solutions to understand the differences.The summary of the omega object is a reduced set of the most useful output. The various objects returned from omega include:
“Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale." (Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonald has proposed coefficient omega_hierarchical (ω_h) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf compare McDonald's ω_h to Cronbach's α and Revelle's β.  They conclude that ω_h is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   One way to find omega_h is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (schmid) transformation to find general factor loadings, and then find omega_h.  Here we present code to do that.  omega_h differs as a function of how the factors are estimated.  Four options are available, three use the fa function but with different factoring methods: the default does a minres factor solution, fm="pa"  does a principle axes factor analysis  fm="mle" does a maximum likelihood solution; fm="pc" does a principal components analysis using (principal).  For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the scoreItems function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).beta, an alternative to ω_h, is defined as the worst split half reliability (Revelle, 1979).  It can be estimated by using ICLUST (a hierarchical clustering algorithm originally developed for main frames and written in Fortran and that is now part of the psych package.  (For a very complimentary review of why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). The omega function uses exploratory factor analysis to estimate the ω_h coefficient.  It is important to remember that  “A recommendation that should be heeded, regardless of the method chosen to estimate ω_h, is to always examine the pattern of the estimated general factor loadings prior to estimating ω_h. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading ω_h estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137).A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (describe) the $schmid$sl results.Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option="first" or option="second" to the call.Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.In addition to ω_h, another of McDonald's coefficients is ω_t.  This is an estimate of the total reliability of a test. McDonald's ω_t, which is similar to Guttman's λ_6, guttman but uses the estimates of uniqueness (u^2) from factor analysis to find e_j^2. This is based on a decomposition of the variance of a test score, V_x  into four parts: that due to a general factor, \vec{g}, that due to a set of group factors, \vec{f},  (factors common to some but not all of the items), specific factors, \vec{s} unique to each item, and \vec{e}, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). Letting x = cg + Af + Ds + ethen the communality of item_j, based upon general as well as group factors,h_j^2 = c_j^2 + sum(f_ij^2)and the unique variance for the itemu_j^2 = σ_j^2 (1-h_j^2)may be used to estimate the test reliability.That is, if h_j^2 is the communality of item_j, based upon general as well as group factors,  then for standardized items,  e_j^2 = 1 - h_j^2 andω_t = (1 cc' 1 + 1 AA' 1')/(V_x)Because h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.It is important to distinguish here between the two ω coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, ω_t and ω_h.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. ω_h = (1 cc' 1')/VxAnother estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1'). Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  omega is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  omegaSem first calls omega and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  ω_h is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. It is also possible to give omega a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  omegaFromSem takes the output from a sem model and uses it to find ω_h.  The estimate of factor indeterminacy, found by the multiple R^2 of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.Finally, and still being tested, is omegaDirect adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (directSl).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures omega and omegaSem.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.Moral: Finding omega_h is tricky and one should probably compare omega, omegaSem,  omegaDirect and even iclust solutions to understand the differences.The summary of the omega object is a reduced set of the most useful output. The various objects returned from omega include:
“Many scales are assumed by their developers and users to be primarily a measure of one latent variable. When it is also assumed that the scale conforms to the effect indicator model of measurement (as is almost always the case in psychological assessment), it is important to support such an interpretation with evidence regarding the internal structure of that scale. In particular, it is important to examine two related properties pertaining to the internal structure of such a scale. The first property relates to whether all the indicators forming the scale measure a latent variable in common. The second internal structural property pertains to the proportion of variance in the scale scores (derived from summing or averaging the indicators) accounted for by this latent variable that is common to all the indicators (Cronbach, 1951; McDonald, 1999; Revelle, 1979). That is, if an effect indicator scale is primarily a measure of one latent variable common to all the indicators forming the scale, then that latent variable should account for the majority of the variance in the scale scores. Put differently, this variance ratio provides important information about the sampling fluctuations when estimating individuals' standing on a latent variable common to all the indicators arising from the sampling of indicators (i.e., when dealing with either Type 2 or Type 12 sampling, to use the terminology of Lord, 1956). That is, this variance proportion can be interpreted as the square of the correlation between the scale score and the latent variable common to all the indicators in the infinite universe of indicators of which the scale indicators are a subset. Put yet another way, this variance ratio is important both as reliability and a validity coefficient. This is a reliability issue as the larger this variance ratio is, the more accurately one can predict an individual's relative standing on the latent variable common to all the scale's indicators based on his or her observed scale score. At the same time, this variance ratio also bears on the construct validity of the scale given that construct validity encompasses the internal structure of a scale." (Zinbarg, Yovel, Revelle, and McDonald, 2006).McDonald has proposed coefficient omega_hierarchical (ω_h) as an estimate of the general factor saturation of a test.  Zinbarg, Revelle, Yovel and Li (2005) https://personality-project.org/revelle/publications/zinbarg.revelle.pmet.05.pdf compare McDonald's ω_h to Cronbach's α and Revelle's β.  They conclude that ω_h is the best estimate. (See also Zinbarg et al., 2006 and Revelle and Zinbarg (2009)).   One way to find omega_h is to do a factor analysis of the original data set, rotate the factors obliquely, factor that correlation matrix, do a   Schmid-Leiman (schmid) transformation to find general factor loadings, and then find omega_h.  Here we present code to do that.  omega_h differs as a function of how the factors are estimated.  Four options are available, three use the fa function but with different factoring methods: the default does a minres factor solution, fm="pa"  does a principle axes factor analysis  fm="mle" does a maximum likelihood solution; fm="pc" does a principal components analysis using (principal).  For ability items, it is typically the case that all items will have positive loadings on the general factor.  However, for non-cognitive items it is frequently the case that some items are to be scored positively, and some negatively.  Although probably better to specify which directions the items are to be scored by specifying a key vector, if flip =TRUE (the default), items will be reversed so that they have positive loadings on the general factor.  The keys are reported so that scores can be found using the scoreItems function.  Arbitrarily reversing items this way can overestimate the general factor. (See the example with a simulated circumplex).beta, an alternative to ω_h, is defined as the worst split half reliability (Revelle, 1979).  It can be estimated by using ICLUST (a hierarchical clustering algorithm originally developed for main frames and written in Fortran and that is now part of the psych package.  (For a very complimentary review of why the ICLUST algorithm is useful in scale construction, see Cooksey and Soutar, 2005)). The omega function uses exploratory factor analysis to estimate the ω_h coefficient.  It is important to remember that  “A recommendation that should be heeded, regardless of the method chosen to estimate ω_h, is to always examine the pattern of the estimated general factor loadings prior to estimating ω_h. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading ω_h estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137).A simple demonstration of the problem of an omega estimate reflecting just one of two group factors can be found in the last example.  Diagnostic statistics that reflect the quality of the omega solution include a comparison of the relative size of the g factor eigen value to the other eigen values, the percent of the common variance for each item that is general factor variance (p2), the mean of p2, and the standard deviation of p2.  Further diagnostics can be done by describing (describe) the $schmid$sl results.Although omega_h is uniquely defined only for cases where 3 or more subfactors are extracted, it is sometimes desired to have a two factor solution.  By default this is done by forcing the schmid extraction to treat the two subfactors as having equal loadings.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. This solution discussed in Zinbarg et al., 2007.  To do this in omega, add the option="first" or option="second" to the call.Although obviously not meaningful for a 1 factor solution, it is of course possible to find the sum of the loadings on the first (and only) factor, square them, and compare them to the overall matrix variance.  This is done, with appropriate complaints.In addition to ω_h, another of McDonald's coefficients is ω_t.  This is an estimate of the total reliability of a test. McDonald's ω_t, which is similar to Guttman's λ_6, guttman but uses the estimates of uniqueness (u^2) from factor analysis to find e_j^2. This is based on a decomposition of the variance of a test score, V_x  into four parts: that due to a general factor, \vec{g}, that due to a set of group factors, \vec{f},  (factors common to some but not all of the items), specific factors, \vec{s} unique to each item, and \vec{e}, random error.  (Because specific variance can not be distinguished from random error unless the test is given at least twice,  some combine these both into error). Letting x = cg + Af + Ds + ethen the communality of item_j, based upon general as well as group factors,h_j^2 = c_j^2 + sum(f_ij^2)and the unique variance for the itemu_j^2 = σ_j^2 (1-h_j^2)may be used to estimate the test reliability.That is, if h_j^2 is the communality of item_j, based upon general as well as group factors,  then for standardized items,  e_j^2 = 1 - h_j^2 andω_t = (1 cc' 1 + 1 AA' 1')/(V_x)Because h_j^2 ≥q r_{smc}^2, ω_t ≥q λ_6.It is important to distinguish here between the two ω coefficients of McDonald, 1978 and Equation 6.20a of McDonald, 1999, ω_t and ω_h.  While the former is based upon the sum of squared loadings on all the factors, the latter is based upon the sum of the squared loadings on the general factor. ω_h = (1 cc' 1')/VxAnother estimate reported is the omega for an infinite length test with a structure similar to the observed test (omega H asymptotic).  This is found by ω_{limit} = (1 cc' 1')/(1 cc' 1' + 1 AA' 1'). Following suggestions by Steve Reise, the Explained Common Variance (ECV) is also reported.  This is the ratio of the general factor eigen value to the sum of all of the eigen values.  As such, it is a better indicator of unidimensionality than of the amount of test variance accounted for by a general factor.The input to omega may be a correlation matrix or a raw data matrix, or a factor pattern matrix with the factor intercorrelations (Phi) matrix.  omega is an exploratory factor analysis function that uses a Schmid-Leiman transformation.  omegaSem first calls omega and then takes the Schmid-Leiman solution, converts this to a confirmatory sem model and then calls the sem package to conduct a confirmatory model.  ω_h is then calculated from the CFA output. Although for well behaved problems, the efa and cfa solutions will be practically identical, the CFA solution will not always agree with the EFA solution. In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)In addition, not all EFA solutions will produce workable CFA solutions.  Model misspecifications will lead to very strange CFA estimates. It is also possible to give omega a factor pattern matrix and the associated factor intercorrelation.  In this case, the analysis will be done on these matrices.  This is particularly useful if one is not satisfied with the exploratory EFA solutions and rotation options and somehow comes up with an alternative. (For instance, one might want to do a EFA using fm='pa' with a Kaiser normalized Promax solution with a specified m value.)  omegaFromSem takes the output from a sem model and uses it to find ω_h.  The estimate of factor indeterminacy, found by the multiple R^2 of the variables with the factors, will not match that found by the EFA model.  In particular, the estimated   R^2 will sometimes exceed 1. (An example of this is the Harman 24 cognitive abilities problem.)The notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory.  Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale.  Thus, we can find a number of different omega estimates:   what percentage of the variance of the items identified with each subfactor is actually due to the general factor.  What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor.   These results are reported in omega.group object and in the last few lines of the normal output.Finally, and still being tested, is omegaDirect adapted from Waller (2017).  This is a direct rotation to a Schmid-Leiman  like solution without doing the hierarchical factoring (directSl).  This rotation is then interpreted in terms of omega.  It is included here to allow for comparisons with the alternative procedures omega and omegaSem.  Preliminary analyses suggests that it produces inappropriate solutions for the case where there is no general factor.Moral: Finding omega_h is tricky and one should probably compare omega, omegaSem,  omegaDirect and even iclust solutions to understand the differences.The summary of the omega object is a reduced set of the most useful output. The various objects returned from omega include:
Adapted from the mahalanobis function and help page from stats.
The conventional Null Hypothesis Significance Test (NHST) is the likelihood of observing the data given the null hypothesis of no effect.  But this tells us nothing about the probability of the null hypothesis.  Peter Killeen (2005) introduced the probability of replication as a more useful measure.  The probability of replication is the probability that an exact replication study will find a result in the same direction as the original result.p.rep is based upon a 1 tailed probability value of the observed statistic.  Other frequently called for statistics are estimates of the effect size, expressed either as Cohen's d, Hedges g, or the equivalent value of the correlation, r. For p.rep.t, if the cell sizes are unequal, the effect size estimates are adjusted by the ratio of the mean cell size to the harmonic mean cell size (see Rownow et al., 2000).   
The conventional Null Hypothesis Significance Test (NHST) is the likelihood of observing the data given the null hypothesis of no effect.  But this tells us nothing about the probability of the null hypothesis.  Peter Killeen (2005) introduced the probability of replication as a more useful measure.  The probability of replication is the probability that an exact replication study will find a result in the same direction as the original result.p.rep is based upon a 1 tailed probability value of the observed statistic.  Other frequently called for statistics are estimates of the effect size, expressed either as Cohen's d, Hedges g, or the equivalent value of the correlation, r. For p.rep.t, if the cell sizes are unequal, the effect size estimates are adjusted by the ratio of the mean cell size to the harmonic mean cell size (see Rownow et al., 2000).   
The conventional Null Hypothesis Significance Test (NHST) is the likelihood of observing the data given the null hypothesis of no effect.  But this tells us nothing about the probability of the null hypothesis.  Peter Killeen (2005) introduced the probability of replication as a more useful measure.  The probability of replication is the probability that an exact replication study will find a result in the same direction as the original result.p.rep is based upon a 1 tailed probability value of the observed statistic.  Other frequently called for statistics are estimates of the effect size, expressed either as Cohen's d, Hedges g, or the equivalent value of the correlation, r. For p.rep.t, if the cell sizes are unequal, the effect size estimates are adjusted by the ratio of the mean cell size to the harmonic mean cell size (see Rownow et al., 2000).   
The conventional Null Hypothesis Significance Test (NHST) is the likelihood of observing the data given the null hypothesis of no effect.  But this tells us nothing about the probability of the null hypothesis.  Peter Killeen (2005) introduced the probability of replication as a more useful measure.  The probability of replication is the probability that an exact replication study will find a result in the same direction as the original result.p.rep is based upon a 1 tailed probability value of the observed statistic.  Other frequently called for statistics are estimates of the effect size, expressed either as Cohen's d, Hedges g, or the equivalent value of the correlation, r. For p.rep.t, if the cell sizes are unequal, the effect size estimates are adjusted by the ratio of the mean cell size to the harmonic mean cell size (see Rownow et al., 2000).   
To find the z of the difference between two independent correlations, first convert them to z scores using the Fisher r-z transform and then find the z of the difference between the two correlations.  The default assumption is that the group sizes are the same, but the test can be done for different size groups by specifying n2.If the correlations are not independent (i.e., they are from the same sample) then the correlation with the third variable r(yz) must be specified. Find a t statistic for the difference of thee two dependent correlations.
Shamelessly adapted from the pairs help page.  Uses panel.cor, panel.cor.scale, and panel.hist, all taken from the help pages for pairs. Also adapts the ellipse function from John Fox's car package. pairs.panels is most useful when the number of variables to plot is less than about 6-10. It is particularly useful for an initial overview of the data.To show different groups with different colors, use a plot character (pch) between 21 and 25 and then set the background color to vary by group. (See the second example).When plotting more than about 10 variables, it is useful to set the gap parameter to something less than 1 (e.g., 0).  Alternatively, consider using cor.plotIn addition, when plotting more than about 100-200 cases, it is useful to set the plotting character to be a point. (pch=".")Sometimes it useful to draw the correlation ellipses and best fitting loess without the points. (points.false=TRUE).
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
When using Massively Missing Completely at Random (MMCAR) designs used by the SAPA project, it is important to count the number of pairwise observations (pairwiseCount).  If there are pairs with 1 or fewer observations, these will produce NA values for correlations making subsequent factor analyses fa or reliability analsyes omega or scoreOverlap impossible.pairwiseCountBig may be used to count the cells of large data sets.  It is analogous to bigCor and returns the cell sizes for each pair of correlations.In order to identify item pairs with counts less than a certain value pairwiseReport reports the names of those pairs with fewer than 'cut' observations.  By default, it just reports the number of offending items. With short=FALSE, the print will give the items with n.obs < cut. Even more detail is available in the returned objects.The specific pairs that have values <= n min in any particular table of the paiwise counts may be given by pairwiseZero.  To remedy the problem of missing correlations, we impute the missing correlations using pairwiseImpute.The technique takes advantage of the scale based structure of SAPA items.  Items within a scale (e.g. Letter Number Series similar to the ability items) are  imputed to correlate with items from another scale (e.g., Matrix Reasoning) at the average of these two between scale inter-item mean correlations.  The average correlations within and between scales are reported by pairwiseImpute and if the fix paremeter is specified, the imputed correlation matrix is returned. Alternative methods of imputing these correlations are not yet implemented.The time to count cell size  varies linearly by the number of subjects and of the number of items squared.  This becomes prohibitive for larger (big n items) data sets. pairwiseSample will take samples of size=size of these bigger data sets and then returns basic descriptive statistics of these counts, including mean, median, and the .05, .25, .5, .75 and .95 quantiles.   
Items used in measuring ability or other aspects of personality are typically not very reliable.  One suggestion has been to form items into homogeneous item composites (HICs), Factorially Homogeneous Item Dimensions (FHIDs) or mini scales (parcels).  Parcelling may be done rationally, factorially, or empirically based upon the structure of the correlation/covariance matrix.  link{parcels} facilitates the finding of parcels by forming a keys matrix suitable for using in score.items.  These keys represent the n/2 most similar pairs or the n/3 most similar triplets.The algorithm is straightforward:  For size = 2, the correlation matrix is searched for the highest correlation.  These two items form the first parcel and are dropped from the matrix.  The procedure is repeated until there are no more pairs to form.For size=3, the three items with the greatest sum of variances and covariances with each other is found.  This triplet is the first parcel.  All three items are removed and the procedure then identifies the next most similar triplet.  The procedure repeats until n/3 parcels are identified.  
There are two ways to use partial.r.  One is to find the complete partial correlation matrix (that is, partial all the other variables out of each variable).  This may be done by simply specifying the raw data or correlation matrix.  (In the case of raw data, correlations will be found according to use and method.)  In this case, just specify the data matrix. Alternatively, if we think of the data as an X matrix and Y matrix, then (D = X + Y) with correlations R.  Then the partial correlations of the X predictors with the Y variables partialled out are just the last column of R^(-1). See the Tal.Or example below.The second usage is to partial a set of variables(y) out of another set (x). It is sometimes convenient to partial the effect of a number of variables (e.g., sex, age, education) out of the correlations of another set of variables.  This could be done laboriously by finding the residuals of various multiple correlations, and then correlating these residuals.  The matrix algebra alternative is to do it directly. To find the confidence intervals and "significance" of the correlations, use the corr.p function with n = n - s where s is the number of covariates. A perhaps easier format is to use formula input compatible with that used in setCor. If using formula input,we specify X and Y with the partialled variables specified by subtraction.    That is X ~ Y - z,This is useful in the case of multiple regression using  which uses this notation.Following a thoughtful request from Fransisco Wilheim, I just find the correlations of the variables specified in the call (previously I  had found the entire correlation matrix, which is a waste of time and breaks if some variables are non-numeric).)In the case of non-positive definite matrices, find the Pinv (pseudo inverse) of the matrix.
Useful for those cases where the correlation matrix is improper (perhaps because of SAPA techniques).There are a number of data reduction techniques including principal components analysis (PCA) and factor analysis (EFA).  Both PC and FA attempt to approximate a given correlation or covariance matrix of rank n with matrix of lower rank (p).  nRn = nFk kFn' + U2 where k is much less than n.  For principal components, the item uniqueness is assumed to be zero and all elements of the correlation or covariance matrix are fitted. That is, nRn = nFk kFn'   The primary empirical difference between a components versus a factor model is the treatment of the variances for each item.  Philosophically, components are weighted composites of observed variables while in the factor model, variables are weighted composites of the factors. As the number of items increases, the difference between the two models gets smaller.  Factor loadings are the asymptotic component loadings as the number of items gets larger. For a n x n correlation matrix, the n principal components completely reproduce the correlation matrix.  However, if just the first k principal components are extracted, this is the best k dimensional approximation of the matrix.It is important to recognize that rotated principal components are not principal components (the axes associated with the eigen value decomposition) but are merely components.  To point this out, unrotated principal components are labelled as PCi, while rotated PCs are now labeled as RCi (for rotated components) and obliquely transformed components as TCi (for transformed components). (Thanks to Ulrike Gromping for this suggestion.)Rotations and transformations are either part of psych (Promax and cluster), of base R (varimax), or of GPArotation (simplimax, quartimax, oblimin, etc.).  Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The “cluster” option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional "keys" parameter, the "target" option will rotate to a target supplied as a keys matrix. (See target.rot.)The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by Phi = Th' ThSome of the statistics reported are more appropriate for (maximum likelihood) factor analysis rather than principal components analysis, and are reported to allow comparisons with these other models. Although for items, it is typical to find component scores by scoring the salient items (using, e.g., scoreItems) component scores are found  by regression where the regression weights are R^(-1) lambda where lambda is the matrix of component loadings.   The regression approach is done  to be parallel with the factor analysis function fa.  The regression weights are found from the inverse of the correlation matrix times the component loadings.   This has the result that the component scores are standard scores (mean=0, sd = 1) of the standardized input.  A comparison to the scores from princomp shows this difference.  princomp does not, by default, standardize the data matrix, nor are the components themselves standardized.  The regression weights are found from the Structure matrix, not the Pattern matrix. If the scores are found with the covar option = TRUE, then the scores are not standardized but are just mean centered.  Jolliffe (2002) discusses why the interpretation of rotated components is complicated.   Rencher (1992) discourages the use of rotated components. The approach used here is consistent with the factor analytic tradition.  The correlations of the items with the component scores closely matches (as it should) the component loadings (as reported in the structure matrix). The output from the print.psych function displays the component loadings (from the pattern matrix), the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the component loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared component loadings. But for an oblique solution, it is the row sum of the (squared) orthogonal component loadings (remember, that rotations or transformations do not change the communality).  This information is returned (invisibly) from the print function as the object Vaccounted.
In many prediction situations, a dichotomous predictor (accept/reject) is validated against a dichotomous criterion (success/failure).  Although a polychoric correlation estimates the underlying Pearson correlation as if the predictor and criteria were continuous and bivariate normal variables, and the tetrachoric correlation if both x and y are assumed to dichotomized normal distributions,  the phi coefficient is the Pearson applied to a matrix of 0's and 1s. The phi coefficient was first reported by Yule (1912), but should not be confused with the Yule Q coefficient. For a very useful discussion of various measures of association given a 2 x 2 table, and why one should probably prefer the Yule Q coefficient, see Warren (2008). Given a two x two table of counts convert all counts to fractions of the total and then  Phi = [a- (a+b)*(a+c)]/sqrt((a+b)(c+d)(a+c)(b+d) ) =  (a - R1 * C1)/sqrt(R1 * R2 * C1 * C2)This is in contrast to the Yule coefficient, Q, where Q = (ad - bc)/(ad+bc) which is the same as [a- (a+b)*(a+c)]/(ad+bc)Since the phi coefficient is just a Pearson correlation applied to dichotomous data, to find a matrix of phis from a data set involves just finding the correlations using cor  or lowerCor or corr.test.  
A demonstration of the problem of different base rates on the phi correlation, and how these are partially solved by using the polychoric correlation. Not one of my more interesting demonstrations.  See https://personality-project.org/r/simulating-personality.html and https://personality-project.org/r/r.datageneration.html for better demonstrations of data generation.
This is almost self explanatory.  See the examples.
used to require the mvtnorm package but this has been replaced with mnormt
These functions call Yule2poly,  Yule2phi or phi2poly for each cell of the matrix. See those functions for more details.  See phi.demo for an example.
used to require the mvtnorm package but this has been replaced with mnormt
 The singular value decomposition of a matrix X is UdV where for full rank matrices, d is the vector of eigen values and U and V are the matrices of eigen vectors. The inverse is just U/d.  If the matrix is less than full rank, many of the d values are effectively zero (at the limit of computational accuracy.) Thus, to solve matrix equations with matrices of less than full rank (e.g. the schmid Schmid-Leiman solution), we need to find the generalized inverse. 
Passes the appropriate values to plot.  For plotting the results of irt.fa, there are three options: type = "IIC" (default) will plot the item characteristic respone function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.Note that plotting an irt result will call either plot.irt or plot.poly depending upon the type of data that were used in the original irt.fa call.  These are calls to the generic plot function that are intercepted for objects of type "psych".  More precise plotting control is available in the separate plot functions.  plot may be used for psych objects returned from fa, irt.fa, ICLUST, omega, principal as well as plot.reliability .A "jiggle" parameter is available in the fa.plot function (called from plot.psych when the type is a factor or cluster.  If jiggle=TRUE, then the points are jittered slightly (controlled by amount) before plotting.  This option is useful when plotting items with identical factor loadings (e.g., when comparing hypothetical models).Objects from irt.fa are plotted according to "type" (Item informations, item characteristics, or test information).  In addition, plots for selected items may be done if using the keys matrix.  Plots of irt information return three invisible objects, a summary of information for each item at  levels of the trait, the average area under the curve (the average information) for each item as well as where the item is most informative.If plotting multiple factor solutions in plot.poly, then main can be a vector of names, one for each factor.  The default is to give main + the factor number.It is also possible to create irt like plots based upon just a scoring key and item difficulties, or from a factor analysis and item difficulties.  These are not true IRT type analyses, in that the parameters are not estimated from the data, but are rather indications of item location and discrimination for arbitrary sets of items.  To do this, find irt.stats.like and then plot the results.plot.residuals allows the user to graphically examine the residuals of models formed by  fa, irt.fa, omega, as well as  principal and display them in a number of ways.  "qq" will show quantiles of standardized or unstandardized residuals, "chi" will show quantiles of the squared standardized or unstandardized residuals plotted against the expected chi square values, "hist" will draw the histogram of the raw or standardized residuals, and "cor" will show a corPlot of the residual correlations. 
Passes the appropriate values to plot.  For plotting the results of irt.fa, there are three options: type = "IIC" (default) will plot the item characteristic respone function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.Note that plotting an irt result will call either plot.irt or plot.poly depending upon the type of data that were used in the original irt.fa call.  These are calls to the generic plot function that are intercepted for objects of type "psych".  More precise plotting control is available in the separate plot functions.  plot may be used for psych objects returned from fa, irt.fa, ICLUST, omega, principal as well as plot.reliability .A "jiggle" parameter is available in the fa.plot function (called from plot.psych when the type is a factor or cluster.  If jiggle=TRUE, then the points are jittered slightly (controlled by amount) before plotting.  This option is useful when plotting items with identical factor loadings (e.g., when comparing hypothetical models).Objects from irt.fa are plotted according to "type" (Item informations, item characteristics, or test information).  In addition, plots for selected items may be done if using the keys matrix.  Plots of irt information return three invisible objects, a summary of information for each item at  levels of the trait, the average area under the curve (the average information) for each item as well as where the item is most informative.If plotting multiple factor solutions in plot.poly, then main can be a vector of names, one for each factor.  The default is to give main + the factor number.It is also possible to create irt like plots based upon just a scoring key and item difficulties, or from a factor analysis and item difficulties.  These are not true IRT type analyses, in that the parameters are not estimated from the data, but are rather indications of item location and discrimination for arbitrary sets of items.  To do this, find irt.stats.like and then plot the results.plot.residuals allows the user to graphically examine the residuals of models formed by  fa, irt.fa, omega, as well as  principal and display them in a number of ways.  "qq" will show quantiles of standardized or unstandardized residuals, "chi" will show quantiles of the squared standardized or unstandardized residuals plotted against the expected chi square values, "hist" will draw the histogram of the raw or standardized residuals, and "cor" will show a corPlot of the residual correlations. 
Cattell's “scree" test is one of most simple tests for the number of factors problem.  Horn's (1965) “parallel" analysis is an equally compelling procedure.  Other procedures for determining the most optimal number of factors include finding the Very Simple Structure (VSS) criterion (VSS ) and Velicer's MAP procedure (included in VSS). Both the VSS and the MAP criteria are included in the nfactors function which also reports the  mean item complexity and the BIC for each of multiple solutions.   fa.parallel plots the eigen values for a principal components and the factor solution (minres by default) and does the same for random matrices of the same size as the original data matrix.  For raw data, the random matrices are 1) a matrix of univariate normal data and 2) random samples (randomized across rows) of the original data.fa.parallelwith the  cor=poly option will do what fa.parallel.polyexplicitly does: parallel analysis for polychoric and tetrachoric factors. If the data are dichotomous, fa.parallel.polywill find tetrachoric correlations for the real and simulated data, otherwise, if the number of categories is less than 10, it will find polychoric correlations.  Note that fa.parallel.poly is slower than fa.parallel because of the complexity of calculating the tetrachoric/polychoric correlations.  The functionality of fa.parallel.poly is included in fa.parallel with cor=poly option (etc.) option but the older fa.parallel.poly is kept for those who call it directly.That is, fa.parallel now will do tetrachorics or polychorics directly if the cor option is set to "tet" or "poly".  As with fa.parallel.poly this will take longer.  The means of (ntrials) random solutions are shown.  Error bars are usually very small and are suppressed by default but can be shown if requested.  If the sim option is set to TRUE (default), then parallel analyses are done on resampled data as well as random normal data. In the interests of speed, the parallel analyses are done just on resampled data if sim=FALSE.    Both procedures tend to agree.  As of version 1.5.4, I added the ability to specify the quantile of the simulated/resampled data, and to plot standard deviations or standard errors.  By default, this is set to the 95th percentile.  Alternative ways to estimate the number of factors problem are discussed in the Very Simple Structure  (Revelle and Rocklin, 1979) documentation (VSS) and include Wayne Velicer's MAP algorithm (Veicer, 1976).  Parallel analysis for factors is actually harder than it seems, for the question is what are the appropriate communalities to use.  If communalities are estimated by the Squared Multiple Correlation (SMC) smc, then the eigen values of the original data will reflect major as well as minor factors (see sim.minor to simulate such data).  Random data will not, of course, have any structure and thus the number of factors will tend to be biased upwards by the presence of the minor factors.  By default, fa.parallel estimates the communalities based upon a one factor minres solution.  Although this will underestimate the communalities, it does seem to lead to better solutions on simulated or real (e.g., the bfi or Harman74) data sets.  For comparability with other algorithms (e.g, the paran function in the paran package), setting smc=TRUE will use smcs as estimates of communalities. This will tend towards identifying more factors than the default option.Yet another option (suggested by Florian Scharf) is to estimate the eigen values based upon a particular factor model (e.g., specify nfactors > 1).   Printing the results will show the eigen values of the original data that are greater than simulated values.A sad observation about parallel analysis is that it is sensitive to sample size.  That is, for large data sets, the eigen values of random data are very close to 1.  This will lead to different estimates of the number of factors as a function of sample size.  Consider factor structure of the bfi data set (the first 25 items are meant to represent a five factor model).  For samples of 200 or less, parallel analysis suggests 5 factors, but for 1000 or more, six factors and components are indicated.  This is not due to an instability of the eigen values of the real data, but rather the closer approximation to 1 of the random data as n increases.Although with nfactors=1, 6 factors are suggested, when specifying nfactors =5, parallel analysis of the bfi suggests 12 factors should be extracted!When simulating dichotomous data in fa.parallel.poly, the simulated data have the same difficulties as the original data.  This functionally means that the simulated and the resampled results will be very similar.  Note that fa.parallel.poly has functionally been replaced with fa.parallel with the cor="poly" option.As with many psych functions, fa.parallel has been changed to allow for multicore processing.  For running a large number of iterations, it is obviously faster to increase the number of cores to the maximum possible (using the options("mc.cores=n) command where n is determined from detectCores(). 
Passes the appropriate values to plot.  For plotting the results of irt.fa, there are three options: type = "IIC" (default) will plot the item characteristic respone function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.Note that plotting an irt result will call either plot.irt or plot.poly depending upon the type of data that were used in the original irt.fa call.  These are calls to the generic plot function that are intercepted for objects of type "psych".  More precise plotting control is available in the separate plot functions.  plot may be used for psych objects returned from fa, irt.fa, ICLUST, omega, principal as well as plot.reliability .A "jiggle" parameter is available in the fa.plot function (called from plot.psych when the type is a factor or cluster.  If jiggle=TRUE, then the points are jittered slightly (controlled by amount) before plotting.  This option is useful when plotting items with identical factor loadings (e.g., when comparing hypothetical models).Objects from irt.fa are plotted according to "type" (Item informations, item characteristics, or test information).  In addition, plots for selected items may be done if using the keys matrix.  Plots of irt information return three invisible objects, a summary of information for each item at  levels of the trait, the average area under the curve (the average information) for each item as well as where the item is most informative.If plotting multiple factor solutions in plot.poly, then main can be a vector of names, one for each factor.  The default is to give main + the factor number.It is also possible to create irt like plots based upon just a scoring key and item difficulties, or from a factor analysis and item difficulties.  These are not true IRT type analyses, in that the parameters are not estimated from the data, but are rather indications of item location and discrimination for arbitrary sets of items.  To do this, find irt.stats.like and then plot the results.plot.residuals allows the user to graphically examine the residuals of models formed by  fa, irt.fa, omega, as well as  principal and display them in a number of ways.  "qq" will show quantiles of standardized or unstandardized residuals, "chi" will show quantiles of the squared standardized or unstandardized residuals plotted against the expected chi square values, "hist" will draw the histogram of the raw or standardized residuals, and "cor" will show a corPlot of the residual correlations. 
reliability is basically just a wrapper for omegah, unidim and splitHalf. Revelle and Condon (2019) recommended reporting at least three reliability statistics for any scale, here we make it easy to do. If the hist option is set to true, histgrams and density plots of the split half values for each test are also shown. The output from reliability can be passed to error.dots to show the reliability statistics for multiple scales graphically.  It is however more useful to just call the plot.reliability function to show the basic information.For detailed analysis of any one scale, it is recommended to do a complete omega analysis, perhaps combined with a splitHalf analysis. The reliability function is just meant for the case where the user has multiple scales (perhaps scored using scoreItems) and then wants to get more complete reliability information for all of the scales.  Following a suggestion, the ability to not bother with keys and just do omega and split half and draw the results has been added.  Either specify that keys=NULL, or just specify the items to use. (See the first example.)plot.reliability provides a dot chart summary of the distributions of the split half values, as well as the estimates of omega and alpha and unidimensionality.  It can also be called by just issuing a plot command.
Passes the appropriate values to plot.  For plotting the results of irt.fa, there are three options: type = "IIC" (default) will plot the item characteristic respone function.  type = "IIC" will plot the item information function, and type= "test" will plot the test information function.Note that plotting an irt result will call either plot.irt or plot.poly depending upon the type of data that were used in the original irt.fa call.  These are calls to the generic plot function that are intercepted for objects of type "psych".  More precise plotting control is available in the separate plot functions.  plot may be used for psych objects returned from fa, irt.fa, ICLUST, omega, principal as well as plot.reliability .A "jiggle" parameter is available in the fa.plot function (called from plot.psych when the type is a factor or cluster.  If jiggle=TRUE, then the points are jittered slightly (controlled by amount) before plotting.  This option is useful when plotting items with identical factor loadings (e.g., when comparing hypothetical models).Objects from irt.fa are plotted according to "type" (Item informations, item characteristics, or test information).  In addition, plots for selected items may be done if using the keys matrix.  Plots of irt information return three invisible objects, a summary of information for each item at  levels of the trait, the average area under the curve (the average information) for each item as well as where the item is most informative.If plotting multiple factor solutions in plot.poly, then main can be a vector of names, one for each factor.  The default is to give main + the factor number.It is also possible to create irt like plots based upon just a scoring key and item difficulties, or from a factor analysis and item difficulties.  These are not true IRT type analyses, in that the parameters are not estimated from the data, but are rather indications of item location and discrimination for arbitrary sets of items.  To do this, find irt.stats.like and then plot the results.plot.residuals allows the user to graphically examine the residuals of models formed by  fa, irt.fa, omega, as well as  principal and display them in a number of ways.  "qq" will show quantiles of standardized or unstandardized residuals, "chi" will show quantiles of the squared standardized or unstandardized residuals plotted against the expected chi square values, "hist" will draw the histogram of the raw or standardized residuals, and "cor" will show a corPlot of the residual correlations. 
Although many uses of factor analysis/cluster analysis assume a simple structure where items have one and only one large loading, some domains such as personality or affect items have a more complex structure and some items have high loadings on two factors.  (These items are said to have complexity 2, see VSS).  By expressing the factor loadings in polar coordinates, this structure is more readily perceived.For each pair of factors, item loadings are converted to an angle with the first factor, and a vector length corresponding to the amount of variance in the item shared with the two factors.  For a two dimensional structure, this will lead to a column of angles and a column of vector lengths.  For n factors, this leads to n* (n-1)/2 columns of angles and an equivalent number of vector lengths.
Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See draw.tetra and draw.cor for  illustrations.)This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.tetrachoric and polychoric can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See fa.extension for an application of this.The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).Examples include the lsat6 and lsat7 data sets in the bock data.The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. For finding one polychoric correlation from a table, see the Olsson example (below).polychoric replaces poly.mat and is recommended.   poly.mat was an alternative wrapper to the polycor function. biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (https://www.sapa-project.org/) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (https://www.icar-project.org/) for similar data.Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, rbi = r s* √(pq)/zp .As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).The 'ad hoc' polyserial correlation, rps is just r = r * sqrt(n-1)/n) σ y /∑(zpi)  where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) All of these were inspired by (and adapted from) John Fox's polychor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  For combinations of continous, categorical, and dichotomous variables, see mixed.cor.If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   mixedCor which calls polychoric for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. (Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See draw.tetra and draw.cor for  illustrations.)This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.tetrachoric and polychoric can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See fa.extension for an application of this.The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).Examples include the lsat6 and lsat7 data sets in the bock data.The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. For finding one polychoric correlation from a table, see the Olsson example (below).polychoric replaces poly.mat and is recommended.   poly.mat was an alternative wrapper to the polycor function. biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (https://www.sapa-project.org/) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (https://www.icar-project.org/) for similar data.Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, rbi = r s* √(pq)/zp .As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).The 'ad hoc' polyserial correlation, rps is just r = r * sqrt(n-1)/n) σ y /∑(zpi)  where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) All of these were inspired by (and adapted from) John Fox's polychor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  For combinations of continous, categorical, and dichotomous variables, see mixed.cor.If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   mixedCor which calls polychoric for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. (Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See draw.tetra and draw.cor for  illustrations.)This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.tetrachoric and polychoric can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See fa.extension for an application of this.The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).Examples include the lsat6 and lsat7 data sets in the bock data.The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. For finding one polychoric correlation from a table, see the Olsson example (below).polychoric replaces poly.mat and is recommended.   poly.mat was an alternative wrapper to the polycor function. biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (https://www.sapa-project.org/) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (https://www.icar-project.org/) for similar data.Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, rbi = r s* √(pq)/zp .As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).The 'ad hoc' polyserial correlation, rps is just r = r * sqrt(n-1)/n) σ y /∑(zpi)  where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) All of these were inspired by (and adapted from) John Fox's polychor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  For combinations of continous, categorical, and dichotomous variables, see mixed.cor.If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   mixedCor which calls polychoric for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. (Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See draw.tetra and draw.cor for  illustrations.)This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.tetrachoric and polychoric can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See fa.extension for an application of this.The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).Examples include the lsat6 and lsat7 data sets in the bock data.The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. For finding one polychoric correlation from a table, see the Olsson example (below).polychoric replaces poly.mat and is recommended.   poly.mat was an alternative wrapper to the polycor function. biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (https://www.sapa-project.org/) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (https://www.icar-project.org/) for similar data.Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, rbi = r s* √(pq)/zp .As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).The 'ad hoc' polyserial correlation, rps is just r = r * sqrt(n-1)/n) σ y /∑(zpi)  where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) All of these were inspired by (and adapted from) John Fox's polychor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  For combinations of continous, categorical, and dichotomous variables, see mixed.cor.If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   mixedCor which calls polychoric for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. (Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
NA
When predicting criteria from a set of items formed into scales, the validity of the scale (that is, the correlations of the scale with each criteria) is a function of the average item validity (r_y), the average intercorrelation of the items in the scale (r_x), and the number of items in the scale (n).  The limit of validity is r_y/sqrt(r_x).  Criteria will differ in their predictability from a set of scales. These asymptotic values may be used to help the decision on which scales to develop further.  
Useful for those cases where the correlation matrix is improper (perhaps because of SAPA techniques).There are a number of data reduction techniques including principal components analysis (PCA) and factor analysis (EFA).  Both PC and FA attempt to approximate a given correlation or covariance matrix of rank n with matrix of lower rank (p).  nRn = nFk kFn' + U2 where k is much less than n.  For principal components, the item uniqueness is assumed to be zero and all elements of the correlation or covariance matrix are fitted. That is, nRn = nFk kFn'   The primary empirical difference between a components versus a factor model is the treatment of the variances for each item.  Philosophically, components are weighted composites of observed variables while in the factor model, variables are weighted composites of the factors. As the number of items increases, the difference between the two models gets smaller.  Factor loadings are the asymptotic component loadings as the number of items gets larger. For a n x n correlation matrix, the n principal components completely reproduce the correlation matrix.  However, if just the first k principal components are extracted, this is the best k dimensional approximation of the matrix.It is important to recognize that rotated principal components are not principal components (the axes associated with the eigen value decomposition) but are merely components.  To point this out, unrotated principal components are labelled as PCi, while rotated PCs are now labeled as RCi (for rotated components) and obliquely transformed components as TCi (for transformed components). (Thanks to Ulrike Gromping for this suggestion.)Rotations and transformations are either part of psych (Promax and cluster), of base R (varimax), or of GPArotation (simplimax, quartimax, oblimin, etc.).  Of the various rotation/transformation options, varimax, Varimax, quartimax, bentlerT, geominT, and bifactor do orthogonal rotations. Promax  transforms obliquely with a target matix equal to the varimax solution. oblimin, quartimin, simplimax,  bentlerQ,  geominQ and biquartimin are oblique transformations. Most of these are just calls to  the GPArotation package. The “cluster” option does a targeted rotation to a structure defined by the cluster representation of a varimax solution.  With the optional "keys" parameter, the "target" option will rotate to a target supplied as a keys matrix. (See target.rot.)The rotation matrix  (rot.mat) is returned from all of these options. This is the inverse of the Th (theta?) object returned by the GPArotation package.  The correlations of the factors may be found by Phi = Th' ThSome of the statistics reported are more appropriate for (maximum likelihood) factor analysis rather than principal components analysis, and are reported to allow comparisons with these other models. Although for items, it is typical to find component scores by scoring the salient items (using, e.g., scoreItems) component scores are found  by regression where the regression weights are R^(-1) lambda where lambda is the matrix of component loadings.   The regression approach is done  to be parallel with the factor analysis function fa.  The regression weights are found from the inverse of the correlation matrix times the component loadings.   This has the result that the component scores are standard scores (mean=0, sd = 1) of the standardized input.  A comparison to the scores from princomp shows this difference.  princomp does not, by default, standardize the data matrix, nor are the components themselves standardized.  The regression weights are found from the Structure matrix, not the Pattern matrix. If the scores are found with the covar option = TRUE, then the scores are not standardized but are just mean centered.  Jolliffe (2002) discusses why the interpretation of rotated components is complicated.   Rencher (1992) discourages the use of rotated components. The approach used here is consistent with the factor analytic tradition.  The correlations of the items with the component scores closely matches (as it should) the component loadings (as reported in the structure matrix). The output from the print.psych function displays the component loadings (from the pattern matrix), the h2 (communalities) the u2 (the uniquenesses), com (the complexity of the component loadings for that variable (see below).  In the case of an orthogonal solution, h2 is merely the row sum of the squared component loadings. But for an oblique solution, it is the row sum of the (squared) orthogonal component loadings (remember, that rotations or transformations do not change the communality).  This information is returned (invisibly) from the print function as the object Vaccounted.
Most of the psych functions produce too much output.  print.psych and summary.psych use generic methods for printing just the highlights.  To see what else is available,  ask for the structure of the particular object: (str(theobject) ).Alternatively, to get complete output, unclass(theobject) and then print it. This may be done by using the all=TRUE option.As an added feature, if the promax function is applied to a factanal loadings matrix, the normal output just provides the rotation matrix.  print.psych will provide the factor correlations. (Following a suggestion by John Fox and Uli Keller to the R-help list).  The alternative is to just use the Promax function directly on the factanal object.
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
Three vignettes (overview.pdf and psych_for_sem.pdf) are useful introductions to the package. They may be found as vignettes in R or may be downloaded from https://personality-project.org/r/psych/intro.pdf https://personality-project.org/r/psych/overview.pdf and https://personality-project.org/r/psych/psych_for_sem.pdf.  In addition, there are a number of "HowTo"s available at https://personality-project.org/r/ The more important functions in the package are for the analysis of multivariate data, with an emphasis upon those functions useful in scale construction of item composites. However, there are a number of very useful functions for basic data manipulation including  read.file, read.clipboard, describe,  pairs.panels, error.bars and error.dots) which are useful for basic data entry and descriptive analyses.When given a set of items from a personality inventory, one goal is to combine these into higher level item composites. This leads to several questions:1) What are the basic properties of the data?  describe reports basic summary statistics (mean, sd, median, mad, range,  minimum, maximum, skew, kurtosis, standard error) for vectors, columns of matrices, or data.frames. describeBy provides descriptive statistics, organized by one or more grouping variables. statsBy provides even more detail for data structured by groups including within and between correlation matrices, ICCs for group differences, as well as basic descriptive statistics organized by group.pairs.panels shows scatter plot matrices (SPLOMs) as well as histograms and the Pearson correlation for scales or items. error.bars will plot variable means with associated confidence intervals. errorCircles will plot confidence intervals for both the x and y coordinates.  corr.test will find the significance values for a matrix of correlations. error.dots creates a dot chart with confidence intervals.2) What is the most appropriate number of item composites to form? After finding  either standard Pearson correlations, or finding tetrachoric or polychoric correlations,  the dimensionality of the correlation matrix may be examined. The number of factors/components problem is a standard question of factor analysis, cluster analysis, or principal components analysis. Unfortunately, there is no agreed upon answer. The Very Simple Structure (VSS) set of procedures has been proposed as on answer to the question of the optimal number of factors.  Other procedures (VSS.scree,  VSS.parallel,  fa.parallel, and MAP)  also address this question.  nfactors combine several of these approaches into one convenient function. Unfortunately, there is no best answer to the problem.3) What are the best composites to form?  Although this may be answered using principal components  (principal), principal axis (factor.pa) or minimum residual (factor.minres)  factor analysis (all part of the fa function) and to show the results graphically (fa.diagram), it is sometimes more useful to address this question using cluster analytic techniques.  Previous versions of ICLUST (e.g., Revelle, 1979)  have been shown to be particularly successful at forming maximally consistent and independent item composites.  Graphical output from ICLUST.graph uses the Graphviz dot language and allows one to write files suitable for Graphviz.  If Rgraphviz is available, these graphs can be done in R.Graphical organizations of cluster and factor analysis output can be done using  cluster.plot which plots items by cluster/factor loadings and assigns items to that dimension with the highest loading.  4) How well does a particular item composite reflect a single construct?  This is a question of reliability and general factor saturation.  Multiple solutions for this problem result in (Cronbach's) alpha (alpha, scoreItems), (Revelle's) Beta (ICLUST), and (McDonald's) omega (both omega hierarchical and omega total). Additional reliability estimates may be found in the guttman function.This can also be examined by applying irt.fa Item Response Theory techniques using factor analysis of the tetrachoric or polychoric correlation matrices and converting the results into the standard two parameter parameterization of item difficulty and item discrimination.  Information functions for the items suggest where they are most effective.5) For some applications, data matrices are synthetically combined from sampling different items for different people.  So called Synthetic Aperture Personality Assessement (SAPA) techniques allow the formation of large correlation or covariance matrices even though no one person has taken all of the items. To analyze such data sets, it is easy to form item composites based upon the covariance matrix of the items, rather than original data set.  These matrices may then be analyzed using a number of functions (e.g., cluster.cor,   fa, ICLUST, principal, mat.regress, and factor2cluster.6) More typically, one has a raw data set to analyze. alpha will report several reliablity estimates as well as item-whole correlations for items forming a single scale, score.items will score data sets on multiple scales, reporting the scale scores, item-scale and scale-scale correlations, as well as coefficient alpha,  alpha-1 and G6+. Using a ‘keys’ matrix (created by make.keys or by hand), scales can have overlapping or independent items. score.multiple.choice scores multiple choice items or converts multiple choice items to dichtomous (0/1) format for other functions. 7) In addition to classical test theory (CTT) based scores of either totals or averages, 1 and 2 parameter IRT based scores may be found with scoreIrt.1pl, scoreIrt.2pl or more generally scoreIrt. Although highly correlated with CTT estimates, these scores take advantage of different item difficulties and are particularly appropriate for the problem of missing data. 8) If the data has a multilevel structure (e.g, items nested within time nested within subjects) the multilevel.reliability aka mlr function will estimate generalizability coefficients for data over subjects, subjects over time, etc. mlPlot will provide plots for each subject of items over time. mlArrange takes the conventional wide output format and converts it to the long format necessary for some multilevel functions. Other functions useful for multilevel data include statsBy and faBy.    An additional set of functions generate simulated data to meet certain structural properties. sim.anova produces data simulating a 3 way analysis of variance (ANOVA) or linear model with or with out repeated measures. sim.item creates simple structure data,  sim.circ will produce circumplex structured data,  sim.dichot produces circumplex or simple structured data for dichotomous items.  These item structures are useful for understanding the effects of skew, differential item endorsement on factor and cluster analytic soutions.  sim.structural will produce correlation matrices and data matrices to match general structural models. (See the vignette).When examining personality items, some people like to discuss them as representing items in a two dimensional space with a circumplex structure.  Tests of circumplex fit circ.tests have been developed.  When representing items in a circumplex, it is convenient to view them in polar coordinates. Additional functions for testing the difference between two independent or dependent correlation r.test, to find the phi or Yule coefficients from a two by table, or to find the confidence interval of a correlation coefficient.    Many data sets are included: bfi represents 25 personality items thought to represent five factors of personality, ability has 14 multiple choice iq items. sat.act has data on self reported test scores by age and gender. galton   Galton's data set of the heights of parents and their children. peas recreates the original Galton data set of the genetics of sweet peas.  heights andcubits provide even more Galton data, vegetables provides the Guilford preference matrix of vegetables.  cities provides airline miles between 11 US cities (demo data for multidimensional scaling).Partial Index:psych      A package for personality, psychometric, and psychological research.Useful data entry and descriptive statisticsData reduction through cluster and factor analysisFunctions for reliability analysis (some are listed above as well).Procedures particularly useful for Synthetic Aperture Personality AssessmentFunctions for generating simulated data sets Graphical functions (require Rgraphviz) – deprecated Graphical functions that do not require Rgraphviz Circular statistics (for circadian data analysis) Miscellaneous functionsFunctions that are under development and not recommended for casual use Data sets included in the psych or psychTools package A debugging function that may also be used as a demonstration of psych.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
NA
NA
Depending upon the input, one of four different tests of correlations is done.1) For a sample size n, find the t value for a single correlation where t = r* sqrt(n-2)/sqrt(1-r^2)  and se = sqrt((1-r^2)/(n-2)).2) For sample sizes of n and n2 (n2 = n if not specified) find the z of the difference between the z transformed correlations divided by the standard error of the difference of two z scores: t = (z_1  - z_2) * sqrt(1/((n_1)-3 + (n_2-3))).3) For sample size n, and correlations r12, r13 and r23 test for the difference of two dependent correlations (r12 vs r13).  4) For sample size n, test for the difference between two dependent correlations involving different variables.Consider the correlations from Steiger (1980), Table 1:  Because these all from the same subjects, any tests must be of dependent correlations.  For dependent correlations, it is necessary to specify at least 3 correlations (e.g., r12, r13, r23) For clarity, correlations may be specified by value.  If specified by location and if doing the test of dependent correlations, if three correlations are specified, they are assumed to be in the order r12, r13, r23.Consider the examples from Steiger: Case A: where Masculinity at time 1 (M1) correlates with Verbal Ability .5 (r12), femininity at time 1 (F1) correlates with Verbal ability  r13 =.4, and M1 correlates with F1 (r23= .1).  Then, given the correlations: r12 = .4, r13 = .5, and r23 = .1, t = -.89 for n =103, i.e.,r.test(n=103, r12=.4, r13=.5,r23=.1)  Case B: Test whether correlation between two variables (e.g., F and V) is the same over time (e.g. F1V1 = F2V2) r.test(n = 103, r12 = 0.5, r34 = 0.6, r23 = 0.5, r13 = 0.7, r14 = 0.5,  r24 = 0.8)
NA
NA
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
NA
Displaying multivariate profiles may be done by a series of lines (see, e.g., matplot), by colors (see, e.g., corPlot, or by radar or spider plots. Spiders are particularly suitable for showing data thought to have circumplex structure. To show just one variable as a function of several others, use radar.  To make multiple plots, use spider.  An additional option when comparing just a few y values is to do overlay plots.  Alternatively, set the plotting options to do several on one page.
When participants in a study are selected on one variable, that will reduce the variance of that variable and the resulting correlation.  Thorndike (1949) considered four cases of range restriction.  Others have continued this discussion but have changed the case numbers.  Can be used to find correlations in a restricted sample as well as the unrestricted sample.  Not the same as the correction to reliability for restriction of range.
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
reliability is basically just a wrapper for omegah, unidim and splitHalf. Revelle and Condon (2019) recommended reporting at least three reliability statistics for any scale, here we make it easy to do. If the hist option is set to true, histgrams and density plots of the split half values for each test are also shown. The output from reliability can be passed to error.dots to show the reliability statistics for multiple scales graphically.  It is however more useful to just call the plot.reliability function to show the basic information.For detailed analysis of any one scale, it is recommended to do a complete omega analysis, perhaps combined with a splitHalf analysis. The reliability function is just meant for the case where the user has multiple scales (perhaps scored using scoreItems) and then wants to get more complete reliability information for all of the scales.  Following a suggestion, the ability to not bother with keys and just do omega and split half and draw the results has been added.  Either specify that keys=NULL, or just specify the items to use. (See the first example.)plot.reliability provides a dot chart summary of the distributions of the split half values, as well as the estimates of omega and alpha and unidimensionality.  It can also be called by just issuing a plot command.
NA
Currently implemented for fa, principal,   omega, irt.fa, and fa.extension. 
Currently implemented for fa, principal,   omega, irt.fa, and fa.extension. 
The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using make.keys or may be entered as a keys.list directly.  Various estimates of scale reliability include “Cronbach's alpha", Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just s/n = n r/(1-nr)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the guttman function. Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (omega_hierchical and omega_total) should be encouraged. Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute="mean"), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute="none").  This is useful for findings means for scales for the SAPA project (see https://www.sapa-project.org/) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The "alpha observed" values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. Using the impute="none" option as well as asking for totals (totals="TRUE") will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:scores$scores[scores$missing >0] <- NA This is shown in the last example.Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.By default, scoreItems will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?scoreItems can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  scoreFast just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!scoreVeryFast is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using make.keys or may be entered as a keys.list directly.  Various estimates of scale reliability include “Cronbach's alpha", Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just s/n = n r/(1-nr)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the guttman function. Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (omega_hierchical and omega_total) should be encouraged. Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute="mean"), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute="none").  This is useful for findings means for scales for the SAPA project (see https://www.sapa-project.org/) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The "alpha observed" values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. Using the impute="none" option as well as asking for totals (totals="TRUE") will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:scores$scores[scores$missing >0] <- NA This is shown in the last example.Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.By default, scoreItems will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?scoreItems can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  scoreFast just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!scoreVeryFast is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
Not a very complicated function, but useful in the case that items need to be reversed prior to using IRT functions from the ltm or eRM packages. Most psych functions do not require reversing prior to analysis, but will do so within the function.
When examining multiple measures within subjects, it is sometimes useful to consider the variability of trial by trial observations in addition to the over all between trial variation.  The Mean Square of Successive Differences (mssd) and root mean square of successive differences (rmssd) is just σ^2 = Σ(x_i - x_{i+1})^2 /(n-lag)  Where n-lag is used because there are only n-lag cases.In the case of multiple subjects  (groups) with multiple observations per subject (group), specify the grouping variable will produce output for each group.   Similar functions are available in the matrixStats package. However, the varDiff function in that package is variance of the difference rather than the MeanSquare. This is just the variance and standard deviation applied to the result from the diff function.Perhaps useful when studying mood, the autoR function finds the autocorrelation for each item for the specified lag.  It also returns the rmssd (root means square successive difference). This is done by finding the correlation of the lag data. 
hese items were collected as part of the  SAPA  project  (https://www.sapa-project.org/)to develop online measures of ability (Revelle, Wilt and Rosenthal, 2009).  The score means are higher than national norms suggesting both self selection for people taking on line personality and ability tests and a self reporting bias in scores.See also the iq.items data set.  
How well does a model fit the data is the classic problem of all of statistics.  One fit statistic for scaling is the just the size of the residual matrix compared to the original estimates. 
Just a straightforward application of layout and barplot, with some tricks taken from pairs.panels.  The various options allow for correlation ellipses (1 and 2 sigma from the mean), lowess smooths, linear fits, density curves on the histograms, and the value of the correlation.  ellipse = TRUE implies smooth = TRUE.  The grid option provides a background grid to the scatterplot.If using grouping variables, will draw ellipses (defaults to 1 sd) around each centroid. This is useful when demonstrating Mahalanobis distances. Formula input allows specification of grouping variables as well.  )For plotting data for two groups, Mahalobnis differences between the groups may be shown by drawing an arrow between the two centroids.  This is a bit messy and it is useful to use pch="." in this case.
Just a straightforward application of layout and barplot, with some tricks taken from pairs.panels.  The various options allow for correlation ellipses (1 and 2 sigma from the mean), lowess smooths, linear fits, density curves on the histograms, and the value of the correlation.  ellipse = TRUE implies smooth = TRUE.  The grid option provides a background grid to the scatterplot.If using grouping variables, will draw ellipses (defaults to 1 sd) around each centroid. This is useful when demonstrating Mahalanobis distances. Formula input allows specification of grouping variables as well.  )For plotting data for two groups, Mahalobnis differences between the groups may be shown by drawing an arrow between the two centroids.  This is a bit messy and it is useful to use pch="." in this case.
Schmid Leiman orthogonalizations are typical in the ability domain, but are not seen as often in the non-cognitive personality domain.  S-L is one way of finding the loadings of items on the general factor for estimating omega. A typical example would be in the study of anxiety and depression.  A general neuroticism factor (g) accounts for much of the variance, but smaller group factors of tense anxiety, panic disorder, depression, etc. also need to be considerd.An alternative model is to consider hierarchical cluster analysis techniques such as ICLUST.Requires the GPArotation package.Although 3 factors are the minimum number necessary to define the solution uniquely, it is occasionally useful to allow for a two factor solution.  There are three possible options for this condition: setting the general factor loadings between the two lower order factors to be "equal" which will be the sqrt(oblique correlations between the factors) or to "first" or "second" in which case the general factor is equated with either the first or second group factor. A  message is issued suggesting that the model is not really well defined. A diagnostic tool for testing the appropriateness of a hierarchical model is p2 which is the percent of the common variance for each variable that is general factor variance.  In general, p2 should not have much variance. 
 Two artificial correlation matrices from Schmid and Leiman (1957). One real and one artificial covariance matrices from Chen et al. (2006).  Schmid: a 12 x 12 artificial correlation matrix created to show the Schmid-Leiman transformation.  schmid.leiman: A 12 x 12 matrix with communalities on the diagonal.  Treating this as a covariance matrix shows the 6 x 6 factor solution Chen: An 18 x 18 covariance matrix of health related quality of life items from Chen et al. (2006). Number of observations = 403.  The first item is a measure of the quality of life.  The remaining 17 items form four subfactors: The items are (a) Cognition subscale: “Have difficulty reasoningand solving problems?"  “React slowly to things that were said or done?"; “Become confused and start several actions at a time?"  “Forget where youput things or appointments?"; “Have difficulty concentrating?"  (b) Vitalitysubscale: “Feel tired?"  “Have enough energy to do the things you want?" (R) “Feel worn out?" ; “Feel full of pep?" (R). (c) Mental health subscale: “Feelcalm and peaceful?"(R)  “Feel downhearted and blue?"; “Feel veryhappy"(R) ; “Feel very nervous?" ; “Feel so down in the dumps nothing couldcheer you up?  (d) Disease worry subscale: “Were you afraid because of your health?"; “Were you frustrated about your health?"; “Was your health a worry in your life?" . West: A 16 x 16 artificial covariance matrix from Chen et al. (2006).
 Two artificial correlation matrices from Schmid and Leiman (1957). One real and one artificial covariance matrices from Chen et al. (2006).  Schmid: a 12 x 12 artificial correlation matrix created to show the Schmid-Leiman transformation.  schmid.leiman: A 12 x 12 matrix with communalities on the diagonal.  Treating this as a covariance matrix shows the 6 x 6 factor solution Chen: An 18 x 18 covariance matrix of health related quality of life items from Chen et al. (2006). Number of observations = 403.  The first item is a measure of the quality of life.  The remaining 17 items form four subfactors: The items are (a) Cognition subscale: “Have difficulty reasoningand solving problems?"  “React slowly to things that were said or done?"; “Become confused and start several actions at a time?"  “Forget where youput things or appointments?"; “Have difficulty concentrating?"  (b) Vitalitysubscale: “Feel tired?"  “Have enough energy to do the things you want?" (R) “Feel worn out?" ; “Feel full of pep?" (R). (c) Mental health subscale: “Feelcalm and peaceful?"(R)  “Feel downhearted and blue?"; “Feel veryhappy"(R) ; “Feel very nervous?" ; “Feel so down in the dumps nothing couldcheer you up?  (d) Disease worry subscale: “Were you afraid because of your health?"; “Were you frustrated about your health?"; “Was your health a worry in your life?" . West: A 16 x 16 artificial covariance matrix from Chen et al. (2006).
This function has been replaced with scoreItems (for multiple scales) and alpha for single scales.The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find the sum or the average scale score.  Various estimates of scale reliability include “Cronbach's alpha", and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Alpha is a poor estimate of the general factor saturation of a test (see Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a useful statistic to report. 
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using make.keys or may be entered as a keys.list directly.  Various estimates of scale reliability include “Cronbach's alpha", Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just s/n = n r/(1-nr)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the guttman function. Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (omega_hierchical and omega_total) should be encouraged. Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute="mean"), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute="none").  This is useful for findings means for scales for the SAPA project (see https://www.sapa-project.org/) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The "alpha observed" values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. Using the impute="none" option as well as asking for totals (totals="TRUE") will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:scores$scores[scores$missing >0] <- NA This is shown in the last example.Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.By default, scoreItems will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?scoreItems can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  scoreFast just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!scoreVeryFast is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
Basically combines score.items with a conversion from multiple choice to right/wrong.The item-whole correlation is inflated because of item overlap.The example data set is taken from the Synthetic Aperture Personality Assessment personality and ability test at https://www.sapa-project.org/.
These  are three of the functions used in the SAPA (https://www.sapa-project.org/) procedures to form synthetic correlation matrices.  Given any correlation matrix of items, it is easy to find the correlation matrix of scales made up of those items. This can also be done from the original data matrix or from the correlation matrix using scoreItems which is probably preferred unless the keys are overlapping.  It is important to remember with SAPA data, that scale correlations should be found from the item correlations, not the raw data.In the case of overlapping keys, (items being scored on multiple scales), scoreOverlap will adjust for this overlap by replacing the overlapping covariances (which are variances when overlapping) with the corresponding best estimate of an item's “true" variance using either the average correlation or the smc estimate for that item.  This parallels the operation done when finding alpha reliability.  This is similar to ideas suggested by Cureton (1966) and Bashaw and Anderson (1966) but uses the smc or the average interitem correlation (default).A typical use in the SAPA project is to form item composites by clustering or factoring (see fa, ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple correlatin procedures using mat.regress.The original correlation is pre and post multiplied by the (transpose) of the keys matrix. If some correlations are missing from the original matrix this will lead to missing values (NA) for scale intercorrelations based upon those lower level correlations. If impute=TRUE (the default), a warning is issued and the correlations are imputed based upon the average correlations of the non-missing elements of each scale. Because the alpha estimate of reliability is based upon the correlations of the items rather than upon the covariances, this estimate of alpha is sometimes called “standardized alpha".  If the raw items are available, it is useful to compare standardized alpha with the raw alpha found using scoreItems.  They will differ substantially only if the items differ a great deal in their variances.   scoreOverlap answers an important question when developing scales and related subscales, or when comparing alternative versions of scales.  For by removing the effect of item overlap, it gives a better estimate the relationship between the latent variables estimated by the observed sum (mean) scores.scoreBy finds the within subject correlations after preprocessing with statsBy.  This is useful if doing an ESM study with multiple occasions for each subject.   It also makes it possible to find the correlations for subsets of subjects. See the example.  Note that it likely that for ESM data with a high level of missingness that the correlation matrices will not be positive-semi-definite. This can lead to composite score correlations that exceed 1.  Smoothing will resolve this problem.scoreBy  is useful when examining multi-level models where we want to examine the correlations within subjects (e.g., for ESM data) or within groups of subjects (when examining the stability of correlational structures by subgroups).  For both cases the data must be processed first by statsBy.  To find the variances of the scales it is necessary to use the cor="cov" option in statsBy.  
The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using make.keys or may be entered as a keys.list directly.  Various estimates of scale reliability include “Cronbach's alpha", Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just s/n = n r/(1-nr)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the guttman function. Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (omega_hierchical and omega_total) should be encouraged. Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute="mean"), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute="none").  This is useful for findings means for scales for the SAPA project (see https://www.sapa-project.org/) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The "alpha observed" values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. Using the impute="none" option as well as asking for totals (totals="TRUE") will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:scores$scores[scores$missing >0] <- NA This is shown in the last example.Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.By default, scoreItems will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?scoreItems can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  scoreFast just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!scoreVeryFast is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
Although there are more elegant ways of finding subject scores given a set of item locations (difficulties) and discriminations, simply finding that value of theta θ that best fits the equation P(x|θ) = 1/(1+exp(β(δ - θ) ) for a score vector X, and location δ and discrimination β provides more information than just total scores.  With complete data, total scores and irt estimates are almost perfectly correlated.  However, the irt estimates provide much more information in the case of missing data.The bounds parameter sets the lower and upper limits to the estimate.  This is relevant for the case of a subject who gives just the lowest score on every item, or just the top score on every item. Formerly (prior to 1.6.12) this was done by  estimating these taial scores  by finding the probability of missing every item taken, converting this to a quantile score based upon the normal distribution, and then assigning a z value equivalent to 1/2 of that quantile.  Similarly, if a person gets all the items they take correct, their score is defined as the quantile of the z equivalent to the probability of getting all of the items correct, and then moving up the distribution half way.  If these estimates exceed either the upper or lower bounds, they are adjusted to those boundaries. As of 1.6.9, the procedure is very different.  We now assume that all items are bounded with one passed item that is easier than all items given, and one failed item that is harder than any item given.  This produces much cleaner results.There are several more elegant packages in R that provide Full Information Maximum Likeliood IRT based estimates. In particular, the MIRT package seems especially good.  The ltm package give equivalent estimates to MIRT for dichotomous data but produces unstable estimates for polytomous data and should be avoided.   Although the scoreIrt estimates are are not FIML based they seem to correlated with  the MIRT estiamtes with values exceeding .99.  Indeed, based upon very limited simulations there are some small hints that the solutions match the true score estimates  slightly better than do the MIRT estimates.  scoreIrt seems to do a good job of recovering the basic structure.If trying to use item parameters from a different data set (e.g. some standardization sample), specify the stats as a data frame with the first column representing the item discriminations, and the next columns the item difficulties. See the last example.The two wrapper functions scoreIrt.1pl and scoreIrt.2pl are very fast and are meant for scoring one or many scales at a time with a one factor model (scoreIrt.2pl) or just Rasch like scoring.  Just specify the scoring direction for a number of scales  (scoreIrt.1pl) or just items to score  for a number of scales scoreIrt.2pl.  scoreIrt.2pl will then apply irt.fa to the items for each scale separately, and then find the 2pl scores. The keys.list is a list of items to score for each scale.  Preceding the item name with a negative sign will reverse score that item (relevant for scoreIrt.1pl.  Alternatively, a keys matrix can be created using make.keys.  The keys matrix is a matrix of 1s, 0s, and -1s reflecting whether an item should be scored or not scored for a particular factor.  See scoreItems or make.keys for details.  The default case is to score all items with absolute discriminations > cut.If one wants to score scales taking advantage of differences in item location but not do a full IRT analysis, then find the item difficulties from the raw data using irt.tau or combine this information with a scoring keys matrix (see scoreItems and make.keys and create quasi-IRT statistics using irt.stats.like.   This is the equivalent of doing a quasi-Rasch model, in that all items are assumed to be equally discriminating.  In this case, tau values may be found first (using irt.tau or just found before doing the scoring.  This is all done for you inside of scoreIrt.1pl. Such irt based scores are particularly useful if finding scales based upon massively missing data (e.g., the SAPA data sets).  Even without doing the full irt analysis, we can take into account different item difficulties. David Condon has added a very nice function to do 2PL analysis for a number of scales at one time.  scoreIrt.2pl takes the raw data file and a list of items to score for each of multiple scales.  These are then factored (currently just one factor for each scale) and the loadings and difficulties are used for scoring.  There are conventionally two different metrics and models that are used.  The logistic metric and model and the normal metric and model. These are chosen using the mod parameter.irt.se finds the standard errors for scores with a particular value.  These are based upon the information curves calculated by irt.fa and are not based upon the particular score of a particular subject.
The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using make.keys or may be entered as a keys.list directly.  Various estimates of scale reliability include “Cronbach's alpha", Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just s/n = n r/(1-nr)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the guttman function. Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (omega_hierchical and omega_total) should be encouraged. Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute="mean"), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute="none").  This is useful for findings means for scales for the SAPA project (see https://www.sapa-project.org/) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The "alpha observed" values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. Using the impute="none" option as well as asking for totals (totals="TRUE") will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:scores$scores[scores$missing >0] <- NA This is shown in the last example.Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.By default, scoreItems will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?scoreItems can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  scoreFast just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!scoreVeryFast is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
These  are three of the functions used in the SAPA (https://www.sapa-project.org/) procedures to form synthetic correlation matrices.  Given any correlation matrix of items, it is easy to find the correlation matrix of scales made up of those items. This can also be done from the original data matrix or from the correlation matrix using scoreItems which is probably preferred unless the keys are overlapping.  It is important to remember with SAPA data, that scale correlations should be found from the item correlations, not the raw data.In the case of overlapping keys, (items being scored on multiple scales), scoreOverlap will adjust for this overlap by replacing the overlapping covariances (which are variances when overlapping) with the corresponding best estimate of an item's “true" variance using either the average correlation or the smc estimate for that item.  This parallels the operation done when finding alpha reliability.  This is similar to ideas suggested by Cureton (1966) and Bashaw and Anderson (1966) but uses the smc or the average interitem correlation (default).A typical use in the SAPA project is to form item composites by clustering or factoring (see fa, ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple correlatin procedures using mat.regress.The original correlation is pre and post multiplied by the (transpose) of the keys matrix. If some correlations are missing from the original matrix this will lead to missing values (NA) for scale intercorrelations based upon those lower level correlations. If impute=TRUE (the default), a warning is issued and the correlations are imputed based upon the average correlations of the non-missing elements of each scale. Because the alpha estimate of reliability is based upon the correlations of the items rather than upon the covariances, this estimate of alpha is sometimes called “standardized alpha".  If the raw items are available, it is useful to compare standardized alpha with the raw alpha found using scoreItems.  They will differ substantially only if the items differ a great deal in their variances.   scoreOverlap answers an important question when developing scales and related subscales, or when comparing alternative versions of scales.  For by removing the effect of item overlap, it gives a better estimate the relationship between the latent variables estimated by the observed sum (mean) scores.scoreBy finds the within subject correlations after preprocessing with statsBy.  This is useful if doing an ESM study with multiple occasions for each subject.   It also makes it possible to find the correlations for subsets of subjects. See the example.  Note that it likely that for ESM data with a high level of missingness that the correlation matrices will not be positive-semi-definite. This can lead to composite score correlations that exceed 1.  Smoothing will resolve this problem.scoreBy  is useful when examining multi-level models where we want to examine the correlations within subjects (e.g., for ESM data) or within groups of subjects (when examining the stability of correlational structures by subgroups).  For both cases the data must be processed first by statsBy.  To find the variances of the scales it is necessary to use the cor="cov" option in statsBy.  
The process of finding sum or average scores for a set of scales given a larger set of items is a typical problem in applied psychometrics and in psychometric research.  Although the structure of scales can be determined from the item intercorrelations, to find scale means, variances, and do further analyses, it is typical to find scores based upon the sum or the average item score.  For some strange reason, personality scale scores are typically given as totals, but attitude scores as averages.  The default for scoreItems is the average as it would seem to make more sense to report scale scores in the metric of the item.  When scoring more than one scale, it is convenient to have a list of the items on each scale and the direction to score the items.  This may be converted to a keys.matrix using make.keys or may be entered as a keys.list directly.  Various estimates of scale reliability include “Cronbach's alpha", Guttman's Lambda 6, and the average interitem correlation.  For k = number of items in a scale, and av.r = average correlation between items in the scale, alpha = k * av.r/(1+ (k-1)*av.r).  Thus, alpha is an increasing function of test length as well as the test homeogeneity.  Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation.alpha (Cronbach, 1951) is the same as Guttman's  lambda3 (Guttman, 1945) and may be found byLambda 3 = (n)/(n-1)(1-tr(Vx)/(Vx)  = (n)/(n-1)(Vx-tr(Vx)/Vx  = alphaPerhaps because it is so easy to calculate and is available in most commercial programs, alpha is without doubt the most frequently reported measure of internal consistency reliability. Alpha is the mean of all possible spit half reliabilities (corrected for test length).  For a unifactorial test, it is a reasonable estimate of the first factor saturation, although if the test has any microstructure (i.e., if it is “lumpy") coefficients beta (Revelle, 1979; see ICLUST) and omega_hierchical (see omega)  (McDonald, 1999; Revelle and Zinbarg, 2009) are more appropriate estimates of the general factor saturation.  omega_total (see omega) is a better estimate of the reliability of the total test.  Guttman's Lambda 6 (G6) considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and islamada 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The squared multiple correlation is a lower bound for the item communality and as the number of items increases, becomes a better estimate.G6 is also sensitive to lumpyness in the test and should not be taken as a measure of unifactorial structure.  For lumpy tests, it will be greater than alpha.  For tests with equal item loadings, alpha > G6, but if the loadings are unequal or if there is a general factor, G6 > alpha. Although it is normal when scoring just a single scale to calculate G6 from just those items within the scale, logically it is appropriate to estimate an item reliability from all items available.  This is done here and is labeled as G6* to identify the subtle difference.Alpha and G6* are both positive functions of the number of items in a test as well as the average intercorrelation of the items in the test.  When calculated from the item variances and total test variance, as is done here, raw alpha is sensitive to differences in the item variances. Standardized alpha is based upon the correlations rather than the covariances.  alpha is a generalization of an earlier estimate of reliability for tests with dichotomous items developed by Kuder and Richardson, known as KR20, and a shortcut approximation, KR21. (See Revelle, in prep; Revelle and Condon, in press.).A useful index is the ratio of reliable variance to unreliable variance and is known as the Signal/Noise ratio.  This is just s/n = n r/(1-nr)  (Cronbach and Gleser, 1964; Revelle and Condon (in press)).Standard errors for unstandardized alpha are reported using the formula from Duhachek and Iacobucci (2005).More complete reliability analyses of a single scale can be done using the omega function which finds omega_hierchical and omega_total based upon a hierarchical factor analysis.  Alternative estimates of the Greatest Lower Bound for the reliability are found in the guttman function. Alpha is a poor estimate of the general factor saturation of a test (see Revelle and Zinbarg, 2009; Zinbarg et al., 2005) for it can seriously overestimate the size of a general factor, and a better but not perfect estimate of total test reliability because it underestimates total reliability. None the less, it is a common statistic to report. In general, the use of alpha should be discouraged and the use of more appropriate estimates (omega_hierchical and omega_total) should be encouraged. Correlations between scales are attenuated by a lack of reliability.  Correcting correlations for reliability (by dividing by the square roots of the reliabilities of each scale) sometimes help show structure.  This is done in the scale intercorrelation matrix with raw correlations below the diagonal and unattenuated correlation above the diagonal.There are several alternative ways to treat missing values.  By default, missing values are replaced with the corresponding median value for that item.  Means can be used instead (impute="mean"), or subjects with missing data can just be dropped (missing = FALSE).  For data with a great deal of missingness, yet another option is to just find the average of the available responses (impute="none").  This is useful for findings means for scales for the SAPA project (see https://www.sapa-project.org/) where most scales are estimated from random sub samples of the items from the scale. In this case, the alpha reliabilities are seriously overinflated because they are based upon the total number of items in each scale.  The "alpha observed" values are based upon the average number of items answered in each scale using the standard form for alpha a function of inter-item correlation and number of items. Using the impute="none" option as well as asking for totals (totals="TRUE") will be done, although a warning will be issued because scores will now reflect the number of items responded to much more than the actual pattern of responses.The number of missing responses for each person for each scale is reported in the missing object.  One possibility is to drop scores just for those scales with missing responses.  This may be done adding the code:scores$scores[scores$missing >0] <- NA This is shown in the last example.Note that the default for scoreItems is to impute missing items with their median, but the default for scoreFAst is to not impute but must return the scale scores based upon the mean or total value for the items scored.By default, scoreItems will drop those items with no variance. This changes the reliability calculations (number of items is reduced), and it mean that those items are not used in finding scores.  Using the delete=FALSE option, these variables will not be dropped and their scores will be found.  Various warnings are issued about the SMC not being correct.  What do you you expect when there is no variance for an item?scoreItems can be applied to correlation matrices to find just the reliability statistics.  This will be done automatically if the items matrix is symmetric.  scoreFast just finds the scores (with or without imputation) and does not report other statistics.  It is much faster!scoreVeryFast is even more stripped down, no imputation, just scores based upon the observed data. No statistics.
Although meant for finding correlation weighted scores using the weights from bestScales, it also possible to use alternative weight matrices, such as those returned by the coefficients in lm.   
Among the many ways to choose the optimal number of factors is the scree test.  A better function to show the scree as well as compare it to randomly parallel solutions is found found in fa.parallel
Solves a tedious problem that can be done directly but that is sometimes awkward.  Will either replace specified values with NA or will recode to values within a range.
Finds the standard deviation of a vector, matrix, or data.frame.  Returns NA if no cases.Just an adaptation of the stats:sd function to return the functionality found in R < 2.7.0 or R >= 2.8.0Because this problem seems to have been fixed, SD will be removed eventually.
The easiest way to prepare keys for scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl  is to specify a keys.list.  This is just a list specifying the name of the scales to be scores and the direction of the items to be used.In earlier versions (prior to 1.6.9) keys were formed as a matrix of -1, 0, and 1s for all the items using make.keys.  This is no longer necessary, but make.keys is kept for compatibility with earlier versions.There are three ways to create keys for the scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl functions. One is to laboriously do it in a spreadsheet and then copy them into R.  The other is to just specify them by item number in a list. make.keys allows one to specify items by name or by location or a mixture of both.keys2list reverses the make.keys process and returns a list of scoring keys with the item names for each  item to be keyed.  If sign=FALSE, this is just a list of the items to be scored. (Useful for scoreIrt.2plselectFromKeys will strip the signs from a keys.list and create a vector of item names (deleting duplicates) associated with those keys.  This is useful if using a keys.list to define scales and then just selecting those items that are in subset of the keys.list.  This is now done in the scoring functions in the interest of speed. Since these scoring functions scoreItems, scoreOverlap, scoreIrt.1pl, or scoreIrt.2pl can now (> version 1.6.9)  just take a keys.list as input, make.keys is not as important, but is kept for documentation purposes.To address items by name it is necessary to specify item names, either by using the item.labels value, or by putting the name of the data file or the colnames of the data file to be scored into the first (nvars) position.If specifying by number (location), then nvars is the total number of items in the object to be scored, not just the number of items used.See the examples for the various options.Note that make.keys was revised in Sept, 2013 to allow for keying by name.It is also possible to do several make.keys operations and then combine them using superMatrix.  The alternative, if using the keys.list features is just to concatenate them. makePositiveKeys is useful for taking subsets of keys (e.g. from bestScales )and create separate keys for the positively and negatively keyed items.
The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.The figure is organized to show the appropriate paths between:The correlations between the X variables (if Rx is specified) The X variables and their latent factors  (if fx is specified) The latent X and the latent Y (if Phi is specified)  The latent Y and the observed Y (if fy is specified) The correlations between the Y variables (if Ry is specified)A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.lavaan.diagram may be called from the diagram function which also will call  fa.diagram, omega.diagram or  iclust.diagram, depending upon the class of the fit.Other diagram functions include fa.diagram, omega.diagram.  All of these functions use the various dia functions such as dia.rect, dia.ellipse, dia.arrow, dia.curve, dia.curved.arrow, and dia.shape.
The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.The figure is organized to show the appropriate paths between:The correlations between the X variables (if Rx is specified) The X variables and their latent factors  (if fx is specified) The latent X and the latent Y (if Phi is specified)  The latent Y and the observed Y (if fy is specified) The correlations between the Y variables (if Ry is specified)A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.lavaan.diagram may be called from the diagram function which also will call  fa.diagram, omega.diagram or  iclust.diagram, depending upon the class of the fit.Other diagram functions include fa.diagram, omega.diagram.  All of these functions use the various dia functions such as dia.rect, dia.ellipse, dia.arrow, dia.curve, dia.curved.arrow, and dia.shape.
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
Although it is more common to calculate multiple regression and canonical correlations from the raw data, it is,  of course, possible to do so from a matrix of correlations or covariances.  In this case, the input to the function is a square covariance or correlation matrix, as well as the column numbers (or names) of the x (predictor),  y (criterion) variables, and if desired z (covariates). The function will find the correlations if given raw data.Input is either the set of y variables and the set of x variables, this can be written in the standard formula style of lm (see last example).  In this case, pairwise  or higher  interactions (product terms) may also be specified.  By default, when finding product terms,  the predictive variables are zero centered (Cohen, Cohen, West and Aiken, 2003), although this option can be turned off (zero=FALSE) to match the results of lm or the results discussed in Hayes (2013).  Covariates to be removed are specified by a negative sign in the formula input or by using the z variable.  Note that when specifying covariates, the regressions are done as if the regressions were done on the partialled variables.  This means that the degrees of freedom and the R2 reflect the regressions of the partialled variables. (See the last example.)  If using covariates, should they be removed from the dependent as well as the independent variables (part = FALSE, the default which means partial correlations  or just the independent variables (part=TRUE or part aka semi-partial correlations).  The difference between part correlations and partial correlations is whether the variance of the covariate is removed from both the DV and IVs (a partial correlation) or from just the IVs (a part correlation).  The slopes of the regressions remain the same, but the amount of variance in the DV (and thus the standard errors) is larger when using part correlations.  Using partial correlations for the covariates is equivalent to using the covariate in the regression when interpreting the effects of the other variables. The output is a set of multiple correlations, one for each dependent variable in the y set, as well as the set of canonical correlations.An additional output is the R2 found using Cohen's set correlation (Cohen, 1982).  This is a measure of how much variance and the x and y set share.Cohen (1982) introduced the set correlation, a multivariate generalization of the multiple correlation to measure the overall relationship between two sets of variables. It is an application of canoncial correlation (Hotelling, 1936) and 1 - ∏(1-ρ_i^2) where ρ_i^2 is the squared canonical correlation.  Set correlation is the amount of shared variance (R2) between two sets of variables.  With the addition of a third, covariate set, set correlation will find multivariate R2, as well as partial and semi partial R2.  (The semi and bipartial options are not yet implemented.) Details on set correlation may be found in Cohen (1982), Cohen (1988) and  Cohen, Cohen, Aiken and West (2003). R2 between two sets is just R2 = 1- |R| /(|Ry| * |Rx|) where R is the  complete correlation matrix of the x and y variables and Rx and Ry are the two sets involved.Unfortunately, the R2 is sensitive to one of the canonical correlations being very high.  An alternative, T2, is the proportion of additive variance and is the average of the squared canonicals.  (Cohen et al., 2003), see also Cramer and Nicewander (1979).  This  average, because it includes some very small canonical correlations, will tend to be too small.  Cohen et al. admonition is appropriate: "In the final analysis, however, analysts must be guided by their substantive and methodological conceptions of the problem at hand in their choice of a measure of association." ( p613). Yet another measure of the association between two sets is just the simple, unweighted correlation between the two sets. That is, Ruw=1Rxy1' / (sqrt(1Ryy1'* 1Rxx1')) where Rxy is the matrix of correlations between the two sets.  This is just  the simple (unweighted) sums of the correlations in each matrix. This technique exemplifies the robust beauty of linear models and  is particularly appropriate in the case of one dimension in both x and y, and will be a drastic underestimate in the case of items where the betas differ in sign. When finding the unweighted correlations, as is done in alpha, items are flipped so that they all are positively signed.  A typical use in the SAPA project is to form item composites by clustering or factoring (see  fa,ICLUST, principal), extract the clusters from these results (factor2cluster), and then form the composite correlation matrix using cluster.cor.  The variables in this reduced matrix may then be used in multiple R procedures using setCor.Although the overall matrix can have missing correlations, the correlations in the subset of the matrix used for prediction must exist.If the number of observations is entered, then the conventional confidence intervals, statistical significance, and shrinkage estimates are  reported.If the input is rectangular (not square), correlations or covariances are found from the data.The print function reports t and p values for the beta weights, the summary function just reports the beta weights.The Variance Inflation Factor is reported but should be taken with the normal cautions of interpretation discussed by Guide and Ketokivi.  That is to say, VIF > 10 is not a magic cuttoff to define colinearity.  It is merely 1/(1-smc(R(x)).The Guide and Ketokivi article is well worth reading for all who want to use various regression models. crossValidation can be used to take the results from setCor or bestScales and apply the weights to a different data set.matPlot can be used to plot the crossValidation values (just a call to matplot with the xaxis given labels). matPlot has been improved to draw legends and to allow for specifications of the line types.setCorLookup will sort the beta weights and report them with item contents if given a dictionary. matReg is primarily a helper function for mediate but is a general multiple regression function given a covariance matrix and the specified x,  y and z variables. Its output includes betas, se, t, p and R2.  The call includes m for mediation variables, but these are only used to adjust the degrees of freedom.matReg does not work on data matrices, nor does it take formula input.  It is really just a helper function for  mediate
fa.lookup and lookup are simple helper functions to summarize correlation matrices or factor loading matrices.  bestItems will sort the specified column (criteria) of x on the basis of the (absolute) value of the column.  The return as a default is just the rowname of the variable with those absolute values > cut.   If there is a dictionary of item content and item names, then include the contents as a two column (or more) matrix with rownames corresponding to the item name and then as many fields as desired for item content. (See the example dictionary bfi.dictionary).lookup is used by bestItems and will find values in c1 of y that match those in x.  It returns those rows of y of that match x.Suppose that you have a "dictionary" of the many variables in a study but you want to consider a small subset of them in a data set x.  Then, you can find the entries in the dictionary corresponding to x by lookup(rownames(x),y)  If the column is not specified, then it will match by rownames(y). fa.lookup is used when examining the output of a factor analysis and one wants the corresponding variable names and contents. The returned object may then be printed in LaTex by using the df2latex function with the char option set to TRUE.fa.lookup will work with output from fa, pca or omega.  For omega output, the items are sorted by the non-general factor loadings.Similarly, given a correlation matrix, r, of the x variables, if you want to find the items that most correlate with another item or scale, and then show the contents of that item from the dictionary, bestItems(r,c1=column number or name of x, contents = y)item.lookup combines the output from a factor analysis fa with simple descriptive statistics (a data frame of means) with a dictionary.  Items are grouped by factor loadings > cut, and then sorted by item mean.  This allows a better understanding of how a scale works, in terms of the meaning of the item endorsements. lookupItems searches a dictionary for all items that have a certain content.  The rownames of the returned object are the item numbers which can then be used in other functions to find statistics (e.g. omega) of a scale with those items.   If an scales by items correlation matrix is given, then the item correlation with that scale are also shown. 
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are the core set of functions.  Others are documented in other help files.sim simulates one (fx) or two (fx and fy) factor structures where both fx and fy respresent factor loadings of variables.  The use of fy is particularly appropriate for simulating sem models.  This is better documentated in the help for sim.structural.Perhaps the easist simulation to understand is just sim.  A factor model (fx) and perhaps fy with intercorrelations between the two factor sets of Phi.  This will produce a correlation matrix R = fx' phi fy.  Factors can differ in their mean values by specifying mu.  The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
A simple simulation for teaching about ANOVA, regression and reliability.  A variety of demonstrations of the relation between anova and lm can be shown.The default is to produce categorical IVs (factors).  For more than two levels of an IV, this will show the difference between the linear model and anova in terms of the comparisons made.The within vector can be used to add congenerically equivalent dependent variables. These will have intercorrelations (reliabilities) of r and means as specified as values of within.To demonstrate the effect of centered versus non-centering, make factors = center=FALSE. The default is to center the IVs. By not centering them, the lower order effects will  be incorrect given the higher order interaction terms. 
Many personality and cognitive tests have a hierarchical factor structure.  For demonstration purposes, it is useful to be able to create such matrices, either with population values, or sample values. Given a matrix of item factor loadings (fload) and of loadings of these factors on a general factor (gload), we create a population correlation matrix by using the general factor law (R = F' theta F where theta = g'g).  The default is to return population correlation matrices. Sample correlation matrices are generated if n > 0.  Raw data are returned if raw = TRUE.The default values for gload and fload create a data matrix discussed by Jensen and Weng, 1994.Although written to create hierarchical structures, if the gload matrix is all 0, then a non-hierarchical structure will be generated.Yet another model is that of Godfrey H. Thomson (1916) who suggested that independent bonds could produce the same factor structure as a g factor model. This is simulated in sim.bonds.  Compare the omega solutions for a sim.hierarchical with a sim.bonds model. Both produce reasonable values of omega, although the one was generated without a general factor.
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
When constructing examples for reliability analysis, it is convenient to simulate congeneric data structures.  These are the most simple of item structures, having just one factor. Mainly used for a discussion of reliability theory as well as factor score estimates. The implied covariance matrix is just pattern %*% t(pattern). 
Given the measurement model, fx and the structure model Phi, the model is  f %*% Phi %*%  t(f).   Reliability is f %*% t(f). f φ f' and the reliability for each test is the items communality or just the diag of the model. If creating a correlation matrix, (uniq=NULL) then the diagonal is set to 1, otherwise the diagonal is diag(model) + uniq and the resulting structure is a covariance matrix.A special case of a structural model are one factor models such as parallel tests, tau equivalent tests, and congeneric tests.  These may be created by letting the structure matrix = 1 and then defining a vector of factor loadings. Alternatively, sim.congeneric will do the same. The general case is to use simCor aka sim.correlation which will create data sampled from a specified correlation matrix for a particular sample size. If desired, it will just return the sample correlation matrix.  With data=TRUE, it will return the sample data as well.  It uses an eigen value decomposition of the original matrix times a matrix of random normal deviates (code adapted from the mvnorm function of Brian Ripley's MASS package).  These resulting scores may be transformed using a number of transforms (see the skew option) or made into dichotomous variables (see quant option) for all or a select set (vars option) of the variables.  
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Many personality and cognitive tests have a hierarchical factor structure.  For demonstration purposes, it is useful to be able to create such matrices, either with population values, or sample values. Given a matrix of item factor loadings (fload) and of loadings of these factors on a general factor (gload), we create a population correlation matrix by using the general factor law (R = F' theta F where theta = g'g).  The default is to return population correlation matrices. Sample correlation matrices are generated if n > 0.  Raw data are returned if raw = TRUE.The default values for gload and fload create a data matrix discussed by Jensen and Weng, 1994.Although written to create hierarchical structures, if the gload matrix is all 0, then a non-hierarchical structure will be generated.Yet another model is that of Godfrey H. Thomson (1916) who suggested that independent bonds could produce the same factor structure as a g factor model. This is simulated in sim.bonds.  Compare the omega solutions for a sim.hierarchical with a sim.bonds model. Both produce reasonable values of omega, although the one was generated without a general factor.
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are the core set of functions.  Others are documented in other help files.sim simulates one (fx) or two (fx and fy) factor structures where both fx and fy respresent factor loadings of variables.  The use of fy is particularly appropriate for simulating sem models.  This is better documentated in the help for sim.structural.Perhaps the easist simulation to understand is just sim.  A factor model (fx) and perhaps fy with intercorrelations between the two factor sets of Phi.  This will produce a correlation matrix R = fx' phi fy.  Factors can differ in their mean values by specifying mu.  The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
The basic concepts of the independence of within group and between group correlations is discussed very clearly by Pedhazur (1997) as well as by Bliese (2009).  sim.multi generates within subject data to model the traditional two level structure of multilevel data.This is meant to show how within subject data measured over ntrials can vary independently within and between subjects.  Furthermore, several variables can correlate within subjects show a person by person factor structure.Factor scores for n.obs subjects are created for nfact factors with loadings on nvar  variables.  A simple structure model is assumed, so that the loadings on nvar/fact are set to loading for each factor, the others are set to 0. Factors are allowed to correlate phi between subjects and phi.i for each subject.  (Which can be different for each subject). Scores can change over time with a slope of beta.i and can vary diurnally as a function of sine and cosine of time (24 hours/day converted to radians).  Error is added to every trial and can be related across trials with a lag of 1.  Thus, if we set AR1=1, then the errors at time t = error + error at t -1. This will lead to auto correlations of about .5. (See autoR ).  sim.multilevel  merely simulates pooled correlations (mixtures of between group and within group correlations) to allow for a better understanding of the problems inherent in multi-level modeling.  Data (wg) are created with a particular within group structure (rwg).  Independent data (bg) are also created with a between group structure (rbg). Note that although there are ncases rows to this data matrix, there are only ngroups independent cases.  That is, every ngroups case is a repeat. The resulting data frame (xy) is a weighted sum of the wg and bg.  This is the inverse procedure for estimating estimating rwg and rbg from an observed rxy which is done by the statsBy function.  r_{xy} = η_{x_{within}} * η_{y_{within}} * r_{xy_{within}} + η_{x_{between}} * η_{y_{between}} * r_{xy_{between}} 
The basic concepts of the independence of within group and between group correlations is discussed very clearly by Pedhazur (1997) as well as by Bliese (2009).  sim.multi generates within subject data to model the traditional two level structure of multilevel data.This is meant to show how within subject data measured over ntrials can vary independently within and between subjects.  Furthermore, several variables can correlate within subjects show a person by person factor structure.Factor scores for n.obs subjects are created for nfact factors with loadings on nvar  variables.  A simple structure model is assumed, so that the loadings on nvar/fact are set to loading for each factor, the others are set to 0. Factors are allowed to correlate phi between subjects and phi.i for each subject.  (Which can be different for each subject). Scores can change over time with a slope of beta.i and can vary diurnally as a function of sine and cosine of time (24 hours/day converted to radians).  Error is added to every trial and can be related across trials with a lag of 1.  Thus, if we set AR1=1, then the errors at time t = error + error at t -1. This will lead to auto correlations of about .5. (See autoR ).  sim.multilevel  merely simulates pooled correlations (mixtures of between group and within group correlations) to allow for a better understanding of the problems inherent in multi-level modeling.  Data (wg) are created with a particular within group structure (rwg).  Independent data (bg) are also created with a between group structure (rbg). Note that although there are ncases rows to this data matrix, there are only ngroups independent cases.  That is, every ngroups case is a repeat. The resulting data frame (xy) is a weighted sum of the wg and bg.  This is the inverse procedure for estimating estimating rwg and rbg from an observed rxy which is done by the statsBy function.  r_{xy} = η_{x_{within}} * η_{y_{within}} * r_{xy_{within}} + η_{x_{between}} * η_{y_{between}} * r_{xy_{between}} 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are a miscellaneous set of functions that will be documented in other help files eventually.The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
Simulation of data structures is a very useful tool in psychometric research and teaching.  By knowing “truth" it is possible to see how well various algorithms can capture it.  For a much longer discussion of the use of simulation in psychometrics, see the accompany vignettes.  The simulations documented here are the core set of functions.  Others are documented in other help files.sim simulates one (fx) or two (fx and fy) factor structures where both fx and fy respresent factor loadings of variables.  The use of fy is particularly appropriate for simulating sem models.  This is better documentated in the help for sim.structural.Perhaps the easist simulation to understand is just sim.  A factor model (fx) and perhaps fy with intercorrelations between the two factor sets of Phi.  This will produce a correlation matrix R = fx' phi fy.  Factors can differ in their mean values by specifying mu.  The default values for sim.structure is to generate a 4 factor, 12 variable data set with a simplex structure between the factors. This, and the simplex of items (sim.simplex) can also be converted in a STARS model with an autoregressive component (alpha) and a stable trait component (lambda). Two data structures that are particular challenges to exploratory factor analysis are the simplex structure and the presence of minor factors.  Simplex structures sim.simplex will typically occur in developmental or learning contexts and have a correlation structure of r between adjacent variables and r^n for variables n apart.  Although just one latent variable (r) needs to be estimated, the structure will have nvar-1 factors.  An alternative version of the simplex is the State-Trait-Auto Regressive Structure (STARS) which has both a simplex state structure, with autoregressive path alpha and a trait structure with path lambda. This simulated in  sim.simplex by specifying a non-zero lambda value.Many simulations of factor structures assume that except for the major factors, all residuals are normally distributed around 0.  An alternative, and perhaps more realistic situation, is that the there are a few major (big) factors and many minor (small) factors.  The challenge is thus to identify the major factors. sim.minor generates such structures.  The structures generated can be thought of as havinga a major factor structure with some small correlated residuals. To make these simulations complete, the possibility of a general factor is considered.  For simplicity, sim.minor allows one to specify a set of loadings to be sampled from for g, fmajor and fminor.  Alternatively, it is possible to specify the complete factor matrix.Another structure worth considering is direct modeling of a general factor with several group factors.  This is done using sim.general.Although coefficient ω is a very useful indicator of the general factor saturation of a unifactorial test (one with perhaps several sub factors), it has problems with the case of multiple, independent factors.  In this situation, one of the factors is labelled as “general” and  the omega estimate is too large.  This situation may be explored using the sim.omega function with general left as NULL.  If there is a general factor, then results from sim.omega suggests that omega estimated either from EFA or from SEM does a pretty good job of identifying it but that the EFA approach using Schmid-Leiman transformation is somewhat more robust than the SEM approach. The four irt simulations, sim.rasch, sim.irt, sim.npl and sim.npn, simulate dichotomous items following the Item Response model.  sim.irt just calls either sim.npl (for logistic models) or sim.npn (for normal models) depending upon the specification of the model. The logistic model is P(i,j) = γ + (ζ-γ)/(1+ exp(α(δ-θ))) where γ is the lower asymptote or guesssing parameter, ζ is the upper asymptote (normally 1), α is item discrimination and δ is item difficulty.  For the 1 Paramater Logistic (Rasch) model, gamma=0, zeta=1, alpha=1 and item difficulty is the only free parameter to specify.For the 2PL and 2PN models, a = α and  d = δ are specified. For the 3PL or 3PN models, items also differ in their guessing parameter c =γ. For the 4PL and 4PN models, the upper asymptote, z= ζ is also specified.  (Graphics of these may be seen in the demonstrations for the logistic function.)The normal model (irt.npn calculates the probability using pnorm instead of the logistic function used in irt.npl, but the meaning of the parameters are otherwise the same.  With the a = α parameter = 1.702 in the logistic model the two models are practically identical.In parallel to the dichotomous IRT simulations are the poly versions which simulate polytomous item models.  They have the additional parameter of how many categories to simulate.  In addition, the sim.poly.ideal functions will simulate an ideal point or unfolding model in which the response probability varies by the distance from each subject's ideal point.  Some have claimed that this is a more appropriate model of the responses to personality questionnaires.  It will lead to simplex like structures which may be fit by a two factor model.  The middle items form one factor, the extreme a bipolar factor.By default, the theta parameter is created in each function as normally distributed with mean mu=0  and sd=1.  In the case where you want to specify the theta to be equivalent from another simulation or fixed for a particular experimental condition, either take the theta object from the output of a previous simulation, or create it using whatever properties are desired. The previous functions all assume one latent trait.  Alternatively, we can simulate dichotomous or polytomous items with a particular structure using the sim.poly.mat function.  This takes as input the population correlation matrix, the population marginals, and the sample size.  It returns categorical items with the specified structure.Other simulation functions in psych are:sim.structure  A function to combine a measurement and structural model into one data matrix.  Useful for understanding structural equation models.  Combined with structure.diagram to see the proposed structure.  sim.congeneric   A function to create congeneric items/tests for demonstrating classical test theory. This is just a special case of sim.structure.sim.hierarchical  A function to create data with a hierarchical (bifactor) structure.  sim.item      A function to create items that either have a simple structure or a circumplex structure.sim.circ    Create data with a circumplex structure.sim.dichot    Create dichotomous item data with a simple or circumplex structure.sim.minor   Create a factor structure for nvar variables defined by nfact major factors and nvar/2 “minor" factors for n observations.  Although the standard factor model assumes that K major factors (K << nvar) will account for the correlations among the variablesR = FF' + U^2where R is of rank P and F is a P x K matrix of factor coefficients and U is a diagonal matrix of uniquenesses.  However, in many cases, particularly when working with items, there are many small factors (sometimes referred to as correlated residuals) that need to be considered as well.  This leads to a data structure such that R = FF' + MM' + U^2where R is a P x P matrix of correlations, F is a  P x K factor loading matrix,  M is a P x P/2 matrix of minor factor loadings, and U is a diagonal matrix (P x P) of uniquenesses.  Such a correlation matrix will have a poor χ^2 value in terms of goodness of fit if just the K factors are extracted, even though for all intents and purposes, it is well fit.  sim.minor will generate such data sets with big factors with loadings of .6 to .8 and small factors with loadings of -.2 to .2.  These may both be adjusted.sim.parallel Create a number of simulated data sets using sim.minor to show how parallel analysis works.  The general observation is that with the presence of minor factors, parallel analysis is probably best done with component eigen values rather than factor eigen values, even when using the factor model. sim.anova    Simulate a 3 way balanced ANOVA or linear model, with or without repeated measures. Useful for teaching research  methods and generating teaching examples. sim.multilevel  To understand some of the basic concepts of multilevel modeling, it is useful to create multilevel structures.  The correlations of aggregated data is sometimes called an 'ecological correlation'.  That group level and individual level correlations are independent makes such inferences problematic.  This simulation allows for demonstrations that correlations within groups do not imply, nor are implied by, correlations between group means. 
This simulation was originally developed to compare the effect of skew on the measurement of affect (see Rafaeli and Revelle, 2005).  It has been extended to allow for a general simulation of affect or personality items with either a simple structure or a circumplex structure.  Items can be continuous normally distributed, or broken down into n categories (e.g, -2, -1, 0, 1, 2).  Items can be distorted by limiting them to these ranges, even though the items have a mean of (e.g., 1).  The addition of item.dichot allows for testing structures with dichotomous items of different difficulty (endorsement) levels.  Two factor data with either simple structure or circumplex structure are generated for two sets of items, one giving a score of 1 for all items greater than the low (easy) value, one giving a 1 for all items greater than the high (hard) value. The default values for low and high are 0.  That is, all items are assumed to have a 50 percent endorsement rate.  To examine the effect of item difficulty, low could be   -1, high  1. This will lead to item endorsements of .84 for the easy and .16 for the hard.  Within each set of difficulties, the first 1/4 are assigned to the first factor factor, the second to the second factor, the third to the first factor (but with negative loadings) and the fourth to the second factor (but with negative loadings). It is useful to compare the results of sim.item with sim.hierarchical.  sim.item will produce a general factor that runs through all the items as well as two orthogonal factors.  This produces a data set that is hard to represent with standard rotation techniques.  Extracting 3 factors without rotation and then rotating the 2nd and 3rd factors reproduces the correct solution.  But simple oblique rotation of 3 factors, or an omega analysis do not capture the underlying structure.  See the last example.Yet another structure that might be appealing is fully complex data in three dimensions.  That is, rather than having items representing the circumference of a circle, items can be structured to represent equally spaced three dimensional points on a sphere. sim.spherical produces such data.
Given the measurement model, fx and the structure model Phi, the model is  f %*% Phi %*%  t(f).   Reliability is f %*% t(f). f φ f' and the reliability for each test is the items communality or just the diag of the model. If creating a correlation matrix, (uniq=NULL) then the diagonal is set to 1, otherwise the diagonal is diag(model) + uniq and the resulting structure is a covariance matrix.A special case of a structural model are one factor models such as parallel tests, tau equivalent tests, and congeneric tests.  These may be created by letting the structure matrix = 1 and then defining a vector of factor loadings. Alternatively, sim.congeneric will do the same. The general case is to use simCor aka sim.correlation which will create data sampled from a specified correlation matrix for a particular sample size. If desired, it will just return the sample correlation matrix.  With data=TRUE, it will return the sample data as well.  It uses an eigen value decomposition of the original matrix times a matrix of random normal deviates (code adapted from the mvnorm function of Brian Ripley's MASS package).  These resulting scores may be transformed using a number of transforms (see the skew option) or made into dichotomous variables (see quant option) for all or a select set (vars option) of the variables.  
Given the measurement model, fx and the structure model Phi, the model is  f %*% Phi %*%  t(f).   Reliability is f %*% t(f). f φ f' and the reliability for each test is the items communality or just the diag of the model. If creating a correlation matrix, (uniq=NULL) then the diagonal is set to 1, otherwise the diagonal is diag(model) + uniq and the resulting structure is a covariance matrix.A special case of a structural model are one factor models such as parallel tests, tau equivalent tests, and congeneric tests.  These may be created by letting the structure matrix = 1 and then defining a vector of factor loadings. Alternatively, sim.congeneric will do the same. The general case is to use simCor aka sim.correlation which will create data sampled from a specified correlation matrix for a particular sample size. If desired, it will just return the sample correlation matrix.  With data=TRUE, it will return the sample data as well.  It uses an eigen value decomposition of the original matrix times a matrix of random normal deviates (code adapted from the mvnorm function of Brian Ripley's MASS package).  These resulting scores may be transformed using a number of transforms (see the skew option) or made into dichotomous variables (see quant option) for all or a select set (vars option) of the variables.  
NA
Given the measurement model, fx and the structure model Phi, the model is  f %*% Phi %*%  t(f).   Reliability is f %*% t(f). f φ f' and the reliability for each test is the items communality or just the diag of the model. If creating a correlation matrix, (uniq=NULL) then the diagonal is set to 1, otherwise the diagonal is diag(model) + uniq and the resulting structure is a covariance matrix.A special case of a structural model are one factor models such as parallel tests, tau equivalent tests, and congeneric tests.  These may be created by letting the structure matrix = 1 and then defining a vector of factor loadings. Alternatively, sim.congeneric will do the same. The general case is to use simCor aka sim.correlation which will create data sampled from a specified correlation matrix for a particular sample size. If desired, it will just return the sample correlation matrix.  With data=TRUE, it will return the sample data as well.  It uses an eigen value decomposition of the original matrix times a matrix of random normal deviates (code adapted from the mvnorm function of Brian Ripley's MASS package).  These resulting scores may be transformed using a number of transforms (see the skew option) or made into dichotomous variables (see quant option) for all or a select set (vars option) of the variables.  
“A common model for representing psychological data is simple structure (Thurstone, 1947). According to one common interpretation, data are simple structured when items or scales have non-zero factor loadings on one and only one factor (Revelle & Rocklin, 1979). Despite the commonplace application of simple structure, some psychological models are defined by a lack of simple structure. Circumplexes (Guttman, 1954) are one kind of model in which simple structure is lacking.“A number of elementary requirements can be teased out of the idea of circumplex structure. First, circumplex structure implies minimally that variables are interrelated; random noise does not a circumplex make. Second, circumplex structure implies that the domain in question is optimally represented by two and only two dimensions. Third, circumplex structure implies that variables do not group or clump along the two axes, as in simple structure, but rather that there are always interstitial variables between any orthogonal pair of axes (Saucier, 1992). In the ideal case, this quality will be reflected in equal spacing of variables along the circumference of the circle (Gurtman, 1994; Wiggins, Steiger, & Gaelick, 1981). Fourth, circumplex structure implies that variables have a constant radius from the center of the circle, which implies that all variables have equal communality on the two circumplex dimensions (Fisher, 1997; Gurtman, 1994). Fifth, circumplex structure implies that all rotations are equally good representations of the domain (Conte & Plutchik, 1981; Larsen & Diener, 1992)." (Acton and Revelle, 2004)Acton and Revelle reviewed the effectiveness of 10 tests of circumplex structure and found that four did a particularly good job of discriminating circumplex structure from simple structure, or circumplexes from ellipsoidal structures. Unfortunately, their work was done in Pascal and is not easily available. Here we release R code to do the four most useful tests:The Gap test of equal spacingFisher's test of equality of axesA test of indifference to RotationA test of equal Variance of squared factor loadings across arbitrary rotations.Included in this set of functions are simple procedure to generate circumplex structured or simple structured data, the four test statistics, and a simple simulation showing the effectiveness of the four procedures.circ.sim.plot compares the four tests for circumplex, ellipsoid and simple structure data as function of the number of variables and the sample size.  What one can see from this plot is that although no one test is sufficient to discriminate these alternative structures, the set of four tests does a very good job of doing so.  When testing a particular data set for structure, comparing the results of all four tests to the simulated data will give a good indication of the structural properties of the data.
given a matrix or data.frame x, find the skew or kurtosis for each column (for skew and kurtosis) or the multivariate skew and kurtosis in the case of mardia.As of version 1.2.3,when finding the skew and the kurtosis, there are three different options available.  These match the choices available in skewness and kurtosis found in the e1071 package (see Joanes and Gill (1998) for the advantages of each one). If we define m_r = [sum(X- mx)^r]/n then Type 1 finds skewness and kurtosis by g_1 = m_3/(m_2)^{3/2}  and g_2 = m_4/(m_2)^2 -3.  Type 2 is G1 = g1 * √{n *(n-1)}/(n-2) and G2 = (n-1)*[(n+1)g2 +6]/((n-2)(n-3)).  Type 3 is b1 = [(n-1)/n]^{3/2} m_3/m_2^{3/2} and b2 =  [(n-1)/n]^{3/2} m_4/m_2^2). For consistency with e1071 and with the Joanes and Gill, the types are now defined as above.However, from revision 1.0.93 to 1.2.3, kurtosi by default gives an unbiased estimate of the kurtosis (DeCarlo, 1997). Prior versions used a different equation which produced a biased estimate.  (See the kurtosis function in the e1071 package for the distinction between these two formulae.  The default, type 1 gave what is called type 2 in e1071.  The other is their type 3.)  For comparison with previous releases, specifying type = 2 will give the old estimate.  These type numbers are now changed.  
NA
Displaying multivariate profiles may be done by a series of lines (see, e.g., matplot), by colors (see, e.g., corPlot, or by radar or spider plots. Spiders are particularly suitable for showing data thought to have circumplex structure. To show just one variable as a function of several others, use radar.  To make multiple plots, use spider.  An additional option when comparing just a few y values is to do overlay plots.  Alternatively, set the plotting options to do several on one page.
Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using splitHalf for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's β (1979) using splitHalf. The companion function, omega calculates omega hierarchical (ω_h)  and omega total (ω_t). Guttman's first estimate λ_1 assumes that all the variance of an item is error:lambda 1= 1-tr(Vx)/VxThis is a clear underestimate.The second bound, λ_2, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' , then λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x)Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  Guttman's 3rd lower bound, λ_3, also  modifies λ_1 and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's α. λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx)  = ((n)/(n-1))(Vx-tr(Vx)/Vx  = αThis is just replacing the diagonal elements with the average off diagonal elements.  λ_2 ≥ λ_3 with  λ_2 > λ_3 if the covariances are not identical.λ_3 and λ_2 are both corrections to λ_1 and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)where p_h = sum(σ^2h,  h = 0, 1, 2, ... r-1  andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978).  Clearly μ_0 = λ_3 = α and  μ_1 = λ_2.  μ_r ≥ μ_{r-1} ≥ … μ_1 ≥ μ_0, although the series does not improve much after the first two steps.Guttman's fourth lower bound, λ_4 was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If \vec{X} is split into  two parts, \vec{X}_a and \vec{X}_b, with correlation r_{ab} thenλ 4 = 4rab/(Va + Vb + 2rabVaVb)which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. λ_5, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariancesλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.) Although superior to λ_1, λ_5 underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in λ_3:λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and isλ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The smc is found from all the items.  A modification to Guttman λ_6, λ_6* reported by the score.items function is to find the smc from the entire pool of items given, not just the items on the selected scale.  Guttman's λ_4 is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using splitHalf to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.The algorithms that had been tried before included:a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.The brute force and the sampling procedures seem to provide more stable and larger estimates. Yet another procedure, implemented in splitHalf is actually form all possible (for n items <= 16) or sample 10,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples.Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, λ_4. This considers the test as set of items and examines how best to partition the items into splits. The other two, glb.fa and glb.algebraic, are alternative ways of weighting the diagonal of the matrix. glb.fa estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/VxThis estimate will differ slightly from that found by  glb.algebraic, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. Compared to glb.algebraic, glb.fa seems to have less (positive) bias for smallish sample sizes (n < 500) but larger for large (> 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  glb.algebraic seems to converge on the population value while glb.fa has a positive bias. 
Multilevel data are endemic in psychological research. In multilevel data, observations are taken on subjects who are nested within some higher level grouping variable.  The data might be experimental (participants are nested within experimental conditions) or observational (students are nested within classrooms, students are nested within college majors.) To analyze this type of data, one uses random effects models or mixed effect models, or more generally, multilevel models.  There are at least two very powerful packages (nlme and multilevel) which allow for complex analysis of hierarchical (multilevel) data structures.  statsBy is a much simpler function to give some of the basic descriptive statistics for two level models.  It is meant to supplement true multilevel modeling.For a group variable (group) for a data.frame or matrix (data), basic descriptive statistics (mean, sd, n) as well as within group correlations (cors=TRUE) are found for each group.  The amount of variance associated with the grouping variable compared to the total variance is the type 1 IntraClass Correlation (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))where npr is the average number of cases within each group. The reliability of the group differences may be found by the ICC2 which reflects how different the means are with respect to the within group variability.  ICC2 = (MSb-MSw)/MSb.Because the mean square between is sensitive to sample size, this estimate will also reflect sample size.Perhaps the most useful part of statsBy is that it decomposes the observed correlations between variables into two parts: the within group and the between group correlation. This follows the decomposition of an observed correlation into the pooled correlation within groups (rwg) and the weighted correlation of the means between groups  discussed by Pedazur (1997) and by Bliese in the multilevel package.  r_{xy} = eta_{x_{wg}} * eta_{y_{wg}} * r_{xy_{wg}}  +  eta_{x_{bg}} * eta_{y_{bg}} * r_{xy_{bg}}  where r_{xy} is the normal correlation which may be decomposed into a within group and between group correlations r_{xy_{wg}} and r_{xy_{bg}} and eta is the correlation of the data with the within group values, or the group means.It is important to realize that the within group and between group correlations are independent of each other.  That is to say, inferring from the 'ecological correlation' (between groups) to the lower level (within group) correlation is inappropriate.  However, these between group correlations are still very meaningful, if inferences are made at the higher level.  There are actually two ways of finding the within group correlations pooled across groups.  We can find the correlations within every group, weight these by the sample size and then report this pooled value (pooled).  This is found if the cors option is set to TRUE.  It is logically  equivalent to doing a sample size weighted meta-analytic correlation.  The other way, rwg, considers the covariances, variances, and thus correlations when each subject's scores are given as deviation score from the group mean.  If finding tetrachoric, polychoric, or mixed correlations, these two estimates will differ, for the pooled value is the weighted polychoric correlation, but the rwg is the Pearson correlation. If the weights parameter is specified, the pooled correlations are found by weighting the groups by the specified weight, rather than sample size.Confidence values and significance  of  r_{xy_{wg}}, pwg, reflect the pooled number of cases within groups, while  r_{xy_{bg}} , pbg, the number of groups. These are not corrected for multiple comparisons.withinBetween is an example data set of the mixture of within and between group correlations. sim.multilevel will generate simulated data with a multilevel structure.The statsBy.boot function will randomize the grouping variable ntrials times and find the statsBy output.  This can take a long time and will produce a great deal of output.  This output can then be summarized for relevant variables using the statsBy.boot.summary function specifying the variable of interest.  These two functions are useful in order to find if the mere act of grouping leads to large between group correlations.Consider the case of the relationship between various tests of ability when the data are grouped by level of education (statsBy(sat.act,"education")) or when affect data are analyzed within and between an affect manipulation (statsBy(flat,group="Film") ). Note in this latter example, that because subjects were randomly assigned to Film condition for the pretest, that the pretest ICC1s cluster around 0. faBy uses the output of statsBy to perform a factor analysis on the correlation matrix within each group. If the free parameter is FALSE, then each solution is rotated towards the group solution (as much as possible).  The output is a list of each factor solution, as well as a summary matrix of loadings and interfactor correlations for  all groups.
Multilevel data are endemic in psychological research. In multilevel data, observations are taken on subjects who are nested within some higher level grouping variable.  The data might be experimental (participants are nested within experimental conditions) or observational (students are nested within classrooms, students are nested within college majors.) To analyze this type of data, one uses random effects models or mixed effect models, or more generally, multilevel models.  There are at least two very powerful packages (nlme and multilevel) which allow for complex analysis of hierarchical (multilevel) data structures.  statsBy is a much simpler function to give some of the basic descriptive statistics for two level models.  It is meant to supplement true multilevel modeling.For a group variable (group) for a data.frame or matrix (data), basic descriptive statistics (mean, sd, n) as well as within group correlations (cors=TRUE) are found for each group.  The amount of variance associated with the grouping variable compared to the total variance is the type 1 IntraClass Correlation (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))where npr is the average number of cases within each group. The reliability of the group differences may be found by the ICC2 which reflects how different the means are with respect to the within group variability.  ICC2 = (MSb-MSw)/MSb.Because the mean square between is sensitive to sample size, this estimate will also reflect sample size.Perhaps the most useful part of statsBy is that it decomposes the observed correlations between variables into two parts: the within group and the between group correlation. This follows the decomposition of an observed correlation into the pooled correlation within groups (rwg) and the weighted correlation of the means between groups  discussed by Pedazur (1997) and by Bliese in the multilevel package.  r_{xy} = eta_{x_{wg}} * eta_{y_{wg}} * r_{xy_{wg}}  +  eta_{x_{bg}} * eta_{y_{bg}} * r_{xy_{bg}}  where r_{xy} is the normal correlation which may be decomposed into a within group and between group correlations r_{xy_{wg}} and r_{xy_{bg}} and eta is the correlation of the data with the within group values, or the group means.It is important to realize that the within group and between group correlations are independent of each other.  That is to say, inferring from the 'ecological correlation' (between groups) to the lower level (within group) correlation is inappropriate.  However, these between group correlations are still very meaningful, if inferences are made at the higher level.  There are actually two ways of finding the within group correlations pooled across groups.  We can find the correlations within every group, weight these by the sample size and then report this pooled value (pooled).  This is found if the cors option is set to TRUE.  It is logically  equivalent to doing a sample size weighted meta-analytic correlation.  The other way, rwg, considers the covariances, variances, and thus correlations when each subject's scores are given as deviation score from the group mean.  If finding tetrachoric, polychoric, or mixed correlations, these two estimates will differ, for the pooled value is the weighted polychoric correlation, but the rwg is the Pearson correlation. If the weights parameter is specified, the pooled correlations are found by weighting the groups by the specified weight, rather than sample size.Confidence values and significance  of  r_{xy_{wg}}, pwg, reflect the pooled number of cases within groups, while  r_{xy_{bg}} , pbg, the number of groups. These are not corrected for multiple comparisons.withinBetween is an example data set of the mixture of within and between group correlations. sim.multilevel will generate simulated data with a multilevel structure.The statsBy.boot function will randomize the grouping variable ntrials times and find the statsBy output.  This can take a long time and will produce a great deal of output.  This output can then be summarized for relevant variables using the statsBy.boot.summary function specifying the variable of interest.  These two functions are useful in order to find if the mere act of grouping leads to large between group correlations.Consider the case of the relationship between various tests of ability when the data are grouped by level of education (statsBy(sat.act,"education")) or when affect data are analyzed within and between an affect manipulation (statsBy(flat,group="Film") ). Note in this latter example, that because subjects were randomly assigned to Film condition for the pretest, that the pretest ICC1s cluster around 0. faBy uses the output of statsBy to perform a factor analysis on the correlation matrix within each group. If the free parameter is FALSE, then each solution is rotated towards the group solution (as much as possible).  The output is a list of each factor solution, as well as a summary matrix of loadings and interfactor correlations for  all groups.
Multilevel data are endemic in psychological research. In multilevel data, observations are taken on subjects who are nested within some higher level grouping variable.  The data might be experimental (participants are nested within experimental conditions) or observational (students are nested within classrooms, students are nested within college majors.) To analyze this type of data, one uses random effects models or mixed effect models, or more generally, multilevel models.  There are at least two very powerful packages (nlme and multilevel) which allow for complex analysis of hierarchical (multilevel) data structures.  statsBy is a much simpler function to give some of the basic descriptive statistics for two level models.  It is meant to supplement true multilevel modeling.For a group variable (group) for a data.frame or matrix (data), basic descriptive statistics (mean, sd, n) as well as within group correlations (cors=TRUE) are found for each group.  The amount of variance associated with the grouping variable compared to the total variance is the type 1 IntraClass Correlation (ICC1):ICC1 = (MSb-MSw)/(MSb + MSw*(npr-1))where npr is the average number of cases within each group. The reliability of the group differences may be found by the ICC2 which reflects how different the means are with respect to the within group variability.  ICC2 = (MSb-MSw)/MSb.Because the mean square between is sensitive to sample size, this estimate will also reflect sample size.Perhaps the most useful part of statsBy is that it decomposes the observed correlations between variables into two parts: the within group and the between group correlation. This follows the decomposition of an observed correlation into the pooled correlation within groups (rwg) and the weighted correlation of the means between groups  discussed by Pedazur (1997) and by Bliese in the multilevel package.  r_{xy} = eta_{x_{wg}} * eta_{y_{wg}} * r_{xy_{wg}}  +  eta_{x_{bg}} * eta_{y_{bg}} * r_{xy_{bg}}  where r_{xy} is the normal correlation which may be decomposed into a within group and between group correlations r_{xy_{wg}} and r_{xy_{bg}} and eta is the correlation of the data with the within group values, or the group means.It is important to realize that the within group and between group correlations are independent of each other.  That is to say, inferring from the 'ecological correlation' (between groups) to the lower level (within group) correlation is inappropriate.  However, these between group correlations are still very meaningful, if inferences are made at the higher level.  There are actually two ways of finding the within group correlations pooled across groups.  We can find the correlations within every group, weight these by the sample size and then report this pooled value (pooled).  This is found if the cors option is set to TRUE.  It is logically  equivalent to doing a sample size weighted meta-analytic correlation.  The other way, rwg, considers the covariances, variances, and thus correlations when each subject's scores are given as deviation score from the group mean.  If finding tetrachoric, polychoric, or mixed correlations, these two estimates will differ, for the pooled value is the weighted polychoric correlation, but the rwg is the Pearson correlation. If the weights parameter is specified, the pooled correlations are found by weighting the groups by the specified weight, rather than sample size.Confidence values and significance  of  r_{xy_{wg}}, pwg, reflect the pooled number of cases within groups, while  r_{xy_{bg}} , pbg, the number of groups. These are not corrected for multiple comparisons.withinBetween is an example data set of the mixture of within and between group correlations. sim.multilevel will generate simulated data with a multilevel structure.The statsBy.boot function will randomize the grouping variable ntrials times and find the statsBy output.  This can take a long time and will produce a great deal of output.  This output can then be summarized for relevant variables using the statsBy.boot.summary function specifying the variable of interest.  These two functions are useful in order to find if the mere act of grouping leads to large between group correlations.Consider the case of the relationship between various tests of ability when the data are grouped by level of education (statsBy(sat.act,"education")) or when affect data are analyzed within and between an affect manipulation (statsBy(flat,group="Film") ). Note in this latter example, that because subjects were randomly assigned to Film condition for the pretest, that the pretest ICC1s cluster around 0. faBy uses the output of statsBy to perform a factor analysis on the correlation matrix within each group. If the free parameter is FALSE, then each solution is rotated towards the group solution (as much as possible).  The output is a list of each factor solution, as well as a summary matrix of loadings and interfactor correlations for  all groups.
The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.The figure is organized to show the appropriate paths between:The correlations between the X variables (if Rx is specified) The X variables and their latent factors  (if fx is specified) The latent X and the latent Y (if Phi is specified)  The latent Y and the observed Y (if fy is specified) The correlations between the Y variables (if Ry is specified)A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.lavaan.diagram may be called from the diagram function which also will call  fa.diagram, omega.diagram or  iclust.diagram, depending upon the class of the fit.Other diagram functions include fa.diagram, omega.diagram.  All of these functions use the various dia functions such as dia.rect, dia.ellipse, dia.arrow, dia.curve, dia.curved.arrow, and dia.shape.
The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.The figure is organized to show the appropriate paths between:The correlations between the X variables (if Rx is specified) The X variables and their latent factors  (if fx is specified) The latent X and the latent Y (if Phi is specified)  The latent Y and the observed Y (if fy is specified) The correlations between the Y variables (if Ry is specified)A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.lavaan.diagram may be called from the diagram function which also will call  fa.diagram, omega.diagram or  iclust.diagram, depending upon the class of the fit.Other diagram functions include fa.diagram, omega.diagram.  All of these functions use the various dia functions such as dia.rect, dia.ellipse, dia.arrow, dia.curve, dia.curved.arrow, and dia.shape.
This is almost self explanatory.  See the examples.
The recommended function is structure.diagram which does not use Rgraphviz but which does not produce dot code either.  All three structure function return a matrix of commands suitable for using in the sem or lavaan packages.  (Specify errors=TRUE to get code that will run directly in the sem package.)The structure.graph output can be directed to an output file for post processing using the dot graphic language but requires that Rgraphviz is installed. lavaan.diagram will create sem, cfa, or mimic diagrams depending upon the lavaan input.sem.diagram and sem.graph convert the output from a simple CFA done with the sem package and draw them using structure.diagram or structure.graph.lavaan.diagram converts the output (fit) from a simple CFA done with the lavaan package and draws them using structure.diagram.The figure is organized to show the appropriate paths between:The correlations between the X variables (if Rx is specified) The X variables and their latent factors  (if fx is specified) The latent X and the latent Y (if Phi is specified)  The latent Y and the observed Y (if fy is specified) The correlations between the Y variables (if Ry is specified)A confirmatory factor model would specify just fx and Phi, a structural model would include fx, Phi, and fy.  The raw correlations could be shown by just including Rx and Ry.lavaan.diagram may be called from the diagram function which also will call  fa.diagram, omega.diagram or  iclust.diagram, depending upon the class of the fit.Other diagram functions include fa.diagram, omega.diagram.  All of these functions use the various dia functions such as dia.rect, dia.ellipse, dia.arrow, dia.curve, dia.curved.arrow, and dia.shape.
Most of the psych functions produce too much output.  print.psych and summary.psych use generic methods for printing just the highlights.  To see what else is available,  ask for the structure of the particular object: (str(theobject) ).Alternatively, to get complete output, unclass(theobject) and then print it. This may be done by using the all=TRUE option.As an added feature, if the promax function is applied to a factanal loadings matrix, the normal output just provides the rotation matrix.  print.psych will provide the factor correlations. (Following a suggestion by John Fox and Uli Keller to the R-help list).  The alternative is to just use the Promax function directly on the factanal object.
Several functions, e.g., sim.structural,structure.graph, make.keys use matrices that can be thought of as formed from a set of submatrices.  In particular, when using make.keys in order to score a set of items (scoreItems or scoreOverlap) or to form specified clusters (cluster.cor), it is convenient to define different sets of scoring keys for different sets of items and to combine these scoring keys into one super key.When developing scales and examining items using bestScales it is sometimes helpful to combine the matrix output from scoreOverlap with the original correlation matrix.  Thus, let x =  correlation of the scales from scoreOverlap, y = the correlations of the items used to form the scales, and xy = the correlation of the scales with the items.   
Several functions, e.g., sim.structural,structure.graph, make.keys use matrices that can be thought of as formed from a set of submatrices.  In particular, when using make.keys in order to score a set of items (scoreItems or scoreOverlap) or to form specified clusters (cluster.cor), it is convenient to define different sets of scoring keys for different sets of items and to combine these scoring keys into one super key.When developing scales and examining items using bestScales it is sometimes helpful to combine the matrix output from scoreOverlap with the original correlation matrix.  Thus, let x =  correlation of the scales from scoreOverlap, y = the correlations of the items used to form the scales, and xy = the correlation of the scales with the items.   
Several functions, e.g., sim.structural,structure.graph, make.keys use matrices that can be thought of as formed from a set of submatrices.  In particular, when using make.keys in order to score a set of items (scoreItems or scoreOverlap) or to form specified clusters (cluster.cor), it is convenient to define different sets of scoring keys for different sets of items and to combine these scoring keys into one super key.When developing scales and examining items using bestScales it is sometimes helpful to combine the matrix output from scoreOverlap with the original correlation matrix.  Thus, let x =  correlation of the scales from scoreOverlap, y = the correlations of the items used to form the scales, and xy = the correlation of the scales with the items.   
There are many ways of reporting how two groups differ.  Cohen's d statistic is just the differences of means expressed in terms of the pooled within group standard deviation.  This is insensitive to sample size.  r is the a universal measure of effect size that is a simple function of d, but is bounded -1 to 1.  The t statistic is merely d * sqrt(n)/2 and thus reflects sample size.   (M2- M1)/Spwhere Sp is the pooled standard deviation.  √{((n1-1)*s1^2 + (n2-1)* s2^2)/{N}  }  Cohens d uses N as the divisor for the pooled sums of squares.  Hedges g uses N-2.  Confidence intervals for Cohen's d may be found by converting the d to a t, finding the confidence intervals for t, and then converting those back to ds.  This take advantage of the uniroot function and the non-centrality parameter of the t distribution.The results of cohen.d may be displayed using the error.dots function.  This will include the labels provided in the dictionary.  In the case of finding the confidence interval (using cohen.d.ci for a comparison against 0 (the one sample case), specify n1.  This will yield a d = t/sqrt(n1)  whereas in the case of the difference between two samples, d = 2*t/sqrt(n) (for equal sample sizes n = n1+ n2) or d = t/sqrt(1/n1 + 1/n2)  for the case of unequal sample sizes.Since we find d and then convert this to t, using d2t,  the question is how to pool the variances. Until 7/14/21 I was using the total n to estimate the t and thus the p values.  In response to a query (see news), I switched to using the actual sample size ns (n1 and n2) and then finding t based upon the hedges g value.  This produces t values as reported by t.test with the var.equal = TRUE option.It is probably useful to comment that the various confidence intervals reported are based upon normal theory and should be interpreted cautiously.  cohen.d.by will find Cohen's d for groups for each subset of the data defined by group2.  The summary of the output produces a simplified listing of the d values for each variable for each group.  May be called directly from cohen.d by using formula input and specifying two grouping variables. d.robust follows Algina et al. 2005) to find trimmed means (trim =.2) and Winsorize variances (trim =.2).  Supposedly, this provides a more robust estimate of effect sizes.m2t reports Student's t.test for two groups given their means, standard deviations, and sample size.  This is convenient when checking statistics where those estimates are provided, but the raw data are not available.  By default, it gives the pooled estimate of variance, but if pooled is FALSE, it applies Welch's correction.The Mahalanobis Distance combines the individual ds and weight them by their unique contribution:  D = √{d' R^{-1}d}.By default, cohen.d will find the Mahalanobis distance between the two groups (if there is more than one DV.)  This requires finding the correlation of all of the DVs and can fail if that matrix is not invertible because some pairs do not exist.  Thus, setting MD=FALSE will prevent the Mahalanobis calculation.Marco del Giudice (2019) has a very helpful paper discussing how to interpret d and Md in terms of various overlap coefficients. These may be found by the use of the d2OVL (percent overlap for 1 distribution), d2OVL2 percent overlap of joint distributions, d2CL (the common language effect size), and d2U3 (proportion in higher group exceeding median of the lower group).OVL = 2 φ(-d/2)  is the proportion of overlap (and gets smaller the larger the d).where  Phi  is the cumulative density function of the normal distribution.OVL_2 = OVL/(2-OVL)The proportion of individuals in one group above the median of the other group is U3U_3 = φ(d).The Common Language Effect size CL = φ(d * √(2) )These last two get larger with (abs (d)).  For graphic displays of Cohen's d and Mahalanobis D, see the scatterHist examples, or the example from the psychTools::GERAS data set. 
NA
The original Galton (1888) of heights by cubits (arm length) is in tabular form. To show this as a correlation or as a scatter plot, it is useful to convert the table to a matrix or data frame of two columns.  This function may also be used to convert an item response pattern table into a data table.  e.g., the Bock data set bock.  
The original Galton (1888) of heights by cubits (arm length) is in tabular form. To show this as a correlation or as a scatter plot, it is useful to convert the table to a matrix or data frame of two columns.  This function may also be used to convert an item response pattern table into a data table.  e.g., the Bock data set bock.  
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
Tal-Or et al. (2010) examined the presumed effect of the media in two experimental studies.  These data are from study 2. '... perceptions regarding the influence of a news story about an expected shortage in sugar were manipulated indirectly, by manipulating the perceived exposure to the news story, and behavioral intentions resulting from the story were consequently measured." (p 801). 
Tal-Or et al. (2010) examined the presumed effect of the media in two experimental studies.  These data are from study 2. '... perceptions regarding the influence of a news story about an expected shortage in sugar were manipulated indirectly, by manipulating the perceived exposure to the news story, and behavioral intentions resulting from the story were consequently measured." (p 801). 
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's α  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using splitHalf for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's β (1979) using splitHalf. The companion function, omega calculates omega hierarchical (ω_h)  and omega total (ω_t). Guttman's first estimate λ_1 assumes that all the variance of an item is error:lambda 1= 1-tr(Vx)/VxThis is a clear underestimate.The second bound, λ_2, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' , then λ_2= λ_1 + sqrt(n *(n-1)C_2)/V_x)Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  Guttman's 3rd lower bound, λ_3, also  modifies λ_1 and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's α. λ 3 = ((n)/(n-1))(1-tr(Vx)/(Vx)  = ((n)/(n-1))(Vx-tr(Vx)/Vx  = αThis is just replacing the diagonal elements with the average off diagonal elements.  λ_2 ≥ λ_3 with  λ_2 > λ_3 if the covariances are not identical.λ_3 and λ_2 are both corrections to λ_1 and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) (1/(Vx))(po + p1 = (p2 + ... (pr1) + pr^.5 )^.5^ ... .5)where p_h = sum(σ^2h,  h = 0, 1, 2, ... r-1  andp_h = n/((n-1) σ^2h) tenberge and Zegers (1978).  Clearly μ_0 = λ_3 = α and  μ_1 = λ_2.  μ_r ≥ μ_{r-1} ≥ … μ_1 ≥ μ_0, although the series does not improve much after the first two steps.Guttman's fourth lower bound, λ_4 was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If \vec{X} is split into  two parts, \vec{X}_a and \vec{X}_b, with correlation r_{ab} thenλ 4 = 4rab/(Va + Vb + 2rabVaVb)which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. λ_5, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariancesλ_5 = λ_1 +2/sqrt(average(C_2)/V_X.) Although superior to λ_1, λ_5 underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in λ_3:λ 5+ = λ 1 + ((n/(n-1))2/sqrt(av covariance 12)/Vxλ_6,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, e_j^2,  and isλ 6 = 1 - sum(e^2)/Vx = 1-sum(1-r^2(smc))/Vx.The smc is found from all the items.  A modification to Guttman λ_6, λ_6* reported by the score.items function is to find the smc from the entire pool of items given, not just the items on the selected scale.  Guttman's λ_4 is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using splitHalf to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.The algorithms that had been tried before included:a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.The brute force and the sampling procedures seem to provide more stable and larger estimates. Yet another procedure, implemented in splitHalf is actually form all possible (for n items <= 16) or sample 10,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples.Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, λ_4. This considers the test as set of items and examines how best to partition the items into splits. The other two, glb.fa and glb.algebraic, are alternative ways of weighting the diagonal of the matrix. glb.fa estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by glb = 1 - sum(e^2)/Vx = 1-sum(1-h^2)/VxThis estimate will differ slightly from that found by  glb.algebraic, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. Compared to glb.algebraic, glb.fa seems to have less (positive) bias for smallish sample sizes (n < 500) but larger for large (> 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  glb.algebraic seems to converge on the population value while glb.fa has a positive bias. 
lowerCor prints out the lower off diagonal matrix rounded to digits with column names abbreviated to digits + 3 characters, but also returns the full and unrounded matrix.  By default, it uses pairwise deletion of variables.  It in turn callslowerMat which does the pretty printing.  It is important to remember to not call lowerCor when all you need is lowerMat!  cs is a direct copy of the Cs function in the Hmisc package by Frank Harrell.  Added to psych to avoid the overhead of the Hmisc package.
n.obs observations (0/1)  on nvar variables are simulated using either a logistic or normal theory model.  Then, a number of different scoring algorithms are applied and shown graphically.  Requires the ltm package to be installed to compare ltm scores.
When modifying the psych package, it is useful to make sure that adding some code does not break something else.  The test.psych function tests the major functions on various standard data sets.  It  also shows off a number of the capabilities of the psych package.Uses 5 standard data sets: USArrests               Violent Crime Rates by US State  (4 variables) attitude               The Chatterjee-Price Attitude Data    Harman23.cor$cov       Harman Example 2.3 8 physical measurements  Harman74.cor$cov        Harman Example 7.4  24 mental measurements ability.cov$cov        8 Ability and Intelligence Tests  It also uses the bfi and ability data sets from psych
There are many ways of measuring reliability. Test - Retest is one way.  If the time interval is very short (or immediate), this is known as a dependability correlation, if the time interval is longer, a stability coefficient.  In all cases, this is a correlation between two measures at different time points.  Given the multi-level nature of these data, it is possible to find variance components associated with individuals, time, item, and time by item, etc.  This leads to several different estimates of reliability (see multilevel.reliability for a discussion and references).It is also possible to find the subject reliability across time (this is the correlation across the items at time 1 with time 2 for each subject).  This is a sign of subject reliability (Wood et al, 2017).  Items can show differing amounts of test-retest reliability over time.  Unfortunately, the within person correlation has problems if people do not differ very much across items.  If all items are in the same keyed direction, and measuring the same construct, then the response profile for an individual is essentially flat. This implies that the even with almost perfect reproducibility, that the correlation can actually be negative.  The within person distance (d2) across items is just the mean of the squared differences for each item.  Although highly negatively correlated with the rqq score, this does distinguish between random responders (high dqq and low rqq) from consistent responders with lower variance (low dqq and low rqq). Several individual statistics are reported in the scores object.  These can be displayed by using pairs.panels for a graphic display of the relationship and ranges of the various measures.Although meant to decompose the variance for tests with items nested within tests, if just given two tests, the variance components for people and for time will also be shown.  The resulting variance ratio of people to total variance is the intraclass correlation between the two tests.  See also ICC for the more general case.
Tetrachoric correlations infer a latent Pearson correlation from a two x two table of frequencies with the assumption of bivariate normality.  The estimation procedure is two stage ML.  Cell frequencies for each pair of items are found. In the case of tetrachorics, cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE). The data typically will be a raw data matrix of responses to a questionnaire scored either true/false (tetrachoric) or with a limited number of responses (polychoric).  In both cases, the marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred)  latent  Pearson correlation that would produce the observed cell frequencies with the observed marginals.  (See draw.tetra and draw.cor for  illustrations.)This is a computationally intensive function which can be speeded up considerably by using multiple cores and using the parallel package.  The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. The greatest step up in speed is going from 1 cores to 2.  This is about a 50% savings.  Going to 4 cores seems to have about at 66% savings, and 8 a 75% savings.  The number of parallel processes defaults to 2 but can be modified by using the options command:  options("mc.cores"=4) will set the number of cores to 4.tetrachoric and polychoric can find non-symmetric correlation matrices of set of x variables (coluumns) and y variable (rows).  This is useful if extending a solution from a base set of items to a different set.  See fa.extension for an application of this.The tetrachoric correlation is used in a variety of contexts, one important one being in Item Response Theory (IRT) analyses of test scores, a second in the conversion of comorbity statistics to correlation coefficients.  It is in this second context that examples of the sensitivity of the coefficient to the cell frequencies becomes apparent:Consider the test data set from Kirk (1973) who reports the effectiveness of a ML algorithm for the tetrachoric correlation (see examples).Examples include the lsat6 and lsat7 data sets in the bock data.The polychoric function forms matrices of polychoric correlations by an local function (polyc) and will also report the tau values for each alternative.  Earlier versions used  John Fox's polychor function which has now been replaced by the polyc function. For finding one polychoric correlation from a table, see the Olsson example (below).polychoric replaces poly.mat and is recommended.   poly.mat was an alternative wrapper to the polycor function. biserial and polyserial correlations are the inferred latent correlations equivalent to the observed point-biserial and point-polyserial correlations (which are themselves just Pearson correlations).The polyserial function is meant to work with matrix or dataframe input and treats missing data by finding the pairwise Pearson r corrected by the overall (all observed cases) probability of response frequency.  This is particularly useful for SAPA procedures (https://www.sapa-project.org/) (Revelle et al. 2010, 2016, 2020)  with large amounts of missing data and no complete cases. See also the International Cognitive Ability Resource (https://www.icar-project.org/) for similar data.Ability tests and personality test matrices will typically have a cleaner structure when using tetrachoric or polychoric correlations than when using the normal Pearson correlation. However, if either alpha or omega is used to find the reliability, this will be an overestimate of the squared correlation of a latent variable with the observed variable.A biserial correlation (not to be confused with the point-biserial correlation which is just a Pearson correlation) is the latent correlation between x and y where y is continuous and x is dichotomous but assumed to represent an (unobserved) continuous normal variable. Let p = probability of x level 1, and q = 1 - p.  Let zp = the normal ordinate of the z score associated with p.  Then, rbi = r s* √(pq)/zp .As nicely discussed by MacCallum et al, 2002, if artificially dichotomizing at the mean, the point biserial will be .8 of the biserial (actually .798).  Similarly, the phi coefficient (a Pearson on dichotmous data will be  2[arcsin(rho)/pi) of the real value (rho).The 'ad hoc' polyserial correlation, rps is just r = r * sqrt(n-1)/n) σ y /∑(zpi)  where zpi are the ordinates of the normal curve at the normal equivalent of the cut point boundaries between the item responses. (Olsson, 1982) All of these were inspired by (and adapted from) John Fox's polychor package which should be used for precise ML estimates of the correlations.  See, in particular, the hetcor function in the polychor package.  The results from polychoric  match the polychor answers to at least 5 decimals when using correct=FALSE, and global = FALSE. Particularly for tetrachoric correlations from sets of data with missing data, the matrix will sometimes not be positive definite.  Various smoothing alternatives are possible, the one done here is to do an eigen value decomposition of the correlation matrix, set all negative eigen values to   10 * .Machine$double.eps, normalize the positive eigen values to sum to the number of variables, and then reconstitute the correlation matrix.   A warning is issued when this is done.For very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.  John Uebersax (2015)  makes the interesting point that both polychoric and tetrachoric correlations should be called latent correlations or latent continuous correlations because of the way they are found and not tetrachoric or polychoric which is the way they were found in the past. That is, what is the correlation between two latent variables that when artificially broken into two (tetrachoric) or more (polychoric) values produces the n x n table of observed frequencies.  For combinations of continous, categorical, and dichotomous variables, see mixed.cor.If using data with a variable number of response alternatives, it is necessary to use the global=FALSE option in polychoric.  (This is set automatically if this condition is detected).For relatively small samples with dichotomous data  if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores (the default if using a Mac). The solution seems to be to not use multi.cores  (e.g., options(mc.cores =1)   mixedCor which calls polychoric for the N = 4,000 with 145 items of the spi data set, on a Mac book Pro with a 2.4 GHz 8-core Intel i9,  1 core took 130 seconds, 2 cores, 68 seconds, 4 37, 8 22 and 16 22.8.  Just finding the polychoric correlations (spi[11:145]) for 4 cores took 34 and for 8 cores, 22 seconds. (Update in 2022: with an M1 max chip) mixedCor for all 145 variables: 1 core = 54, 2 cores 28, 4 core = 15.8 seconds, 8 cores= 8.9 seconds.  For just the polychorics 4 cores = 13.7, 8 cores = 7.4 .     As the number of categories increases, the computation time does as well.  For more than about 8 categories, the difference between a normal Pearson correlation and the polychoric is not very large*.  However, the number of categories (num.cat) may be set to large values if desired.  (Added for version 2.0.6).*A recent article by Foldnes and Gronneberg suggests using polychoric is appropriate for categorical data even if the number of categories is large.  This particularly the case for very non-normal distributions of the category frequencies.
Louis L. Thurstone was a pioneer in psychometric theory and measurement of attitudes, interests, and abilities.  Among his many contributions was a systematic analysis of the process of comparative judgment (thurstone, 1927).  He considered the case of asking subjects to successively compare pairs of objects. If the same subject does this repeatedly, or if  subjects act as random replicates of each other, their judgments can be thought of as sampled from a normal distribution of underlying (latent) scale  scores for each object, Thurstone  proposed that the comparison between the value of two objects could be represented as representing the differences of the average value for each object compared to the standard deviation of the differences between objects.  The basic model is that each item has a normal distribution of response strength and that choice represents the stronger of the two response strengths.  A justification for the normality assumption is that each decision represents the sum of many independent  inputs and thus, through the central limit theorem, is normally distributed. Thurstone considered five different sets of assumptions about the equality and independence of the variances for each item (Thurston, 1927). Torgerson expanded this analysis slightly by considering three classes of data collection (with individuals, between individuals and mixes of within and between) crossed with three sets of assumptions (equal covariance of decision process, equal correlations and small differences in variance, equal variances).  The data may be either a square matrix of dataframe of preferences (as proportions with the probability of the column variable being chosen over the row variable) or a matrix or dataframe of rank orders ( 1 being prefered to 2, etc.)The second example creates 100 random permutations of ranks 1-5.  These data are then converted to a matrix of choices and then scaled.  The goodness of fit is practically perfect, even though the data are meaningless.   This suggests a better goodness of fit test should be applied.  
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
Holzinger and Swineford (1937) introduced the bifactor model (one general factor and several group factors) for mental abilities.  This is a nice demonstration data set of a hierarchical factor structure that can be analyzed using the omega function or using sem. The bifactor model is typically used in measures of cognitive ability.There are several ways to analyze such data.  One is to use the omega function to do a hierarchical factoring using the Schmid-Leiman transformation. This can then be done as an exploratory and then as a confirmatory model using omegaSem. Another way is to do a regular factor analysis and use either a bifactor or  biquartimin rotation. These latter two functions implement the Jennrich and Bentler  (2011) bifactor and biquartimin transformations.  The bifactor rotation suffers from the problem of local minima (Mansolf and Reise, 2016) and thus a mixture of exploratory and confirmatory analysis might be preferred. The 14 variables are ordered to reflect 3 spatial tests, 3 mental speed tests, 4 motor speed tests, and 4 verbal tests.  The sample size is 355.Another data set from Holzinger (Holzinger.9) represents 9 cognitive abilities (Holzinger, 1939) and is used as an example by Karl Joreskog (2003) for factor analysis by the MINRES algorithm and also appears in the LISREL manual as example NPV.KM.  This data set represents  the scores from the Grant White middle school for 9 tests:  "t01_visperc"  "t02_cubes"    "t04_lozenges" "t06_paracomp" "t07_sentcomp" "t09_wordmean" "t10_addition" "t12_countdot" and  "t13_sccaps" and as variables x1 ... x9 (for the Grant-White school) in the lavaan package.Another classic data set is the 9 variable Thurstone problem which is discussed in detail by R. P. McDonald (1985, 1999) and and is used as example in the sem package as well as in the PROC CALIS manual for SAS.  These nine tests were grouped by Thurstone and Thurstone, 1941 (based on other data) into three factors: Verbal Comprehension, Word Fluency, and Reasoning. The original data came from Thurstone and Thurstone (1941) but were reanalyzed by Bechthold (1961) who broke the data set into two. McDonald, in turn, selected these nine variables from the larger set of 17 found in Bechtoldt.2. The sample size is 213.Another set of 9 cognitive variables attributed to Thurstone (1933) is the data set of 4,175 students reported by Professor Brigham of Princeton to the College Entrance Examination Board.  This set does not show a clear bifactor solution but is included as a demonstration of the differences between a maximimum likelihood factor analysis solution versus a principal axis factor solution.Tucker (1958) uses 9 variables from Thurstone and Thburstone (1941) for his example of interbattery factor analysis.  More recent applications of the bifactor model are to the measurement of psychological status. The Reise data set is a correlation matrix based upon >35,000 observations to the Consumer Assessment of Health Care Provideers and Systems survey instrument. Reise, Morizot, and Hays (2007) describe a bifactor solution based upon 1,000 cases.    The five factors from Reise et al. reflect Getting care quickly (1-3), Doctor communicates well (4-7), Courteous and helpful staff (8,9), Getting needed care (10-13), and  Health plan customer service (14-16). The two Bechtoldt data sets are two samples from Thurstone and Thurstone (1941).  They include 17 variables, 9 of which were used by McDonald to form the Thurstone data set. The sample sizes are 212 and 213 respectively. The six proposed factors reflect  memory, verbal, words,  space, number and reasoning with three markers for all expect the rote memory factor. 9 variables from this set appear in the Thurstone data set.Two more data sets with  similar structures are found in the Harman data set.  This includes the another 9 variables (with 696 subjects) from Holzinger used by Harman link{Harman.Holzinger} as well as 8 affective variables from link{burt}. Another data set that is worth examining for tests of bifactor structure is the holzinger.swineford  data set which includes the original data from Holzinger and Swineford (1939) supplied by Keith Widaman.  This is in the  psychTools package. Bechtoldt.1: 17 x 17 correlation matrix of ability tests, N = 212. Bechtoldt.2: 17 x 17 correlation matrix of ability tests, N = 213. Holzinger: 14 x 14 correlation matrix of ability tests, N = 355 Holzinger.9: 9 x 9 correlation matrix of ability tests, N = 145 Reise: 16 x 16 correlation matrix of health satisfaction items.  N = 35,000 Thurstone: 9 x 9 correlation matrix of ability tests, N = 213 Thurstone.33: Another 9 x 9 correlation matrix of ability tests, N=4175 Thurstone:9: And yet another 9 x 9 correlation matrix of ability tests,  N =710
NA
The tr function is used in various matrix operations and is the sum of the diagonal elements of a matrix.
The correlation matrix from Tucker (1958) was used in Tucker and Lewis (1973) for the Tucker-Lewis Index of factoring reliability.
This is set of exploratory indices that are still under development.  A number of test cases suggest that u provides high values when the data are in fact unidimensional, low values when they are not.The logic is deceptively simple:  Unidimensionality implies that a one factor model of the data fits the covariances of the data.  If this is the case, then factor model implies R = FF' + U2 will have residuals of 0.  Similarly, this also implies that the observed correlations will equal the model.  Thus, the sum of the observed correlations (with the diagonal replaced by the communalities) should match the factor model.  Compare these two models:  R - U2  versus FF'.  This is the unidim.A estimate. Also reported, and probably better, is the fit of the one factor model to the correlations. This is merely the sumsq of the residual correlations/sumsq of the original correlations.  When the factor model is perfect, this will be 1. This works well, but when some of the loadings are very small, even though 1 factor is correct, it is probably not a good idea to think of the items as forming a unidimensional scale.  An alternative model (the av.r.fit statistic) considers the residuals found by subtracting the average correlation from the observed correlations.  This will achieve a maximum if the item covariances are all identical (a tau equivalent model).The product of fa.fit and av.r.fit is the measure of unidimensionality, u.  All of the results are reported in the uni list.
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
The two most useful of these  functions is probably biquartimin which implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011). The second is TargetQ which allows for missing NA values in the target. Next best is the orthogonal case, bifactor.  None of these seem to be implemented in GPArotation (yet). TargetT is an orthogonal target rotation function which allows for missing NA values in the target. faRotate is merely a convenient way to call the various GPArotation functions as well as the additional ones added here.The difference between biquartimin and bifactor is just that the latter is the orthogonal case which is documented in Jennrich and Bentler (2011).  It seems as if these two functions are sensitive to the starting values and random  restarts (modifying T) might be called for.bifactor output for the 24 cognitive variable of Holzinger matches that of Jennrich and Bentler as does output for the Chen et al. problem when fm="mle" is used and the Jennrich and Bentler solution is rescaled from covariances to correlations. Promax is a very direct adaptation of the stats::promax function.  The addition is that it will return the interfactor correlations as well as the loadings and rotation matrix. varimin implements the varimin criterion proposed by Suitbert Ertl (2013).  Rather than maximize the varimax criterion, it minimizes it.  For a discussion of the benefits of this procedure, consult Ertel (2013).In addition, these functions will take output from either the factanal, fa or earlier (factor.pa, factor.minres or principal)  functions and select just the loadings matrix for analysis.equamax is just a call to GPArotation's cFT function (for the Crawford Ferguson family of rotations. TargetQ implements Michael Browne's algorithm and allows specification of NA values. The Target input is a list (see examples).  It is interesting to note how powerful specifying what a factor isn't works in defining a factor.  That is, by specifying the pattern of 0s and letting most other elements be NA, the factor structure is still clearly defined.The target.rot function is an adaptation of a function of Michael Browne's to do rotations to arbitrary target matrices.  Suggested by Pat Shrout. The default for target.rot is to rotate to an independent cluster structure (every items is assigned to a group with its highest loading.) target.rot will not handle targets that have linear dependencies (e.g., a pure bifactor model where there is a g loading and a group factor for all variables).
Describe the data using a violin plot. Change alpha to modify the shading.  The grp variable may be used to draw separate violin plots for each of multiple groups.For relatively smallish data sets (< 500-1000), it is informative to also show the actual data points. This done with the dots=TRUE option. The jitter value is arbitrarily set to .05, but making it larger (say .1 or .2) will display more points.
Describe the data using a violin plot. Change alpha to modify the shading.  The grp variable may be used to draw separate violin plots for each of multiple groups.For relatively smallish data sets (< 500-1000), it is informative to also show the actual data points. This done with the dots=TRUE option. The jitter value is arbitrarily set to .05, but making it larger (say .1 or .2) will display more points.
Determining the most interpretable number of factors from a factor analysis is perhaps one of the greatest challenges in factor analysis.  There are many solutions to this problem, none of which is uniformly the best.  "Solving the number of factors problem is easy, I do it everyday before breakfast."  But knowing the right solution is harder. (Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  Techniques most commonly used include1)  Extracting factors until the chi square of the residual matrix is not significant.2) Extracting factors until the change in chi square from factor n to factor n+1 is not significant.3) Extracting factors until the eigen values of the real data are less than the corresponding eigen values of a random data set of the same size (parallel analysis) fa.parallel.4) Plotting the magnitude of the successive eigen values and applying the scree test (a sudden drop in eigen values analogous to the change in slope seen when scrambling up the talus slope of a mountain and approaching the rock face.5) Extracting principal components until the eigen value < 1. 6) Extracting factors as long as they are interpetable.7) Using the Very Simple Structure Criterion (VSS).8) Using Wayne Velicer's Minimum Average Partial (MAP) criterion. Each of the procedures has its advantages and disadvantages.  Using either the chi square test or the change in square test is, of course, sensitive to the number of subjects and leads to the nonsensical condition that if one wants to find many factors, one simply runs more subjects. Parallel analysis is partially sensitive to sample size in that for large samples the eigen values of random factors will be very small.  The scree test is quite appealling but can lead to differences of interpretation as to when the scree "breaks". The eigen value of 1 rule, although the default for many programs, seems to be a rough way of dividing the number of variables by 3.  Extracting interpretable factors means that the number of factors reflects the investigators creativity more than the data.  VSS, while very simple to understand, will not work very well if the data are very factorially complex. (Simulations suggests it will work fine if the complexities of some of the items are no more than 2).Most users of factor analysis tend to interpret factor output by focusing their attention on the largest loadings for every variable and ignoring the smaller ones.  Very Simple Structure operationalizes this tendency by  comparing the original correlation matrix to that reproduced by a simplified version (S) of the original factor matrix (F).  R = SS' + U2.   S is composed of just the c greatest (in absolute value) loadings for each variable.  C (or complexity) is a parameter of the model and may vary from 1 to the number of factors.  The VSS criterion compares the fit of the simplified model to the original correlations: VSS = 1 -sumsquares(r*)/sumsquares(r)  where R* is the residual matrix R* = R - SS' and r* and r are the elements of R* and R respectively. VSS for a given complexity will tend to peak at the optimal (most interpretable) number of factors (Revelle and Rocklin, 1979). Although originally written in Fortran for main frame computers, VSS has been adapted to micro computers (e.g., Macintosh OS 6-9) using Pascal. We now release R code for calculating VSS. Note that if using a correlation matrix (e.g., my.matrix) and doing a factor analysis, the parameters n.obs should be specified for the factor analysis:e.g., the call is VSS(my.matrix,n.obs=500).  Otherwise it defaults to 1000. Wayne Velicer's MAP criterion has been added as an additional test for the optimal number of components to extract.  Note that VSS and MAP will not always agree as to the optimal number.The nfactors function will do a VSS, find MAP, and report a number of other criteria (e.g., BIC, complexity, chi square, ...)A variety of rotation options are available. These include varimax, promax, and oblimin. Others can be added.  Suggestions are welcome.
Determining the most interpretable number of factors from a factor analysis is perhaps one of the greatest challenges in factor analysis.  There are many solutions to this problem, none of which is uniformly the best.  "Solving the number of factors problem is easy, I do it everyday before breakfast."  But knowing the right solution is harder. (Horn and Engstrom, 1979) (Henry Kaiser in personal communication with J.L. Horn, as cited by Horn and Engstrom, 1979, MBR p 283).  Techniques most commonly used include1)  Extracting factors until the chi square of the residual matrix is not significant.2) Extracting factors until the change in chi square from factor n to factor n+1 is not significant.3) Extracting factors until the eigen values of the real data are less than the corresponding eigen values of a random data set of the same size (parallel analysis) fa.parallel.4) Plotting the magnitude of the successive eigen values and applying the scree test (a sudden drop in eigen values analogous to the change in slope seen when scrambling up the talus slope of a mountain and approaching the rock face.5) Extracting principal components until the eigen value < 1. 6) Extracting factors as long as they are interpetable.7) Using the Very Simple Structure Criterion (VSS).8) Using Wayne Velicer's Minimum Average Partial (MAP) criterion. Each of the procedures has its advantages and disadvantages.  Using either the chi square test or the change in square test is, of course, sensitive to the number of subjects and leads to the nonsensical condition that if one wants to find many factors, one simply runs more subjects. Parallel analysis is partially sensitive to sample size in that for large samples the eigen values of random factors will be very small.  The scree test is quite appealling but can lead to differences of interpretation as to when the scree "breaks". The eigen value of 1 rule, although the default for many programs, seems to be a rough way of dividing the number of variables by 3.  Extracting interpretable factors means that the number of factors reflects the investigators creativity more than the data.  VSS, while very simple to understand, will not work very well if the data are very factorially complex. (Simulations suggests it will work fine if the complexities of some of the items are no more than 2).Most users of factor analysis tend to interpret factor output by focusing their attention on the largest loadings for every variable and ignoring the smaller ones.  Very Simple Structure operationalizes this tendency by  comparing the original correlation matrix to that reproduced by a simplified version (S) of the original factor matrix (F).  R = SS' + U2.   S is composed of just the c greatest (in absolute value) loadings for each variable.  C (or complexity) is a parameter of the model and may vary from 1 to the number of factors.  The VSS criterion compares the fit of the simplified model to the original correlations: VSS = 1 -sumsquares(r*)/sumsquares(r)  where R* is the residual matrix R* = R - SS' and r* and r are the elements of R* and R respectively. VSS for a given complexity will tend to peak at the optimal (most interpretable) number of factors (Revelle and Rocklin, 1979). Although originally written in Fortran for main frame computers, VSS has been adapted to micro computers (e.g., Macintosh OS 6-9) using Pascal. We now release R code for calculating VSS. Note that if using a correlation matrix (e.g., my.matrix) and doing a factor analysis, the parameters n.obs should be specified for the factor analysis:e.g., the call is VSS(my.matrix,n.obs=500).  Otherwise it defaults to 1000. Wayne Velicer's MAP criterion has been added as an additional test for the optimal number of components to extract.  Note that VSS and MAP will not always agree as to the optimal number.The nfactors function will do a VSS, find MAP, and report a number of other criteria (e.g., BIC, complexity, chi square, ...)A variety of rotation options are available. These include varimax, promax, and oblimin. Others can be added.  Suggestions are welcome.
NA
Item-factor models differ in their "complexity".  Complexity 1 means that all except the greatest (absolute) loading for an item are ignored. Basically a cluster model (e.g., ICLUST). Complexity 2 implies all except the greatest two, etc.  Different complexities can suggest different number of optimal number of factors to extract.  For personality items, complexity 1 and 2 are probably the most meaningful.The Very Simple Structure criterion will tend to peak at the number of factors that are most interpretable for a given level of complexity.  Note that some problems, the most interpretable number of factors will differ as a function of complexity.  For instance, when doing the Harman 24 psychological variable problems, an unrotated solution of complexity one suggests one factor (g), while a complexity two solution suggests that a four factor solution is most appropriate.  This latter probably reflects a bi-factor structure.  For examples of VSS.plot output, see https://personality-project.org/r/r.vss.html
Among the many ways to choose the optimal number of factors is the scree test.  A better function to show the scree as well as compare it to randomly parallel solutions is found found in fa.parallel
NA
NA
 Two artificial correlation matrices from Schmid and Leiman (1957). One real and one artificial covariance matrices from Chen et al. (2006).  Schmid: a 12 x 12 artificial correlation matrix created to show the Schmid-Leiman transformation.  schmid.leiman: A 12 x 12 matrix with communalities on the diagonal.  Treating this as a covariance matrix shows the 6 x 6 factor solution Chen: An 18 x 18 covariance matrix of health related quality of life items from Chen et al. (2006). Number of observations = 403.  The first item is a measure of the quality of life.  The remaining 17 items form four subfactors: The items are (a) Cognition subscale: “Have difficulty reasoningand solving problems?"  “React slowly to things that were said or done?"; “Become confused and start several actions at a time?"  “Forget where youput things or appointments?"; “Have difficulty concentrating?"  (b) Vitalitysubscale: “Feel tired?"  “Have enough energy to do the things you want?" (R) “Feel worn out?" ; “Feel full of pep?" (R). (c) Mental health subscale: “Feelcalm and peaceful?"(R)  “Feel downhearted and blue?"; “Feel veryhappy"(R) ; “Feel very nervous?" ; “Feel so down in the dumps nothing couldcheer you up?  (d) Disease worry subscale: “Were you afraid because of your health?"; “Were you frustrated about your health?"; “Was your health a worry in your life?" . West: A 16 x 16 artificial covariance matrix from Chen et al. (2006).
Among the many robust estimates of central tendency, some recommend the Winsorized mean.  Rather than just dropping the top and bottom trim percent, these extreme values are replaced with values at the trim and 1- trim quantiles.
Among the many robust estimates of central tendency, some recommend the Winsorized mean.  Rather than just dropping the top and bottom trim percent, these extreme values are replaced with values at the trim and 1- trim quantiles.
Among the many robust estimates of central tendency, some recommend the Winsorized mean.  Rather than just dropping the top and bottom trim percent, these extreme values are replaced with values at the trim and 1- trim quantiles.
Among the many robust estimates of central tendency, some recommend the Winsorized mean.  Rather than just dropping the top and bottom trim percent, these extreme values are replaced with values at the trim and 1- trim quantiles.
Among the many robust estimates of central tendency, some recommend the Winsorized mean.  Rather than just dropping the top and bottom trim percent, these extreme values are replaced with values at the trim and 1- trim quantiles.
Correlations between individuals who belong to different natural groups (based upon e.g., ethnicity, age, gender, college major,or country) reflect an unknown mixture of the pooled correlation within each group as well as the correlation of the means of these groups. These two correlations are independent and do not allow inferences from one level (the group) to the other level (the individual).  This data set shows this independence.  The within group correlations between 9 variables are set to be 1, 0, and -1 while those between groups are also set to be 1, 0, -1.  These two sets of correlations are crossed such that V1, V4, and V7 have within group correlations of 1, as do V2, V5 and V8, and V3, V6 and V9.  V1 has a within group correlation of 0 with V2, V5, and V8, and a -1 within group correlation with V3, V6 and V9.  V1, V2, and V3 share a between group correlation of 1, as do V4, V5 and V6, and V7, V8 and V9.  The first group has a 0 between group correlation with the second and a -1 with the third group.  statsBy can decompose the observed correlation in the between and within correlations.  sim.multilevel can produce similar data.
When cateogorical judgments are made with two cateories, a measure of relationship is the phi coefficient.  However, some categorical judgments are made using more than two outcomes.  For example, two diagnosticians might be asked to categorize patients three ways (e.g., Personality disorder, Neurosis, Psychosis) or to categorize the stages of a disease.  Just as base rates affect observed cell frequencies in a two by two table, they need to be considered in the n-way table (Cohen, 1960). Kappa considers the matches on the main diagonal.  A penalty function (weight) may be applied to the off diagonal matches.  If the weights increase by the square of the distance from the diagonal, weighted kappa is similar to an Intra Class Correlation (ICC).Derivations of weighted kappa are sometimes expressed in terms of similarities, and sometimes in terms of dissimilarities. In the latter case, the weights on the diagonal are 1 and the weights off the diagonal are less than one. In this  case, if the weights are 1 - squared distance from the diagonal / k, then the result is similar to the ICC (for any positive k).  cohen.kappa may use either similarity weighting (diagonal = 0) or dissimilarity weighting (diagonal = 1) in order to match various published examples. The input may be a two column data.frame or matrix with columns representing the two judges and rows the subjects being rated. Alternatively, the input may be a square n x n matrix of counts or proportion of matches.  If proportions are used, it is necessary to specify the number of observations (n.obs) in order to correctly find the confidence intervals.The confidence intervals are based upon the variance estimates discussed by Fleiss, Cohen, and Everitt who corrected the formulae of Cohen (1968) and Blashfield.Some data sets will include data with numeric categories with some category values missing completely.  In the sense that kappa is a measure of category relationship, this should not matter.  But when finding weighted kappa, the number of categories weighted will be less than the number of categories potentially in the data.  This can be remedied by specifying the levels parameter.  This is a vector of the levels potentially in the data (even if some are missing).   See the examples.If there are more than 2 raters, then the average of all raters is known as Light's kappa. (Conger, 1980).  
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
These functions call Yule2poly,  Yule2phi or phi2poly for each cell of the matrix. See those functions for more details.  See phi.demo for an example.
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
These functions call Yule2poly,  Yule2phi or phi2poly for each cell of the matrix. See those functions for more details.  See phi.demo for an example.
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
Yule developed two measures of association for two by two tables.  Both are functions of the odds ratio 
For type = "correlation" and "covariance", theestimates are based on the sample covariance. (The lag 0 autocorrelationis fixed at 1 by convention.)By default, no missing values are allowed.  If the na.actionfunction passes through missing values (as na.pass does), thecovariances are computed from the complete cases.  This means that theestimate computed may well not be a valid autocorrelation sequence,and may contain missing values.  Missing values are not allowed whencomputing the PACF of a multivariate time series.The partial correlation coefficient is estimated by fittingautoregressive models of successively higher orders up tolag.max.The generic function plot has a method for objects of class"acf".The lag is returned and plotted in units of time, and not numbers ofobservations.There are print and subsetting methods for objects of class"acf".
NA
factor.scope is not intended to be called directly by users.
For drop1 methods, a missing scope is taken to be allterms in the model. The hierarchy is respected when considering termsto be added or dropped: all main effects contained in a second-orderinteraction must remain, and so on.In a scope formula . means ‘what is already there’.The methods for lm and glm are moreefficient in that they do not recompute the model matrix and call thefit methods directly.The default output table gives AIC, defined as minus twice loglikelihood plus 2p where p is the rank of the model (thenumber of effective parameters).  This is only defined up to anadditive constant (like log-likelihoods).  For linear Gaussian modelswith fixed scale, the constant is chosen to give Mallows' Cp,RSS/scale + 2p - n.  Where Cp is used,the column is labelled as Cp rather than AIC.The F tests for the "glm" methods are based on analysis ofdeviance tests, so if the dispersion is estimated it is based on theresidual deviance, unlike the F tests of anova.glm.
If the functions used to form margins are not commutative the resultdepends on the order in which margins are computed.  Annotationof margins is done via naming the FUN list.
aggregate is a generic function with methods for data framesand time series.The default method, aggregate.default, uses the time seriesmethod if x is a time series, and otherwise coerces xto a data frame and calls the data frame method.aggregate.data.frame is the data frame method.  If x isnot a data frame, it is coerced to one, which must have a non-zeronumber of rows.  Then, each of the variables (columns) in x issplit into subsets of cases (rows) of identical combinations of thecomponents of by, and FUN is applied to each such subsetwith further arguments in ... passed to it.  The result isreformatted into a data frame containing the variables in byand x.  The ones arising from by contain the uniquecombinations of grouping values used for determining the subsets, andthe ones arising from x the corresponding summaries for thesubset of the respective variables in x.  If simplify istrue, summaries are simplified to vectors or matrices if they have acommon length of one or greater than one, respectively; otherwise,lists of summary results according to subsets are obtained.  Rows withmissing values in any of the by variables will be omitted fromthe result.  (Note that versions of R prior to 2.11.0 requiredFUN to be a scalar function.)aggregate.formula is a standard formula interface toaggregate.data.frame.aggregate.ts is the time series method, and requires FUNto be a scalar function.  If x is not a time series, it iscoerced to one.  Then, the variables in x are split intoappropriate blocks of length frequency(x) / nfrequency, andFUN is applied to each such block, with further (named)arguments in ... passed to it.  The result returned is a timeseries with frequency nfrequency holding the aggregated values.Note that this make most sense for a quarterly or yearly result whenthe original series covers a whole number of quarters or years: inparticular aggregating a monthly series to quarters starting inFebruary does not give a conventional quarterly series.FUN is passed to match.fun, and hence it can be afunction or a symbol or character string naming a function.
aggregate is a generic function with methods for data framesand time series.The default method, aggregate.default, uses the time seriesmethod if x is a time series, and otherwise coerces xto a data frame and calls the data frame method.aggregate.data.frame is the data frame method.  If x isnot a data frame, it is coerced to one, which must have a non-zeronumber of rows.  Then, each of the variables (columns) in x issplit into subsets of cases (rows) of identical combinations of thecomponents of by, and FUN is applied to each such subsetwith further arguments in ... passed to it.  The result isreformatted into a data frame containing the variables in byand x.  The ones arising from by contain the uniquecombinations of grouping values used for determining the subsets, andthe ones arising from x the corresponding summaries for thesubset of the respective variables in x.  If simplify istrue, summaries are simplified to vectors or matrices if they have acommon length of one or greater than one, respectively; otherwise,lists of summary results according to subsets are obtained.  Rows withmissing values in any of the by variables will be omitted fromthe result.  (Note that versions of R prior to 2.11.0 requiredFUN to be a scalar function.)aggregate.formula is a standard formula interface toaggregate.data.frame.aggregate.ts is the time series method, and requires FUNto be a scalar function.  If x is not a time series, it iscoerced to one.  Then, the variables in x are split intoappropriate blocks of length frequency(x) / nfrequency, andFUN is applied to each such block, with further (named)arguments in ... passed to it.  The result returned is a timeseries with frequency nfrequency holding the aggregated values.Note that this make most sense for a quarterly or yearly result whenthe original series covers a whole number of quarters or years: inparticular aggregating a monthly series to quarters starting inFebruary does not give a conventional quarterly series.FUN is passed to match.fun, and hence it can be afunction or a symbol or character string naming a function.
aggregate is a generic function with methods for data framesand time series.The default method, aggregate.default, uses the time seriesmethod if x is a time series, and otherwise coerces xto a data frame and calls the data frame method.aggregate.data.frame is the data frame method.  If x isnot a data frame, it is coerced to one, which must have a non-zeronumber of rows.  Then, each of the variables (columns) in x issplit into subsets of cases (rows) of identical combinations of thecomponents of by, and FUN is applied to each such subsetwith further arguments in ... passed to it.  The result isreformatted into a data frame containing the variables in byand x.  The ones arising from by contain the uniquecombinations of grouping values used for determining the subsets, andthe ones arising from x the corresponding summaries for thesubset of the respective variables in x.  If simplify istrue, summaries are simplified to vectors or matrices if they have acommon length of one or greater than one, respectively; otherwise,lists of summary results according to subsets are obtained.  Rows withmissing values in any of the by variables will be omitted fromthe result.  (Note that versions of R prior to 2.11.0 requiredFUN to be a scalar function.)aggregate.formula is a standard formula interface toaggregate.data.frame.aggregate.ts is the time series method, and requires FUNto be a scalar function.  If x is not a time series, it iscoerced to one.  Then, the variables in x are split intoappropriate blocks of length frequency(x) / nfrequency, andFUN is applied to each such block, with further (named)arguments in ... passed to it.  The result returned is a timeseries with frequency nfrequency holding the aggregated values.Note that this make most sense for a quarterly or yearly result whenthe original series covers a whole number of quarters or years: inparticular aggregating a monthly series to quarters starting inFebruary does not give a conventional quarterly series.FUN is passed to match.fun, and hence it can be afunction or a symbol or character string naming a function.
When comparing models fitted by maximum likelihood to the same data,the smaller the AIC or BIC, the better the fit.The theory of AIC requires that the log-likelihood has been maximized:whereas AIC can be computed for models not fitted by maximumlikelihood, their AIC values should not be compared.Examples of models not ‘fitted to the same data’ are where theresponse is transformed (accelerated-life models are fitted tolog-times) and where contingency tables have been used to summarizedata.These are generic functions (with S4 generics defined in packagestats4): however methods should be defined for thelog-likelihood function logLik rather than thesefunctions: the action of their default methods is to call logLikon all the supplied objects and assemble the results.  Note that inseveral common cases logLik does not return the value atthe MLE: see its help page.The log-likelihood and hence the AIC/BIC is only defined up to anadditive constant.  Different constants have conventionally been usedfor different purposes and so extractAIC and AICmay give different values (and do for models of class "lm": seethe help for extractAIC).  Particular care is neededwhen comparing fits of different classes (with, for example, acomparison of a Poisson and gamma GLM being meaningless since one hasa discrete response, the other continuous).BIC is defined asAIC(object, ..., k = log(nobs(object))).This needs the number of observations to be known: the default methodlooks first for a "nobs" attribute on the return value from thelogLik method, then tries the nobsgeneric, and if neither succeed returns BIC as NA.
Although the main method is for class "lm", alias ismost useful for experimental designs and so is used with fits fromaov.Complete aliasing refers to effects in linear models that cannot be estimatedindependently of the terms which occur earlier in the model and sohave their coefficients omitted from the fit. Partial aliasing refersto effects that can be estimated less precisely because ofcorrelations induced by the design.Some parts of the "lm" method require recommended packageMASS to be installed.
NA
Suppose that x and y are independent samples fromdistributions with densities f((t-m)/s)/s and f(t-m),respectively, where m is an unknown nuisance parameter ands, the ratio of scales, is the parameter of interest.  TheAnsari-Bradley test is used for testing the null that s equals1, the two-sided alternative being that s != 1 (thedistributions differ only in variance), and the one-sided alternativesbeing s > 1 (the distribution underlying x has a largervariance, "greater") or s < 1 ("less").By default (if exact is not specified), an exact p-valueis computed if both samples contain less than 50 finite values andthere are no ties.  Otherwise, a normal approximation is used.Optionally, a nonparametric confidence interval and an estimator fors are computed.  If exact p-values are available, an exactconfidence interval is obtained by the algorithm described in Bauer(1972), and the Hodges-Lehmann estimator is employed.  Otherwise, thereturned confidence interval and point estimate are based on normalapproximations.Note that mid-ranks are used in the case of ties rather than averagescores as employed in Hollander & Wolfe (1973).  See, e.g., Hajek,Sidak and Sen (1999), pages 131ff, for more information.
This provides a wrapper to lm for fitting linear models tobalanced or unbalanced experimental designs.The main difference from lm is in the way print,summary and so on handle the fit: this is expressed in thetraditional language of the analysis of variance rather than that oflinear models.If the formula contains a single Error term, this is used tospecify error strata, and appropriate models are fitted within eacherror stratum.The formula can specify multiple responses.Weights can be specified by a weights argument, but should notbe used with an Error term, and are incompletely supported(e.g., not by model.tables).
The inputs can contain missing values which are deleted (if na.rmis true, i.e., by default), so at leasttwo complete (x, y) pairs are required (for method =  "linear", one otherwise).  If there are duplicated (tied) xvalues and ties contains a function it is applied to the yvalues for each distinct x value to produce (x,y) pairswith unique x.Useful functions in this context include mean,min, and max.If ties = "ordered" the x values are assumed to be alreadyordered (and unique) and ties are not checked but kept if present.This is the fastest option for large length(x).If ties is a list of length two, ties[[2]]must be a function to be applied to ties, see above, but ifties[[1]] is identical to "ordered", the x valuesare assumed to be sorted and are only checked for ties.  Consequently,ties = list("ordered", mean) will be slightly more efficient thanthe default ties = mean in such a case.The first y value will be used for interpolation to the left and the lastone for interpolation to the right.
The inputs can contain missing values which are deleted (if na.rmis true, i.e., by default), so at leasttwo complete (x, y) pairs are required (for method =  "linear", one otherwise).  If there are duplicated (tied) xvalues and ties contains a function it is applied to the yvalues for each distinct x value to produce (x,y) pairswith unique x.Useful functions in this context include mean,min, and max.If ties = "ordered" the x values are assumed to be alreadyordered (and unique) and ties are not checked but kept if present.This is the fastest option for large length(x).If ties is a list of length two, ties[[2]]must be a function to be applied to ties, see above, but ifties[[1]] is identical to "ordered", the x valuesare assumed to be sorted and are only checked for ties.  Consequently,ties = list("ordered", mean) will be slightly more efficient thanthe default ties = mean in such a case.The first y value will be used for interpolation to the left and the lastone for interpolation to the right.
For definiteness, note that the AR coefficients have the sign inx[t] - m = a[1]*(x[t-1] - m) + … +  a[p]*(x[t-p] - m) + e[t]ar is just a wrapper for the functions ar.yw,ar.burg, ar.ols and ar.mle.Order selection is done by AIC if aic is true. This isproblematic, as of the methods here only ar.mle performstrue maximum likelihood estimation. The AIC is computed as if the varianceestimate were the MLE, omitting the determinant term from thelikelihood. Note that this is not the same as the Gaussian likelihoodevaluated at the estimated parameter values.  In ar.yw thevariance matrix of the innovations is computed from the fittedcoefficients and the autocovariance of x.ar.burg allows two methods to estimate the innovationsvariance and hence AIC. Method 1 is to use the update given bythe Levinson-Durbin recursion (Brockwell and Davis, 1991, (8.2.6)on page 242), and follows S-PLUS. Method 2 is the mean of the sumof squares of the forward and backward prediction errors(as in Brockwell and Davis, 1996, page 145). Percival and Walden(1998) discuss both. In the multivariate case the estimatedcoefficients will depend (slightly) on the variance estimation method.Remember that ar includes by default a constant in the model, byremoving the overall mean of x before fitting the AR model,or (ar.mle) estimating a constant to subtract.
For definiteness, note that the AR coefficients have the sign inx[t] - m = a[1]*(x[t-1] - m) + … +  a[p]*(x[t-p] - m) + e[t]ar is just a wrapper for the functions ar.yw,ar.burg, ar.ols and ar.mle.Order selection is done by AIC if aic is true. This isproblematic, as of the methods here only ar.mle performstrue maximum likelihood estimation. The AIC is computed as if the varianceestimate were the MLE, omitting the determinant term from thelikelihood. Note that this is not the same as the Gaussian likelihoodevaluated at the estimated parameter values.  In ar.yw thevariance matrix of the innovations is computed from the fittedcoefficients and the autocovariance of x.ar.burg allows two methods to estimate the innovationsvariance and hence AIC. Method 1 is to use the update given bythe Levinson-Durbin recursion (Brockwell and Davis, 1991, (8.2.6)on page 242), and follows S-PLUS. Method 2 is the mean of the sumof squares of the forward and backward prediction errors(as in Brockwell and Davis, 1996, page 145). Percival and Walden(1998) discuss both. In the multivariate case the estimatedcoefficients will depend (slightly) on the variance estimation method.Remember that ar includes by default a constant in the model, byremoving the overall mean of x before fitting the AR model,or (ar.mle) estimating a constant to subtract.
For definiteness, note that the AR coefficients have the sign inx[t] - m = a[1]*(x[t-1] - m) + … +  a[p]*(x[t-p] - m) + e[t]ar is just a wrapper for the functions ar.yw,ar.burg, ar.ols and ar.mle.Order selection is done by AIC if aic is true. This isproblematic, as of the methods here only ar.mle performstrue maximum likelihood estimation. The AIC is computed as if the varianceestimate were the MLE, omitting the determinant term from thelikelihood. Note that this is not the same as the Gaussian likelihoodevaluated at the estimated parameter values.  In ar.yw thevariance matrix of the innovations is computed from the fittedcoefficients and the autocovariance of x.ar.burg allows two methods to estimate the innovationsvariance and hence AIC. Method 1 is to use the update given bythe Levinson-Durbin recursion (Brockwell and Davis, 1991, (8.2.6)on page 242), and follows S-PLUS. Method 2 is the mean of the sumof squares of the forward and backward prediction errors(as in Brockwell and Davis, 1996, page 145). Percival and Walden(1998) discuss both. In the multivariate case the estimatedcoefficients will depend (slightly) on the variance estimation method.Remember that ar includes by default a constant in the model, byremoving the overall mean of x before fitting the AR model,or (ar.mle) estimating a constant to subtract.
ar.ols fits the general AR model to a possibly non-stationaryand/or multivariate system of series x. The resultingunconstrained least squares estimates are consistent, even ifsome of the series are non-stationary and/or co-integrated.For definiteness, note that the AR coefficients have the sign in(x[t] - m) = a[0] + a[1]*(x[t-1] - m) + … +  a[p]*(x[t-p] - m) + e[t]where a[0] is zero unless intercept is true, andm is the sample mean if demean is true, zerootherwise.Order selection is done by AIC if aic is true. This isproblematic, as ar.ols does not performtrue maximum likelihood estimation. The AIC is computed as ifthe variance estimate (computed from the variance matrix of theresiduals) were the MLE, omitting the determinant term from thelikelihood. Note that this is not the same as the Gaussianlikelihood evaluated at the estimated parameter values.Some care is needed if intercept is true and demean isfalse. Only use this is the series are roughly centred onzero. Otherwise the computations may be inaccurate or fail entirely.
For definiteness, note that the AR coefficients have the sign inx[t] - m = a[1]*(x[t-1] - m) + … +  a[p]*(x[t-p] - m) + e[t]ar is just a wrapper for the functions ar.yw,ar.burg, ar.ols and ar.mle.Order selection is done by AIC if aic is true. This isproblematic, as of the methods here only ar.mle performstrue maximum likelihood estimation. The AIC is computed as if the varianceestimate were the MLE, omitting the determinant term from thelikelihood. Note that this is not the same as the Gaussian likelihoodevaluated at the estimated parameter values.  In ar.yw thevariance matrix of the innovations is computed from the fittedcoefficients and the autocovariance of x.ar.burg allows two methods to estimate the innovationsvariance and hence AIC. Method 1 is to use the update given bythe Levinson-Durbin recursion (Brockwell and Davis, 1991, (8.2.6)on page 242), and follows S-PLUS. Method 2 is the mean of the sumof squares of the forward and backward prediction errors(as in Brockwell and Davis, 1996, page 145). Percival and Walden(1998) discuss both. In the multivariate case the estimatedcoefficients will depend (slightly) on the variance estimation method.Remember that ar includes by default a constant in the model, byremoving the overall mean of x before fitting the AR model,or (ar.mle) estimating a constant to subtract.
Different definitions of ARMA models have different signs for theAR and/or MA coefficients.  The definition used here hasX[t] = a[1]X[t-1] + … + a[p]X[t-p] + e[t] + b[1]e[t-1] + … + b[q]e[t-q]and so the MA coefficients differ in sign from those of S-PLUS.Further, if include.mean is true (the default for an ARMAmodel), this formula applies to X - m rather than X.  ForARIMA models with differencing, the differenced series follows azero-mean ARMA model. If an xreg term is included, a linearregression (with a constant term if include.mean is true andthere is no differencing) is fitted with an ARMA model for the errorterm.The variance matrix of the estimates is found from the Hessian ofthe log-likelihood, and so may only be a rough guide.Optimization is done by optim.  It will workbest if the columns in xreg are roughly scaled to zero meanand unit variance, but does attempt to estimate suitable scalings.
See arima for the precise definition of an ARIMA model.The ARMA model is checked for stationarity.ARIMA models are specified via the order component ofmodel, in the same way as for arima.  Otheraspects of the order component are ignored, but inconsistentspecifications of the MA and AR orders are detected.  Theun-differencing assumes previous values of zero, and to remind theuser of this, those values are returned.Random inputs for the ‘burn-in’ period are generated by callingrand.gen.
Different definitions of ARMA models have different signs for theAR and/or MA coefficients. The definition here hasX[t] = a[1]X[t-1] + … + a[p]X[t-p] + e[t] + b[1]e[t-1] + … + b[q]e[t-q]and so the MA coefficients differ in sign from those ofS-PLUS.  Further, if include.mean is true, this formulaapplies to X-m rather than X.  For ARIMA models withdifferencing, the differenced series follows a zero-mean ARMA model.The variance matrix of the estimates is found from the Hessian ofthe log-likelihood, and so may only be a rough guide, especially forfits close to the boundary of invertibility.Optimization is done by optim. It will workbest if the columns in xreg are roughly scaled to zero meanand unit variance, but does attempt to estimate suitable scalings.Finite-history prediction is used. This is only statisticallyefficient if the MA part of the fit is invertible, sopredict.arima0 will give a warning for non-invertible MAmodels.
The *chisq() functions now take an optional non-centralityargument, so the *nchisq() functions are no longer needed.reshape*, which were experimental, are replaced byreshape.  This has a different syntax and allowsmultiple time-varying variables.arima0.diag has been replaced by tsdiag.arima0.plot.mts has been removed, as plot.ts now has thesame functionality.print.coefmat was an older name for printCoefmatwith a different default for na.print.anovalist.lm was replaced by anova.lmlist inR 1.2.0.lm.fit.null and lm.wfit.null are superseded bylm.fit and lm.wfit which handle null models now.Similarly, glm.fit.null is superseded by glm.fit.mauchley.test was a misspelling of Mauchly's name, correctedby the introduction of mauchly.test.clearNames had been introduced at about the same time asunname, but is less general and has been used rarely.plclust has been drawing dendrograms (“cluster - trees”) and been replaced by theplot() method for class "dendrogram".
The methods used follow Brockwell & Davis (1991, section 3.3).  Theirequations (3.3.8) are solved for the autocovariances at lags0, …, max(p, q+1),and the remaining autocorrelations are given by a recursive filter.
NA
The dendrogram is directly represented as a nested list where eachcomponent corresponds to a branch of the tree.  Hence, the firstbranch of tree z is z[[1]], the second branch of thecorresponding subtree is z[[1]][[2]], or shorterz[[c(1,2)]], etc..  Each node of the treecarries some information needed for efficient plotting or cutting asattributes, of which only members, height andleaf for leaves are compulsory:total number of leaves in the branchnumeric non-negative height at which the nodeis plotted.numeric horizontal distance of the node fromthe left border (the leftmost leaf) of the branch (unit 1 betweenall leaves).  This is used for plot(*, center = FALSE).character; the label of the nodefor cut()$upper,the number of former members; more generally a substitutefor the members component used for ‘horizontal’(when horiz = FALSE, else ‘vertical’) alignment.character; the label for the edge leading tothe nodea named list (of length-1 components)specifying node-specific attributes for pointsplotting, see the nodePar argument above.a named list (of length-1 components)specifying attributes for segments plotting of theedge leading to the node, and drawing of the edgetext ifavailable, see the edgePar argument above.logical, if TRUE, the node is a leaf ofthe tree.cut.dendrogram() returns a list with components $upperand $lower, the first is a truncated version of the originaltree, also of class dendrogram, the latter a list with thebranches obtained from cutting the tree, each a dendrogram.There are [[, print, and strmethods for "dendrogram" objects where the first one(extraction) ensures that selecting sub-branches keeps the class,i.e., returns a dendrogram even if only a leaf.On the other hand, [ (single bracket) extractionreturns the underlying list structure.Objects of class "hclust" can be converted to class"dendrogram" using method as.dendrogram(), and since R2.13.0, there is also a as.hclust() method as an inverse.rev.dendrogram simply returns the dendrogram x withreversed nodes, see also reorder.dendrogram.The merge(x, y, ...) method merges two or moredendrograms into a new one which has x and y (andoptional further arguments) as branches.  Note that before R 3.1.2,adjust = "none" was used implicitly, which is invalid when,e.g., the dendrograms are from as.dendrogram(hclust(..)).nobs(object) returns the total number of leaves (themembers attribute, see above).is.leaf(object) returns logical indicating if object is aleaf (the most simple dendrogram).plotNode() and plotNodeLimit() are helper functions.
Available distance measures are (written for two vectors x andy):Usual distance between the two vectors (2norm aka L_2), sqrt(sum((x_i - y_i)^2)).Maximum distance between two components of xand y (supremum norm)Absolute distance between the two vectors (1 norm aka L_1).sum(|x_i - y_i| / (|x_i| + |y_i|)).Terms with zero numerator and denominator are omitted from the sumand treated as if the values were missing.This is intended for non-negative values (e.g., counts), in whichcase the denominator can be written in various equivalent ways;Originally, R used x_i + y_i, then from 1998 to 2017,|x_i + y_i|, and then the correct |x_i| + |y_i|.(aka asymmetric binary): The vectorsare regarded as binary bits, so non-zero elements are ‘on’and zero elements are ‘off’.  The distance is theproportion of bits in which only one is on amongst those inwhich at least one is on.The p norm, the pth root of thesum of the pth powers of the differences of the components.Missing values are allowed, and are excluded from all computationsinvolving the rows within which they occur.Further, when Inf values are involved, all pairs of values areexcluded when their contribution to the distance gave NaN orNA.If some columns are excluded in calculating a Euclidean, Manhattan,Canberra or Minkowski distance, the sum is scaled up proportionally tothe number of columns used.  If all pairs are excluded whencalculating a particular distance, the value is NA.The "dist" method of as.matrix() and as.dist()can be used for conversion between objects of class "dist"and conventional distance matrices.as.dist() is a generic function.  Its default method handlesobjects inheriting from class "dist", or coercible to matricesusing as.matrix().  Support for classes representingdistances (also known as dissimilarities) can be added by providing anas.matrix() or, more directly, an as.dist methodfor such a class.
The models fit by, e.g., the lm and glm functionsare specified in a compact symbolic form.The ~ operator is basic in the formation of such models.An expression of the form y ~ model is interpretedas a specification that the response y is modelledby a linear predictor specified symbolically by model.Such a model consists of a series of terms separatedby + operators.The terms themselves consist of variable and factornames separated by : operators.Such a term is interpreted as the interaction ofall the variables and factors appearing in the term.In addition to + and :, a number of other operators areuseful in model formulae.  The * operator denotes factorcrossing: a*b interpreted as a+b+a:b.  The ^operator indicates crossing to the specified degree.  For example(a+b+c)^2 is identical to (a+b+c)*(a+b+c) which in turnexpands to a formula containing the main effects for a,b and c together with their second-order interactions.The %in% operator indicates that the terms on its left arenested within those on the right.  For example a + b %in% aexpands to the formula a + a:b.  The - operator removesthe specified terms, so that (a+b+c)^2 - a:b is identical toa + b + c + b:c + a:c.  It can also used to remove theintercept term: when fitting a linear model y ~ x - 1 specifiesa line through the origin.  A model with no intercept can be alsospecified as y ~ x + 0 or y ~ 0 + x.While formulae usually involve just variable and factornames, they can also involve arithmetic expressions.The formula log(y) ~ a + log(x) is quite legal.When such arithmetic expressions involveoperators which are also used symbolicallyin model formulae, there can be confusion betweenarithmetic and symbolic operator use.To avoid this confusion, the function I()can be used to bracket those portions of a modelformula where the operators are used in theirarithmetic sense.  For example, in the formulay ~ a + I(b+c), the term b+c is to beinterpreted as the sum of b and c.Variable names can be quoted by backticks `like this` informulae, although there is no guarantee that all code using formulaewill accept such non-syntactic names.Most model-fitting functions accept formulae with right-hand-sideincluding the function offset to indicate terms with afixed coefficient of one.  Some functions accept other‘specials’ such as strata or cluster (see thespecials argument of terms.formula).There are two special interpretations of . in a formula.  Theusual one is in the context of a data argument of modelfitting functions and means ‘all columns not otherwise in theformula’: see terms.formula.  In the context ofupdate.formula, only, it means ‘what waspreviously in this part of the formula’.When formula is called on a fitted model object, either aspecific method is used (such as that for class "nls") or thedefault method.  The default first looks for a "formula"component of the object (and evaluates it), then a "terms"component, then a formula parameter of the call (and evaluatesits value) and finally a "formula" attribute.There is a formula method for data frames.  When there's"terms" attribute with a formula, e.g., for amodel.frame(), that formula is returned.  If you'd like theprevious (R <= 3.5.x) behavior, use the auxiliaryDF2formula() which does not consider a "terms" attribute.Otherwise, ifthere is onlyone column this forms the RHS with an empty LHS.  For more columns,the first column is the LHS of the formula and the remaining columnsseparated by + form the RHS.
Currently there is only support for converting objects ofclass "twins" as produced by the functions diana andagnes from the package cluster.  The default methodthrows an error unless passed an "hclust" object.
NA
The function ts is used to create time-series objects.  Theseare vectors or matrices with class of "ts" (and additionalattributes) which represent data which has been sampled at equispacedpoints in time.  In the matrix case, each column of the matrixdata is assumed to contain a single (univariate) time series.Time series must have at least one observation, and although they neednot be numeric there is very limited support for non-numeric series.Class "ts" has a number of methods.  In particular arithmeticwill attempt to align time axes, and subsetting to extract subsets ofseries can be used (e.g., EuStockMarkets[, "DAX"]).  However,subsetting the first (or only) dimension will return a matrix orvector, as will matrix subsetting.  Subassignment can be used toreplace values but not to extend a series (see window).There is a method for t that transposes the series as amatrix (a one-column matrix if a vector) and hence returns a resultthat does not inherit from class "ts".Argument frequency indicates the sampling frequency of thetime series, with the default value 1 indicating one sample ineach unit time interval.  Forexample, one could use a value of 7 for frequency whenthe data are sampled daily, and the natural time period is a week, or12 when the data are sampled monthly and the natural timeperiod is a year.  Values of 4 and 12 are assumed in(e.g.) print methods to imply a quarterly and monthly seriesrespectively.  As from R 4.0.0, frequency need not be a wholenumber.  For example, frequency = 0.2 would imply samplingonce every five time units.as.ts is generic.  Its default method will use thetsp attribute of the object if it has one to set thestart and end times and frequency.is.ts tests if an object is a time series.  It is generic: youcan write methods to handle specific classes of objects,see InternalMethods.
NA
NA
kernel is used to construct a general kernel or named specifickernels.  The modified Daniell kernel halves the end coefficients (asused by S-PLUS).The [ method allows natural indexing of kernel objectswith indices in (-m) : m.  The normalization is such that fork <- kernel(*), sum(k[ -k$m : k$m ]) is one.df.kernel returns the ‘equivalent degrees of freedom’ ofa smoothing kernel as defined in Brockwell and Davis (1991), page362, and bandwidth.kernel returns the equivalent bandwidth asdefined in Bloomfield (1976), p. 201, with a continuity correction.
If x is a list, its elements are taken as the samples or fittedlinear models to be compared for homogeneity of variances.  In thiscase, the elements must either all be numeric data vectors or fittedlinear model objects, g is ignored, and one can simply usebartlett.test(x) to perform the test.  If the samples are notyet contained in a list, use bartlett.test(list(x, ...)).Otherwise, x must be a numeric data vector, and g mustbe a vector or factor object of the same length as x giving thegroup for the corresponding elements of x.
When comparing models fitted by maximum likelihood to the same data,the smaller the AIC or BIC, the better the fit.The theory of AIC requires that the log-likelihood has been maximized:whereas AIC can be computed for models not fitted by maximumlikelihood, their AIC values should not be compared.Examples of models not ‘fitted to the same data’ are where theresponse is transformed (accelerated-life models are fitted tolog-times) and where contingency tables have been used to summarizedata.These are generic functions (with S4 generics defined in packagestats4): however methods should be defined for thelog-likelihood function logLik rather than thesefunctions: the action of their default methods is to call logLikon all the supplied objects and assemble the results.  Note that inseveral common cases logLik does not return the value atthe MLE: see its help page.The log-likelihood and hence the AIC/BIC is only defined up to anadditive constant.  Different constants have conventionally been usedfor different purposes and so extractAIC and AICmay give different values (and do for models of class "lm": seethe help for extractAIC).  Particular care is neededwhen comparing fits of different classes (with, for example, acomparison of a Poisson and gamma GLM being meaningless since one hasa discrete response, the other continuous).BIC is defined asAIC(object, ..., k = log(nobs(object))).This needs the number of observations to be known: the default methodlooks first for a "nobs" attribute on the return value from thelogLik method, then tries the nobsgeneric, and if neither succeed returns BIC as NA.
Confidence intervals are obtained by a procedure first given inClopper and Pearson (1934).  This guarantees that the confidence levelis at least conf.level, but in general does not give theshortest-length confidence intervals.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
A biplot is plot which aims to represent both the observations andvariables of a matrix of multivariate data on the same plot. There aremany variations on biplots (see the references) and perhaps the mostwidely used one is implemented by biplot.princomp.The function biplot.default merely provides theunderlying code to plot two sets of variables on the same figure.Graphical parameters can also be given to biplot: the size ofxlabs and ylabs is controlled by cex.
These tests are sometimes applied to the residuals from anARMA(p, q) fit, in which case the references suggest a betterapproximation to the null-hypothesis distribution is obtained bysetting fitdf = p+q, provided of course that lag > fitdf.
bw.nrd0 implements a rule-of-thumb forchoosing the bandwidth of a Gaussian kernel density estimator.It defaults to 0.9 times theminimum of the standard deviation and the interquartile range divided by1.34 times the sample size to the negative one-fifth power(= Silverman's ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31)))unless the quartiles coincide when a positive resultwill be guaranteed.bw.nrd is the more common variation given by Scott (1992),using factor 1.06.bw.ucv and bw.bcv implement unbiased andbiased cross-validation respectively.bw.SJ implements the methods of Sheather & Jones (1991)to select the bandwidth using pilot estimation of derivatives.The algorithm for method "ste" solves an equation (viauniroot) and because of that, enlarges the intervalc(lower, upper) when the boundaries were not user-specified anddo not bracket the root.The last three methods use all pairwise binned distances: they are ofcomplexity O(n^2) up to n = nb/2 and O(n)thereafter.  Because of the binning, the results differ slightly whenx is translated or sign-flipped.
bw.nrd0 implements a rule-of-thumb forchoosing the bandwidth of a Gaussian kernel density estimator.It defaults to 0.9 times theminimum of the standard deviation and the interquartile range divided by1.34 times the sample size to the negative one-fifth power(= Silverman's ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31)))unless the quartiles coincide when a positive resultwill be guaranteed.bw.nrd is the more common variation given by Scott (1992),using factor 1.06.bw.ucv and bw.bcv implement unbiased andbiased cross-validation respectively.bw.SJ implements the methods of Sheather & Jones (1991)to select the bandwidth using pilot estimation of derivatives.The algorithm for method "ste" solves an equation (viauniroot) and because of that, enlarges the intervalc(lower, upper) when the boundaries were not user-specified anddo not bracket the root.The last three methods use all pairwise binned distances: they are ofcomplexity O(n^2) up to n = nb/2 and O(n)thereafter.  Because of the binning, the results differ slightly whenx is translated or sign-flipped.
bw.nrd0 implements a rule-of-thumb forchoosing the bandwidth of a Gaussian kernel density estimator.It defaults to 0.9 times theminimum of the standard deviation and the interquartile range divided by1.34 times the sample size to the negative one-fifth power(= Silverman's ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31)))unless the quartiles coincide when a positive resultwill be guaranteed.bw.nrd is the more common variation given by Scott (1992),using factor 1.06.bw.ucv and bw.bcv implement unbiased andbiased cross-validation respectively.bw.SJ implements the methods of Sheather & Jones (1991)to select the bandwidth using pilot estimation of derivatives.The algorithm for method "ste" solves an equation (viauniroot) and because of that, enlarges the intervalc(lower, upper) when the boundaries were not user-specified anddo not bracket the root.The last three methods use all pairwise binned distances: they are ofcomplexity O(n^2) up to n = nb/2 and O(n)thereafter.  Because of the binning, the results differ slightly whenx is translated or sign-flipped.
bw.nrd0 implements a rule-of-thumb forchoosing the bandwidth of a Gaussian kernel density estimator.It defaults to 0.9 times theminimum of the standard deviation and the interquartile range divided by1.34 times the sample size to the negative one-fifth power(= Silverman's ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31)))unless the quartiles coincide when a positive resultwill be guaranteed.bw.nrd is the more common variation given by Scott (1992),using factor 1.06.bw.ucv and bw.bcv implement unbiased andbiased cross-validation respectively.bw.SJ implements the methods of Sheather & Jones (1991)to select the bandwidth using pilot estimation of derivatives.The algorithm for method "ste" solves an equation (viauniroot) and because of that, enlarges the intervalc(lower, upper) when the boundaries were not user-specified anddo not bracket the root.The last three methods use all pairwise binned distances: they are ofcomplexity O(n^2) up to n = nb/2 and O(n)thereafter.  Because of the binning, the results differ slightly whenx is translated or sign-flipped.
bw.nrd0 implements a rule-of-thumb forchoosing the bandwidth of a Gaussian kernel density estimator.It defaults to 0.9 times theminimum of the standard deviation and the interquartile range divided by1.34 times the sample size to the negative one-fifth power(= Silverman's ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31)))unless the quartiles coincide when a positive resultwill be guaranteed.bw.nrd is the more common variation given by Scott (1992),using factor 1.06.bw.ucv and bw.bcv implement unbiased andbiased cross-validation respectively.bw.SJ implements the methods of Sheather & Jones (1991)to select the bandwidth using pilot estimation of derivatives.The algorithm for method "ste" solves an equation (viauniroot) and because of that, enlarges the intervalc(lower, upper) when the boundaries were not user-specified anddo not bracket the root.The last three methods use all pairwise binned distances: they are ofcomplexity O(n^2) up to n = nb/2 and O(n)thereafter.  Because of the binning, the results differ slightly whenx is translated or sign-flipped.
For compatibility with S, contr can be treatment,helmert, sum or poly (without quotes) as shorthandfor contr.treatment and so on.
The canonical correlation analysis seeks linear combinations of they variables which are well explained by linear combinationsof the x variables. The relationship is symmetric as‘well explained’ is measured by correlations.
NA
For type = "correlation" and "covariance", theestimates are based on the sample covariance. (The lag 0 autocorrelationis fixed at 1 by convention.)By default, no missing values are allowed.  If the na.actionfunction passes through missing values (as na.pass does), thecovariances are computed from the complete cases.  This means that theestimate computed may well not be a valid autocorrelation sequence,and may contain missing values.  Missing values are not allowed whencomputing the PACF of a multivariate time series.The partial correlation coefficient is estimated by fittingautoregressive models of successively higher orders up tolag.max.The generic function plot has a method for objects of class"acf".The lag is returned and plotted in units of time, and not numbers ofobservations.There are print and subsetting methods for objects of class"acf".
If x is a matrix with one row or column, or if x is avector and y is not given, then a goodness-of-fit testis performed (x is treated as a one-dimensionalcontingency table).  The entries of x must be non-negativeintegers.  In this case, the hypothesis tested is whether thepopulation probabilities equal those in p, or are all equal ifp is not given.If x is a matrix with at least two rows and columns, it istaken as a two-dimensional contingency table: the entries of xmust be non-negative integers.  Otherwise, x and y mustbe vectors or factors of the same length; cases with missing valuesare removed, the objects are coerced to factors, and the contingencytable is computed from these.  Then Pearson's chi-squared test isperformed of the null hypothesis that the joint distribution of thecell counts in a 2-dimensional contingency table is the product of therow and column marginals.If simulate.p.value is FALSE, the p-value is computedfrom the asymptotic chi-squared distribution of the test statistic;continuity correction is only used in the 2-by-2 case (if correctis TRUE, the default).  Otherwise the p-value is computed for aMonte Carlo test (Hope, 1968) with B replicates.In the contingency table case simulation is done by random samplingfrom the set of all contingency tables with given marginals, and worksonly if the marginals are strictly positive.  Continuity correction isnever used, and the statistic is quoted without it.  Note that this isnot the usual sampling situation assumed for the chi-squared test butrather that for Fisher's exact test.In the goodness-of-fit case simulation is done by random sampling fromthe discrete distribution specified by p, each sample beingof size n = sum(x).  This simulation is done in R and may beslow.
Multidimensional scaling takes a set of dissimilarities and returns aset of points such that the distances between the points areapproximately equal to the dissimilarities.  (It is a major part ofwhat ecologists call ‘ordination’.)A set of Euclidean distances on n points can be representedexactly in at most n - 1 dimensions.  cmdscale followsthe analysis of Mardia (1978), and returns the best-fittingk-dimensional representation, where k may be less than theargument k.The representation is only determined up to location (cmdscaletakes the column means of the configuration to be at the origin),rotations and reflections.  The configuration returned is given inprincipal-component axes, so the reflection chosen may differ betweenR platforms (see prcomp).When add = TRUE, a minimal additive constant c* iscomputed such that the dissimilarities d[i,j] +  c* are Euclidean and hence can be represented in n - 1dimensions.  Whereas S (Becker et al, 1988) computes thisconstant using an approximation suggested by Torgerson, R uses theanalytical solution of Cailliez (1983), see also Cox and Cox (2001).Note that because of numerical errors the computed eigenvalues neednot all be non-negative, and even theoretically the representationcould be in fewer than n - 1 dimensions.
All object classes which are returned by model fitting functionsshould provide a coef method or use the default one.(Note that the method is for coef and not coefficients.)The "aov" method does not report aliased coefficients (seealias) by default where complete = FALSE.The complete argument also exists for compatibility withvcov methods, and coef and aov methods forother classes should typically also keep the complete = *behavior in sync.  By that, with p <- length(coef(obj, complete = TF)),dim(vcov(obj, complete = TF)) == c(p,p) will be fulfilled for bothcomplete settings and the default.
All object classes which are returned by model fitting functionsshould provide a coef method or use the default one.(Note that the method is for coef and not coefficients.)The "aov" method does not report aliased coefficients (seealias) by default where complete = FALSE.The complete argument also exists for compatibility withvcov methods, and coef and aov methods forother classes should typically also keep the complete = *behavior in sync.  By that, with p <- length(coef(obj, complete = TF)),dim(vcov(obj, complete = TF)) == c(p,p) will be fulfilled for bothcomplete settings and the default.
NA
confint is a generic function.  The default method assumesnormality, and needs suitable coef andvcov methods to be available.  The default method can becalled directly for comparison with other methods.For objects of class "lm" the direct formulae based on tvalues are used.There are stub methods in package stats for classes "glm"and "nls" which call those in package MASS (ifinstalled): if the MASS namespace has been loaded, itsmethods will be used directly.  (Those methods are based on profilelikelihood.)
confint is a generic function.  The default method assumesnormality, and needs suitable coef andvcov methods to be available.  The default method can becalled directly for comparison with other methods.For objects of class "lm" the direct formulae based on tvalues are used.There are stub methods in package stats for classes "glm"and "nls" which call those in package MASS (ifinstalled): if the MASS namespace has been loaded, itsmethods will be used directly.  (Those methods are based on profilelikelihood.)
confint is a generic function.  The default method assumesnormality, and needs suitable coef andvcov methods to be available.  The default method can becalled directly for comparison with other methods.For objects of class "lm" the direct formulae based on tvalues are used.There are stub methods in package stats for classes "glm"and "nls" which call those in package MASS (ifinstalled): if the MASS namespace has been loaded, itsmethods will be used directly.  (Those methods are based on profilelikelihood.)
The feasible region is defined by ui %*% theta - ci >= 0. Thestarting value must be in the interior of the feasible region, but theminimum may be on the boundary.A logarithmic barrier is added to enforce the constraints and thenoptim is called. The barrier function is chosen so thatthe objective function should decrease at each outer iteration. Minimain the interior of the feasible region are typically found quitequickly, but a substantial number of outer iterations may be neededfor a minimum on the boundary.The tuning parameter mu multiplies the barrier term. Its precisevalue is often relatively unimportant. As mu increases theaugmented objective function becomes closer to the original objectivefunction but also less smooth near the boundary of the feasibleregion.Any optim method that permits infinite values for theobjective function may be used (currently all but "L-BFGS-B").The objective function f takes as first argument the vectorof parameters over which minimisation is to take place.  It shouldreturn a scalar result. Optional arguments ... will bepassed to optim and then (if not used by optim) tof. As with optim, the default is to minimise, butmaximisation can be performed by setting control$fnscale to anegative value.The gradient function grad must be supplied except withmethod = "Nelder-Mead".  It should take arguments matchingthose of f and return a vector containing the gradient.
These functions are used for creating contrast matrices for use infitting analysis of variance and regression models.  The columns ofthe resulting matrices contain contrasts which can be used for codinga factor with n levels.  The returned value contains thecomputed contrasts.  If the argument contrasts is FALSEa square indicator matrix (the dummy coding) is returned exceptfor contr.poly (which includes the 0-degree, i.e. constant,polynomial when contrasts = FALSE).contr.helmert returns Helmert contrasts, which contrast thesecond level with the first, the third with the average of the firsttwo, and so on.  contr.poly returns contrasts based onorthogonal polynomials. contr.sum uses ‘sum to zerocontrasts’.contr.treatment contrasts each level with the baseline level(specified by base): the baseline level is omitted.  Note thatthis does not produce ‘contrasts’ as defined in the standardtheory for linear models as they are not orthogonal to the intercept.contr.SAS is a wrapper for contr.treatment that setsthe base level to be the last level of the factor.  The coefficientsproduced when using these contrasts should be equivalent to thoseproduced by many (but not all) SAS procedures.For consistency, sparse is an argument to all these contrastfunctions, however sparse = TRUE for contr.polyis typically pointless and is rarely useful forcontr.helmert.
These functions are used for creating contrast matrices for use infitting analysis of variance and regression models.  The columns ofthe resulting matrices contain contrasts which can be used for codinga factor with n levels.  The returned value contains thecomputed contrasts.  If the argument contrasts is FALSEa square indicator matrix (the dummy coding) is returned exceptfor contr.poly (which includes the 0-degree, i.e. constant,polynomial when contrasts = FALSE).contr.helmert returns Helmert contrasts, which contrast thesecond level with the first, the third with the average of the firsttwo, and so on.  contr.poly returns contrasts based onorthogonal polynomials. contr.sum uses ‘sum to zerocontrasts’.contr.treatment contrasts each level with the baseline level(specified by base): the baseline level is omitted.  Note thatthis does not produce ‘contrasts’ as defined in the standardtheory for linear models as they are not orthogonal to the intercept.contr.SAS is a wrapper for contr.treatment that setsthe base level to be the last level of the factor.  The coefficientsproduced when using these contrasts should be equivalent to thoseproduced by many (but not all) SAS procedures.For consistency, sparse is an argument to all these contrastfunctions, however sparse = TRUE for contr.polyis typically pointless and is rarely useful forcontr.helmert.
These functions are used for creating contrast matrices for use infitting analysis of variance and regression models.  The columns ofthe resulting matrices contain contrasts which can be used for codinga factor with n levels.  The returned value contains thecomputed contrasts.  If the argument contrasts is FALSEa square indicator matrix (the dummy coding) is returned exceptfor contr.poly (which includes the 0-degree, i.e. constant,polynomial when contrasts = FALSE).contr.helmert returns Helmert contrasts, which contrast thesecond level with the first, the third with the average of the firsttwo, and so on.  contr.poly returns contrasts based onorthogonal polynomials. contr.sum uses ‘sum to zerocontrasts’.contr.treatment contrasts each level with the baseline level(specified by base): the baseline level is omitted.  Note thatthis does not produce ‘contrasts’ as defined in the standardtheory for linear models as they are not orthogonal to the intercept.contr.SAS is a wrapper for contr.treatment that setsthe base level to be the last level of the factor.  The coefficientsproduced when using these contrasts should be equivalent to thoseproduced by many (but not all) SAS procedures.For consistency, sparse is an argument to all these contrastfunctions, however sparse = TRUE for contr.polyis typically pointless and is rarely useful forcontr.helmert.
These functions are used for creating contrast matrices for use infitting analysis of variance and regression models.  The columns ofthe resulting matrices contain contrasts which can be used for codinga factor with n levels.  The returned value contains thecomputed contrasts.  If the argument contrasts is FALSEa square indicator matrix (the dummy coding) is returned exceptfor contr.poly (which includes the 0-degree, i.e. constant,polynomial when contrasts = FALSE).contr.helmert returns Helmert contrasts, which contrast thesecond level with the first, the third with the average of the firsttwo, and so on.  contr.poly returns contrasts based onorthogonal polynomials. contr.sum uses ‘sum to zerocontrasts’.contr.treatment contrasts each level with the baseline level(specified by base): the baseline level is omitted.  Note thatthis does not produce ‘contrasts’ as defined in the standardtheory for linear models as they are not orthogonal to the intercept.contr.SAS is a wrapper for contr.treatment that setsthe base level to be the last level of the factor.  The coefficientsproduced when using these contrasts should be equivalent to thoseproduced by many (but not all) SAS procedures.For consistency, sparse is an argument to all these contrastfunctions, however sparse = TRUE for contr.polyis typically pointless and is rarely useful forcontr.helmert.
These functions are used for creating contrast matrices for use infitting analysis of variance and regression models.  The columns ofthe resulting matrices contain contrasts which can be used for codinga factor with n levels.  The returned value contains thecomputed contrasts.  If the argument contrasts is FALSEa square indicator matrix (the dummy coding) is returned exceptfor contr.poly (which includes the 0-degree, i.e. constant,polynomial when contrasts = FALSE).contr.helmert returns Helmert contrasts, which contrast thesecond level with the first, the third with the average of the firsttwo, and so on.  contr.poly returns contrasts based onorthogonal polynomials. contr.sum uses ‘sum to zerocontrasts’.contr.treatment contrasts each level with the baseline level(specified by base): the baseline level is omitted.  Note thatthis does not produce ‘contrasts’ as defined in the standardtheory for linear models as they are not orthogonal to the intercept.contr.SAS is a wrapper for contr.treatment that setsthe base level to be the last level of the factor.  The coefficientsproduced when using these contrasts should be equivalent to thoseproduced by many (but not all) SAS procedures.For consistency, sparse is an argument to all these contrastfunctions, however sparse = TRUE for contr.polyis typically pointless and is rarely useful forcontr.helmert.
If contrasts are not set for a factor the default functions fromoptions("contrasts") are used.A logical vector x is converted into a two-level factor withlevels c(FALSE, TRUE) (regardless of which levels occur in thevariable).The argument contrasts is ignored if x has a matrixcontrasts attribute set.  Otherwise if contrasts = TRUEit is passed to a contrasts function such ascontr.treatment and if contrasts = FALSEan identity matrix is returned.  Suitable functions have a firstargument which is the character vector of levels, a named argumentcontrasts (always called with contrasts = TRUE) andoptionally a logical argument sparse.If value supplies more than how.many contrasts, thefirst how.many are used.  If too few are supplied, a suitablecontrast matrix is created by extending value after ensuringits columns are contrasts (orthogonal to the constant term) and notcollinear.
If contrasts are not set for a factor the default functions fromoptions("contrasts") are used.A logical vector x is converted into a two-level factor withlevels c(FALSE, TRUE) (regardless of which levels occur in thevariable).The argument contrasts is ignored if x has a matrixcontrasts attribute set.  Otherwise if contrasts = TRUEit is passed to a contrasts function such ascontr.treatment and if contrasts = FALSEan identity matrix is returned.  Suitable functions have a firstargument which is the character vector of levels, a named argumentcontrasts (always called with contrasts = TRUE) andoptionally a logical argument sparse.If value supplies more than how.many contrasts, thefirst how.many are used.  If too few are supplied, a suitablecontrast matrix is created by extending value after ensuringits columns are contrasts (orthogonal to the constant term) and notcollinear.
The Fast Fourier Transform, fft, is used for efficiency.The input sequences x and  y must have the same length ifcircular is true.Note that the usual definition of convolution of two sequencesx and y is given by convolve(x, rev(y), type = "o").
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
The cophenetic distance between two observations that have beenclustered is defined to be the intergroup dissimilarity at which thetwo observations are first combined into a single cluster.Note that this distance has many ties and restrictions.It can be argued that a dendrogram is an appropriate summary of somedata if the correlation between the original distances and thecophenetic distances is high.  Otherwise, it should simply be viewed asthe description of the output of the clustering algorithm.cophenetic is a generic function.  Support for classes whichrepresent hierarchical clusterings (total indexed hierarchies) can beadded by providing an as.hclust() or, more directly, acophenetic() method for such a class.The method for objects of class "dendrogram" requiresthat all leaves of the dendrogram object have non-null labels.
For cov and cor one must either give a matrix ordata frame for x or give both x and y.The inputs must be numeric (as determined by is.numeric:logical values are also allowed for historical compatibility): the"kendall" and "spearman" methods make sense for orderedinputs but xtfrm can be used to find a suitable priortransformation to numbers.var is just another interface to cov, wherena.rm is used to determine the default for use when thatis unspecified.  If na.rm is TRUE then the completeobservations (rows) are used (use = "na.or.complete") tocompute the variance.  Otherwise, by default use = "everything".If use is "everything", NAs willpropagate conceptually, i.e., a resulting value will be NAwhenever one of its contributing observations is NA.If use is "all.obs", then the presence of missingobservations will produce an error.  If use is"complete.obs" then missing values are handled by casewisedeletion (and if there are no complete cases, that gives an error)."na.or.complete" is the same unless there are no completecases, that gives NA.Finally, if use has the value "pairwise.complete.obs"then the correlation or covariance between each pair of variables iscomputed using all complete pairs of observations on those variables.This can result in covariance or correlation matrices which are not positivesemi-definite, as well as NA entries if there are no completepairs for that pair of variables.   For cov and var,"pairwise.complete.obs" only works with the "pearson"method.Note that (the equivalent of) var(double(0), use = *) givesNA for use = "everything" and "na.or.complete",and gives an error in the other cases.The denominator n - 1 is used which gives an unbiased estimatorof the (co)variance for i.i.d. observations.These functions return NA when there is only oneobservation (whereas S-PLUS has been returning NaN).For cor(), if method is "kendall" or"spearman", Kendall's tau or Spearman'srho statistic is used to estimate a rank-based measure ofassociation.  These are more robust and have been recommended if thedata do not necessarily come from a bivariate normal distribution.For cov(), a non-Pearson method is unusual but available forthe sake of completeness.  Note that "spearman" basicallycomputes cor(R(x), R(y)) (or cov(., .)) where R(u)  := rank(u, na.last = "keep"). In the case of missing values, theranks are calculated depending on the value of use, eitherbased on complete observations, or based on pairwise completeness withreranking for each pair.When there are ties, Kendall's tau_b is computed, asproposed by Kendall (1945).Scaling a covariance matrix into a correlation one can be achieved inmany ways, mathematically most appealing by multiplication with adiagonal matrix from left and right, or more efficiently by usingsweep(.., FUN = "/") twice.  The cov2cor functionis even a bit more efficient, and provided mostly for didacticalreasons.
The three methods each estimate the association between paired samplesand compute a test of the value being zero.  They use differentmeasures of association, all in the range [-1, 1] with 0indicating no association.  These are sometimes referred to as testsof no correlation, but that term is often confined to thedefault method.If method is "pearson", the test statistic is based onPearson's product moment correlation coefficient cor(x, y) andfollows a t distribution with length(x)-2 degrees of freedomif the samples follow independent normal distributions.  If there areat least 4 complete pairs of observation, an asymptotic confidenceinterval is given based on Fisher's Z transform.If method is "kendall" or "spearman", Kendall'stau or Spearman's rho statistic is used toestimate a rank-based measure of association.  These tests may be usedif the data do not necessarily come from a bivariate normaldistribution.For Kendall's test, by default (if exact is NULL), an exactp-value is computed if there are less than 50 paired samples containingfinite values and there are no ties.  Otherwise, the test statistic isthe estimate scaled to zero mean and unit variance, and is approximatelynormally distributed.For Spearman's test, p-values are computed using algorithm AS 89 forn < 1290 and exact = TRUE, otherwise via the asymptotict approximation.  Note that these are ‘exact’ for n  < 10, and use an Edgeworth series approximation for larger samplesizes (the cutoff has been changed from the original paper).
For cov and cor one must either give a matrix ordata frame for x or give both x and y.The inputs must be numeric (as determined by is.numeric:logical values are also allowed for historical compatibility): the"kendall" and "spearman" methods make sense for orderedinputs but xtfrm can be used to find a suitable priortransformation to numbers.var is just another interface to cov, wherena.rm is used to determine the default for use when thatis unspecified.  If na.rm is TRUE then the completeobservations (rows) are used (use = "na.or.complete") tocompute the variance.  Otherwise, by default use = "everything".If use is "everything", NAs willpropagate conceptually, i.e., a resulting value will be NAwhenever one of its contributing observations is NA.If use is "all.obs", then the presence of missingobservations will produce an error.  If use is"complete.obs" then missing values are handled by casewisedeletion (and if there are no complete cases, that gives an error)."na.or.complete" is the same unless there are no completecases, that gives NA.Finally, if use has the value "pairwise.complete.obs"then the correlation or covariance between each pair of variables iscomputed using all complete pairs of observations on those variables.This can result in covariance or correlation matrices which are not positivesemi-definite, as well as NA entries if there are no completepairs for that pair of variables.   For cov and var,"pairwise.complete.obs" only works with the "pearson"method.Note that (the equivalent of) var(double(0), use = *) givesNA for use = "everything" and "na.or.complete",and gives an error in the other cases.The denominator n - 1 is used which gives an unbiased estimatorof the (co)variance for i.i.d. observations.These functions return NA when there is only oneobservation (whereas S-PLUS has been returning NaN).For cor(), if method is "kendall" or"spearman", Kendall's tau or Spearman'srho statistic is used to estimate a rank-based measure ofassociation.  These are more robust and have been recommended if thedata do not necessarily come from a bivariate normal distribution.For cov(), a non-Pearson method is unusual but available forthe sake of completeness.  Note that "spearman" basicallycomputes cor(R(x), R(y)) (or cov(., .)) where R(u)  := rank(u, na.last = "keep"). In the case of missing values, theranks are calculated depending on the value of use, eitherbased on complete observations, or based on pairwise completeness withreranking for each pair.When there are ties, Kendall's tau_b is computed, asproposed by Kendall (1945).Scaling a covariance matrix into a correlation one can be achieved inmany ways, mathematically most appealing by multiplication with adiagonal matrix from left and right, or more efficiently by usingsweep(.., FUN = "/") twice.  The cov2cor functionis even a bit more efficient, and provided mostly for didacticalreasons.
By default, method = "unbiased",The covariance matrix is divided by one minus the sum of squares ofthe weights, so if the weights are the default (1/n) the conventionalunbiased estimate of the covariance matrix with divisor (n - 1)is obtained.  This differs from the behaviour in S-PLUS whichcorresponds to method = "ML" and does not divide.
For cov and cor one must either give a matrix ordata frame for x or give both x and y.The inputs must be numeric (as determined by is.numeric:logical values are also allowed for historical compatibility): the"kendall" and "spearman" methods make sense for orderedinputs but xtfrm can be used to find a suitable priortransformation to numbers.var is just another interface to cov, wherena.rm is used to determine the default for use when thatis unspecified.  If na.rm is TRUE then the completeobservations (rows) are used (use = "na.or.complete") tocompute the variance.  Otherwise, by default use = "everything".If use is "everything", NAs willpropagate conceptually, i.e., a resulting value will be NAwhenever one of its contributing observations is NA.If use is "all.obs", then the presence of missingobservations will produce an error.  If use is"complete.obs" then missing values are handled by casewisedeletion (and if there are no complete cases, that gives an error)."na.or.complete" is the same unless there are no completecases, that gives NA.Finally, if use has the value "pairwise.complete.obs"then the correlation or covariance between each pair of variables iscomputed using all complete pairs of observations on those variables.This can result in covariance or correlation matrices which are not positivesemi-definite, as well as NA entries if there are no completepairs for that pair of variables.   For cov and var,"pairwise.complete.obs" only works with the "pearson"method.Note that (the equivalent of) var(double(0), use = *) givesNA for use = "everything" and "na.or.complete",and gives an error in the other cases.The denominator n - 1 is used which gives an unbiased estimatorof the (co)variance for i.i.d. observations.These functions return NA when there is only oneobservation (whereas S-PLUS has been returning NaN).For cor(), if method is "kendall" or"spearman", Kendall's tau or Spearman'srho statistic is used to estimate a rank-based measure ofassociation.  These are more robust and have been recommended if thedata do not necessarily come from a bivariate normal distribution.For cov(), a non-Pearson method is unusual but available forthe sake of completeness.  Note that "spearman" basicallycomputes cor(R(x), R(y)) (or cov(., .)) where R(u)  := rank(u, na.last = "keep"). In the case of missing values, theranks are calculated depending on the value of use, eitherbased on complete observations, or based on pairwise completeness withreranking for each pair.When there are ties, Kendall's tau_b is computed, asproposed by Kendall (1945).Scaling a covariance matrix into a correlation one can be achieved inmany ways, mathematically most appealing by multiplication with adiagonal matrix from left and right, or more efficiently by usingsweep(.., FUN = "/") twice.  The cov2cor functionis even a bit more efficient, and provided mostly for didacticalreasons.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
NA
Cutting trees at a given height is only possible for ultrametric trees(with monotone clustering heights).
These are all generic functions, which will use thetsp attribute of x if it exists. timeand cycle have methods for class ts that coercethe result to that class.
D is modelled after its S namesake for taking simple symbolicderivatives.deriv is a generic function with a default and aformula method.  It returns a call forcomputing the expr and its (partial) derivatives,simultaneously.  It uses so-called algorithmic derivatives.  Iffunction.arg is a function, its arguments can have defaultvalues, see the fx example below.Currently, deriv.formula just calls deriv.default afterextracting the expression to the right of ~.deriv3 and its methods are equivalent to deriv and itsmethods except that hessian defaults to TRUE forderiv3.The internal code knows about the arithmetic operators +,-, *, / and ^, and the single-variablefunctions exp, log, sin, cos, tan,sinh, cosh, sqrt, pnorm, dnorm,asin, acos, atan, gamma, lgamma,digamma and trigamma, as well as psigamma for oneor two arguments (but derivative only with respect to the first).(Note that only the standard normal distribution is considered.)Since R 3.4.0, the single-variable functions log1p,expm1, log2, log10, cospi,sinpi, tanpi, factorial, andlfactorial are supported as well.
The Beta distribution with parameters shape1 = a andshape2 = b has densityΓ(a+b)/(Γ(a)Γ(b))x^(a-1)(1-x)^(b-1)for a > 0, b > 0 and 0 ≤ x ≤ 1where the boundary values at x=0 or x=1 are defined asby continuity (as limits).The mean is a/(a+b) and the variance is ab/((a+b)^2 (a+b+1)).These moments and all distributional properties can be defined aslimits (leading to point masses at 0, 1/2, or 1) when a orb are zero or infinite, and the corresponding[dpqr]beta() functions are defined correspondingly.pbeta is closely related to the incomplete beta function.  Asdefined by Abramowitz and Stegun 6.6.1B_x(a,b) =    integral_0^x t^(a-1) (1-t)^(b-1) dt,and 6.6.2 I_x(a,b) = B_x(a,b) / B(a,b) whereB(a,b) = B_1(a,b) is the Beta function (beta).I_x(a,b) is pbeta(x, a, b).The noncentral Beta distribution (with ncp  = λ)is defined (Johnson et al, 1995, pp. 502) as the distribution ofX/(X+Y) where X ~ chi^2_2a(λ)and Y ~ chi^2_2b.
The binomial distribution with size = n andprob = p has density    p(x) = choose(n, x) p^x (1-p)^(n-x)for x = 0, …, n.Note that binomial coefficients can be computed bychoose in R.If an element of x is not integer, the result of dbinomis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference below.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
If location or scale are not specified, they assumethe default values of 0 and 1 respectively.The Cauchy distribution with location l and scale s hasdensityf(x) = 1 / (π s (1 + ((x-l)/s)^2))for all x.
The chi-squared distribution with df= n ≥ 0degrees of freedom has densityf_n(x) = 1 / (2^(n/2) Γ(n/2))  x^(n/2-1) e^(-x/2)for x > 0, where f_0(x) := \lim_{n \to 0} f_n(x) =  δ_0(x), a point mass at zero, is not a density function proper, buta “δ distribution”.The mean and variance are n and 2n.The non-central chi-squared distribution with df= ndegrees of freedom and non-centrality parameter ncp= λ has densityf(x) = exp(-λ/2) SUM_{r=0}^∞ ((λ/2)^r / r!) dchisq(x, df + 2r)  for x ≥ 0.  For integer n, this is the distribution ofthe sum of squares of n normals each with variance one,λ being the sum of squares of the normal means; further,E(X) = n + λ, Var(X) = 2(n + 2*λ), andE((X - E(X))^3) = 8(n + 3*λ).Note that the degrees of freedom df= n, can benon-integer, and also n = 0 which is relevant fornon-centrality λ > 0,see Johnson et al (1995, chapter 29).In that (noncentral, zero df) case, the distribution is a mixture of apoint mass at x = 0 (of size pchisq(0, df=0, ncp=ncp)) anda continuous part, and dchisq() is not a density withrespect to that mixture measure but rather the limit of the densityfor df -> 0.Note that ncp values larger than about 1e5 (and even smaller)  may give inaccurateresults with many warnings for pchisq and qchisq.
The additive model used is:Y[t] = T[t] + S[t] + e[t]The multiplicative model used is:Y[t] = T[t] * S[t] * e[t]The function first determines the trend component using a movingaverage (if filter is NULL, a symmetric window withequal weights is used), and removes it from the time series.  Then,the seasonal figure is computed by averaging, for each time unit, overall periods.  The seasonal figure is then centered.   Finally, the errorcomponent is determined by removing trend and seasonal figure(recycled as needed) from the original time series.This only works well if x covers an integer number of completeperiods.
NA
These are all generic functions, which will use thetsp attribute of x if it exists. timeand cycle have methods for class ts that coercethe result to that class.
NA
The algorithm used in density.default disperses the mass of theempirical distribution function over a regular grid of at least 512points and then uses the fast Fourier transform to convolve thisapproximation with a discretized version of the kernel and then useslinear approximation to evaluate the density at the specified points.The statistical properties of a kernel are determined bysig^2 (K) = int(t^2 K(t) dt)which is always = 1 for our kernels (and hence the bandwidthbw is the standard deviation of the kernel) andR(K) = int(K^2(t) dt).MSE-equivalent bandwidths (for different kernels) are proportional tosig(K) R(K) which is scale invariant and for ourkernels equal to R(K).  This value is returned whengive.Rkern = TRUE.  See the examples for using exact equivalentbandwidths.Infinite values in x are assumed to correspond to a point mass at+/-Inf and the density estimate is of the sub-density on(-Inf, +Inf).
The algorithm used in density.default disperses the mass of theempirical distribution function over a regular grid of at least 512points and then uses the fast Fourier transform to convolve thisapproximation with a discretized version of the kernel and then useslinear approximation to evaluate the density at the specified points.The statistical properties of a kernel are determined bysig^2 (K) = int(t^2 K(t) dt)which is always = 1 for our kernels (and hence the bandwidthbw is the standard deviation of the kernel) andR(K) = int(K^2(t) dt).MSE-equivalent bandwidths (for different kernels) are proportional tosig(K) R(K) which is scale invariant and for ourkernels equal to R(K).  This value is returned whengive.Rkern = TRUE.  See the examples for using exact equivalentbandwidths.Infinite values in x are assumed to correspond to a point mass at+/-Inf and the density estimate is of the sub-density on(-Inf, +Inf).
D is modelled after its S namesake for taking simple symbolicderivatives.deriv is a generic function with a default and aformula method.  It returns a call forcomputing the expr and its (partial) derivatives,simultaneously.  It uses so-called algorithmic derivatives.  Iffunction.arg is a function, its arguments can have defaultvalues, see the fx example below.Currently, deriv.formula just calls deriv.default afterextracting the expression to the right of ~.deriv3 and its methods are equivalent to deriv and itsmethods except that hessian defaults to TRUE forderiv3.The internal code knows about the arithmetic operators +,-, *, / and ^, and the single-variablefunctions exp, log, sin, cos, tan,sinh, cosh, sqrt, pnorm, dnorm,asin, acos, atan, gamma, lgamma,digamma and trigamma, as well as psigamma for oneor two arguments (but derivative only with respect to the first).(Note that only the standard normal distribution is considered.)Since R 3.4.0, the single-variable functions log1p,expm1, log2, log10, cospi,sinpi, tanpi, factorial, andlfactorial are supported as well.
D is modelled after its S namesake for taking simple symbolicderivatives.deriv is a generic function with a default and aformula method.  It returns a call forcomputing the expr and its (partial) derivatives,simultaneously.  It uses so-called algorithmic derivatives.  Iffunction.arg is a function, its arguments can have defaultvalues, see the fx example below.Currently, deriv.formula just calls deriv.default afterextracting the expression to the right of ~.deriv3 and its methods are equivalent to deriv and itsmethods except that hessian defaults to TRUE forderiv3.The internal code knows about the arithmetic operators +,-, *, / and ^, and the single-variablefunctions exp, log, sin, cos, tan,sinh, cosh, sqrt, pnorm, dnorm,asin, acos, atan, gamma, lgamma,digamma and trigamma, as well as psigamma for oneor two arguments (but derivative only with respect to the first).(Note that only the standard normal distribution is considered.)Since R 3.4.0, the single-variable functions log1p,expm1, log2, log10, cospi,sinpi, tanpi, factorial, andlfactorial are supported as well.
This is a generic function which can be used to extract deviances forfitted models.  Consult the individual modeling functions for detailson how to use this function.
If rate is not specified, it assumes the default value of1.The exponential distribution with rate λ has densityf(x) = λ {e}^{- λ x} for x ≥ 0.
The F distribution with df1 = n1 and df2 =n2 degrees of freedom has densityf(x) = Γ((n1 + n2)/2) / (Γ(n1/2) Γ(n2/2))    (n1/n2)^(n1/2) x^(n1/2 - 1)    (1 + (n1/n2) x)^-(n1 + n2)/2for x > 0.It is the distribution of the ratio of the mean squares ofn1 and n2 independent standard normals, and henceof the ratio of two independent chi-squared variates each divided by itsdegrees of freedom.  Since the ratio of a normal and the rootmean-square of m independent normals has a Student's t_mdistribution, the square of a t_m variate has a F distribution on1 and m degrees of freedom.The non-central F distribution is again the ratio of mean squares ofindependent normals of unit variance, but those in the numerator areallowed to have non-zero means and ncp is the sum of squares ofthe means.  See Chisquare for further details onnon-central distributions.
kernel is used to construct a general kernel or named specifickernels.  The modified Daniell kernel halves the end coefficients (asused by S-PLUS).The [ method allows natural indexing of kernel objectswith indices in (-m) : m.  The normalization is such that fork <- kernel(*), sum(k[ -k$m : k$m ]) is one.df.kernel returns the ‘equivalent degrees of freedom’ ofa smoothing kernel as defined in Brockwell and Davis (1991), page362, and bandwidth.kernel returns the equivalent bandwidth asdefined in Bloomfield (1976), p. 201, with a continuity correction.
This is a generic function which can be used to extract residualdegrees-of-freedom for fitted models.  Consult the individual modelingfunctions for details on how to use this function.The default method just extracts the df.residual component.
The models fit by, e.g., the lm and glm functionsare specified in a compact symbolic form.The ~ operator is basic in the formation of such models.An expression of the form y ~ model is interpretedas a specification that the response y is modelledby a linear predictor specified symbolically by model.Such a model consists of a series of terms separatedby + operators.The terms themselves consist of variable and factornames separated by : operators.Such a term is interpreted as the interaction ofall the variables and factors appearing in the term.In addition to + and :, a number of other operators areuseful in model formulae.  The * operator denotes factorcrossing: a*b interpreted as a+b+a:b.  The ^operator indicates crossing to the specified degree.  For example(a+b+c)^2 is identical to (a+b+c)*(a+b+c) which in turnexpands to a formula containing the main effects for a,b and c together with their second-order interactions.The %in% operator indicates that the terms on its left arenested within those on the right.  For example a + b %in% aexpands to the formula a + a:b.  The - operator removesthe specified terms, so that (a+b+c)^2 - a:b is identical toa + b + c + b:c + a:c.  It can also used to remove theintercept term: when fitting a linear model y ~ x - 1 specifiesa line through the origin.  A model with no intercept can be alsospecified as y ~ x + 0 or y ~ 0 + x.While formulae usually involve just variable and factornames, they can also involve arithmetic expressions.The formula log(y) ~ a + log(x) is quite legal.When such arithmetic expressions involveoperators which are also used symbolicallyin model formulae, there can be confusion betweenarithmetic and symbolic operator use.To avoid this confusion, the function I()can be used to bracket those portions of a modelformula where the operators are used in theirarithmetic sense.  For example, in the formulay ~ a + I(b+c), the term b+c is to beinterpreted as the sum of b and c.Variable names can be quoted by backticks `like this` informulae, although there is no guarantee that all code using formulaewill accept such non-syntactic names.Most model-fitting functions accept formulae with right-hand-sideincluding the function offset to indicate terms with afixed coefficient of one.  Some functions accept other‘specials’ such as strata or cluster (see thespecials argument of terms.formula).There are two special interpretations of . in a formula.  Theusual one is in the context of a data argument of modelfitting functions and means ‘all columns not otherwise in theformula’: see terms.formula.  In the context ofupdate.formula, only, it means ‘what waspreviously in this part of the formula’.When formula is called on a fitted model object, either aspecific method is used (such as that for class "nls") or thedefault method.  The default first looks for a "formula"component of the object (and evaluates it), then a "terms"component, then a formula parameter of the call (and evaluatesits value) and finally a "formula" attribute.There is a formula method for data frames.  When there's"terms" attribute with a formula, e.g., for amodel.frame(), that formula is returned.  If you'd like theprevious (R <= 3.5.x) behavior, use the auxiliaryDF2formula() which does not consider a "terms" attribute.Otherwise, ifthere is onlyone column this forms the RHS with an empty LHS.  For more columns,the first column is the LHS of the formula and the remaining columnsseparated by + form the RHS.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
If scale is omitted, it assumes the default value of 1.The Gamma distribution with parameters shape = aand scale = s has densityf(x)= 1/(s^a Gamma(a)) x^(a-1) e^-(x/s)for x ≥ 0, a > 0 and s > 0.(Here Gamma(a) is the function implemented by R'sgamma() and defined in its help.  Note that a = 0corresponds to the trivial distribution with all mass at point 0.)The mean and variance areE(X) = a*s andVar(X) = a*s^2.The cumulative hazard H(t) = - log(1 - F(t))isNote that for smallish values of shape (and moderatescale) a large parts of the mass of the Gamma distribution ison values of x so near zero that they will be represented aszero in computer arithmetic.  So rgamma may well return valueswhich will be represented as zero.  (This will also happen for verylarge values of scale since the actual generation is done forscale = 1.)
The geometric distribution with prob = p has densityp(x) = p (1-p)^xfor x = 0, 1, 2, …, 0 < p ≤ 1.If an element of x is not integer, the result of dgeomis zero, with a warning.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
The hypergeometric distribution is used for sampling withoutreplacement.  The density of this distribution with parametersm, n and k (named Np, N-Np, andn, respectively in the reference below, where N := m+n is also usedin other references) is given byp(x) =      choose(m, x) choose(n, k-x) / choose(m+n, k)for x = 0, …, k.Note that p(x) is non-zero only formax(0, k-n) <= x <= min(k, m).With p := m/(m+n) (hence Np = N \times p in thereference's notation), the first two moments are meanE[X] = μ = k p and variance               Var(X) = k p (1 - p) * (m+n-k)/(m+n-1),which shows the closeness to the Binomial(k,p) (where thehypergeometric has smaller variance unless k = 1).The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.In rhyper(), if one of m, n, k exceeds .Machine$integer.max,currently the equivalent of qhyper(runif(nn), m,n,k) is usedwhich is comparably slow while instead a binomial approximation may beconsiderably more efficient.
diffinv is a generic function with methods for class "ts"and default for vectors and matrices.Missing values are not handled.
Available distance measures are (written for two vectors x andy):Usual distance between the two vectors (2norm aka L_2), sqrt(sum((x_i - y_i)^2)).Maximum distance between two components of xand y (supremum norm)Absolute distance between the two vectors (1 norm aka L_1).sum(|x_i - y_i| / (|x_i| + |y_i|)).Terms with zero numerator and denominator are omitted from the sumand treated as if the values were missing.This is intended for non-negative values (e.g., counts), in whichcase the denominator can be written in various equivalent ways;Originally, R used x_i + y_i, then from 1998 to 2017,|x_i + y_i|, and then the correct |x_i| + |y_i|.(aka asymmetric binary): The vectorsare regarded as binary bits, so non-zero elements are ‘on’and zero elements are ‘off’.  The distance is theproportion of bits in which only one is on amongst those inwhich at least one is on.The p norm, the pth root of thesum of the pth powers of the differences of the components.Missing values are allowed, and are excluded from all computationsinvolving the rows within which they occur.Further, when Inf values are involved, all pairs of values areexcluded when their contribution to the distance gave NaN orNA.If some columns are excluded in calculating a Euclidean, Manhattan,Canberra or Minkowski distance, the sum is scaled up proportionally tothe number of columns used.  If all pairs are excluded whencalculating a particular distance, the value is NA.The "dist" method of as.matrix() and as.dist()can be used for conversion between objects of class "dist"and conventional distance matrices.as.dist() is a generic function.  Its default method handlesobjects inheriting from class "dist", or coercible to matricesusing as.matrix().  Support for classes representingdistances (also known as dissimilarities) can be added by providing anas.matrix() or, more directly, an as.dist methodfor such a class.
The log normal distribution has densityf(x) = 1/(√(2 π) σ x) e^-((log x - μ)^2 / (2 σ^2))where μ and σ are the mean and standarddeviation of the logarithm.The mean is E(X) = exp(μ + 1/2 σ^2),the median is med(X) = exp(μ), and the varianceVar(X) = exp(2*μ + σ^2)*(exp(σ^2) - 1)and hence the coefficient of variation issqrt(exp(σ^2) - 1) which isapproximately σ when that is small (e.g., σ < 1/2).
If location or scale are omitted, they assume thedefault values of 0 and 1 respectively.The Logistic distribution with location = m andscale = s has distribution functionF(x) = 1 / (1 + exp(-(x-m)/s))  and densityf(x) = 1/s exp((x-m)/s) (1 + exp((x-m)/s))^-2.It is a long-tailed distribution with mean m and varianceπ^2 /3 s^2.
If x is a K-component vector, dmultinom(x, prob)is the probabilityP(X[1]=x[1], … , X[K]=x[k]) = C * prod(j=1 , …, K) p[j]^x[j]where C is the ‘multinomial coefficient’C = N! / (x[1]! * … * x[K]!)and N = sum(j=1, …, K) x[j].By definition, each component X[j] is binomially distributed asBin(size, prob[j]) for j = 1, …, K.The rmultinom() algorithm draws binomials X[j] fromBin(n[j], P[j]) sequentially, wheren[1] = N (N := size),P[1] = p[1] (p is prob scaled to sum 1),and for j ≥ 2, recursively,n[j] = N - sum(k=1, …, j-1) X[k]andP[j] = p[j] / (1 - sum(p[1:(j-1)])).
The negative binomial distribution with size = n andprob = p has density    Γ(x+n)/(Γ(n) x!) p^n (1-p)^xfor x = 0, 1, 2, …, n > 0 and 0 < p ≤ 1.This represents the number of failures which occur in a sequence ofBernoulli trials before a target number of successes is reached.The mean is μ = n(1-p)/p and variance n(1-p)/p^2.A negative binomial distribution can also arise as a mixture ofPoisson distributions with mean distributed as a gamma distribution(see pgamma) with scale parameter (1 - prob)/proband shape parameter size.  (This definition allows non-integervalues of size.)An alternative parametrization (often used in ecology) is by themean mu (see above), and size, the dispersionparameter, where prob = size/(size+mu).  The varianceis mu + mu^2/size in this parametrization.If an element of x is not integer, the result of dnbinomis zero, with a warning.The case size == 0 is the distribution concentrated at zero.This is the limiting distribution for size approaching zero,even if mu rather than prob is held constant.  Noticethough, that the mean of the limit distribution is 0, whatever thevalue of mu.The quantile is defined as the smallest value x such thatF(x) ≥ p, where F is the distribution function.
If mean or sd are not specified they assume the defaultvalues of 0 and 1, respectively.The normal distribution has density    f(x) = 1/(√(2 π) σ) e^-((x - μ)^2/(2 σ^2))  where μ is the mean of the distribution andσ the standard deviation.
The Poisson distribution has densityp(x) = λ^x exp(-λ)/x!for x = 0, 1, 2, … .The mean and variance are E(X) = Var(X) = λ.Note that λ = 0 is really a limit case (setting0^0 = 1) resulting in a point mass at 0, see also the example.If an element of x is not integer, the result of dpoisis zero, with a warning.p(x) is computed using Loader's algorithm, see the reference indbinom.The quantile is right continuous: qpois(p, lambda) is the smallestinteger x such that P(X ≤ x) ≥ p.Setting lower.tail = FALSE allows to get much more preciseresults when the default, lower.tail = TRUE would return 1, seethe example below.
factor.scope is not intended to be called directly by users.
NA
For drop1 methods, a missing scope is taken to be allterms in the model. The hierarchy is respected when considering termsto be added or dropped: all main effects contained in a second-orderinteraction must remain, and so on.In a scope formula . means ‘what is already there’.The methods for lm and glm are moreefficient in that they do not recompute the model matrix and call thefit methods directly.The default output table gives AIC, defined as minus twice loglikelihood plus 2p where p is the rank of the model (thenumber of effective parameters).  This is only defined up to anadditive constant (like log-likelihoods).  For linear Gaussian modelswith fixed scale, the constant is chosen to give Mallows' Cp,RSS/scale + 2p - n.  Where Cp is used,the column is labelled as Cp rather than AIC.The F tests for the "glm" methods are based on analysis ofdeviance tests, so if the dispersion is estimated it is based on theresidual deviance, unlike the F tests of anova.glm.
This distribution is obtained as follows.  Let x be a sample ofsize n from a continuous distribution symmetric about theorigin.  Then the Wilcoxon signed rank statistic is the sum of theranks of the absolute values x[i] for which x[i] ispositive.  This statistic takes values between 0 andn(n+1)/2, and its mean and variance are n(n+1)/4 andn(n+1)(2n+1)/24, respectively.If either of the first two arguments is a vector, the recycling rule isused to do the calculations for all combinations of the two up tothe length of the longer vector.
The t distribution with df = n degrees offreedom has densityf(x) = Γ((n+1)/2) / (√(n π) Γ(n/2)) (1 + x^2/n)^-((n+1)/2)for all real x.It has mean 0 (for n > 1) andvariance n/(n-2) (for n > 2).The general non-central twith parameters (df, Del) = (df, ncp)is defined as the distribution ofT(df, Del) := (U + Del) / √(V/df) where U and V  are independent randomvariables, U ~ N(0,1) andV ~ χ^2(df) (see Chisquare).The most used applications are power calculations for t-tests:Let T= (mX - m0) / (S/sqrt(n))wheremX is the mean and S the sample standarddeviation (sd) of X_1, X_2, …, X_n which arei.i.d. N(μ, σ^2)Then T is distributed as non-central t withdf= n - 1degrees of freedom and non-centrality parameterncp = (μ - m0) * sqrt(n)/σ.
A fitted linear model has coefficients for the contrasts of the factorterms, usually one less in number than the number of levels.  Thisfunction re-expresses the coefficients in the original coding; as thecoefficients will have been fitted in the reduced basis, any impliedconstraints (e.g., zero sum for contr.helmert or contr.sum)will be respected.  There will be little point in usingdummy.coef for contr.treatment contrasts, as the missingcoefficients are by definition zero.The method used has some limitations, and will give incomplete resultsfor terms such as poly(x, 2).  However, it is adequate forits main purpose, aov models.
A fitted linear model has coefficients for the contrasts of the factorterms, usually one less in number than the number of levels.  Thisfunction re-expresses the coefficients in the original coding; as thecoefficients will have been fitted in the reduced basis, any impliedconstraints (e.g., zero sum for contr.helmert or contr.sum)will be respected.  There will be little point in usingdummy.coef for contr.treatment contrasts, as the missingcoefficients are by definition zero.The method used has some limitations, and will give incomplete resultsfor terms such as poly(x, 2).  However, it is adequate forits main purpose, aov models.
If min or max are not specified they assume the defaultvalues of 0 and 1 respectively.The uniform distribution has densityf(x) = 1/(max-min)for min ≤ x ≤ max.For the case of u := min == max, the limit case ofX == u is assumed, although there is no density inthat case and dunif will return NaN (the error condition).runif will not generate either of the extreme values unlessmax = min or max-min is small compared to min,and in particular not for the default arguments.
The Weibull distribution with shape parameter a andscale parameter b has density given byf(x) = (a/b) (x/b)^(a-1) exp(- (x/b)^a) for x > 0.The cumulative distribution function isF(x) = 1 - exp(- (x/b)^a)on x > 0, themean is E(X) = b Γ(1 + 1/a), andthe Var(X) = b^2 * (Γ(1 + 2/a) - (Γ(1 + 1/a))^2).
This distribution is obtained as follows.  Let x and ybe two random, independent samples of size m and n.Then the Wilcoxon rank sum statistic is the number of all pairs(x[i], y[j]) for which y[j] is not greater thanx[i].  This statistic takes values between 0 andm * n, and its mean and variance are m * n / 2 andm * n * (m + n + 1) / 12, respectively.If any of the first three arguments are vectors, the recycling rule isused to do the calculations for all combinations of the three up tothe length of the longest vector.
The e.c.d.f. (empirical cumulative distribution function)Fn is a step function with jumps i/n atobservation values, where i is the number of tied observationsat that value.  Missing values are ignored.For observationsx= (x1,x2, ... xn),Fn is the fraction of observations less or equal to t,i.e.,    Fn(t) = #{xi <= t}/n  =  1/n sum(i=1,n) Indicator(xi <= t).The function plot.ecdf which implements the plotmethod for ecdf objects, is implemented via a call toplot.stepfun; see its documentation.
Fixed-effect terms in an analysis of variance model with multiple stratamay be estimable in more than one stratum, in which case there is lessthan complete information in each.  The efficiency for a termis the fraction of the maximum possible precision (inverse variance)obtainable by estimating in just that stratum.  Under the assumptionof balance, this is the same for all contrasts involving that term.This function is used to pick strata in which to estimate terms inmodel.tables.aovlist andse.contrast.aovlist.In many cases terms will only occur in one stratum, when all theefficiencies will be one: this is detected and no further calculationsare done.The calculation used requires orthogonal contrasts for each term, andwill throw an error if non-orthogonal contrasts (e.g., treatmentcontrasts or an unbalanced design) are detected.
For a linear model fitted by lm or aov,the effects are the uncorrelated single-degree-of-freedom valuesobtained by projecting the data onto the successive orthogonalsubspaces generated by the QR decomposition during the fittingprocess. The first r (the rank of the model) are associated withcoefficients and the remainder span the space of residuals (but arenot associated with particular residuals).Empty models do not have effects.
Each row of the resulting matrix consists of sequencesx[t], x[t-1], ..., x[t-dimension+1], wheret is the original index of x. If x is a matrix,i.e., x contains more than one variable, then x[t]consists of the tth observation on each variable.
These are generic functions, which will use thetsp attribute of x if it exists.Their default methods decode the start time from the original timeunits, so that for a monthly series 1995.5 is representedas c(1995, 7). For a series of frequency f, timen+i/f is presented as c(n, i+1) (even for i = 0and f = 1).
NA
If na.expand = FALSE then NA values in the extra variableswill be passed to the na.action function used inmodel.  This may result in a shorter data frame (withna.omit) or an error (with na.fail).  Ifna.expand = TRUE the returned data frame will have precisely thesame rows as model.frame(model), but the columns corresponding tothe extra variables may contain NA.
This is a generic function, with methods in base R for classes"aov", "glm" and "lm" as well as for"negbin" (package MASS) and "coxph" and"survreg" (package survival).The criterion used isAIC = - 2*log L +  k * edf,where L is the likelihood and edf the equivalent degreesof freedom (i.e., the number of free parameters for usual parametricmodels) of fit.For linear models with unknown scale (i.e., for lm andaov), -2 log L is computed from thedeviance and uses a different additive constant tologLik and hence AIC.  If RSSdenotes the (weighted) residual sum of squares then extractAICuses for -2 log L the formulae RSS/s - n (correspondingto Mallows' Cp) in the case of known scale s andn log (RSS/n) for unknown scale.AIC only handles unknown scale and uses the formulan*log(RSS/n) + n + n*log 2pi - sum(log w)where w are the weights.  Further AIC counts the scaleestimation as a parameter in the edf and extractAIC does not.For glm fits the family's aic() function is used tocompute the AIC: see the note under logLik about theassumptions this makes.k = 2 corresponds to the traditional AIC, using k =    log(n) provides the BIC (Bayesian IC) instead.Note that the methods for this function may differ in theirassumptions from those of methods for AIC (usuallyvia a method for logLik).  We have alreadymentioned the case of "lm" models with estimated scale, andthere are similar issues in the "glm" and "negbin"methods where the dispersion parameter may or may not be taken as‘free’.  This is immaterial as extractAIC is only usedto compare models of the same class (where only differences in AICvalues are considered).
The factor analysis model isx = Λ f + efor a p–element vector x, a p x kmatrix Λ of loadings, a k–element vectorf of scores and a p–element vector e oferrors.  None of the components other than x is observed, butthe major restriction is that the scores be uncorrelated and of unitvariance, and that the errors be independent with variancesPsi, the uniquenesses.  It is also common toscale the observed variables to unit variance, and done in this function.Thus factor analysis is in essence a model for the correlation matrixof x,Σ = Λ Λ' + ΨThere is still some indeterminacy in the model for it is unchangedif Λ is replaced by G Λ forany orthogonal matrix G.  Such matrices G are known asrotations (although the term is applied also to non-orthogonalinvertible matrices).If covmat is supplied it is used.  Otherwise x is usedif it is a matrix, or a formula x is used with data toconstruct a model matrix, and that is used to construct a covariancematrix.  (It makes no sense for the formula to have a response, andall the variables must be numeric.)  Once a covariance matrix is foundor calculated from x, it is converted to a correlation matrixfor analysis.  The correlation matrix is returned as componentcorrelation of the result.The fit is done by optimizing the log likelihood assuming multivariatenormality over the uniquenesses.  (The maximizing loadings for givenuniquenesses can be found analytically: Lawley & Maxwell (1971,p. 27).)  All the starting values supplied in start are triedin turn and the best fit obtained is used.  If start = NULLthen the first fit is started at the value suggested byJöreskog (1963) and given by Lawley & Maxwell(1971, p. 31), and then control$nstart - 1 other values aretried, randomly selected as equal values of the uniquenesses.The uniquenesses are technically constrained to lie in [0, 1],but near-zero values are problematical, and the optimization isdone with a lower bound of control$lower, default 0.005(Lawley & Maxwell, 1971, p. 32).Scores can only be produced if a data matrix is supplied and used.The first method is the regression method of Thomson (1951), thesecond the weighted least squares method of Bartlett (1937, 8).Both are estimates of the unobserved scores f.  Thomson's methodregresses (in the population) the unknown f on x to yieldhat f = Λ' Σ^-1 xand then substitutes the sample estimates of the quantities on theright-hand side.  Bartlett's method minimizes the sum of squares ofstandardized errors over the choice of f, given (the fitted)Λ.If x is a formula then the standard NA-handling isapplied to the scores (if requested): see napredict.The print method (documented under loadings)follows the factor analysis convention of drawing attention to thepatterns of the results, so the default precision is three decimalplaces, and small loadings are suppressed.
factor.scope is not intended to be called directly by users.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
NA
Missing values are allowed in x but not in filter(where they would lead to missing values everywhere in the output).Note that there is an implied coefficient 1 at lag 0 in therecursive filter, which givesy[i] =      x[i] + f[1]*y[i-1] + … + f[p]*y[i-p]No check is made to see if recursive filter is invertible:the output may diverge if it is not.The convolution filter isy[i] = f[1]*x[i+o] + … + f[p]*x[i+o-(p-1)]where o is the offset: see sides for how it is determined.
If x is a matrix, it is taken as a two-dimensional contingencytable, and hence its entries should be nonnegative integers.Otherwise, both x and y must be vectors of the samelength.  Incomplete cases are removed, the vectors are coerced intofactor objects, and the contingency table is computed from these.For 2 by 2 cases, p-values are obtained directlyusing the (central or non-central) hypergeometricdistribution. Otherwise, computations are based on a C version of theFORTRAN subroutine FEXACT which implements the network developed byMehta and Patel (1983, 1986) and improved by Clarkson, Fan and Joe (1993).The FORTRAN code can be obtained fromhttps://www.netlib.org/toms/643.  Note this fails (with an errormessage) when the entries of the table are too large.  (It transposesthe table if necessary so it has no more rows than columns.  Oneconstraint is that the product of the row marginals be less than2^31 - 1.)For 2 by 2 tables, the null of conditionalindependence is equivalent to the hypothesis that the odds ratioequals one.  ‘Exact’ inference can be based on observing that ingeneral, given all marginal totals fixed, the first element of thecontingency table has a non-central hypergeometric distribution withnon-centrality parameter given by the odds ratio (Fisher, 1935).  Thealternative for a one-sided test is based on the odds ratio, soalternative = "greater" is a test of the odds ratio being biggerthan or.Two-sided tests are based on the probabilities of the tables, and takeas ‘more extreme’ all tables with probabilities less than orequal to that of the observed table, the p-value being the sum of suchprobabilities.For larger than 2 by 2 tables and hybrid = TRUE,asymptotic chi-squared probabilities are only used if the‘Cochran conditions’ (or modified version thereof) specified byhybridPars = c(expect = 5, percent = 80, Emin = 1) aresatisfied, that is if no cell has expected counts less than1 (= Emin) and more than 80% (= percent) of thecells have expected counts at least 5 (= expect), otherwisethe exact calculation is used.  A corresponding if() decisionis made for all sub-tables considered.Accidentally, R has used 180 instead of 80 aspercent, i.e., hybridPars[2] in R versions between3.0.0 and 3.4.1 (inclusive), i.e., the 2nd of the hybridPars(all of which used to be hard-coded previous to R 3.5.0).Consequently, in these versions of R, hybrid=TRUE never made adifference.In the r x c case with r > 2 or c > 2,internal tables can get too large for the exact test in which case anerror is signalled.  Apart from increasing workspacesufficiently, which then may lead to very long running times, usingsimulate.p.value = TRUE may then often be sufficient and henceadvisable.Simulation is done conditional on the row and column marginals, andworks only if the marginals are strictly positive.  (A C translationof the algorithm of Patefield (1981) is used.)
NA
NA
NA
If x is a list, its elements are taken as the samples to becompared for homogeneity of variances, and hence have to be numericdata vectors.  In this case, g is ignored, and one can simplyuse fligner.test(x) to perform the test.  If the samples arenot yet contained in a list, use fligner.test(list(x, ...)).Otherwise, x must be a numeric data vector, and g mustbe a vector or factor object of the same length as x giving thegroup for the corresponding elements of x.The Fligner-Killeen (median) test has been determined in a simulationstudy as one of the many tests for homogeneity of variances which ismost robust against departures from normality, see Conover, Johnson &Johnson (1981).  It is a k-sample simple linear rank which usesthe ranks of the absolute values of the centered samples and weightsa(i) = qnorm((1 +    i/(n+1))/2).  The version implemented here uses median centering ineach of the samples (F-K:med X^2 in the reference).
The models fit by, e.g., the lm and glm functionsare specified in a compact symbolic form.The ~ operator is basic in the formation of such models.An expression of the form y ~ model is interpretedas a specification that the response y is modelledby a linear predictor specified symbolically by model.Such a model consists of a series of terms separatedby + operators.The terms themselves consist of variable and factornames separated by : operators.Such a term is interpreted as the interaction ofall the variables and factors appearing in the term.In addition to + and :, a number of other operators areuseful in model formulae.  The * operator denotes factorcrossing: a*b interpreted as a+b+a:b.  The ^operator indicates crossing to the specified degree.  For example(a+b+c)^2 is identical to (a+b+c)*(a+b+c) which in turnexpands to a formula containing the main effects for a,b and c together with their second-order interactions.The %in% operator indicates that the terms on its left arenested within those on the right.  For example a + b %in% aexpands to the formula a + a:b.  The - operator removesthe specified terms, so that (a+b+c)^2 - a:b is identical toa + b + c + b:c + a:c.  It can also used to remove theintercept term: when fitting a linear model y ~ x - 1 specifiesa line through the origin.  A model with no intercept can be alsospecified as y ~ x + 0 or y ~ 0 + x.While formulae usually involve just variable and factornames, they can also involve arithmetic expressions.The formula log(y) ~ a + log(x) is quite legal.When such arithmetic expressions involveoperators which are also used symbolicallyin model formulae, there can be confusion betweenarithmetic and symbolic operator use.To avoid this confusion, the function I()can be used to bracket those portions of a modelformula where the operators are used in theirarithmetic sense.  For example, in the formulay ~ a + I(b+c), the term b+c is to beinterpreted as the sum of b and c.Variable names can be quoted by backticks `like this` informulae, although there is no guarantee that all code using formulaewill accept such non-syntactic names.Most model-fitting functions accept formulae with right-hand-sideincluding the function offset to indicate terms with afixed coefficient of one.  Some functions accept other‘specials’ such as strata or cluster (see thespecials argument of terms.formula).There are two special interpretations of . in a formula.  Theusual one is in the context of a data argument of modelfitting functions and means ‘all columns not otherwise in theformula’: see terms.formula.  In the context ofupdate.formula, only, it means ‘what waspreviously in this part of the formula’.When formula is called on a fitted model object, either aspecific method is used (such as that for class "nls") or thedefault method.  The default first looks for a "formula"component of the object (and evaluates it), then a "terms"component, then a formula parameter of the call (and evaluatesits value) and finally a "formula" attribute.There is a formula method for data frames.  When there's"terms" attribute with a formula, e.g., for amodel.frame(), that formula is returned.  If you'd like theprevious (R <= 3.5.x) behavior, use the auxiliaryDF2formula() which does not consider a "terms" attribute.Otherwise, ifthere is onlyone column this forms the RHS with an empty LHS.  For more columns,the first column is the LHS of the formula and the remaining columnsseparated by + form the RHS.
These are all generic functions, which will use thetsp attribute of x if it exists. timeand cycle have methods for class ts that coercethe result to that class.
friedman.test can be used for analyzing unreplicated completeblock designs (i.e., there is exactly one observation in yfor each combination of levels of groups and blocks)where the normality assumption may be violated.The null hypothesis is that apart from an effect of blocks,the location parameter of y is the same in each of thegroups.If y is a matrix, groups and blocks areobtained from the column and row indices, respectively.  NA'sare not allowed in groups or blocks;  if ycontains NA's, corresponding blocks are removed.
ftable creates ‘flat’ contingency tables.  Similar to theusual contingency tables, these contain the counts of each combinationof the levels of the variables (factors) involved.  This informationis then re-arranged as a matrix whose rows and columns correspond tounique combinations of the levels of the row and column variables (asspecified by row.vars and col.vars, respectively).  Thecombinations are created by looping over the variables in reverseorder (so that the levels of the left-most variable vary theslowest).  Displaying a contingency table in this flat matrix form(via print.ftable, the print method for objects of class"ftable") is often preferable to showing it as ahigher-dimensional array.ftable is a generic function.  Its default method,ftable.default, first creates a contingency table in arrayform from all arguments except row.vars and col.vars.If the first argument is of class "table", it represents acontingency table and is used as is; if it is a flat table of class"ftable", the information it contains is converted to the usualarray representation using as.ftable.  Otherwise, the argumentsshould be R objects which can be interpreted as factors (includingcharacter strings), or a list (or data frame) whose components can beso interpreted, which are cross-tabulated using table.Then, the arguments row.vars and col.vars are used tocollapse the contingency table into flat form.  If neither of thesetwo is given, the last variable is used for the columns.  If both aregiven and their union is a proper subset of all variables involved,the other variables are summed out.When the arguments are R expressions interpreted as factors,additional arguments will be passed to table to control howthe variable names are displayed; see the last example below.Function ftable.formula provides a formula method forcreating flat contingency tables.There are methods for as.table, as.matrixand as.data.frame.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
Exactly what happens depends on the class and attributes of the objectformula.  If this is an object of fitted-model class such as"lm", the method will either return the saved model frameused when fitting the model (if any, often selected by argumentmodel = TRUE) or pass the call used when fitting on to thedefault method.  The default method itself can cope with ratherstandard model objects such as those of class"lqs" from package MASS if no otherarguments are supplied.The rest of this section applies only to the default method.If either formula or data is already a model frame (adata frame with a "terms" attribute) and the other is missing,the model frame is returned.  Unless formula is a terms object,as.formula and then terms is called on it.  (If you wishto use the keep.order argument of terms.formula, pass aterms object rather than a formula.)Row names for the model frame are taken from the data argumentif present, then from the names of the response in the formula (orrownames if it is a matrix), if there is one.All the variables in formula, subset and in ...are looked for first in data and then in the environment offormula (see the help for formula() for furtherdetails) and collected into a data frame.  Then the subsetexpression is evaluated, and it is used as a row index to the dataframe.  Then the na.action function is applied to the data frame(and may well add attributes).  The levels of any factors in the dataframe are adjusted according to the drop.unused.levels andxlev arguments: if xlev specifies a factor and acharacter variable is found, it is converted to a factor (as from R2.10.0).Unless na.action = NULL, time-series attributes will be removedfrom the variables found (since they will be wrong if NAs areremoved).Note that all the variables in the formula are included in thedata frame, even those preceded by -.Only variables whose type is raw, logical, integer, real, complex orcharacter can be included in a model frame: this includes classedvariables such as factors (whose underlying type is integer), butexcludes lists.get_all_vars returns a data.frame containing thevariables used in formula plus those specified in ...which are recycled to the number of data frame rows.Unlike model.frame.default, it returns the input variables andnot those resulting from function calls in formula.
NA
NA
A typical predictor has the form response ~ terms whereresponse is the (numeric) response vector and terms is aseries of terms which specifies a linear predictor forresponse.  For binomial and quasibinomialfamilies the response can also be specified as a factor(when the first level denotes failure and all others success) or as atwo-column matrix with the columns giving the numbers of successes andfailures.  A terms specification of the form first + secondindicates all the terms in first together with all the terms insecond with any duplicates removed.A specification of the form first:second indicates the setof terms obtained by taking the interactions of all terms infirst with all terms in second.  The specificationfirst*second indicates the cross of first andsecond.  This is the same as first + second +  first:second.The terms in the formula will be re-ordered so that main effects comefirst, followed by the interactions, all second-order, all third-orderand so on: to avoid this pass a terms object as the formula.Non-NULL weights can be used to indicate that differentobservations have different dispersions (with the values inweights being inversely proportional to the dispersions); orequivalently, when the elements of weights are positiveintegers w_i, that each response y_i is the mean ofw_i unit-weight observations.  For a binomial GLM prior weightsare used to give the number of trials when the response is theproportion of successes: they would rarely be used for a Poisson GLM.glm.fit is the workhorse function: it is not normally calleddirectly but can be more efficient where the response vector, designmatrix and family have already been calculated.If more than one of etastart, start and mustartis specified, the first in the list will be used.  It is oftenadvisable to supply starting values for a quasi family,and also for families with unusual links such as gaussian("log").All of weights, subset, offset, etastartand mustart are evaluated in the same way as variables informula, that is first in data and then in theenvironment of formula.For the background to warning messages about ‘fitted probabilitiesnumerically 0 or 1 occurred’ for binomial GLMs, see Venables &Ripley (2002, pp. 197–8).
The control argument of glm is by default passedto the control argument of glm.fit, which usesits elements as arguments to glm.control: the latter providesdefaults and sanity checking.If epsilon is small (less than 1e-10) it isalso used as the tolerance for the detection of collinearity in theleast squares solution.When trace is true, calls to cat produce theoutput for each IWLS iteration.  Hence, options(digits = *)can be used to increase the precision, see the example.
A typical predictor has the form response ~ terms whereresponse is the (numeric) response vector and terms is aseries of terms which specifies a linear predictor forresponse.  For binomial and quasibinomialfamilies the response can also be specified as a factor(when the first level denotes failure and all others success) or as atwo-column matrix with the columns giving the numbers of successes andfailures.  A terms specification of the form first + secondindicates all the terms in first together with all the terms insecond with any duplicates removed.A specification of the form first:second indicates the setof terms obtained by taking the interactions of all terms infirst with all terms in second.  The specificationfirst*second indicates the cross of first andsecond.  This is the same as first + second +  first:second.The terms in the formula will be re-ordered so that main effects comefirst, followed by the interactions, all second-order, all third-orderand so on: to avoid this pass a terms object as the formula.Non-NULL weights can be used to indicate that differentobservations have different dispersions (with the values inweights being inversely proportional to the dispersions); orequivalently, when the elements of weights are positiveintegers w_i, that each response y_i is the mean ofw_i unit-weight observations.  For a binomial GLM prior weightsare used to give the number of trials when the response is theproportion of successes: they would rarely be used for a Poisson GLM.glm.fit is the workhorse function: it is not normally calleddirectly but can be more efficient where the response vector, designmatrix and family have already been calculated.If more than one of etastart, start and mustartis specified, the first in the list will be used.  It is oftenadvisable to supply starting values for a quasi family,and also for families with unusual links such as gaussian("log").All of weights, subset, offset, etastartand mustart are evaluated in the same way as variables informula, that is first in data and then in theenvironment of formula.For the background to warning messages about ‘fitted probabilitiesnumerically 0 or 1 occurred’ for binomial GLMs, see Venables &Ripley (2002, pp. 197–8).
The tsp attribute gives the start time in time units,the end time and the frequency (the number of observations per unit oftime, e.g. 12 for a monthly series).Assignments are checked for consistency.Assigning NULL which removes the tsp attributeand any "ts" (or "mts") class of x.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
This function performs a hierarchical cluster analysisusing a set of dissimilarities for the n objects beingclustered.  Initially, each object is assigned to its owncluster and then the algorithm proceeds iteratively,at each stage joining the two most similar clusters,continuing until there is just a single cluster.At each stage distances between clusters are recomputedby the Lance–Williams dissimilarity update formulaaccording to the particular clustering method being used.A number of different clustering methods are provided.  Ward'sminimum variance method aims at finding compact, spherical clusters.The complete linkage method finds similar clusters. Thesingle linkage method (which is closely related to the minimalspanning tree) adopts a ‘friends of friends’ clusteringstrategy.  The other methods can be regarded as aiming for clusterswith characteristics somewhere between the single and complete linkmethods.  Note however, that methods "median" and"centroid" are not leading to a monotone distancemeasure, or equivalently the resulting dendrograms can have so calledinversions or reversals which are hard to interpret,but note the trichotomies in Legendre and Legendre (2012).Two different algorithms are found in the literature for Ward clustering.The one used by option "ward.D" (equivalent to the only Wardoption "ward" in R versions <= 3.0.3) does not implementWard's (1963) clustering criterion, whereas option "ward.D2" implementsthat criterion (Murtagh and Legendre 2014).  With the latter, thedissimilarities are squared before cluster updating.Note that agnes(*, method="ward") correspondsto hclust(*, "ward.D2").If members != NULL, then d is taken to be adissimilarity matrix between clusters instead of dissimilaritiesbetween singletons and members gives the number of observationsper cluster.  This way the hierarchical cluster algorithm can be‘started in the middle of the dendrogram’, e.g., in order toreconstruct the part of the tree above a cut (see examples).Dissimilarities between clusters can be efficiently computed (i.e.,without hclust itself) only for a limited number ofdistance/linkage combinations, the simplest one being squaredEuclidean distance and centroid linkage.  In this case thedissimilarities between the clusters are the squared Euclideandistances between cluster means.In hierarchical cluster displays, a decision is needed at each merge tospecify which subtree should go on the left and which on the right.Since, for n observations there are n-1 merges,there are 2^{(n-1)} possible orderings for the leavesin a cluster tree, or dendrogram.The algorithm used in hclust is to order the subtree so thatthe tighter cluster is on the left (the last, i.e., most recent,merge of the left subtree is at a lower value than the lastmerge of the right subtree).Single observations are the tightest clusters possible,and merges involving two observations place them in order by theirobservation sequence number.
If either Rowv or Colv are dendrograms they are honored(and not reordered).  Otherwise, dendrograms are computed asdd <- as.dendrogram(hclustfun(distfun(X))) where X iseither x or t(x).If either is a vector (of ‘weights’) then the appropriatedendrogram is reordered according to the supplied values subject tothe constraints imposed by the dendrogram, by reorder(dd,    Rowv), in the row case.If either is missing, as by default, then the ordering of thecorresponding dendrogram is by the mean value of the rows/columns,i.e., in the case of rows, Rowv <- rowMeans(x, na.rm = na.rm).If either is NA, no reordering will be done forthe corresponding side.By default (scale = "row") the rows are scaled to have meanzero and standard deviation one.  There is some empirical evidencefrom genomic plotting that this is useful.The default colors are not pretty.  Consider using enhancements suchas the RColorBrewer package.
The additive Holt-Winters prediction function (for time series withperiod length p) is    Yhat[t+h] = a[t] + h * b[t] + s[t - p + 1 + (h - 1) mod p],where a[t], b[t] and s[t] are given by  a[t] = α (Y[t] - s[t-p])  + (1-α) (a[t-1] + b[t-1])  b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1]  s[t] = γ (Y[t] - a[t]) + (1-γ) s[t-p]The multiplicative Holt-Winters prediction function (for time serieswith period length p) is    Yhat[t+h] = (a[t] + h * b[t]) * s[t - p + 1 + (h - 1) mod p],where a[t], b[t] and s[t] are given by  a[t] = α (Y[t] / s[t-p])  + (1-α) (a[t-1] + b[t-1])  b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1]  s[t] = γ  (Y[t] / a[t])  + (1-γ) s[t-p]The data in x are required to be non-zero for a multiplicativemodel, but it makes most sense if they are all positive.The function tries to find the optimal values of α and/orβ and/or γ by minimizing the squared one-stepprediction error if they are NULL (the default). optimizewill be used for the single-parameter case, and optim otherwise.For seasonal models, start values for a, b and sare inferred by performing a simple decomposition in trend andseasonal component using moving averages (see functiondecompose) on the start.periods first periods (a simplelinear regression on the trend component is used for starting leveland trend). For level/trend-models (no seasonal component), startvalues for a and b are x[2] and x[2] -  x[1], respectively. For level-only models (ordinary exponentialsmoothing), the start value for a is x[1].
The influence.measures() and other functions listed inSee Also provide a more user oriented way of computing avariety of regression diagnostics.  These all build onlm.influence.  Note that for GLMs (other than the Gaussianfamily with identity link) these are based on one-step approximationswhich may be inadequate if a case has high influence.An attempt is made to ensure that computed hat values that areprobably one are treated as one, and the corresponding rows insigma and coefficients are NaN.  (Dropping such acase would normally result in a variable being dropped, so it is notpossible to give simple drop-one diagnostics.)naresid is applied to the results and so will fill inwith NAs it the fit had na.action = na.exclude.
The primary high-level function is influence.measures which produces aclass "infl" object tabular display showing the DFBETAS foreach model variable, DFFITS, covariance ratios, Cook's distances andthe diagonal elements of the hat matrix.  Cases which are influentialwith respect to any of these measures are marked with an asterisk.The functions dfbetas, dffits,covratio and cooks.distance provide direct access to thecorresponding diagnostic quantities.  Functions rstandard andrstudent give the standardized and Studentized residualsrespectively. (These re-normalize the residuals to have unit variance,using an overall and leave-one-out measure of the error variancerespectively.)Note that for multivariate lm() models (of class"mlm"), these functions return 3d arrays instead of matrices,or matrices instead of vectors.Values for generalized linear models are approximations, as describedin Williams (1987) (except that Cook's distances are scaled asF rather than as chi-square values).  The approximations can bepoor when some cases have large influence.The optional infl, res and sd arguments are thereto encourage the use of these direct access functions, in situationswhere, e.g., the underlying basic influence measures (fromlm.influence or the generic influence) arealready available.Note that cases with weights == 0 are dropped from allthese functions, but that if a linear model has been fitted withna.action = na.exclude, suitable values are filled in for thecases excluded during fitting.For linear models, rstandard(*, type = "predictive") providesleave-one-out cross validation residuals, and the “PRESS”statistic (PREdictive Sum of Squares, the same asthe CV score) of model model is The function hat() exists mainly for S (version 2)compatibility; we recommend using hatvalues() instead.
Note that arguments after ... must be matched exactly.If one or both limits are infinite, the infinite range is mapped ontoa finite interval.For a finite interval, globally adaptive interval subdivision is usedin connection with extrapolation by Wynn's Epsilon algorithm, with thebasic step being Gauss–Kronrod quadrature.rel.tol cannot be less than max(50*.Machine$double.eps,    0.5e-28) if abs.tol <= 0.Note that the comments in the C source code in‘<R>/src/appl/integrate.c’ give more details, particularly aboutreasons for failure (internal error code ier >= 1).In R versions <= 3.2.x, the first entries oflower and upper were used whereas an error is signallednow if they are not of length one.
By default the levels of x.factor are plotted on the x axis intheir given order, with extra space left at the right for the legend(if specified). If x.factor is an ordered factor and the levelsare numeric, these numeric values are used for the x axis.The response and hence its summary can contain missing values. If so,the missing values and the line segments joining them are omitted fromthe plot (and this can be somewhat disconcerting).The graphics parameters xlab, ylab, ylim,lty, col and pch are given suitable defaults(and xlim and xaxs are set and cannot be overridden).The defaults are to cycle through the line types, use the foregroundcolour, and to use the symbols 1:9, 0, and the capital letters to plotthe traces.
family is a generic function with methods for classes"glm" and "lm" (the latter returning gaussian()).For the binomial and quasibinomial families the responsecan be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor nothaving the first level (and hence usually of having the second level). As a numerical vector with values  between 0 and1, interpreted as the proportion of successful cases (with thetotal number of cases given by the weights). As a two-column integer matrix: the first column gives thenumber of successes and the second the number of failures.The quasibinomial and quasipoisson families differ fromthe binomial and poisson families only in that thedispersion parameter is not fixed at one, so they can modelover-dispersion.  For the binomial case see McCullagh and Nelder(1989, pp. 124–8).  Although they show that there is (under somerestrictions) a model withvariance proportional to mean as in the quasi-binomial model, notethat glm does not compute maximum-likelihood estimates in thatmodel.  The behaviour of S is closer to the quasi- variants.
Note that this function computes the quartiles using thequantile function rather than followingTukey's recommendations,i.e., IQR(x) = quantile(x, 3/4) - quantile(x, 1/4).For normally N(m,1) distributed X, the expected value ofIQR(X) is 2*qnorm(3/4) = 1.3490, i.e., for a normal-consistentestimate of the standard deviation, use IQR(x) / 1.349.
NA
The dendrogram is directly represented as a nested list where eachcomponent corresponds to a branch of the tree.  Hence, the firstbranch of tree z is z[[1]], the second branch of thecorresponding subtree is z[[1]][[2]], or shorterz[[c(1,2)]], etc..  Each node of the treecarries some information needed for efficient plotting or cutting asattributes, of which only members, height andleaf for leaves are compulsory:total number of leaves in the branchnumeric non-negative height at which the nodeis plotted.numeric horizontal distance of the node fromthe left border (the leftmost leaf) of the branch (unit 1 betweenall leaves).  This is used for plot(*, center = FALSE).character; the label of the nodefor cut()$upper,the number of former members; more generally a substitutefor the members component used for ‘horizontal’(when horiz = FALSE, else ‘vertical’) alignment.character; the label for the edge leading tothe nodea named list (of length-1 components)specifying node-specific attributes for pointsplotting, see the nodePar argument above.a named list (of length-1 components)specifying attributes for segments plotting of theedge leading to the node, and drawing of the edgetext ifavailable, see the edgePar argument above.logical, if TRUE, the node is a leaf ofthe tree.cut.dendrogram() returns a list with components $upperand $lower, the first is a truncated version of the originaltree, also of class dendrogram, the latter a list with thebranches obtained from cutting the tree, each a dendrogram.There are [[, print, and strmethods for "dendrogram" objects where the first one(extraction) ensures that selecting sub-branches keeps the class,i.e., returns a dendrogram even if only a leaf.On the other hand, [ (single bracket) extractionreturns the underlying list structure.Objects of class "hclust" can be converted to class"dendrogram" using method as.dendrogram(), and since R2.13.0, there is also a as.hclust() method as an inverse.rev.dendrogram simply returns the dendrogram x withreversed nodes, see also reorder.dendrogram.The merge(x, y, ...) method merges two or moredendrograms into a new one which has x and y (andoptional further arguments) as branches.  Note that before R 3.1.2,adjust = "none" was used implicitly, which is invalid when,e.g., the dendrograms are from as.dendrogram(hclust(..)).nobs(object) returns the total number of leaves (themembers attribute, see above).is.leaf(object) returns logical indicating if object is aleaf (the most simple dendrogram).plotNode() and plotNodeLimit() are helper functions.
The function ts is used to create time-series objects.  Theseare vectors or matrices with class of "ts" (and additionalattributes) which represent data which has been sampled at equispacedpoints in time.  In the matrix case, each column of the matrixdata is assumed to contain a single (univariate) time series.Time series must have at least one observation, and although they neednot be numeric there is very limited support for non-numeric series.Class "ts" has a number of methods.  In particular arithmeticwill attempt to align time axes, and subsetting to extract subsets ofseries can be used (e.g., EuStockMarkets[, "DAX"]).  However,subsetting the first (or only) dimension will return a matrix orvector, as will matrix subsetting.  Subassignment can be used toreplace values but not to extend a series (see window).There is a method for t that transposes the series as amatrix (a one-column matrix if a vector) and hence returns a resultthat does not inherit from class "ts".Argument frequency indicates the sampling frequency of thetime series, with the default value 1 indicating one sample ineach unit time interval.  Forexample, one could use a value of 7 for frequency whenthe data are sampled daily, and the natural time period is a week, or12 when the data are sampled monthly and the natural timeperiod is a year.  Values of 4 and 12 are assumed in(e.g.) print methods to imply a quarterly and monthly seriesrespectively.  As from R 4.0.0, frequency need not be a wholenumber.  For example, frequency = 0.2 would imply samplingonce every five time units.as.ts is generic.  Its default method will use thetsp attribute of the object if it has one to set thestart and end times and frequency.is.ts tests if an object is a time series.  It is generic: youcan write methods to handle specific classes of objects,see InternalMethods.
NA
The function ts is used to create time-series objects.  Theseare vectors or matrices with class of "ts" (and additionalattributes) which represent data which has been sampled at equispacedpoints in time.  In the matrix case, each column of the matrixdata is assumed to contain a single (univariate) time series.Time series must have at least one observation, and although they neednot be numeric there is very limited support for non-numeric series.Class "ts" has a number of methods.  In particular arithmeticwill attempt to align time axes, and subsetting to extract subsets ofseries can be used (e.g., EuStockMarkets[, "DAX"]).  However,subsetting the first (or only) dimension will return a matrix orvector, as will matrix subsetting.  Subassignment can be used toreplace values but not to extend a series (see window).There is a method for t that transposes the series as amatrix (a one-column matrix if a vector) and hence returns a resultthat does not inherit from class "ts".Argument frequency indicates the sampling frequency of thetime series, with the default value 1 indicating one sample ineach unit time interval.  Forexample, one could use a value of 7 for frequency whenthe data are sampled daily, and the natural time period is a week, or12 when the data are sampled monthly and the natural timeperiod is a year.  Values of 4 and 12 are assumed in(e.g.) print methods to imply a quarterly and monthly seriesrespectively.  As from R 4.0.0, frequency need not be a wholenumber.  For example, frequency = 0.2 would imply samplingonce every five time units.as.ts is generic.  Its default method will use thetsp attribute of the object if it has one to set thestart and end times and frequency.is.ts tests if an object is a time series.  It is generic: youcan write methods to handle specific classes of objects,see InternalMethods.
kernel is used to construct a general kernel or named specifickernels.  The modified Daniell kernel halves the end coefficients (asused by S-PLUS).The [ method allows natural indexing of kernel objectswith indices in (-m) : m.  The normalization is such that fork <- kernel(*), sum(k[ -k$m : k$m ]) is one.df.kernel returns the ‘equivalent degrees of freedom’ ofa smoothing kernel as defined in Brockwell and Davis (1991), page362, and bandwidth.kernel returns the equivalent bandwidth asdefined in Bloomfield (1976), p. 201, with a continuity correction.
The algorithm determines the convex minorant m(x) of thecumulative data (i.e., cumsum(y)) which is piecewiselinear and the result is m'(x), a step function with levelchanges at locations where the convex m(x) touches thecumulative data polygon and changes slope.as.stepfun() returns a stepfunobject which can be more parsimonious.
These functions work with a general univariate state-space modelwith state vector a, transitions a <- T a + R e,e ~ N(0, kappa Q) and observationequation y = Z'a + eta,eta ~ N(0, kappa h).The likelihood is a profile likelihood after estimation ofkappa.The model is specified as a list with at least componentsthe transition matrixthe observation coefficientsthe observation varianceRQR'the current state estimatethe current estimate of the state uncertainty matrix Qthe estimate at time t-1 of the stateuncertainty matrix Q (not updated by KalmanForecast).KalmanSmooth is the workhorse function for tsSmooth.makeARIMA constructs the state-space model for an ARIMA model,see also arima.The state-space initialization has used Gardner et al's method(SSinit = "Gardner1980"), as only method for years.  However,that suffers sometimes from deficiencies when close to non-stationarity.For this reason, it may be replaced as default in the future and onlykept for reproducibility reasons.  Explicit specification ofSSinit is therefore recommended, notably also inarima().The "Rossignol2011" method has been proposed and partlydocumented by Raphael Rossignol, Univ. Grenoble, on 2011-09-20 (seePR#14682, below), and later been ported to C by Matwey V. Kornilov.It computes the covariance matrix of(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})by the method of difference equations (page 93 of Brockwell and Davis),apparently suggested by a referee of  Gardner et al (see p.314 oftheir paper).
These functions work with a general univariate state-space modelwith state vector a, transitions a <- T a + R e,e ~ N(0, kappa Q) and observationequation y = Z'a + eta,eta ~ N(0, kappa h).The likelihood is a profile likelihood after estimation ofkappa.The model is specified as a list with at least componentsthe transition matrixthe observation coefficientsthe observation varianceRQR'the current state estimatethe current estimate of the state uncertainty matrix Qthe estimate at time t-1 of the stateuncertainty matrix Q (not updated by KalmanForecast).KalmanSmooth is the workhorse function for tsSmooth.makeARIMA constructs the state-space model for an ARIMA model,see also arima.The state-space initialization has used Gardner et al's method(SSinit = "Gardner1980"), as only method for years.  However,that suffers sometimes from deficiencies when close to non-stationarity.For this reason, it may be replaced as default in the future and onlykept for reproducibility reasons.  Explicit specification ofSSinit is therefore recommended, notably also inarima().The "Rossignol2011" method has been proposed and partlydocumented by Raphael Rossignol, Univ. Grenoble, on 2011-09-20 (seePR#14682, below), and later been ported to C by Matwey V. Kornilov.It computes the covariance matrix of(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})by the method of difference equations (page 93 of Brockwell and Davis),apparently suggested by a referee of  Gardner et al (see p.314 oftheir paper).
These functions work with a general univariate state-space modelwith state vector a, transitions a <- T a + R e,e ~ N(0, kappa Q) and observationequation y = Z'a + eta,eta ~ N(0, kappa h).The likelihood is a profile likelihood after estimation ofkappa.The model is specified as a list with at least componentsthe transition matrixthe observation coefficientsthe observation varianceRQR'the current state estimatethe current estimate of the state uncertainty matrix Qthe estimate at time t-1 of the stateuncertainty matrix Q (not updated by KalmanForecast).KalmanSmooth is the workhorse function for tsSmooth.makeARIMA constructs the state-space model for an ARIMA model,see also arima.The state-space initialization has used Gardner et al's method(SSinit = "Gardner1980"), as only method for years.  However,that suffers sometimes from deficiencies when close to non-stationarity.For this reason, it may be replaced as default in the future and onlykept for reproducibility reasons.  Explicit specification ofSSinit is therefore recommended, notably also inarima().The "Rossignol2011" method has been proposed and partlydocumented by Raphael Rossignol, Univ. Grenoble, on 2011-09-20 (seePR#14682, below), and later been ported to C by Matwey V. Kornilov.It computes the covariance matrix of(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})by the method of difference equations (page 93 of Brockwell and Davis),apparently suggested by a referee of  Gardner et al (see p.314 oftheir paper).
These functions work with a general univariate state-space modelwith state vector a, transitions a <- T a + R e,e ~ N(0, kappa Q) and observationequation y = Z'a + eta,eta ~ N(0, kappa h).The likelihood is a profile likelihood after estimation ofkappa.The model is specified as a list with at least componentsthe transition matrixthe observation coefficientsthe observation varianceRQR'the current state estimatethe current estimate of the state uncertainty matrix Qthe estimate at time t-1 of the stateuncertainty matrix Q (not updated by KalmanForecast).KalmanSmooth is the workhorse function for tsSmooth.makeARIMA constructs the state-space model for an ARIMA model,see also arima.The state-space initialization has used Gardner et al's method(SSinit = "Gardner1980"), as only method for years.  However,that suffers sometimes from deficiencies when close to non-stationarity.For this reason, it may be replaced as default in the future and onlykept for reproducibility reasons.  Explicit specification ofSSinit is therefore recommended, notably also inarima().The "Rossignol2011" method has been proposed and partlydocumented by Raphael Rossignol, Univ. Grenoble, on 2011-09-20 (seePR#14682, below), and later been ported to C by Matwey V. Kornilov.It computes the covariance matrix of(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})by the method of difference equations (page 93 of Brockwell and Davis),apparently suggested by a referee of  Gardner et al (see p.314 oftheir paper).
NA
kernel is used to construct a general kernel or named specifickernels.  The modified Daniell kernel halves the end coefficients (asused by S-PLUS).The [ method allows natural indexing of kernel objectswith indices in (-m) : m.  The normalization is such that fork <- kernel(*), sum(k[ -k$m : k$m ]) is one.df.kernel returns the ‘equivalent degrees of freedom’ ofa smoothing kernel as defined in Brockwell and Davis (1991), page362, and bandwidth.kernel returns the equivalent bandwidth asdefined in Bloomfield (1976), p. 201, with a continuity correction.
The data given by x are clustered by the k-means method,which aims to partition the points into k groups such that thesum of squares from points to the assigned cluster centres is minimized.At the minimum, all cluster centres are at the mean of their Voronoisets (the set of data points which are nearest to the cluster centre).The algorithm of Hartigan and Wong (1979) is used by default.  Notethat some authors use k-means to refer to a specific algorithmrather than the general method: most commonly the algorithm given byMacQueen (1967) but sometimes that given by Lloyd (1957) and Forgy(1965).  The Hartigan–Wong algorithm generally does a better job thaneither of those, but trying several random starts (nstart>  1) is often recommended.  In rare cases, when some of the points(rows of x) are extremely close, the algorithm may not convergein the “Quick-Transfer” stage, signalling a warning (andreturning ifault = 4).  Slightrounding of the data may be advisable in that case.For ease of programmatic exploration, k=1 is allowed, notablyreturning the center and withinss.Except for the Lloyd–Forgy method, k clusters will always bereturned if a number is specified.If an initial matrix of centres is supplied, it is possible thatno point will be closest to one or more centres, which is currentlyan error for the Hartigan–Wong method.
